{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61f337ee-4c53-4dea-ae0c-e44cbe8093b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (4.66.2)\n",
      "Requirement already satisfied: langchain in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (0.1.13)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.29 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (0.0.29)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (0.1.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (0.1.31)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: chromadb in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (0.4.24)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (1.1.1)\n",
      "Requirement already satisfied: requests>=2.28 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (2.6.4)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (0.110.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (4.10.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (3.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (1.17.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (4.66.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (1.62.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from chromadb) (3.9.15)\n",
      "Requirement already satisfied: packaging>=19.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (0.36.3)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.7)\n",
      "Requirement already satisfied: protobuf in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.23.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.44b0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.2.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests>=2.28->chromadb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests>=2.28->chromadb) (3.6)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.21.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (4.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
      "Requirement already satisfied: gpt4all in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: requests in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from gpt4all) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from gpt4all) (4.66.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests->gpt4all) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests->gpt4all) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests->gpt4all) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests->gpt4all) (2024.2.2)\n",
      "Requirement already satisfied: langchainhub in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (0.1.15)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchainhub) (2.31.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from langchainhub) (2.31.0.20240311)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests<3,>=2->langchainhub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests<3,>=2->langchainhub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ssankar/miniconda3/envs/rag/lib/python3.12/site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "# for preprocessing html data  \n",
    "!pip install beautifulsoup4\n",
    "\n",
    "# For RAG\n",
    "!pip install langchain\n",
    "!pip install langchainhub\n",
    "!pip install chromadb\n",
    "!pip install gpt4all\n",
    "\n",
    "!pip install tqdm\n",
    "\n",
    "# Needed if using LlamaCpp from LangChain\n",
    "# !pip install llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b682100-2f68-4f9b-b6bb-b81f50492d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# To deal with emails \n",
    "import email\n",
    "from email.policy import default\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5372c615-e865-46c8-be5c-930a8182c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.docstore.document import Document\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6338f115-f502-4efb-9e90-88e020bd8baf",
   "metadata": {},
   "source": [
    "# Preprocess the ArXiv Email Digests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ae5ca6-52c0-42a1-b71a-43f1fce31d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from Stack Overflow:\n",
    "# https://stackoverflow.com/questions/59681461/read-a-big-mbox-file-with-python\n",
    "class MboxReader:\n",
    "    def __init__(self, filename):\n",
    "        self.handle = open(filename, 'rb')\n",
    "        assert self.handle.readline().startswith(b'From ')\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.handle.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.__next__())\n",
    "\n",
    "    def __next__(self):\n",
    "        lines = []\n",
    "        while True:\n",
    "            line = self.handle.readline()\n",
    "            if line == b'' or line.startswith(b'From '):\n",
    "                yield email.message_from_bytes(b''.join(lines), policy=default)\n",
    "                if line == b'':\n",
    "                    break\n",
    "                lines = []\n",
    "                continue\n",
    "            lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6232b056-da8d-48c5-aa5a-f3c5f53ec9b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 63.80it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"./Takeout/Mail/Arxiv.mbox\"\n",
    "mbox = MboxReader(path)\n",
    "emails_to_process = 10\n",
    "\n",
    "current_mails = 0\n",
    "arxiv_contents = \"\"\n",
    "for __, message in tqdm(enumerate(mbox)):\n",
    "    payload = message.get_payload(decode=True)\n",
    "    if payload:\n",
    "        current_mails += 1\n",
    "        if current_mails > emails_to_process:\n",
    "            break\n",
    "        soup = BeautifulSoup(payload, 'html.parser')\n",
    "        body_text = soup.get_text().replace('\"','').replace(\"\\n\", \"\").replace(\"\\t\", \"\").strip()\n",
    "        arxiv_contents += body_text + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9ad6023-2562-4ce0-884e-c9268e56ee84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\rSend any comments regarding submissions directly to submitter.\\r------------------------------------------------------------------------------\\rArchives at http://arxiv.org/\\rTo unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel\\r------------------------------------------------------------------------------\\r Submissions to:\\rComputation and Language\\rComputer Vision and Pattern Recognition\\rGraphics\\r received from  Tue 30 Jan 24 19:00:00 GMT  to  Wed 31 Jan 24 19:00:00 GMT\\r------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17373\\rDate: Tue, 30 Jan 2024 19:01:24 GMT   (1296kb)\\r\\rTitle: Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for\\r  Classifying Arabic Speech Acts on Twitter\\rAuthors: Khadejaa Alshehri, Areej Alhothali and Nahed Alowidi\\rCategories: cs.CL cs.AI\\rComments: 16 pages, 6 figures\\r\\\\\\\\\\r  Speech acts are a speakers actions when performing an utterance within a\\rconversation, such as asking, recommending, greeting, or thanking someone,\\rexpressing a thought, or making a suggestion. Understanding speech acts helps\\rinterpret the intended meaning and actions behind a speakers or writers words.\\rThis paper proposes a Twitter dialectal Arabic speech act classification\\rapproach based on a transformer deep learning neural network. Twitter and\\rsocial media, are becoming more and more integrated into daily life. As a\\rresult, they have evolved into a vital source of information that represents\\rthe views and attitudes of their users. We proposed a BERT based weighted\\rensemble learning approach to integrate the advantages of various BERT models\\rin dialectal Arabic speech acts classification. We compared the proposed model\\ragainst several variants of Arabic BERT models and sequence-based models. We\\rdeveloped a dialectal Arabic tweet act dataset by annotating a subset of a\\rlarge existing Arabic sentiment analysis dataset (ASAD) based on six speech act\\rcategories. We also evaluated the models on a previously developed Arabic Tweet\\rAct dataset (ArSAS). To overcome the class imbalance issue commonly observed in\\rspeech act problems, a transformer-based data augmentation model was\\rimplemented to generate an equal proportion of speech act categories. The\\rresults show that the best BERT model is araBERTv2-Twitter models with a\\rmacro-averaged F1 score and an accuracy of 0.73 and 0.84, respectively. The\\rperformance improved using a BERT-based ensemble method with a 0.74 and 0.85\\raveraged F1 score and accuracy on our dataset, respectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17373 ,  1296kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17377\\rDate: Tue, 30 Jan 2024 19:03:49 GMT   (6464kb,D)\\r\\rTitle: Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion\\r  Tokens\\rAuthors: Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh\\r  Hajishirzi\\rCategories: cs.CL cs.AI cs.IR\\r\\\\\\\\\\r  Are n-gram language models still relevant in this era of neural large\\rlanguage models (LLMs)? Our answer is yes, and we show their values in both\\rtext analysis and improving neural LLMs. Yet this necessitates modernizing\\rn-gram models in two aspects. First, we train them at the same data scale as\\rneural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever\\rbuilt. Second, existing n-gram models use small n which hinders their\\rperformance; we instead allow n to be arbitrarily large, by introducing a new\\r$\\\\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables\\r(which would be very expensive), we develop an engine named infini-gram --\\rpowered by suffix arrays -- that can compute $\\\\infty$-gram (as well as n-gram\\rwith arbitrary n) probabilities with millisecond-level latency. The\\r$\\\\infty$-gram framework and infini-gram engine enable us to conduct many novel\\rand interesting analyses of human-written and machine-generated text: we find\\rthat the $\\\\infty$-gram LM has fairly high accuracy for next-token prediction\\r(47%), and can complement neural LLMs to greatly reduce their language modeling\\rperplexities. When analyzing machine-generated text, we also observe\\rirregularities in the machine--$\\\\infty$-gram agreement level with respect to\\rthe suffix length, which indicates deficiencies in neural LLM pretraining and\\rthe positional embeddings of Transformers. We open-source our infini-gram\\rengine in the hopes of enabling more study on how to best use verbatim\\rinformation retrieved from large text corpora.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17377 ,  6464kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17390\\rDate: Tue, 30 Jan 2024 19:13:12 GMT   (650kb,D)\\r\\rTitle: Customizing Language Model Responses with Contrastive In-Context\\r  Learning\\rAuthors: Xiang Gao, Kamalika Das\\rCategories: cs.CL cs.AI\\rComments: Accepted to appear at AAAI 2024\\r\\\\\\\\\\r  Large language models (LLMs) are becoming increasingly important for machine\\rlearning applications. However, it can be challenging to align LLMs with our\\rintent, particularly when we want to generate content that is preferable over\\rothers or when we want the LLM to respond in a certain style or tone that is\\rhard to describe. To address this challenge, we propose an approach that uses\\rcontrastive examples to better describe our intent. This involves providing\\rpositive examples that illustrate the true intent, along with negative examples\\rthat show what characteristics we want LLMs to avoid. The negative examples can\\rbe retrieved from labeled data, written by a human, or generated by the LLM\\ritself. Before generating an answer, we ask the model to analyze the examples\\rto teach itself what to avoid. This reasoning step provides the model with the\\rappropriate articulation of the user's need and guides it towards generting a\\rbetter answer. We tested our approach on both synthesized and real-world\\rdatasets, including StackExchange and Reddit, and found that it significantly\\rimproves performance compared to standard few-shot prompting\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17390 ,  650kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17396\\rDate: Tue, 30 Jan 2024 19:27:04 GMT   (560kb)\\r\\rTitle: Fine-tuning Transformer-based Encoder for Turkish Language Understanding\\r  Tasks\\rAuthors: Savas Yildirim\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Deep learning-based and lately Transformer-based language models have been\\rdominating the studies of natural language processing in the last years. Thanks\\rto their accurate and fast fine-tuning characteristics, they have outperformed\\rtraditional machine learning-based approaches and achieved state-of-the-art\\rresults for many challenging natural language understanding (NLU) problems.\\rRecent studies showed that the Transformer-based models such as BERT, which is\\rBidirectional Encoder Representations from Transformers, have reached\\rimpressive achievements on many tasks. Moreover, thanks to their transfer\\rlearning capacity, these architectures allow us to transfer pre-built models\\rand fine-tune them to specific NLU tasks such as question answering. In this\\rstudy, we provide a Transformer-based model and a baseline benchmark for the\\rTurkish Language. We successfully fine-tuned a Turkish BERT model, namely\\rBERTurk that is trained with base settings, to many downstream tasks and\\revaluated with a the Turkish Benchmark dataset. We showed that our studies\\rsignificantly outperformed other existing baseline approaches for Named-Entity\\rRecognition, Sentiment Analysis, Question Answering and Text Classification in\\rTurkish Language. We publicly released these four fine-tuned models and\\rresources in reproducibility and with the view of supporting other Turkish\\rresearchers and applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17396 ,  560kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17461\\rDate: Tue, 30 Jan 2024 21:49:30 GMT   (7654kb,D)\\r\\rTitle: Synthetic Dialogue Dataset Generation using LLM Agents\\rAuthors: Yelaman Abdullin, Diego Molla-Aliod, Bahadorreza Ofoghi, John\\r  Yearwood, Qingyang Li\\rCategories: cs.CL cs.AI\\rComments: GEM Workshop @ EMNLP 2023\\r\\\\\\\\\\r  Linear programming (LP) problems are pervasive in real-life applications.\\rHowever, despite their apparent simplicity, an untrained user may find it\\rdifficult to determine the linear model of their specific problem. We envisage\\rthe creation of a goal-oriented conversational agent that will engage in\\rconversation with the user to elicit all information required so that a\\rsubsequent agent can generate the linear model. In this paper, we present an\\rapproach for the generation of sample dialogues that can be used to develop and\\rtrain such a conversational agent. Using prompt engineering, we develop two\\ragents that talk to each other, one acting as the conversational agent, and\\rthe other acting as the user. Using a set of text descriptions of linear\\rproblems from NL4Opt available to the user only, the agent and the user engage\\rin conversation until the agent has retrieved all key information from the\\roriginal problem description. We also propose an extrinsic evaluation of the\\rdialogues by assessing how well the summaries generated by the dialogues match\\rthe original problem descriptions. We conduct human and automatic evaluations,\\rincluding an evaluation approach that uses GPT-4 to mimic the human evaluation\\rmetrics. The evaluation results show an overall good quality of the dialogues,\\rthough research is still needed to improve the quality of the GPT-4 evaluation\\rmetrics. The resulting dialogues, including the human annotations of a subset,\\rare available to the research community. The conversational agent used for the\\rgeneration of the dialogues can be used as a baseline.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17461 ,  7654kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17464\\rDate: Tue, 30 Jan 2024 21:53:30 GMT   (734kb,D)\\r\\rTitle: Efficient Tool Use with Chain-of-Abstraction Reasoning\\rAuthors: Silin Gao, Jane Dwivedi-Yu, Ping Yu, Xiaoqing Ellen Tan, Ramakanth\\r  Pasunuru, Olga Golovneva, Koustuv Sinha, Asli Celikyilmaz, Antoine Bosselut,\\r  Tianlu Wang\\rCategories: cs.CL\\r\\\\\\\\\\r  To achieve faithful reasoning that aligns with human expectations, large\\rlanguage models (LLMs) need to ground their reasoning to real-world knowledge\\r(e.g., web facts, math and physical rules). Tools help LLMs access this\\rexternal knowledge, but there remains challenges for fine-tuning LLM agents\\r(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\\rinter-connected tool calls require holistic and efficient tool usage planning.\\r  In this work, we propose a new method for LLMs to better leverage tools in\\rmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\\rfirst decode reasoning chains with abstract placeholders, and then call domain\\rtools to reify each reasoning chain by filling in specific knowledge. This\\rplanning with abstract chains enables LLMs to learn more general reasoning\\rstrategies, which are robust to shifts of domain knowledge (e.g., math results)\\rrelevant to different reasoning questions. It also allows LLMs to perform\\rdecoding and calling of external tools in parallel, which avoids the inference\\rdelay caused by waiting for tool responses. In mathematical reasoning and Wiki\\rQA domains, we show that our method consistently outperforms previous\\rchain-of-thought and tool-augmented baselines on both in-distribution and\\rout-of-distribution test sets, with an average ~6% absolute QA accuracy\\rimprovement. LLM agents trained with our method also show more efficient tool\\ruse, with inference speed being on average ~1.4x faster than baseline\\rtool-augmented LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17464 ,  734kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17477\\rDate: Tue, 30 Jan 2024 22:22:55 GMT   (1161kb)\\r\\rTitle: Detecting mental disorder on social media: a ChatGPT-augmented\\r  explainable approach\\rAuthors: Loris Belcastro, Riccardo Cantini, Fabrizio Marozzo, Domenico Talia,\\r  Paolo Trunfio\\rCategories: cs.CL cs.AI cs.LG cs.SI\\r\\\\\\\\\\r  In the digital era, the prevalence of depressive symptoms expressed on social\\rmedia has raised serious concerns, necessitating advanced methodologies for\\rtimely detection. This paper addresses the challenge of interpretable\\rdepression detection by proposing a novel methodology that effectively combines\\rLarge Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and\\rconversational agents like ChatGPT. In our methodology, explanations are\\rachieved by integrating BERTweet, a Twitter-specific variant of BERT, into a\\rnovel self-explanatory model, namely BERT-XDD, capable of providing both\\rclassification and explanations via masked attention. The interpretability is\\rfurther enhanced using ChatGPT to transform technical explanations into\\rhuman-readable commentaries. By introducing an effective and modular approach\\rfor interpretable depression detection, our methodology can contribute to the\\rdevelopment of socially responsible digital platforms, fostering early\\rintervention and support for mental health challenges under the guidance of\\rqualified healthcare professionals.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17477 ,  1161kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17498\\rDate: Tue, 30 Jan 2024 23:08:26 GMT   (721kb,D)\\r\\rTitle: Improving QA Model Performance with Cartographic Inoculation\\rAuthors: Allen Chen (UT Austin), Okan Tankirulu (UT Austin)\\rCategories: cs.CL\\rComments: 9 pages, 6 figures\\rACM-class: I.2.7\\r\\\\\\\\\\r  QA models are faced with complex and open-ended contextual reasoning\\rproblems, but can often learn well-performing solution heuristics by exploiting\\rdataset-specific patterns in their training data. These patterns, or dataset\\rartifacts, reduce the model's ability to generalize to real-world QA problems.\\rUtilizing an ElectraSmallDiscriminator model trained for QA, we analyze the\\rimpacts and incidence of dataset artifacts using an adversarial challenge set\\rdesigned to confuse models reliant on artifacts for prediction. Extending\\rexisting work on methods for mitigating artifact impacts, we propose\\rcartographic inoculation, a novel method that fine-tunes models on an optimized\\rsubset of the challenge data to reduce model reliance on dataset artifacts. We\\rshow that by selectively fine-tuning a model on ambiguous adversarial examples\\rfrom a challenge set, significant performance improvements can be made on the\\rfull challenge dataset with minimal loss of model generalizability to other\\rchallenging environments and QA datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17498 ,  721kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17514\\rDate: Wed, 31 Jan 2024 00:15:34 GMT   (220kb,D)\\r\\rTitle: FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation\\rAuthors: Rheeya Uppaal, Yixuan Li, Junjie Hu\\rCategories: cs.CL\\r\\\\\\\\\\r  A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled\\rdata from both source and target domains to learn domain-invariant\\rrepresentations for adaptation. However, these methods showcase certain\\rlimitations, encouraging the use of self-supervised learning through continued\\rpre-training. The necessity of continued pre-training or learning\\rdomain-invariant representations is still unclear in the prompt-based\\rclassification framework, where an input example is modified by a template and\\rthen fed into a language model (LM) to generate a label string. To examine this\\rnew paradigm of UDA in the prompt-based setup, we propose a frustratingly easy\\rUDA method (FEUDA) that trains an autoregressive LM on both unlabeled and\\rlabeled examples using two different instruction-tuning tasks. Specifically,\\rthe first task trains the LM on unlabeled texts from both domains via masked\\rlanguage modeling (MLM), and the other uses supervised instruction-tuning on\\rsource-labeled data for classification. We conduct extensive experiments on 24\\rreal-world domain pairs to show the effectiveness of our method over strong\\rdomain-invariant learning methods. Our analysis sheds light on why masked\\rlanguage modeling improves target-domain classification performance in\\rprompt-based UDA. We discover that MLM helps the model learn both semantic and\\rbackground knowledge of a domain, which are both beneficial for downstream\\rclassification.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17514 ,  220kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17536\\rDate: Wed, 31 Jan 2024 01:37:33 GMT   (466kb,D)\\r\\rTitle: PipeNet: Question Answering with Semantic Pruning over Knowledge Graphs\\rAuthors: Ying Su, Jipeng Zhang, Yangqiu Song, Tong Zhang\\rCategories: cs.CL\\rComments: 8 pages, 4 figures\\r\\\\\\\\\\r  It is well acknowledged that incorporating explicit knowledge graphs (KGs)\\rcan benefit question answering. Existing approaches typically follow a\\rgrounding-reasoning pipeline in which entity nodes are first grounded for the\\rquery (question and candidate answers), and then a reasoning module reasons\\rover the matched multi-hop subgraph for answer prediction. Although the\\rpipeline largely alleviates the issue of extracting essential information from\\rgiant KGs, efficiency is still an open challenge when scaling up hops in\\rgrounding the subgraphs. In this paper, we target at finding semantically\\rrelated entity nodes in the subgraph to improve the efficiency of graph\\rreasoning with KG. We propose a grounding-pruning-reasoning pipeline to prune\\rnoisy nodes, remarkably reducing the computation cost and memory usage while\\ralso obtaining decent subgraph representation. In detail, the pruning module\\rfirst scores concept nodes based on the dependency distance between matched\\rspans and then prunes the nodes according to score ranks. To facilitate the\\revaluation of pruned subgraphs, we also propose a graph attention network (GAT)\\rbased module to reason with the subgraph data. Experimental results on\\rCommonsenseQA and OpenBookQA demonstrate the effectiveness of our method.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17536 ,  466kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17574\\rDate: Wed, 31 Jan 2024 03:39:07 GMT   (2255kb,D)\\r\\rTitle: Scavenging Hyena: Distilling Transformers into Long Convolution Models\\rAuthors: Tokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad\\r  Sami Nur Islam, Wassim Jabbour, Laurence Liang\\rCategories: cs.CL cs.LG\\rComments: 9 pages, 2 figures\\r\\\\\\\\\\r  The rapid evolution of Large Language Models (LLMs), epitomized by\\rarchitectures like GPT-4, has reshaped the landscape of natural language\\rprocessing. This paper introduces a pioneering approach to address the\\refficiency concerns associated with LLM pre-training, proposing the use of\\rknowledge distillation for cross-architecture transfer. Leveraging insights\\rfrom the efficient Hyena mechanism, our method replaces attention heads in\\rtransformer models by Hyena, offering a cost-effective alternative to\\rtraditional pre-training while confronting the challenge of processing long\\rcontextual information, inherent in quadratic attention mechanisms. Unlike\\rconventional compression-focused methods, our technique not only enhances\\rinference speed but also surpasses pre-training in terms of both accuracy and\\refficiency. In the era of evolving LLMs, our work contributes to the pursuit of\\rsustainable AI solutions, striking a balance between computational power and\\renvironmental impact.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17574 ,  2255kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17585\\rDate: Wed, 31 Jan 2024 04:12:59 GMT   (12992kb,D)\\r\\rTitle: Propagation and Pitfalls: Reasoning-based Assessment of Knowledge\\r  Editing through Counterfactual Tasks\\rAuthors: Wenyue Hua, Jiang Guo, Mingwen Dong, Henghui Zhu, Patrick Ng, Zhiguo\\r  Wang\\rCategories: cs.CL cs.AI cs.LG stat.ME\\rComments: 22 pages, 14 figures, 5 tables\\r\\\\\\\\\\r  Current approaches of knowledge editing struggle to effectively propagate\\rupdates to interconnected facts. In this work, we delve into the barriers that\\rhinder the appropriate propagation of updated knowledge within these models for\\raccurate reasoning. To support our analysis, we introduce a novel\\rreasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing\\rdataset) -- which covers six common reasoning schemes in real world. We conduct\\ra thorough analysis of existing knowledge editing techniques, including input\\raugmentation, finetuning, and locate-and-edit. We found that all model editing\\rmethods show notably low performance on this dataset, especially in certain\\rreasoning schemes. Our analysis over the chain-of-thought generation of edited\\rmodels further uncover key reasons behind the inadequacy of existing knowledge\\rediting methods from a reasoning standpoint, involving aspects on fact-wise\\rediting, fact recall ability, and coherence in generation. We will make our\\rbenchmark publicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17585 ,  12992kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17588\\rDate: Wed, 31 Jan 2024 04:19:22 GMT   (8001kb,D)\\r\\rTitle: Local and Global Contexts for Conversation\\rAuthors: Zuoquan Lin and Xinyi Shen\\rCategories: cs.CL\\rComments: 11 pages, 3 figures\\r\\\\\\\\\\r  The context in conversation is the dialog history crucial for multi-turn\\rdialogue. Learning from the relevant contexts in dialog history for grounded\\rconversation is a challenging problem. Local context is the most neighbor and\\rmore sensitive to the subsequent response, and global context is relevant to a\\rwhole conversation far beyond neighboring utterances. Currently, pretrained\\rtransformer models for conversation challenge capturing the correlation and\\rconnection between local and global contexts. We introduce a local and global\\rconversation model (LGCM) for general-purpose conversation in open domain. It\\ris a local-global hierarchical transformer model that excels at accurately\\rdiscerning and assimilating the relevant contexts necessary for generating\\rresponses. It employs a local encoder to grasp the local context at the level\\rof individual utterances and a global encoder to understand the broader context\\rat the dialogue level. The seamless fusion of these locally and globally\\rcontextualized encodings ensures a comprehensive comprehension of the\\rconversation. Experiments on popular datasets show that LGCM outperforms the\\rexisting conversation models on the performance of automatic metrics with\\rsignificant margins.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17588 ,  8001kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17597\\rDate: Wed, 31 Jan 2024 04:50:00 GMT   (398kb,D)\\r\\rTitle: SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization\\rAuthors: Sangwoo Cho, Kaiqiang Song, Chao Zhao, Xiaoyang Wang, Dong Yu\\rCategories: cs.CL\\rComments: 11 pages, 2 figures\\r\\\\\\\\\\r  Multi-turn dialogues are characterized by their extended length and the\\rpresence of turn-taking conversations. Traditional language models often\\roverlook the distinct features of these dialogues by treating them as regular\\rtext. In this paper, we propose a speaker-enhanced pre-training method for long\\rdialogue summarization, which leverages the inherent structure of multiple-turn\\rdialogues. To support our study, we curate a diverse dataset that includes\\rtranscripts from real-world scenarios, movie or TV show transcripts, and\\rdialogues generated by a Large Language Model. We then perform a pre-training,\\rwhich encompasses the detection of speaker changes, and masked utterance\\rgeneration. Experimental results of fine-tuned models demonstrate that our\\rmodel achieves state-of-the-art performance on downstream benchmarks with long\\rcontext, surpassing baseline models and highlighting the effectiveness of our\\rapproach. Our findings highlight the importance of curating pre-training\\rdatasets that exhibit diversity and variations in length distribution to ensure\\reffective alignment with downstream datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17597 ,  398kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17600\\rDate: Wed, 31 Jan 2024 04:57:12 GMT   (14711kb,D)\\r\\rTitle: Good at captioning, bad at counting: Benchmarking GPT-4V on Earth\\r  observation data\\rAuthors: Chenhui Zhang, Sherrie Wang\\rCategories: cs.CL cs.AI cs.CV\\rComments: 62 pages; work in progress\\r\\\\\\\\\\r  Large Vision-Language Models (VLMs) have demonstrated impressive performance\\ron complex tasks involving visual input with natural language instructions.\\rHowever, it remains unclear to what extent capabilities on natural images\\rtransfer to Earth observation (EO) data, which are predominantly satellite and\\raerial images less common in VLM training data. In this work, we propose a\\rcomprehensive benchmark to gauge the progress of VLMs toward being useful tools\\rfor EO data by assessing their abilities on scene understanding, localization\\rand counting, and change detection tasks. Motivated by real-world applications,\\rour benchmark includes scenarios like urban monitoring, disaster relief, land\\ruse, and conservation. We discover that, although state-of-the-art VLMs like\\rGPT-4V possess extensive world knowledge that leads to strong performance on\\ropen-ended tasks like location understanding and image captioning, their poor\\rspatial reasoning limits usefulness on object localization and counting tasks.\\rOur benchmark will be made publicly available at https://vleo.danielz.ch/ and\\ron Hugging Face at\\rhttps://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70\\rfor easy model evaluation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17600 ,  14711kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17602\\rDate: Wed, 31 Jan 2024 05:11:00 GMT   (583kb,D)\\r\\rTitle: Assertion Detection Large Language Model In-context Learning LoRA\\r  Fine-tuning\\rAuthors: Yuelyu Ji, Zeshui Yu and Yanshan Wang\\rCategories: cs.CL\\r\\\\\\\\\\r  In this study, we aim to address the task of assertion detection when\\rextracting medical concepts from clinical notes, a key process in clinical\\rnatural language processing (NLP). Assertion detection in clinical NLP usually\\rinvolves identifying assertion types for medical concepts in the clinical text,\\rnamely certainty (whether the medical concept is positive, negated, possible,\\ror hypothetical), temporality (whether the medical concept is for present or\\rthe past history), and experiencer (whether the medical concept is described\\rfor the patient or a family member). These assertion types are essential for\\rhealthcare professionals to quickly and clearly understand the context of\\rmedical conditions from unstructured clinical texts, directly influencing the\\rquality and outcomes of patient care. Although widely used, traditional\\rmethods, particularly rule-based NLP systems and machine learning or deep\\rlearning models, demand intensive manual efforts to create patterns and tend to\\roverlook less common assertion types, leading to an incomplete understanding of\\rthe context. To address this challenge, our research introduces a novel\\rmethodology that utilizes Large Language Models (LLMs) pre-trained on a vast\\rarray of medical data for assertion detection. We enhanced the current method\\rwith advanced reasoning techniques, including Tree of Thought (ToT), Chain of\\rThought (CoT), and Self-Consistency (SC), and refine it further with Low-Rank\\rAdaptation (LoRA) fine-tuning. We first evaluated the model on the i2b2 2010\\rassertion dataset. Our method achieved a micro-averaged F-1 of 0.89, with 0.11\\rimprovements over the previous works. To further assess the generalizability of\\rour approach, we extended our evaluation to a local dataset that focused on\\rsleep concept extraction. Our approach achieved an F-1 of 0.74, which is 0.31\\rhigher than the previous method.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17602 ,  583kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17623\\rDate: Wed, 31 Jan 2024 06:49:36 GMT   (561kb,D)\\r\\rTitle: Neighboring Perturbations of Knowledge Editing on Large Language Models\\rAuthors: Jun-Yu Ma, Jia-Chen Gu, Ningyu Zhang, Zhen-Hua Ling\\rCategories: cs.CL\\r\\\\\\\\\\r  Despite their exceptional capabilities, large language models (LLMs) are\\rprone to generating unintended text due to false or outdated knowledge. Given\\rthe resource-intensive nature of retraining LLMs, there has been a notable\\rincrease in the development of knowledge editing. However, current approaches\\rand evaluations rarely explore the perturbation of editing on neighboring\\rknowledge. This paper studies whether updating new knowledge to LLMs perturbs\\rthe neighboring knowledge encapsulated within them. Specifically, we seek to\\rfigure out whether appending a new answer into an answer list to a factual\\rquestion leads to catastrophic forgetting of original correct answers in this\\rlist, as well as unintentional inclusion of incorrect answers. A metric of\\radditivity is introduced and a benchmark dubbed as Perturbation Evaluation of\\rAppending Knowledge (PEAK) is constructed to evaluate the degree of\\rperturbation to neighboring knowledge when appending new knowledge. Besides, a\\rplug-and-play framework termed Appending via Preservation and Prevention (APP)\\ris proposed to mitigate the neighboring perturbation by maintaining the\\rintegrity of the answer list. Experiments demonstrate the effectiveness of APP\\rcoupling with four editing methods on three LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17623 ,  561kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17632\\rDate: Wed, 31 Jan 2024 07:23:22 GMT   (124kb,D)\\r\\rTitle: What Do Self-Supervised Speech and Speaker Models Learn? New Findings\\r  From a Cross Model Layer-Wise Analysis\\rAuthors: Takanori Ashihara, Marc Delcroix, Takafumi Moriya, Kohei Matsuura,\\r  Taichi Asami, Yusuke Ijima\\rCategories: cs.CL cs.SD eess.AS\\rComments: Accepted at ICASSP 2024\\r\\\\\\\\\\r  Self-supervised learning (SSL) has attracted increased attention for learning\\rmeaningful speech representations. Speech SSL models, such as WavLM, employ\\rmasked prediction training to encode general-purpose representations. In\\rcontrast, speaker SSL models, exemplified by DINO-based models, adopt\\rutterance-level training objectives primarily for speaker representation.\\rUnderstanding how these models represent information is essential for refining\\rmodel efficiency and effectiveness. Unlike the various analyses of speech SSL,\\rthere has been limited investigation into what information speaker SSL captures\\rand how its representation differs from speech SSL or other fully-supervised\\rspeaker models. This paper addresses these fundamental questions. We explore\\rthe capacity to capture various speech properties by applying SUPERB evaluation\\rprobing tasks to speech and speaker SSL models. We also examine which layers\\rare predominantly utilized for each task to identify differences in how speech\\ris represented. Furthermore, we conduct direct comparisons to measure the\\rsimilarities between layers within and across models. Our analysis unveils that\\r1) the capacity to represent content information is somewhat unrelated to\\renhanced speaker representation, 2) specific layers of speech SSL models would\\rbe partly specialized in capturing linguistic information, and 3) speaker SSL\\rmodels tend to disregard linguistic information but exhibit more sophisticated\\rspeaker representation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17632 ,  124kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17633\\rDate: Wed, 31 Jan 2024 07:26:47 GMT   (7807kb,D)\\r\\rTitle: Navigating the OverKill in Large Language Models\\rAuthors: Chenyu Shi, Xiao Wang, Qiming Ge, Songyang Gao, Xianjun Yang, Tao Gui,\\r  Qi Zhang, Xuanjing Huang, Xun Zhao, Dahua Lin\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Large language models are meticulously aligned to be both helpful and\\rharmless. However, recent research points to a potential overkill which means\\rmodels may refuse to answer benign queries. In this paper, we investigate the\\rfactors for overkill by exploring how models handle and determine the safety of\\rqueries. Our findings reveal the presence of shortcuts within models, leading\\rto an over-attention of harmful words like 'kill' and prompts emphasizing\\rsafety will exacerbate overkill. Based on these insights, we introduce\\rSelf-Contrastive Decoding (Self-CD), a training-free and model-agnostic\\rstrategy, to alleviate this phenomenon. We first extract such over-attention by\\ramplifying the difference in the model's output distributions when responding\\rto system prompts that either include or omit an emphasis on safety. Then we\\rdetermine the final next-token predictions by downplaying the over-attention\\rfrom the model via contrastive decoding. Empirical results indicate that our\\rmethod has achieved an average reduction of the refusal rate by 20\\\\% while\\rhaving almost no impact on safety.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17633 ,  7807kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17658\\rDate: Wed, 31 Jan 2024 08:28:06 GMT   (7958kb,D)\\r\\rTitle: Document Structure in Long Document Transformers\\rAuthors: Jan Buchmann, Max Eichler, Jan-Micha Bodensohn, Ilia Kuznetsov, Iryna\\r  Gurevych\\rCategories: cs.CL\\rComments: Accepted at EACL 2024. Code and data:\\r  http://github.com/UKPLab/eacl2024-doc-structure\\r\\\\\\\\\\r  Long documents often exhibit structure with hierarchically organized elements\\rof different functions, such as section headers and paragraphs. Despite the\\romnipresence of document structure, its role in natural language processing\\r(NLP) remains opaque. Do long-document Transformer models acquire an internal\\rrepresentation of document structure during pre-training? How can structural\\rinformation be communicated to a model after pre-training, and how does it\\rinfluence downstream performance? To answer these questions, we develop a novel\\rsuite of probing tasks to assess structure-awareness of long-document\\rTransformers, propose general-purpose structure infusion methods, and evaluate\\rthe effects of structure infusion on QASPER and Evidence Inference, two\\rchallenging long-document NLP tasks. Results on LED and LongT5 suggest that\\rthey acquire implicit understanding of document structure during pre-training,\\rwhich can be further enhanced by structure infusion, leading to improved\\rend-task performance. To foster research on the role of document structure in\\rNLP modeling, we make our data and code publicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17658 ,  7958kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17671\\rDate: Wed, 31 Jan 2024 08:48:35 GMT   (4224kb,D)\\r\\rTitle: Contextual Feature Extraction Hierarchies Converge in Large Language\\r  Models and the Brain\\rAuthors: Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta and\\r  Nima Mesgarani\\rCategories: cs.CL cs.AI q-bio.NC\\rComments: 19 pages, 5 figures and 4 supplementary figures\\r\\\\\\\\\\r  Recent advancements in artificial intelligence have sparked interest in the\\rparallels between large language models (LLMs) and human neural processing,\\rparticularly in language comprehension. While prior research has established\\rsimilarities in the representation of LLMs and the brain, the underlying\\rcomputational principles that cause this convergence, especially in the context\\rof evolving LLMs, remain elusive. Here, we examined a diverse selection of\\rhigh-performance LLMs with similar parameter sizes to investigate the factors\\rcontributing to their alignment with the brain's language processing\\rmechanisms. We find that as LLMs achieve higher performance on benchmark tasks,\\rthey not only become more brain-like as measured by higher performance when\\rpredicting neural responses from LLM embeddings, but also their hierarchical\\rfeature extraction pathways map more closely onto the brain's while using fewer\\rlayers to do the same encoding. We also compare the feature extraction pathways\\rof the LLMs to each other and identify new ways in which high-performing models\\rhave converged toward similar hierarchical processing mechanisms. Finally, we\\rshow the importance of contextual information in improving model performance\\rand brain similarity. Our findings reveal the converging aspects of language\\rprocessing in the brain and LLMs and offer new directions for developing models\\rthat align more closely with human cognitive processing.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17671 ,  4224kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17686\\rDate: Wed, 31 Jan 2024 09:16:35 GMT   (8133kb,D)\\r\\rTitle: Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought\\r  Reasoning\\rAuthors: Tinghui Zhu, Kai Zhang, Jian Xie, Yu Su\\rCategories: cs.CL\\r\\\\\\\\\\r  Recent advancements have significantly augmented the reasoning capabilities\\rof Large Language Models (LLMs) through various methodologies, especially\\rchain-of-thought (CoT) reasoning. However, previous methods fail to address\\rreasoning errors in intermediate steps, leading to accumulative errors.In this\\rpaper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT\\rand deductive reasoning with step-wise beam search for LLMs. Our approach\\rdeploys a verifier, verifying the deducibility of a reasoning step and its\\rpremises, thus alleviating the error accumulation. Furthermore, we introduce a\\rscalable and labor-free data construction method to amplify our model's\\rverification capabilities. Extensive experiments demonstrate that our approach\\rsignificantly enhances the base performance of LLMs of various scales (7B, 13B,\\r70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres,\\rincluding arithmetic, commonsense, and symbolic. Moreover, our analysis proves\\rDBS's capability of detecting diverse and subtle reasoning errors and\\rrobustness on different model scales.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17686 ,  8133kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17692\\rDate: Wed, 31 Jan 2024 09:28:06 GMT   (229kb,D)\\r\\rTitle: Mitigating the Problem of Strong Priors in LMs with Context\\r  Extrapolation\\rAuthors: Raymond Douglas, Andis Draguns, Tom\\\\'a\\\\v{s} Gaven\\\\v{c}iak\\rCategories: cs.CL\\rComments: 12 pages, 4 figures\\r\\\\\\\\\\r  Language models (LMs) have become important tools in a variety of\\rapplications, from data processing to the creation of instruction-following\\rassistants. But despite their advantages, LMs have certain idiosyncratic\\rlimitations such as the problem of `strong priors', where a model learns to\\routput typical continuations in response to certain, usually local, portions of\\rthe input regardless of any earlier instructions. For example, prompt injection\\rattacks can induce models to ignore explicit directives. In some cases, larger\\rmodels have been shown to be more susceptible to these problems than similar\\rsmaller models, an example of the phenomenon of `inverse scaling'. We develop a\\rnew technique for mitigating the problem of strong priors: we take the original\\rset of instructions, produce a weakened version of the original prompt that is\\reven more susceptible to the strong priors problem, and then extrapolate the\\rcontinuation away from the weakened prompt. This lets us infer how the model\\rwould continue a hypothetical strengthened set of instructions. Our technique\\rconceptualises LMs as mixture models which combine a family of data generation\\rprocesses, reinforcing the desired elements of the mixture. Our approach works\\rat inference time, removing any need for retraining. We apply it to eleven\\rmodels including GPT-2, GPT-3, Llama 2, and Mistral on four tasks, and find\\rimprovements in 41/44. Across all 44 combinations the median increase in\\rproportion of tasks completed is 40%.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17692 ,  229kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17703\\rDate: Wed, 31 Jan 2024 09:49:22 GMT   (8626kb,D)\\r\\rTitle: WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts\\rAuthors: Pardis Sadat Zahraei, Ali Emami\\rCategories: cs.CL cs.CY\\rComments: Accepted for publication in main proceedings of EACL 2024 conference,\\r  22 pages, 16 figures\\rACM-class: I.2.7; K.4.1\\r\\\\\\\\\\r  The Winograd Schema Challenge (WSC) serves as a prominent benchmark for\\revaluating machine understanding. While Large Language Models (LLMs) excel at\\ranswering WSC questions, their ability to generate such questions remains less\\rexplored. In this work, we propose Tree-of-Experts (ToE), a novel prompting\\rmethod which enhances the generation of WSC instances (50% valid cases vs. 10%\\rin recent methods). Using this approach, we introduce WSC+, a novel dataset\\rcomprising 3,026 LLM-generated sentences. Notably, we extend the WSC framework\\rby incorporating new 'ambiguous' and 'offensive' categories, providing a deeper\\rinsight into model overconfidence and bias. Our analysis reveals nuances in\\rgeneration-evaluation consistency, suggesting that LLMs may not always\\routperform in evaluating their own generated questions when compared to those\\rcrafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves an\\raccuracy of 68.7%, significantly below the human benchmark of 95.1%.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17703 ,  8626kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17716\\rDate: Wed, 31 Jan 2024 10:20:01 GMT   (265kb,D)\\r\\rTitle: Enhancing Large Language Model with Decomposed Reasoning for Emotion\\r  Cause Pair Extraction\\rAuthors: Jialiang Wu, Yi Shen, Ziheng Zhang, Longjun Cai\\rCategories: cs.CL\\rComments: 13 pages, 5 figures\\r\\\\\\\\\\r  Emotion-Cause Pair Extraction (ECPE) involves extracting clause pairs\\rrepresenting emotions and their causes in a document. Existing methods tend to\\roverfit spurious correlations, such as positional bias in existing benchmark\\rdatasets, rather than capturing semantic features. Inspired by recent work, we\\rexplore leveraging large language model (LLM) to address ECPE task without\\radditional training. Despite strong capabilities, LLMs suffer from\\runcontrollable outputs, resulting in mediocre performance. To address this, we\\rintroduce chain-of-thought to mimic human cognitive process and propose the\\rDecomposed Emotion-Cause Chain (DECC) framework. Combining inducing inference\\rand logical pruning, DECC guides LLMs to tackle ECPE task. We further enhance\\rthe framework by incorporating in-context learning. Experiment results\\rdemonstrate the strength of DECC compared to state-of-the-art supervised\\rfine-tuning methods. Finally, we analyze the effectiveness of each component\\rand the robustness of the method in various scenarios, including different LLM\\rbases, rebalanced datasets, and multi-pair extraction.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17716 ,  265kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17755\\rDate: Wed, 31 Jan 2024 11:30:24 GMT   (440kb,D)\\r\\rTitle: CauESC: A Causal Aware Model for Emotional Support Conversation\\rAuthors: Wei Chen, Hengxu Lin, Qun Zhang, Xiaojin Zhang, Xiang Bai, Xuanjing\\r  Huang, Zhongyu Wei\\rCategories: cs.CL\\rComments: 15 pages, 5 figures\\rACM-class: I.2.7\\r\\\\\\\\\\r  Emotional Support Conversation aims at reducing the seeker's emotional\\rdistress through supportive response. Existing approaches have two limitations:\\r(1) They ignore the emotion causes of the distress, which is important for\\rfine-grained emotion understanding; (2) They focus on the seeker's own mental\\rstate rather than the emotional dynamics during interaction between speakers.\\rTo address these issues, we propose a novel framework CauESC, which firstly\\rrecognizes the emotion causes of the distress, as well as the emotion effects\\rtriggered by the causes, and then understands each strategy of verbal grooming\\rindependently and integrates them skillfully. Experimental results on the\\rbenchmark dataset demonstrate the effectiveness of our approach and show the\\rbenefits of emotion understanding from cause to effect and\\rindependent-integrated strategy modeling.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17755 ,  440kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17809\\rDate: Wed, 31 Jan 2024 13:08:45 GMT   (206kb,D)\\r\\rTitle: SWEA: Changing Factual Knowledge in Large Language Models via Subject\\r  Word Embedding Altering\\rAuthors: Xiaopeng Li, Shasha Li, Bin Ji, Shezheng Song, Xi Wang, Jun Ma, Jie\\r  Yu, Xiaodong Liu, Jing Wang and Weimin Zhang\\rCategories: cs.CL cs.AI cs.LG\\rComments: Work in progress; Our code will be released\\r\\\\\\\\\\r  Model editing has recently gained widespread attention. Current model editing\\rmethods primarily involve modifying model parameters or adding additional\\rmodules to the existing model. However, the former causes irreversible damage\\rto LLMs, while the latter incurs additional inference overhead and fuzzy vector\\rmatching is not always reliable. To address these issues, we propose an\\rexpandable Subject Word Embedding Altering (SWEA) framework, which modifies the\\rrepresentation of subjects and achieve the goal of editing knowledge during the\\rinference stage. SWEA uses precise key matching outside the model and performs\\rreliable subject word embedding altering, thus protecting the original weights\\rof the model without increasing inference overhead. We then propose optimizing\\rthen suppressing fusion method, which first optimizes the embedding vector for\\rthe editing target and then suppresses the Knowledge Embedding Dimension (KED)\\rto obtain the final fused embedding. We thus propose SWEAOS method for editing\\rfactual knowledge in LLMs. We demonstrate the state-of-the-art performance of\\rSWEAOS on the COUNTERFACT and zsRE datasets. To further validate the reasoning\\rability of SWEAOS in editing knowledge, we evaluate it on the more complex\\rRIPPLEEDITS benchmark. The results on two subdatasets demonstrate that our\\rSWEAOS possesses state-of-the-art reasoning ability.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17809 ,  206kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17824\\rDate: Wed, 31 Jan 2024 13:35:07 GMT   (1438kb,D)\\r\\rTitle: A Survey of Pre-trained Language Models for Processing Scientific Text\\rAuthors: Xanh Ho, Anh Khoa Duong Nguyen, An Tuan Dao, Junfeng Jiang, Yuki\\r  Chida, Kaito Sugimoto, Huy Quoc To, Florian Boudin and Akiko Aizawa\\rCategories: cs.CL\\rComments: Resources are available at https://github.com/Alab-NII/Awesome-SciLM\\r\\\\\\\\\\r  The number of Language Models (LMs) dedicated to processing scientific text\\ris on the rise. Keeping pace with the rapid growth of scientific LMs (SciLMs)\\rhas become a daunting task for researchers. To date, no comprehensive surveys\\ron SciLMs have been undertaken, leaving this issue unaddressed. Given the\\rconstant stream of new SciLMs, appraising the state-of-the-art and how they\\rcompare to each other remain largely unknown. This work fills that gap and\\rprovides a comprehensive review of SciLMs, including an extensive analysis of\\rtheir effectiveness across different domains, tasks and datasets, and a\\rdiscussion on the challenges that lie ahead.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17824 ,  1438kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17827\\rDate: Wed, 31 Jan 2024 13:40:00 GMT   (282kb,D)\\r\\rTitle: Neural Machine Translation for Malayalam Paraphrase Generation\\rAuthors: Christeena Varghese, Sergey Koshelev, Ivan P. Yamshchikov\\rCategories: cs.CL cs.AI\\rACM-class: I.7.0; I.2.7\\r\\\\\\\\\\r  This study explores four methods of generating paraphrases in Malayalam,\\rutilizing resources available for English paraphrasing and pre-trained Neural\\rMachine Translation (NMT) models. We evaluate the resulting paraphrases using\\rboth automated metrics, such as BLEU, METEOR, and cosine similarity, as well as\\rhuman annotation. Our findings suggest that automated evaluation measures may\\rnot be fully appropriate for Malayalam, as they do not consistently align with\\rhuman judgment. This discrepancy underscores the need for more nuanced\\rparaphrase evaluation approaches especially for highly agglutinative languages.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17827 ,  282kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17839\\rDate: Wed, 31 Jan 2024 13:57:24 GMT   (6685kb,D)\\r\\rTitle: Global-Liar: Factuality of LLMs over Time and Geographic Regions\\rAuthors: Shujaat Mirza, Bruno Coelho, Yuyuan Cui, Christina P\\\\opper, Damon\\r  McCoy\\rCategories: cs.CL cs.AI cs.IR\\rComments: 24 pages, 12 figures, 9 tables\\r\\\\\\\\\\r  The increasing reliance on AI-driven solutions, particularly Large Language\\rModels (LLMs) like the GPT series, for information retrieval highlights the\\rcritical need for their factuality and fairness, especially amidst the rampant\\rspread of misinformation and disinformation online. Our study evaluates the\\rfactual accuracy, stability, and biases in widely adopted GPT models, including\\rGPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated\\rinformation dissemination.\\r  We introduce 'Global-Liar,' a dataset uniquely balanced in terms of\\rgeographic and temporal representation, facilitating a more nuanced evaluation\\rof LLM biases. Our analysis reveals that newer iterations of GPT models do not\\ralways equate to improved performance. Notably, the GPT-4 version from March\\rdemonstrates higher factual accuracy than its subsequent June release.\\rFurthermore, a concerning bias is observed, privileging statements from the\\rGlobal North over the Global South, thus potentially exacerbating existing\\rinformational inequities. Regions such as Africa and the Middle East are at a\\rdisadvantage, with much lower factual accuracy. The performance fluctuations\\rover time suggest that model updates may not consistently benefit all regions\\requally.\\r  Our study also offers insights into the impact of various LLM configuration\\rsettings, such as binary decision forcing, model re-runs and temperature, on\\rmodel's factuality. Models constrained to binary (true/false) choices exhibit\\rreduced factuality compared to those allowing an 'unclear' option. Single\\rinference at a low temperature setting matches the reliability of majority\\rvoting across various configurations. The insights gained highlight the need\\rfor culturally diverse and geographically inclusive model training and\\revaluation. This approach is key to achieving global equity in technology,\\rdistributing AI benefits fairly worldwide.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17839 ,  6685kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17858\\rDate: Wed, 31 Jan 2024 14:19:03 GMT   (1308kb,D)\\r\\rTitle: Probing Language Models' Gesture Understanding for Enhanced Human-AI\\r  Interaction\\rAuthors: Philipp Wicke\\rCategories: cs.CL\\rComments: Preprint\\r\\\\\\\\\\r  The rise of Large Language Models (LLMs) has affected various disciplines\\rthat got beyond mere text generation. Going beyond their textual nature, this\\rproject proposal aims to investigate the interaction between LLMs and\\rnon-verbal communication, specifically focusing on gestures. The proposal sets\\rout a plan to examine the proficiency of LLMs in deciphering both explicit and\\rimplicit non-verbal cues within textual prompts and their ability to associate\\rthese gestures with various contextual factors. The research proposes to test\\restablished psycholinguistic study designs to construct a comprehensive dataset\\rthat pairs textual prompts with detailed gesture descriptions, encompassing\\rdiverse regional variations, and semantic labels. To assess LLMs' comprehension\\rof gestures, experiments are planned, evaluating their ability to simulate\\rhuman behaviour in order to replicate psycholinguistic experiments. These\\rexperiments consider cultural dimensions and measure the agreement between\\rLLM-identified gestures and the dataset, shedding light on the models'\\rcontextual interpretation of non-verbal cues (e.g. gestures).\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17858 ,  1308kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17882\\rDate: Wed, 31 Jan 2024 14:41:23 GMT   (224kb,D)\\r\\rTitle: I Think, Therefore I am: Awareness in Large Language Models\\rAuthors: Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan and Lichao Sun\\rCategories: cs.CL\\r\\\\\\\\\\r  Do large language models (LLMs) exhibit any forms of awareness similar to\\rhumans? In this paper, we introduce the concept of awareness to LLMs, arguing\\rthat awareness is an essential aspect of trustworthiness for LLMs to enhance\\rtheir interaction with humans while ensuring ethical responses. We define\\rawareness in LLMs as the ability to perceive and understand themselves as AI\\rmodels and to exhibit social intelligence. We identify four key dimensions of\\rawareness: capability, mission, emotion, and perspective. To assess LLMs on\\rthese dimensions, we introduce a specialized dataset, AwareLLM dataset. Our\\rfindings reveal that LLMs demonstrate a decent degree of awareness, though they\\rstill lack substantial capability awareness.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17882 ,  224kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17897\\rDate: Wed, 31 Jan 2024 15:04:01 GMT   (113kb)\\r\\rTitle: Employing Label Models on ChatGPT Answers Improves Legal Text Entailment\\r  Performance\\rAuthors: Chau Nguyen and Le-Minh Nguyen\\rCategories: cs.CL\\rComments: 15 pages\\r\\\\\\\\\\r  The objective of legal text entailment is to ascertain whether the assertions\\rin a legal query logically follow from the information provided in one or\\rmultiple legal articles. ChatGPT, a large language model, is robust in many\\rnatural language processing tasks, including legal text entailment: when we set\\rthe temperature = 0 (the ChatGPT answers are deterministic) and prompt the\\rmodel, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperforms\\rthe previous SOTA of 67.89%. On the other hand, if the temperature is larger\\rthan zero, ChatGPT answers are not deterministic, leading to inconsistent\\ranswers and fluctuating results. We propose to leverage label models (a\\rfundamental component of weak supervision techniques) to integrate the\\rprovisional answers by ChatGPT into consolidated labels. By that way, we treat\\rChatGPT provisional answers as noisy predictions which can be consolidated by\\rlabel models. The experimental results demonstrate that this approach can\\rattain an accuracy of 76.15%, marking a significant improvement of 8.26% over\\rthe prior state-of-the-art benchmark. Additionally, we perform an analysis of\\rthe instances where ChatGPT produces incorrect answers, then we classify the\\rerrors, offering insights that could guide potential enhancements for future\\rresearch endeavors.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17897 ,  113kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17911\\rDate: Wed, 31 Jan 2024 15:16:25 GMT   (5429kb,D)\\r\\rTitle: SNNLP: Energy-Efficient Natural Language Processing Using Spiking Neural\\r  Networks\\rAuthors: R. Alexander Knipper, Kaniz Mishty, Mehdi Sadi, Shubhra Kanti Karmaker\\r  Santu\\rCategories: cs.CL\\r\\\\\\\\\\r  As spiking neural networks receive more attention, we look toward\\rapplications of this computing paradigm in fields other than computer vision\\rand signal processing. One major field, underexplored in the neuromorphic\\rsetting, is Natural Language Processing (NLP), where most state-of-the-art\\rsolutions still heavily rely on resource-consuming and power-hungry traditional\\rdeep learning architectures. Therefore, it is compelling to design NLP models\\rfor neuromorphic architectures due to their low energy requirements, with the\\radditional benefit of a more human-brain-like operating model for processing\\rinformation. However, one of the biggest issues with bringing NLP to the\\rneuromorphic setting is in properly encoding text into a spike train so that it\\rcan be seamlessly handled by both current and future SNN architectures. In this\\rpaper, we compare various methods of encoding text as spikes and assess each\\rmethod's performance in an associated SNN on a downstream NLP task, namely,\\rsentiment analysis. Furthermore, we go on to propose a new method of encoding\\rtext as spikes that outperforms a widely-used rate-coding technique, Poisson\\rrate-coding, by around 13\\\\% on our benchmark NLP tasks. Subsequently, we\\rdemonstrate the energy efficiency of SNNs implemented in hardware for the\\rsentiment analysis task compared to traditional deep neural networks, observing\\ran energy efficiency increase of more than 32x during inference and 60x during\\rtraining while incurring the expected energy-performance tradeoff.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17911 ,  5429kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17919\\rDate: Wed, 31 Jan 2024 15:33:37 GMT   (15405kb,D)\\r\\rTitle: LOCOST: State-Space Models for Long Document Abstractive Summarization\\rAuthors: Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen,\\r  Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick\\r  Gallinari\\rCategories: cs.CL cs.LG\\rComments: 9 pages, 5 figures, 7 tables, EACL 2024 conference\\r\\\\\\\\\\r  State-space models are a low-complexity alternative to transformers for\\rencoding long sequences and capturing long-term dependencies. We propose\\rLOCOST: an encoder-decoder architecture based on state-space models for\\rconditional text generation with long context inputs. With a computational\\rcomplexity of $O(L \\\\log L)$, this architecture can handle significantly longer\\rsequences than state-of-the-art models that are based on sparse attention\\rpatterns. We evaluate our model on a series of long document abstractive\\rsummarization tasks. The model reaches a performance level that is 93-96%\\rcomparable to the top-performing sparse transformers of the same size while\\rsaving up to 50% memory during training and up to 87% during inference.\\rAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\\rinference time, setting new state-of-the-art results on full-book summarization\\rand opening new perspectives for long input processing.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17919 ,  15405kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17922\\rDate: Wed, 31 Jan 2024 15:35:21 GMT   (6627kb,D)\\r\\rTitle: [Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference\\r  Annotation with LLMs\\rAuthors: Rebecca M. M. Hicke and David Mimno\\rCategories: cs.CL\\rComments: Accepted to LaTeCH-CLfL 2024\\r\\\\\\\\\\r  Coreference annotation and resolution is a vital component of computational\\rliterary studies. However, it has previously been difficult to build high\\rquality systems for fiction. Coreference requires complicated structured\\routputs, and literary text involves subtle inferences and highly varied\\rlanguage. New language-model-based seq2seq systems present the opportunity to\\rsolve both these problems by learning to directly generate a copy of an input\\rsentence with markdown-like annotations. We create, evaluate, and release\\rseveral trained models for coreference, as well as a workflow for training new\\rmodels.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17922 ,  6627kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17974\\rDate: Wed, 31 Jan 2024 16:30:50 GMT   (9927kb,D)\\r\\rTitle: GUMsley: Evaluating Entity Salience in Summarization for 12 English\\r  Genres\\rAuthors: Jessica Lin, Amir Zeldes\\rCategories: cs.CL\\rComments: Camera-ready for EACL 2024\\r\\\\\\\\\\r  As NLP models become increasingly capable of understanding documents in terms\\rof coherent entities rather than strings, obtaining the most salient entities\\rfor each document is not only an important end task in itself but also vital\\rfor Information Retrieval (IR) and other downstream applications such as\\rcontrollable summarization. In this paper, we present and evaluate GUMsley, the\\rfirst entity salience dataset covering all named and non-named salient entities\\rfor 12 genres of English text, aligned with entity types, Wikification links\\rand full coreference resolution annotations. We promote a strict definition of\\rsalience using human summaries and demonstrate high inter-annotator agreement\\rfor salience based on whether a source entity is mentioned in the summary. Our\\revaluation shows poor performance by pre-trained SOTA summarization models and\\rzero-shot LLM prompting in capturing salient entities in generated summaries.\\rWe also show that predicting or providing salient entities to several model\\rarchitectures enhances performance and helps derive higher-quality summaries by\\ralleviating the entity hallucination problem in existing abstractive\\rsummarization.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17974 ,  9927kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17979\\rDate: Wed, 31 Jan 2024 16:34:10 GMT   (7563kb,D)\\r\\rTitle: Entity Linking in the Job Market Domain\\rAuthors: Mike Zhang and Rob van der Goot and Barbara Plank\\rCategories: cs.CL\\rComments: Accepted at EACL 2024 Findings\\r\\\\\\\\\\r  In Natural Language Processing, entity linking (EL) has centered around\\rWikipedia, but yet remains underexplored for the job market domain.\\rDisambiguating skill mentions can help us get insight into the current labor\\rmarket demands. In this work, we are the first to explore EL in this domain,\\rspecifically targeting the linkage of occupational skills to the ESCO taxonomy\\r(le Vrang et al., 2014). Previous efforts linked coarse-grained (full)\\rsentences to a corresponding ESCO skill. In this work, we link more\\rfine-grained span-level mentions of skills. We tune two high-performing neural\\rEL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et\\ral., 2021), on a synthetically generated mention--skill pair dataset and\\revaluate them on a human-annotated skill-linking benchmark. Our findings reveal\\rthat both models are capable of linking implicit mentions of skills to their\\rcorrect taxonomy counterparts. Empirically, BLINK outperforms GENRE in strict\\revaluation, but GENRE performs better in loose evaluation (accuracy@$k$).\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17979 ,  7563kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18001\\rDate: Wed, 31 Jan 2024 17:02:31 GMT   (6698kb,D)\\r\\rTitle: Desiderata for the Context Use of Question Answering Systems\\rAuthors: Sagi Shaier, Lawrence E Hunter, Katharina von der Wense\\rCategories: cs.CL\\rComments: Accepted to EACL 2024\\r\\\\\\\\\\r  Prior work has uncovered a set of common problems in state-of-the-art\\rcontext-based question answering (QA) systems: a lack of attention to the\\rcontext when the latter conflicts with a model's parametric knowledge, little\\rrobustness to noise, and a lack of consistency with their answers. However,\\rmost prior work focus on one or two of those problems in isolation, which makes\\rit difficult to see trends across them. We aim to close this gap, by first\\routlining a set of -- previously discussed as well as novel -- desiderata for\\rQA models. We then survey relevant analysis and methods papers to provide an\\roverview of the state of the field. The second part of our work presents\\rexperiments where we evaluate 15 QA systems on 5 datasets according to all\\rdesiderata at once. We find many novel trends, including (1) systems that are\\rless susceptible to noise are not necessarily more consistent with their\\ranswers when given irrelevant context; (2) most systems that are more\\rsusceptible to noise are more likely to correctly answer according to a context\\rthat conflicts with their parametric knowledge; and (3) the combination of\\rconflicting knowledge and noise can reduce system performance by up to 96%. As\\rsuch, our desiderata help increase our understanding of how these models work\\rand reveal potential avenues for improvements.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18001 ,  6698kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18028\\rDate: Wed, 31 Jan 2024 17:43:04 GMT   (112kb)\\r\\rTitle: Supporting Anticipatory Governance using LLMs: Evaluating and Aligning\\r  Large Language Models with the News Media to Anticipate the Negative Impacts\\r  of AI\\rAuthors: Mowafak Allaham, Nicholas Diakopoulos\\rCategories: cs.CL cs.AI cs.CY\\rComments: 14 pages + research ethics and social impact statement, references,\\r  and appendix. Under conference review\\r\\\\\\\\\\r  Anticipating the negative impacts of emerging AI technologies is a challenge,\\respecially in the early stages of development. An understudied approach to such\\ranticipation is the use of LLMs to enhance and guide this process. Despite\\radvancements in LLMs and evaluation metrics to account for biases in generated\\rtext, it is unclear how well these models perform in anticipatory tasks.\\rSpecifically, the use of LLMs to anticipate AI impacts raises questions about\\rthe quality and range of categories of negative impacts these models are\\rcapable of generating. In this paper we leverage news media, a diverse data\\rsource that is rich with normative assessments of emerging technologies, to\\rformulate a taxonomy of impacts to act as a baseline for comparing against. By\\rcomputationally analyzing thousands of news articles published by hundreds of\\ronline news domains around the world, we develop a taxonomy consisting of ten\\rcategories of AI impacts. We then evaluate both instruction-based (GPT-4 and\\rMistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3)\\rusing a sample from this baseline. We find that the generated impacts using\\rMistral-7B, fine-tuned on impacts from the news media, tend to be qualitatively\\ron par with impacts generated using a larger scale model such as GPT-4.\\rMoreover, we find that these LLMs generate impacts that largely reflect the\\rtaxonomy of negative impacts identified in the news media, however the impacts\\rproduced by instruction-based models had gaps in the production of certain\\rcategories of impacts in comparison to fine-tuned models. This research\\rhighlights a potential bias in state-of-the-art LLMs when used for anticipating\\rimpacts and demonstrates the advantages of aligning smaller LLMs with a diverse\\rrange of impacts, such as those reflected in the news media, to better reflect\\rsuch impacts during anticipatory exercises.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18028 ,  112kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18034\\rDate: Wed, 31 Jan 2024 17:58:10 GMT   (6220kb)\\r\\rTitle: Paramanu: A Family of Novel Efficient Indic Generative Foundation\\r  Language Models\\rAuthors: Mitodru Niyogi and Arnab Bhattacharya\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  We present Gyan AI Paramanu (atom), a family of novel language models for\\rIndian languages. It is a collection of auto-regressive monolingual, bilingual,\\rand multilingual Indic language models pretrained from scratch on a single GPU\\rfor 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi,\\rOdia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia,\\rTamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are\\rpretrained with a context size of 1024 on a single GPU. The models are very\\refficient, small, fast, and powerful. We have also developed an efficient most\\radvanced Indic tokenizer that can even tokenize unseen languages. In order to\\ravoid the curse of multi-linguality in our multilingual mParamanu model, we\\rpretrained on comparable corpora by typological grouping using the same script.\\rWe performed human evaluation of our pretrained models for open end text\\rgeneration on grammar, coherence, creativity, and factuality metrics for\\rBangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit models\\routperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B,\\rGPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite\\rbeing smaller in size by 66 to 20 times compared to standard 7B LLMs. To run\\rinference on our pretrained models, CPU is enough, and GPU is not needed. We\\ralso instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu\\rmodels on 23k instructions in respective languages. Our pretrained and\\rinstruction-tuned models which are first of its kind, most powerful efficient\\rsmall generative language models ever developed for Indic languages, and the\\rvarious results lead to the conclusion that high quality generative language\\rmodels are possible without high amount of compute power and humongous number\\rof parameters. We plan to release our models at https://www.bharatgpts.com.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18034 ,  6220kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18040\\rDate: Wed, 31 Jan 2024 18:03:39 GMT   (336kb)\\r\\rTitle: Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic\\r  Motivation Reinforcement Learning Algorithms for Improved Training and\\r  Adaptability\\rAuthors: Navin Kamuni, Hardik Shah, Sathishkumar Chintala, Naveen Kunchakuri,\\r  Sujatha Alla Old Dominion\\rCategories: cs.CL cs.AI\\rComments: 6 pages, 1 figure, 18th IEEE International Conference on Semantic\\r  Computing\\r\\\\\\\\\\r  End-to-end multi-task dialogue systems are usually designed with separate\\rmodules for the dialogue pipeline. Among these, the policy module is essential\\rfor deciding what to do in response to user input. This policy is trained by\\rreinforcement learning algorithms by taking advantage of an environment in\\rwhich an agent receives feedback in the form of a reward signal. The current\\rdialogue systems, however, only provide meagre and simplistic rewards.\\rInvestigating intrinsic motivation reinforcement learning algorithms is the\\rgoal of this study. Through this, the agent can quickly accelerate training and\\rimprove its capacity to judge the quality of its actions by teaching it an\\rinternal incentive system. In particular, we adapt techniques for random\\rnetwork distillation and curiosity-driven reinforcement learning to measure the\\rfrequency of state visits and encourage exploration by using semantic\\rsimilarity between utterances. Experimental results on MultiWOZ, a\\rheterogeneous dataset, show that intrinsic motivation-based debate systems\\routperform policies that depend on extrinsic incentives. By adopting random\\rnetwork distillation, for example, which is trained using semantic similarity\\rbetween user-system dialogues, an astounding average success rate of 73% is\\rachieved. This is a significant improvement over the baseline Proximal Policy\\rOptimization (PPO), which has an average success rate of 60%. In addition,\\rperformance indicators such as booking rates and completion rates show a 10%\\rrise over the baseline. Furthermore, these intrinsic incentive models help\\rimprove the system's policy's resilience in an increasing amount of domains.\\rThis implies that they could be useful in scaling up to settings that cover a\\rwider range of domains.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18040 ,  336kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18045\\rDate: Wed, 31 Jan 2024 18:06:29 GMT   (232kb,D)\\r\\rTitle: SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition\\rAuthors: Yihan Wu, Soumi Maiti, Yifan Peng, Wangyou Zhang, Chenda Li, Yuyue\\r  Wang, Xihua Wang, Shinji Watanabe, Ruihua Song\\rCategories: cs.CL cs.AI cs.SD eess.AS\\rComments: 11 pages, 2 figures\\r\\\\\\\\\\r  Recent advancements in language models have significantly enhanced\\rperformance in multiple speech-related tasks. Existing speech language models\\rtypically utilize task-dependent prompt tokens to unify various speech tasks in\\ra single model. However, this design omits the intrinsic connections between\\rdifferent speech tasks, which can potentially boost the performance of each\\rtask. In this work, we propose a novel decoder-only speech language model,\\rSpeechComposer, that can unify common speech tasks by composing a fixed set of\\rprompt tokens. Built upon four primary tasks -- speech synthesis, speech\\rrecognition, speech language modeling, and text language modeling --\\rSpeechComposer can easily extend to more speech tasks via compositions of\\rwell-designed prompt tokens, like voice conversion and speech enhancement. The\\runification of prompt tokens also makes it possible for knowledge sharing among\\rdifferent speech tasks in a more structured manner. Experimental results\\rdemonstrate that our proposed SpeechComposer can improve the performance of\\rboth primary tasks and composite tasks, showing the effectiveness of the shared\\rprompt tokens. Remarkably, the unified decoder-only model achieves a comparable\\rand even better performance than the baselines which are expert models designed\\rfor single tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18045 ,  232kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18046\\rDate: Wed, 31 Jan 2024 18:07:12 GMT   (1302kb,D)\\r\\rTitle: Multipath parsing in the brain\\rAuthors: Berta Franzluebbers, Donald Dunagan, Milo\\\\v{s} Stanojevi\\\\'c, Jan Buys,\\r  John T. Hale\\rCategories: cs.CL\\rComments: 15 pages\\r\\\\\\\\\\r  Humans understand sentences word-by-word, in the order that they hear them.\\rThis incrementality entails resolving temporary ambiguities about syntactic\\rrelationships. We investigate how humans process these syntactic ambiguities by\\rcorrelating predictions from incremental generative dependency parsers with\\rtimecourse data from people undergoing functional neuroimaging while listening\\rto an audiobook. In particular, we compare competing hypotheses regarding the\\rnumber of developing syntactic analyses in play during word-by-word\\rcomprehension: one vs more than one. This comparison involves evaluating\\rsyntactic surprisal from a state-of-the-art dependency parser with LLM-adapted\\rencodings against an existing fMRI dataset. In both English and Chinese data,\\rwe find evidence for multipath parsing. Brain regions associated with this\\rmultipath effect include bilateral superior temporal gyrus.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18046 ,  1302kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18058\\rDate: Wed, 31 Jan 2024 18:29:39 GMT   (664kb,D)\\r\\rTitle: LongAlign: A Recipe for Long Context Alignment of Large Language Models\\rAuthors: Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang,\\r  Yuxiao Dong, Juanzi Li\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  Extending large language models to effectively handle long contexts requires\\rinstruction fine-tuning on input sequences of similar length. To address this,\\rwe present LongAlign -- a recipe of the instruction data, training, and\\revaluation for long context alignment. First, we construct a long\\rinstruction-following dataset using Self-Instruct. To ensure the data\\rdiversity, it covers a broad range of tasks from various long context sources.\\rSecond, we adopt the packing and sorted batching strategies to speed up\\rsupervised fine-tuning on data with varied length distributions. Additionally,\\rwe develop a loss weighting method to balance the contribution to the loss\\racross different sequences during packing training. Third, we introduce the\\rLongBench-Chat benchmark for evaluating instruction-following capabilities on\\rqueries of 10k-100k in length. Experiments show that LongAlign outperforms\\rexisting recipes for LLMs in long context tasks by up to 30\\\\%, while also\\rmaintaining their proficiency in handling short, generic tasks. The code, data,\\rand long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18058 ,  664kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18059\\rDate: Wed, 31 Jan 2024 18:30:21 GMT   (2334kb,D)\\r\\rTitle: RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\\rAuthors: Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie,\\r  Christopher D. Manning\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  Retrieval-augmented language models can better adapt to changes in world\\rstate and incorporate long-tail knowledge. However, most existing methods\\rretrieve only short contiguous chunks from a retrieval corpus, limiting\\rholistic understanding of the overall document context. We introduce the novel\\rapproach of recursively embedding, clustering, and summarizing chunks of text,\\rconstructing a tree with differing levels of summarization from the bottom up.\\rAt inference time, our RAPTOR model retrieves from this tree, integrating\\rinformation across lengthy documents at different levels of abstraction.\\rControlled experiments show that retrieval with recursive summaries offers\\rsignificant improvements over traditional retrieval-augmented LMs on several\\rtasks. On question-answering tasks that involve complex, multi-step reasoning,\\rwe show state-of-the-art results; for example, by coupling RAPTOR retrieval\\rwith the use of GPT-4, we can improve the best performance on the QuALITY\\rbenchmark by 20% in absolute accuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18059 ,  2334kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18070\\rDate: Wed, 31 Jan 2024 18:48:20 GMT   (1023kb,D)\\r\\rTitle: Do Language Models Exhibit the Same Cognitive Biases in Problem Solving\\r  as Human Learners?\\rAuthors: Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan\\r  Cotterell, Bernhard Sch\\\\olkopf, Abulhair Saparov, Mrinmaya Sachan\\rCategories: cs.CL cs.AI cs.LG\\rComments: Preprint\\r\\\\\\\\\\r  There is increasing interest in employing large language models (LLMs) as\\rcognitive models. For such purposes, it is central to understand which\\rcognitive properties are well-modeled by LLMs, and which are not. In this work,\\rwe study the biases of LLMs in relation to those known in children when solving\\rarithmetic word problems. Surveying the learning science literature, we posit\\rthat the problem-solving process can be split into three distinct steps: text\\rcomprehension, solution planning and solution execution. We construct tests for\\reach one in order to understand which parts of this process can be faithfully\\rmodeled by current state-of-the-art LLMs. We generate a novel set of word\\rproblems for each of these tests, using a neuro-symbolic method that enables\\rfine-grained control over the problem features. We find evidence that LLMs,\\rwith and without instruction-tuning, exhibit human-like biases in both the\\rtext-comprehension and the solution-planning steps of the solving process, but\\rnot during the final step which relies on the problem's arithmetic expressions\\r(solution execution).\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18070 ,  1023kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17343\\rDate: Tue, 30 Jan 2024 14:18:37 GMT   (17298kb,D)\\r\\rTitle: YTCommentQA: Video Question Answerability in Instructional Videos\\rAuthors: Saelyne Yang, Sunghyun Park, Yunseok Jang, Moontae Lee\\rCategories: cs.CV cs.AI\\rComments: AAAI 2024\\r\\\\\\\\\\r  Instructional videos provide detailed how-to guides for various tasks, with\\rviewers often posing questions regarding the content. Addressing these\\rquestions is vital for comprehending the content, yet receiving immediate\\ranswers is difficult. While numerous computational models have been developed\\rfor Video Question Answering (Video QA) tasks, they are primarily trained on\\rquestions generated based on video content, aiming to produce answers from\\rwithin the content. However, in real-world situations, users may pose questions\\rthat go beyond the video's informational boundaries, highlighting the necessity\\rto determine if a video can provide the answer. Discerning whether a question\\rcan be answered by video content is challenging due to the multi-modal nature\\rof videos, where visual and verbal information are intertwined. To bridge this\\rgap, we present the YTCommentQA dataset, which contains naturally-generated\\rquestions from YouTube, categorized by their answerability and required\\rmodality to answer -- visual, script, or both. Experiments with answerability\\rclassification tasks demonstrate the complexity of YTCommentQA and emphasize\\rthe need to comprehend the combined role of visual and script information in\\rvideo reasoning. The dataset is available at\\rhttps://github.com/lgresearch/YTCommentQA.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17343 ,  17298kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17400\\rDate: Tue, 30 Jan 2024 19:35:07 GMT   (718kb,D)\\r\\rTitle: CALM: Convolution As Local Mixture\\rAuthors: Lifan Liang\\rCategories: cs.CV\\r\\\\\\\\\\r  In this paper, we showed that the feature map of a convolution layer is\\requivalent to the unnormalized log posterior of a special kind of Gaussian\\rmixture for image modeling. Then we expanded the model to drive diverse\\rfeatures and proposed a corresponding EM algorithm to learn the model. Learning\\rconvolution weights using this approach is efficient, guaranteed to converge,\\rand does not need supervised information. Code is available at:\\rhttps://github.com/LifanLiang/CALM.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17400 ,  718kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17417\\rDate: Tue, 30 Jan 2024 20:17:51 GMT   (876kb,D)\\r\\rTitle: Through-Wall Imaging based on WiFi Channel State Information\\rAuthors: Julian Strohmayer, Rafael Sterzinger, Christian Stippel, Martin Kampel\\rCategories: cs.CV cs.AI cs.LG\\r\\\\\\\\\\r  This work presents a seminal approach for synthesizing images from WiFi\\rChannel State Information (CSI) in through-wall scenarios. Leveraging the\\rstrengths of WiFi, such as cost-effectiveness, illumination invariance, and\\rwall-penetrating capabilities, our approach enables visual monitoring of indoor\\renvironments beyond room boundaries and without the need for cameras. More\\rgenerally, it improves the interpretability of WiFi CSI by unlocking the option\\rto perform image-based downstream tasks, e.g., visual activity recognition. In\\rorder to achieve this crossmodal translation from WiFi CSI to images, we rely\\ron a multimodal Variational Autoencoder (VAE) adapted to our problem specifics.\\rWe extensively evaluate our proposed methodology through an ablation study on\\rarchitecture configuration and a quantitative/qualitative assessment of\\rreconstructed images. Our results demonstrate the viability of our method and\\rhighlight its potential for practical applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17417 ,  876kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17497\\rDate: Tue, 30 Jan 2024 23:05:43 GMT   (25165kb,D)\\r\\rTitle: Towards Visual Syntactical Understanding\\rAuthors: Sayeed Shafayet Chowdhury, Soumyadeep Chandra, and Kaushik Roy\\rCategories: cs.CV\\r\\\\\\\\\\r  Syntax is usually studied in the realm of linguistics and refers to the\\rarrangement of words in a sentence. Similarly, an image can be considered as a\\rvisual 'sentence', with the semantic parts of the image acting as 'words'.\\rWhile visual syntactic understanding occurs naturally to humans, it is\\rinteresting to explore whether deep neural networks (DNNs) are equipped with\\rsuch reasoning. To that end, we alter the syntax of natural images (e.g.\\rswapping the eye and nose of a face), referred to as 'incorrect' images, to\\rinvestigate the sensitivity of DNNs to such syntactic anomaly. Through our\\rexperiments, we discover an intriguing property of DNNs where we observe that\\rstate-of-the-art convolutional neural networks, as well as vision transformers,\\rfail to discriminate between syntactically correct and incorrect images when\\rtrained on only correct ones. To counter this issue and enable visual syntactic\\runderstanding with DNNs, we propose a three-stage framework- (i) the 'words'\\r(or the sub-features) in the image are detected, (ii) the detected words are\\rsequentially masked and reconstructed using an autoencoder, (iii) the original\\rand reconstructed parts are compared at each location to determine syntactic\\rcorrectness. The reconstruction module is trained with BERT-like masked\\rautoencoding for images, with the motivation to leverage language model\\rinspired training to better capture the syntax. Note, our proposed approach is\\runsupervised in the sense that the incorrect images are only used during\\rtesting and the correct versus incorrect labels are never used for training. We\\rperform experiments on CelebA, and AFHQ datasets and obtain classification\\raccuracy of 92.10%, and 90.89%, respectively. Notably, the approach generalizes\\rwell to ImageNet samples which share common classes with CelebA and AFHQ\\rwithout explicitly training on them.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17497 ,  25165kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17499\\rDate: Tue, 30 Jan 2024 23:13:41 GMT   (47570kb,D)\\r\\rTitle: AdvGPS: Adversarial GPS for Multi-Agent Perception Attack\\rAuthors: Jinlong Li, Baolu Li, Xinyu Liu, Jianwu Fang, Felix Juefei-Xu, Qing\\r  Guo, Hongkai Yu\\rCategories: cs.CV\\rComments: Accepted by the 2024 IEEE International Conference on Robotics and\\r  Automation (ICRA)\\r\\\\\\\\\\r  The multi-agent perception system collects visual data from sensors located\\ron various agents and leverages their relative poses determined by GPS signals\\rto effectively fuse information, mitigating the limitations of single-agent\\rsensing, such as occlusion. However, the precision of GPS signals can be\\rinfluenced by a range of factors, including wireless transmission and\\robstructions like buildings. Given the pivotal role of GPS signals in\\rperception fusion and the potential for various interference, it becomes\\rimperative to investigate whether specific GPS signals can easily mislead the\\rmulti-agent perception system. To address this concern, we frame the task as an\\radversarial attack challenge and introduce \\\\textsc{AdvGPS}, a method capable of\\rgenerating adversarial GPS signals which are also stealthy for individual\\ragents within the system, significantly reducing object detection accuracy. To\\renhance the success rates of these attacks in a black-box scenario, we\\rintroduce three types of statistically sensitive natural discrepancies:\\rappearance-based discrepancy, distribution-based discrepancy, and task-aware\\rdiscrepancy. Our extensive experiments on the OPV2V dataset demonstrate that\\rthese attacks substantially undermine the performance of state-of-the-art\\rmethods, showcasing remarkable transferability across different point cloud\\rbased 3D detection systems. This alarming revelation underscores the pressing\\rneed to address security implications within multi-agent perception systems,\\rthereby underscoring a critical area of research.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17499 ,  47570kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17509\\rDate: Tue, 30 Jan 2024 23:54:43 GMT   (42756kb,D)\\r\\rTitle: Anything in Any Scene: Photorealistic Video Object Insertion\\rAuthors: Chen Bai, Zeman Shao, Guoxiang Zhang, Di Liang, Jie Yang, Zhuorui\\r  Zhang, Yujian Guo, Chengzhang Zhong, Yiqiao Qiu, Zhendong Wang, Yichen Guan,\\r  Xiaoyin Zheng, Tao Wang, Cheng Lu\\rCategories: cs.CV\\r\\\\\\\\\\r  Realistic video simulation has shown significant potential across diverse\\rapplications, from virtual reality to film production. This is particularly\\rtrue for scenarios where capturing videos in real-world settings is either\\rimpractical or expensive. Existing approaches in video simulation often fail to\\raccurately model the lighting environment, represent the object geometry, or\\rachieve high levels of photorealism. In this paper, we propose Anything in Any\\rScene, a novel and generic framework for realistic video simulation that\\rseamlessly inserts any object into an existing dynamic video with a strong\\remphasis on physical realism. Our proposed general framework encompasses three\\rkey processes: 1) integrating a realistic object into a given scene video with\\rproper placement to ensure geometric realism; 2) estimating the sky and\\renvironmental lighting distribution and simulating realistic shadows to enhance\\rthe light realism; 3) employing a style transfer network that refines the final\\rvideo output to maximize photorealism. We experimentally demonstrate that\\rAnything in Any Scene framework produces simulated videos of great geometric\\rrealism, lighting realism, and photorealism. By significantly mitigating the\\rchallenges associated with video data generation, our framework offers an\\refficient and cost-effective solution for acquiring high-quality videos.\\rFurthermore, its applications extend well beyond video data augmentation,\\rshowing promising potential in virtual reality, video editing, and various\\rother video-centric applications. Please check our project website\\rhttps://anythinginanyscene.github.io for access to our project code and more\\rhigh-resolution video results.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17509 ,  42756kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17515\\rDate: Wed, 31 Jan 2024 00:16:02 GMT   (17342kb,D)\\r\\rTitle: Towards Image Semantics and Syntax Sequence Learning\\rAuthors: Chun Tao, Timur Ibrayev, Kaushik Roy\\rCategories: cs.CV\\rComments: 21 pages, 22 figures, 5 tables\\r\\\\\\\\\\r  Convolutional neural networks and vision transformers have achieved\\routstanding performance in machine perception, particularly for image\\rclassification. Although these image classifiers excel at predicting\\rimage-level class labels, they may not discriminate missing or shifted parts\\rwithin an object. As a result, they may fail to detect corrupted images that\\rinvolve missing or disarrayed semantic information in the object composition.\\rOn the contrary, human perception easily distinguishes such corruptions. To\\rmitigate this gap, we introduce the concept of image grammar, consisting of\\rimage semantics and image syntax, to denote the semantics of parts or\\rpatches of an image and the order in which these parts are arranged to create a\\rmeaningful object. To learn the image grammar relative to a class of visual\\robjects/scenes, we propose a weakly supervised two-stage approach. In the first\\rstage, we use a deep clustering framework that relies on iterative clustering\\rand feature refinement to produce part-semantic segmentation. In the second\\rstage, we incorporate a recurrent bi-LSTM module to process a sequence of\\rsemantic segmentation patches to capture the image syntax. Our framework is\\rtrained to reason over patch semantics and detect faulty syntax. We benchmark\\rthe performance of several grammar learning models in detecting patch\\rcorruptions. Finally, we verify the capabilities of our framework in Celeb and\\rSUNRGBD datasets and demonstrate that it can achieve a grammar validation\\raccuracy of 70 to 90% in a wide variety of semantic and syntactical corruption\\rscenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17515 ,  17342kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17547\\rDate: Wed, 31 Jan 2024 02:25:52 GMT   (10197kb,D)\\r\\rTitle: Task-Oriented Diffusion Model Compression\\rAuthors: Geonung Kim, Beomsu Kim, Eunhyeok Park, Sunghyun Cho\\rCategories: cs.CV\\r\\\\\\\\\\r  As recent advancements in large-scale Text-to-Image (T2I) diffusion models\\rhave yielded remarkable high-quality image generation, diverse downstream\\rImage-to-Image (I2I) applications have emerged. Despite the impressive results\\rachieved by these I2I models, their practical utility is hampered by their\\rlarge model size and the computational burden of the iterative denoising\\rprocess. In this paper, we explore the compression potential of these I2I\\rmodels in a task-oriented manner and introduce a novel method for reducing both\\rmodel size and the number of timesteps. Through extensive experiments, we\\robserve key insights and use our empirical knowledge to develop practical\\rsolutions that aim for near-optimal results with minimal exploration costs. We\\rvalidate the effectiveness of our method by applying it to InstructPix2Pix for\\rimage editing and StableSR for image restoration. Our approach achieves\\rsatisfactory output quality with 39.2% and 56.4% reduction in model footprint\\rand 81.4% and 68.7% decrease in latency to InstructPix2Pix and StableSR,\\rrespectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17547 ,  10197kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17592\\rDate: Wed, 31 Jan 2024 04:32:41 GMT   (9179kb,D)\\r\\rTitle: Local Feature Matching Using Deep Learning: A Survey\\rAuthors: Shibiao Xu, Shunpeng Chen, Rongtao Xu, Changwei Wang, Peng Lu, Li Guo\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Local feature matching enjoys wide-ranging applications in the realm of\\rcomputer vision, encompassing domains such as image retrieval, 3D\\rreconstruction, and object recognition. However, challenges persist in\\rimproving the accuracy and robustness of matching due to factors like viewpoint\\rand lighting variations. In recent years, the introduction of deep learning\\rmodels has sparked widespread exploration into local feature matching\\rtechniques. The objective of this endeavor is to furnish a comprehensive\\roverview of local feature matching methods. These methods are categorized into\\rtwo key segments based on the presence of detectors. The Detector-based\\rcategory encompasses models inclusive of Detect-then-Describe, Joint Detection\\rand Description, Describe-then-Detect, as well as Graph Based techniques. In\\rcontrast, the Detector-free category comprises CNN Based, Transformer Based,\\rand Patch Based methods. Our study extends beyond methodological analysis,\\rincorporating evaluations of prevalent datasets and metrics to facilitate a\\rquantitative comparison of state-of-the-art techniques. The paper also explores\\rthe practical application of local feature matching in diverse domains such as\\rStructure from Motion, Remote Sensing Image Registration, and Medical Image\\rRegistration, underscoring its versatility and significance across various\\rfields. Ultimately, we endeavor to outline the current challenges faced in this\\rdomain and furnish future research directions, thereby serving as a reference\\rfor researchers involved in local feature matching and its interconnected\\rdomains.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17592 ,  9179kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17603\\rDate: Wed, 31 Jan 2024 05:13:53 GMT   (47413kb,D)\\r\\rTitle: Topology-Aware Latent Diffusion for 3D Shape Generation\\rAuthors: Jiangbei Hu, Ben Fei, Baixin Xu, Fei Hou, Weidong Yang, Shengfa Wang,\\r  Na Lei, Chen Qian, Ying He\\rCategories: cs.CV\\rComments: 16 pages, 9 figures\\rACM-class: I.3.5; I.2.10\\r\\\\\\\\\\r  We introduce a new generative model that combines latent diffusion with\\rpersistent homology to create 3D shapes with high diversity, with a special\\remphasis on their topological characteristics. Our method involves representing\\r3D shapes as implicit fields, then employing persistent homology to extract\\rtopological features, including Betti numbers and persistence diagrams. The\\rshape generation process consists of two steps. Initially, we employ a\\rtransformer-based autoencoding module to embed the implicit representation of\\reach 3D shape into a set of latent vectors. Subsequently, we navigate through\\rthe learned latent space via a diffusion model. By strategically incorporating\\rtopological features into the diffusion process, our generative module is able\\rto produce a richer variety of 3D shapes with different topological structures.\\rFurthermore, our framework is flexible, supporting generation tasks constrained\\rby a variety of inputs, including sparse and partial point clouds, as well as\\rsketches. By modifying the persistence diagrams, we can alter the topology of\\rthe shapes generated from these input modalities.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17603 ,  47413kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17604\\rDate: Wed, 31 Jan 2024 05:20:29 GMT   (3699kb,D)\\r\\rTitle: Computation and Parameter Efficient Multi-Modal Fusion Transformer for\\r  Cued Speech Recognition\\rAuthors: Lei Liu and Li Liu and Haizhou Li\\rCategories: cs.CV cs.SD eess.AS\\rComments: Accepted by TASLP\\r\\\\\\\\\\r  Cued Speech (CS) is a pure visual coding method used by hearing-impaired\\rpeople that combines lip reading with several specific hand shapes to make the\\rspoken language visible. Automatic CS recognition (ACSR) seeks to transcribe\\rvisual cues of speech into text, which can help hearing-impaired people to\\rcommunicate effectively. The visual information of CS contains lip reading and\\rhand cueing, thus the fusion of them plays an important role in ACSR. However,\\rmost previous fusion methods struggle to capture the global dependency present\\rin long sequence inputs of multi-modal CS data. As a result, these methods\\rgenerally fail to learn the effective cross-modal relationships that contribute\\rto the fusion. Recently, attention-based transformers have been a prevalent\\ridea for capturing the global dependency over the long sequence in multi-modal\\rfusion, but existing multi-modal fusion transformers suffer from both poor\\rrecognition accuracy and inefficient computation for the ACSR task. To address\\rthese problems, we develop a novel computation and parameter efficient\\rmulti-modal fusion transformer by proposing a novel Token-Importance-Aware\\rAttention mechanism (TIAA), where a token utilization rate (TUR) is formulated\\rto select the important tokens from the multi-modal streams. More precisely,\\rTIAA firstly models the modality-specific fine-grained temporal dependencies\\rover all tokens of each modality, and then learns the efficient cross-modal\\rinteraction for the modality-shared coarse-grained temporal dependencies over\\rthe important tokens of different modalities. Besides, a light-weight gated\\rhidden projection is designed to control the feature flows of TIAA. The\\rresulting model, named Economical Cued Speech Fusion Transformer (EcoCued),\\rachieves state-of-the-art performance on all existing CS datasets, compared\\rwith existing transformer-based fusion methods and ACSR fusion methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17604 ,  3699kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17609\\rDate: Wed, 31 Jan 2024 05:44:01 GMT   (3727kb,D)\\r\\rTitle: LaneGraph2Seq: Lane Topology Extraction with Language Model via\\r  Vertex-Edge Encoding and Connectivity Enhancement\\rAuthors: Renyuan Peng, Xinyue Cai, Hang Xu, Jiachen Lu, Feng Wen, Wei Zhang, Li\\r  Zhang\\rCategories: cs.CV\\rComments: AAAI 2024\\r\\\\\\\\\\r  Understanding road structures is crucial for autonomous driving. Intricate\\rroad structures are often depicted using lane graphs, which include centerline\\rcurves and connections forming a Directed Acyclic Graph (DAG). Accurate\\rextraction of lane graphs relies on precisely estimating vertex and edge\\rinformation within the DAG. Recent research highlights Transformer-based\\rlanguage models' impressive sequence prediction abilities, making them\\reffective for learning graph representations when graph data are encoded as\\rsequences. However, existing studies focus mainly on modeling vertices\\rexplicitly, leaving edge information simply embedded in the network.\\rConsequently, these approaches fall short in the task of lane graph extraction.\\rTo address this, we introduce LaneGraph2Seq, a novel approach for lane graph\\rextraction. It leverages a language model with vertex-edge encoding and\\rconnectivity enhancement. Our serialization strategy includes a vertex-centric\\rdepth-first traversal and a concise edge-based partition sequence.\\rAdditionally, we use classifier-free guidance combined with nucleus sampling to\\rimprove lane connectivity. We validate our method on prominent datasets,\\rnuScenes and Argoverse 2, showcasing consistent and compelling results. Our\\rLaneGraph2Seq approach demonstrates superior performance compared to\\rstate-of-the-art techniques in lane graph extraction.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17609 ,  3727kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17617\\rDate: Wed, 31 Jan 2024 06:12:28 GMT   (9061kb,D)\\r\\rTitle: Unveiling the Power of Self-supervision for Multi-view Multi-human\\r  Association and Tracking\\rAuthors: Wei Feng, Feifan Wang, Ruize Han, Zekun Qian and Song Wang\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Multi-view multi-human association and tracking (MvMHAT), is a new but\\rimportant problem for multi-person scene video surveillance, aiming to track a\\rgroup of people over time in each view, as well as to identify the same person\\racross different views at the same time, which is different from previous MOT\\rand multi-camera MOT tasks only considering the over-time human tracking. This\\rway, the videos for MvMHAT require more complex annotations while containing\\rmore information for self learning. In this work, we tackle this problem with a\\rself-supervised learning aware end-to-end network. Specifically, we propose to\\rtake advantage of the spatial-temporal self-consistency rationale by\\rconsidering three properties of reflexivity, symmetry and transitivity. Besides\\rthe reflexivity property that naturally holds, we design the self-supervised\\rlearning losses based on the properties of symmetry and transitivity, for both\\rappearance feature learning and assignment matrix optimization, to associate\\rthe multiple humans over time and across views. Furthermore, to promote the\\rresearch on MvMHAT, we build two new large-scale benchmarks for the network\\rtraining and testing of different algorithms. Extensive experiments on the\\rproposed benchmarks verify the effectiveness of our method. We have released\\rthe benchmark and code to the public.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17617 ,  9061kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17629\\rDate: Wed, 31 Jan 2024 07:11:01 GMT   (46102kb,D)\\r\\rTitle: Spatial-and-Frequency-aware Restoration method for Images based on\\r  Diffusion Models\\rAuthors: Kyungsung Lee, Donggyu Lee, Myungjoo Kang\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Diffusion models have recently emerged as a promising framework for Image\\rRestoration (IR), owing to their ability to produce high-quality\\rreconstructions and their compatibility with established methods. Existing\\rmethods for solving noisy inverse problems in IR, considers the pixel-wise\\rdata-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware\\rdiffusion model for IR with Gaussian noise. Our model encourages images to\\rpreserve data-fidelity in both the spatial and frequency domains, resulting in\\renhanced reconstruction quality. We comprehensively evaluate the performance of\\rour model on a variety of noisy inverse problems, including inpainting,\\rdenoising, and super-resolution. Our thorough evaluation demonstrates that\\rSaFaRI achieves state-of-the-art performance on both the ImageNet datasets and\\rFFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS\\rand FID metrics.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17629 ,  46102kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17642\\rDate: Wed, 31 Jan 2024 07:51:52 GMT   (19392kb,D)\\r\\rTitle: Exploring the Common Appearance-Boundary Adaptation for Nighttime\\r  Optical Flow\\rAuthors: Hanyu Zhou, Yi Chang, Haoyue Liu, Wending Yan, Yuxing Duan, Zhiwei\\r  Shi, Luxin Yan\\rCategories: cs.CV\\rJournal-ref: International Conference on Learning Representations (ICLR), 2024\\r\\\\\\\\\\r  We investigate a challenging task of nighttime optical flow, which suffers\\rfrom weakened texture and amplified noise. These degradations weaken\\rdiscriminative visual features, thus causing invalid motion feature matching.\\rTypically, existing methods employ domain adaptation to transfer knowledge from\\rauxiliary domain to nighttime domain in either input visual space or output\\rmotion space. However, this direct adaptation is ineffective, since there\\rexists a large domain gap due to the intrinsic heterogeneous nature of the\\rfeature representations between auxiliary and nighttime domains. To overcome\\rthis issue, we explore a common-latent space as the intermediate bridge to\\rreinforce the feature alignment between auxiliary and nighttime domains. In\\rthis work, we exploit two auxiliary daytime and event domains, and propose a\\rnovel common appearance-boundary adaptation framework for nighttime optical\\rflow. In appearance adaptation, we employ the intrinsic image decomposition to\\rembed the auxiliary daytime image and the nighttime image into a\\rreflectance-aligned common space. We discover that motion distributions of the\\rtwo reflectance maps are very similar, benefiting us to consistently transfer\\rmotion appearance knowledge from daytime to nighttime domain. In boundary\\radaptation, we theoretically derive the motion correlation formula between\\rnighttime image and accumulated events within a spatiotemporal gradient-aligned\\rcommon space. We figure out that the correlation of the two spatiotemporal\\rgradient maps shares significant discrepancy, benefitting us to contrastively\\rtransfer boundary knowledge from event to nighttime domain. Moreover,\\rappearance adaptation and boundary adaptation are complementary to each other,\\rsince they could jointly transfer global motion and local boundary knowledge to\\rthe nighttime domain.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17642 ,  19392kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17654\\rDate: Wed, 31 Jan 2024 08:16:32 GMT   (489kb,D)\\r\\rTitle: All Beings Are Equal in Open Set Recognition\\rAuthors: Chaohua Li, Enhao Zhang, Chuanxing Geng, SongCan Chen\\rCategories: cs.CV\\rComments: Accepted by the main track The 38th Annual AAAI Conference on\\r  Artificial Intelligence (AAAI 2024)\\r\\\\\\\\\\r  In open-set recognition (OSR), a promising strategy is exploiting\\rpseudo-unknown data outside given $K$ known classes as an additional $K$+$1$-th\\rclass to explicitly model potential open space. However, treating unknown\\rclasses without distinction is unequal for them relative to known classes due\\rto the category-agnostic and scale-agnostic of the unknowns. This inevitably\\rnot only disrupts the inherent distributions of unknown classes but also incurs\\rboth class-wise and instance-wise imbalances between known and unknown classes.\\rIdeally, the OSR problem should model the whole class space as $K$+$\\\\infty$,\\rbut enumerating all unknowns is impractical. Since the core of OSR is to\\reffectively model the boundaries of known classes, this means just focusing on\\rthe unknowns nearing the boundaries of targeted known classes seems sufficient.\\rThus, as a compromise, we convert the open classes from infinite to $K$, with a\\rnovel concept Target-Aware Universum (TAU) and propose a simple yet effective\\rframework Dual Contrastive Learning with Target-Aware Universum (DCTAU). In\\rdetails, guided by the targeted known classes, TAU automatically expands the\\runknown classes from the previous $1$ to $K$, effectively alleviating the\\rdistribution disruption and the imbalance issues mentioned above. Then, a novel\\rDual Contrastive (DC) loss is designed, where all instances irrespective of\\rknown or TAU are considered as positives to contrast with their respective\\rnegatives. Experimental results indicate DCTAU sets a new state-of-the-art.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17654 ,  489kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17664\\rDate: Wed, 31 Jan 2024 08:35:40 GMT   (4308kb,D)\\r\\rTitle: Image Anything: Towards Reasoning-coherent and Training-free Multi-modal\\r  Image Generation\\rAuthors: Yuanhuiyi Lyu, Xu Zheng, Lin Wang\\rCategories: cs.CV cs.GR\\r\\\\\\\\\\r  The multifaceted nature of human perception and comprehension indicates that,\\rwhen we think, our body can naturally take any combination of senses, a.k.a.,\\rmodalities and form a beautiful picture in our brain. For example, when we see\\ra cattery and simultaneously perceive the cat's purring sound, our brain can\\rconstruct a picture of a cat in the cattery. Intuitively, generative AI models\\rshould hold the versatility of humans and be capable of generating images from\\rany combination of modalities efficiently and collaboratively. This paper\\rpresents ImgAny, a novel end-to-end multi-modal generative model that can mimic\\rhuman reasoning and generate high-quality images. Our method serves as the\\rfirst attempt in its capacity of efficiently and flexibly taking any\\rcombination of seven modalities, ranging from language, audio to vision\\rmodalities, including image, point cloud, thermal, depth, and event data. Our\\rkey idea is inspired by human-level cognitive processes and involves the\\rintegration and harmonization of multiple input modalities at both the entity\\rand attribute levels without specific tuning across modalities. Accordingly,\\rour method brings two novel training-free technical branches: 1) Entity Fusion\\rBranch ensures the coherence between inputs and outputs. It extracts entity\\rfeatures from the multi-modal representations powered by our specially\\rconstructed entity knowledge graph; 2) Attribute Fusion Branch adeptly\\rpreserves and processes the attributes. It efficiently amalgamates distinct\\rattributes from diverse input modalities via our proposed attribute knowledge\\rgraph. Lastly, the entity and attribute features are adaptively fused as the\\rconditional inputs to the pre-trained Stable Diffusion model for image\\rgeneration. Extensive experiments under diverse modality combinations\\rdemonstrate its exceptional capability for visual content creation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17664 ,  4308kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17699\\rDate: Wed, 31 Jan 2024 09:38:44 GMT   (9732kb,D)\\r\\rTitle: Unified Physical-Digital Face Attack Detection\\rAuthors: Hao Fang, Ajian Liu, Haocheng Yuan, Junze Zheng, Dingheng Zeng,\\r  Yanhong Liu, Jiankang Deng, Sergio Escalera, Xiaoming Liu, Jun Wan, Zhen Lei\\rCategories: cs.CV\\rComments: 12 pages, 8 figures\\r\\\\\\\\\\r  Face Recognition (FR) systems can suffer from physical (i.e., print photo)\\rand digital (i.e., DeepFake) attacks. However, previous related work rarely\\rconsiders both situations at the same time. This implies the deployment of\\rmultiple models and thus more computational burden. The main reasons for this\\rlack of an integrated model are caused by two factors: (1) The lack of a\\rdataset including both physical and digital attacks with ID consistency which\\rmeans the same ID covers the real face and all attack types; (2) Given the\\rlarge intra-class variance between these two attacks, it is difficult to learn\\ra compact feature space to detect both attacks simultaneously. To address these\\rissues, we collect a Unified physical-digital Attack dataset, called\\rUniAttackData. The dataset consists of $1,800$ participations of 2 and 12\\rphysical and digital attacks, respectively, resulting in a total of 29,706\\rvideos. Then, we propose a Unified Attack Detection framework based on\\rVision-Language Models (VLMs), namely UniAttackDetection, which includes three\\rmain modules: the Teacher-Student Prompts (TSP) module, focused on acquiring\\runified and specific knowledge respectively; the Unified Knowledge Mining (UKM)\\rmodule, designed to capture a comprehensive feature space; and the Sample-Level\\rPrompt Interaction (SLPI) module, aimed at grasping sample-level semantics.\\rThese three modules seamlessly form a robust unified attack detection\\rframework. Extensive experiments on UniAttackData and three other datasets\\rdemonstrate the superiority of our approach for unified face attack detection.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17699 ,  9732kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17714\\rDate: Wed, 31 Jan 2024 10:09:26 GMT   (943kb,D)\\r\\rTitle: 3D-Plotting Algorithm for Insects using YOLOv5\\rAuthors: Daisuke Mori, Hiroki Hayami, Yasufumi Fujimoto, Isao Goto\\rCategories: cs.CV\\r\\\\\\\\\\r  In ecological research, accurately collecting spatiotemporal position data is\\ra fundamental task for understanding the behavior and ecology of insects and\\rother organisms. In recent years, advancements in computer vision techniques\\rhave reached a stage of maturity where they can support, and in some cases,\\rreplace manual observation. In this study, a simple and inexpensive method for\\rmonitoring insects in three dimensions (3D) was developed so that their\\rbehavior could be observed automatically in experimental environments. The main\\rachievements of this study have been to create a 3D monitoring algorithm using\\rinexpensive cameras and other equipment to design an adjusting algorithm for\\rdepth error, and to validate how our plotting algorithm is quantitatively\\rprecise, all of which had not been realized in conventional studies. By\\roffering detailed 3D visualizations of insects, the plotting algorithm aids\\rresearchers in more effectively comprehending how insects interact within their\\renvironments.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17714 ,  943kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17728\\rDate: Wed, 31 Jan 2024 10:47:25 GMT   (378kb,D)\\r\\rTitle: COMET: Contrastive Mean Teacher for Online Source-Free Universal Domain\\r  Adaptation\\rAuthors: Pascal Schlachter, Bin Yang\\rCategories: cs.CV\\r\\\\\\\\\\r  In real-world applications, there is often a domain shift from training to\\rtest data. This observation resulted in the development of test-time adaptation\\r(TTA). It aims to adapt a pre-trained source model to the test data without\\rrequiring access to the source data. Thereby, most existing works are limited\\rto the closed-set assumption, i.e. there is no category shift between source\\rand target domain. We argue that in a realistic open-world setting a category\\rshift can appear in addition to a domain shift. This means, individual source\\rclasses may not appear in the target domain anymore, samples of new classes may\\rbe part of the target domain or even both at the same time. Moreover, in many\\rreal-world scenarios the test data is not accessible all at once but arrives\\rsequentially as a stream of batches demanding an immediate prediction. Hence,\\rTTA must be applied in an online manner. To the best of our knowledge, the\\rcombination of these aspects, i.e. online source-free universal domain\\radaptation (online SF-UniDA), has not been studied yet. In this paper, we\\rintroduce a Contrastive Mean Teacher (COMET) tailored to this novel scenario.\\rIt applies a contrastive loss to rebuild a feature space where the samples of\\rknown classes build distinct clusters and the samples of new classes separate\\rwell from them. It is complemented by an entropy loss which ensures that the\\rclassifier output has a small entropy for samples of known classes and a large\\rentropy for samples of new classes to be easily detected and rejected as\\runknown. To provide the losses with reliable pseudo labels, they are embedded\\rinto a mean teacher (MT) framework. We evaluate our method across two datasets\\rand all category shifts to set an initial benchmark for online SF-UniDA.\\rThereby, COMET yields state-of-the-art performance and proves to be consistent\\rand robust across a variety of different scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17728 ,  378kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17736\\rDate: Wed, 31 Jan 2024 10:57:07 GMT   (9517kb,D)\\r\\rTitle: Leveraging Human-Machine Interactions for Computer Vision Dataset\\r  Quality Enhancement\\rAuthors: Esla Timothy Anzaku (1,2,3), Hyesoo Hong (1), Jin-Woo Park (1), Wonjun\\r  Yang (1), Kangmin Kim (1), JongBum Won (1), Deshika Vinoshani Kumari Herath\\r  (6), Arnout Van Messem (5) and Wesley De Neve (1,2,3)\\rCategories: cs.CV\\r\\\\\\\\\\r  Large-scale datasets for single-label multi-class classification, such as\\r\\\\emph{ImageNet-1k}, have been instrumental in advancing deep learning and\\rcomputer vision. However, a critical and often understudied aspect is the\\rcomprehensive quality assessment of these datasets, especially regarding\\rpotential multi-label annotation errors. In this paper, we introduce a\\rlightweight, user-friendly, and scalable framework that synergizes human and\\rmachine intelligence for efficient dataset validation and quality enhancement.\\rWe term this novel framework \\\\emph{Multilabelfy}. Central to Multilabelfy is an\\radaptable web-based platform that systematically guides annotators through the\\rre-evaluation process, effectively leveraging human-machine interactions to\\renhance dataset quality. By using Multilabelfy on the ImageNetV2 dataset, we\\rfound that approximately $47.88\\\\%$ of the images contained at least two labels,\\runderscoring the need for more rigorous assessments of such influential\\rdatasets. Furthermore, our analysis showed a negative correlation between the\\rnumber of potential labels per image and model top-1 accuracy, illuminating a\\rcrucial factor in model evaluation and selection. Our open-source framework,\\rMultilabelfy, offers a convenient, lightweight solution for dataset\\renhancement, emphasizing multi-label proportions. This study tackles major\\rchallenges in dataset integrity and provides key insights into model\\rperformance evaluation. Moreover, it underscores the advantages of integrating\\rhuman expertise with machine capabilities to produce more robust models and\\rtrustworthy data development. The source code for Multilabelfy will be\\ravailable at https://github.com/esla/Multilabelfy.\\r  \\\\keywords{Computer Vision \\\\and Dataset Quality Enhancement \\\\and Dataset\\rValidation \\\\and Human-Computer Interaction \\\\and Multi-label Annotation.}\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17736 ,  9517kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17759\\rDate: Wed, 31 Jan 2024 11:36:12 GMT   (3440kb)\\r\\rTitle: Tiered approach for rapid damage characterisation of infrastructure\\r  enabled by remote sensing and deep learning technologies\\rAuthors: Nadiia Kopiika, Andreas Karavias, Pavlos Krassakis, Zehao Ye, Jelena\\r  Ninic, Nataliya Shakhovska, Nikolaos Koukouzas, Sotirios Argyroudis,\\r  Stergios-Aristoteles Mitoulis\\rCategories: cs.CV eess.IV\\rComments: Main text (34 pages,18 figures); Supplementary materials (13 pages)\\r\\\\\\\\\\r  Critical infrastructure such as bridges are systematically targeted during\\rwars and conflicts. This is because critical infrastructure is vital for\\renabling connectivity and transportation of people and goods, and hence,\\runderpinning the national and international defence planning and economic\\rgrowth. Mass destruction of bridges, along with minimal or no accessibility to\\rthese assets during natural and anthropogenic disasters, prevents us from\\rdelivering rapid recovery. As a result, systemic resilience is drastically\\rreduced. A solution to this challenge is to use technology for stand-off\\robservations. Yet, no method exists to characterise damage at different scales,\\ri.e. regional, asset, and structural (component), and more so there is little\\ror no systematic correlation between assessments at scale. We propose an\\rintegrated three-level tiered approach to fill this capability gap, and we\\rdemonstrate the methods for damage characterisation enabled by fit-for-purpose\\rdigital technologies. Next, this method is applied and validated to a case\\rstudy in Ukraine that includes 17 bridges. From macro to micro, we deploy\\rtechnology at scale, from Sentinel-1 SAR images, crowdsourced information, and\\rhigh-resolution images to deep learning for damaged infrastructure. For the\\rfirst time, the interferometric coherence difference and semantic segmentation\\rof images were deployed to improve the reliability of damage characterisations\\rfrom regional to infrastructure component level, when enhanced assessment\\raccuracy is required. This integrated method improves the speed of\\rdecision-making, and thus, enhances resilience. Keywords: critical\\rinfrastructure, damage characterisation, targeted attacks, restoration\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17759 ,  3440kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17766\\rDate: Wed, 31 Jan 2024 11:51:24 GMT   (259kb,D)\\r\\rTitle: Fine-Grained Zero-Shot Learning: Advances, Challenges, and Prospects\\rAuthors: Jingcai Guo, Zhijie Rao, Song Guo, Jingren Zhou, Dacheng Tao\\rCategories: cs.CV\\rComments: 11 pages, 1 figure, 4 tables\\r\\\\\\\\\\r  Recent zero-shot learning (ZSL) approaches have integrated fine-grained\\ranalysis, i.e., fine-grained ZSL, to mitigate the commonly known seen/unseen\\rdomain bias and misaligned visual-semantics mapping problems, and have made\\rprofound progress. Notably, this paradigm differs from existing close-set\\rfine-grained methods and, therefore, can pose unique and nontrivial challenges.\\rHowever, to the best of our knowledge, there remains a lack of systematic\\rsummaries of this topic. To enrich the literature of this domain and provide a\\rsound basis for its future development, in this paper, we present a broad\\rreview of recent advances for fine-grained analysis in ZSL. Concretely, we\\rfirst provide a taxonomy of existing methods and techniques with a thorough\\ranalysis of each category. Then, we summarize the benchmark, covering publicly\\ravailable datasets, models, implementations, and some more details as a\\rlibrary. Last, we sketch out some related applications. In addition, we discuss\\rvital challenges and suggest potential future directions.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17766 ,  259kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17773\\rDate: Wed, 31 Jan 2024 12:12:56 GMT   (3471kb,D)\\r\\rTitle: SNP-S3: Shared Network Pre-training and Significant Semantic\\r  Strengthening for Various Video-Text Tasks\\rAuthors: Xingning Dong, Qingpei Guo, Tian Gan, Qing Wang, Jianlong Wu,\\r  Xiangyuan Ren, Yuan Cheng, Wei Chu\\rCategories: cs.CV cs.MM\\rComments: Accepted by TCSVT (IEEE Transactions on Circuits and Systems for\\r  Video Technology)\\rDOI: 10.1109/TCSVT.2023.3303945\\r\\\\\\\\\\r  We present a framework for learning cross-modal video representations by\\rdirectly pre-training on raw data to facilitate various downstream video-text\\rtasks. Our main contributions lie in the pre-training framework and proxy\\rtasks. First, based on the shortcomings of two mainstream pixel-level\\rpre-training architectures (limited applications or less efficient), we propose\\rShared Network Pre-training (SNP). By employing one shared BERT-type network to\\rrefine textual and cross-modal features simultaneously, SNP is lightweight and\\rcould support various downstream applications. Second, based on the intuition\\rthat people always pay attention to several significant words when\\runderstanding a sentence, we propose the Significant Semantic Strengthening\\r(S3) strategy, which includes a novel masking and matching proxy task to\\rpromote the pre-training performance. Experiments conducted on three downstream\\rvideo-text tasks and six datasets demonstrate that, we establish a new\\rstate-of-the-art in pixel-level video-text pre-training; we also achieve a\\rsatisfactory balance between the pre-training efficiency and the fine-tuning\\rperformance. The codebase are available at\\rhttps://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/snps3_vtp.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17773 ,  3471kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17776\\rDate: Wed, 31 Jan 2024 12:16:39 GMT   (3486kb,D)\\r\\rTitle: Double InfoGAN for Contrastive Analysis\\rAuthors: Florence Carton, Robin Louiset, Pietro Gori\\rCategories: cs.CV cs.AI stat.ML\\rComments: Accepted at AISTATS 2024\\r\\\\\\\\\\r  Contrastive Analysis (CA) deals with the discovery of what is common and what\\ris distinctive of a target domain compared to a background one. This is of\\rgreat interest in many applications, such as medical imaging. Current\\rstate-of-the-art (SOTA) methods are latent variable models based on VAE\\r(CA-VAEs). However, they all either ignore important constraints or they don't\\renforce fundamental assumptions. This may lead to sub-optimal solutions where\\rdistinctive factors are mistaken for common ones (or viceversa). Furthermore,\\rthe generated images have a rather poor quality, typical of VAEs, decreasing\\rtheir interpretability and usefulness. Here, we propose Double InfoGAN, the\\rfirst GAN based method for CA that leverages the high-quality synthesis of GAN\\rand the separation power of InfoGAN. Experimental results on four visual\\rdatasets, from simple synthetic examples to complex medical images, show that\\rthe proposed method outperforms SOTA CA-VAEs in terms of latent separation and\\rimage quality. Datasets and code are available online.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17776 ,  3486kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17789\\rDate: Wed, 31 Jan 2024 12:32:17 GMT   (3365kb,D)\\r\\rTitle: Robustly overfitting latents for flexible neural image compression\\rAuthors: Yura Perugachi-Diaz, Arwin Gansekoele, Sandjai Bhulai\\rCategories: cs.CV cs.LG stat.ML\\r\\\\\\\\\\r  Neural image compression has made a great deal of progress. State-of-the-art\\rmodels are based on variational autoencoders and are outperforming classical\\rmodels. Neural compression models learn to encode an image into a quantized\\rlatent representation that can be efficiently sent to the decoder, which\\rdecodes the quantized latent into a reconstructed image. While these models\\rhave proven successful in practice, they lead to sub-optimal results due to\\rimperfect optimization and limitations in the encoder and decoder capacity.\\rRecent work shows how to use stochastic Gumbel annealing (SGA) to refine the\\rlatents of pre-trained neural image compression models. We extend this idea by\\rintroducing SGA+, which contains three different methods that build upon SGA.\\rFurther, we give a detailed analysis of our proposed methods, show how they\\rimprove performance, and show that they are less sensitive to hyperparameter\\rchoices. Besides, we show how each method can be extended to three- instead of\\rtwo-class rounding. Finally, we show how refinement of the latents with our\\rbest-performing method improves the compression performance on the Tecnick\\rdataset and how it can be deployed to partly move along the rate-distortion\\rcurve.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17789 ,  3365kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17797\\rDate: Wed, 31 Jan 2024 12:45:44 GMT   (1745kb,D)\\r\\rTitle: M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based\\r  Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval\\rAuthors: Xingning Dong, Zipeng Feng, Chunluan Zhou, Xuzheng Yu, Ming Yang,\\r  Qingpei Guo\\rCategories: cs.CV\\r\\\\\\\\\\r  We present a Multi-Modal Recipe for Advancing Adaptation-based Pre-training\\rtowards effective and efficient zero-shot video-text retrieval, dubbed M2-RAAP.\\rUpon popular image-text models like CLIP, most current adaptation-based\\rvideo-text pre-training methods are confronted by three major issues, i.e.,\\rnoisy data corpus, time-consuming pre-training, and limited performance gain.\\rTowards this end, we conduct a comprehensive study including four critical\\rsteps in video-text pre-training. Specifically, we investigate 1) data\\rfiltering and refinement, 2) video input type selection, 3) temporal modeling,\\rand 4) video feature enhancement. We then summarize this empirical study into\\rthe M2-RAAP recipe, where our technical contributions lie in 1) the data\\rfiltering and text re-writing pipeline resulting in 1M high-quality bilingual\\rvideo-text pairs, 2) the replacement of video inputs with key-frames to\\raccelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy to\\renhance video features. We conduct extensive experiments by adapting three\\rimage-text foundation models on two refined video-text datasets from different\\rlanguages, validating the robustness and reproducibility of M2-RAAP for\\radaptation-based pre-training. Results demonstrate that M2-RAAP yields superior\\rperformance with significantly reduced data (-90%) and time consumption (-95%),\\restablishing a new SOTA on four English zero-shot retrieval datasets and two\\rChinese ones. We are preparing our refined bilingual data annotations and\\rcodebase, which will be available at\\rhttps://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17797 ,  1745kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17803\\rDate: Wed, 31 Jan 2024 12:53:11 GMT   (17318kb,D)\\r\\rTitle: SimAda: A Simple Unified Framework for Adapting Segment Anything Model\\r  in Underperformed Scenes\\rAuthors: Yiran Song, Qianyu Zhou, Xuequan Lu, Zhiwen Shao, Lizhuang Ma\\rCategories: cs.CV\\r\\\\\\\\\\r  Segment anything model (SAM) has demonstrated excellent generalization\\rcapabilities in common vision scenarios, yet lacking an understanding of\\rspecialized data. Although numerous works have focused on optimizing SAM for\\rdownstream tasks, these task-specific approaches usually limit the\\rgeneralizability to other downstream tasks. In this paper, we aim to\\rinvestigate the impact of the general vision modules on finetuning SAM and\\renable them to generalize across all downstream tasks. We propose a simple\\runified framework called SimAda for adapting SAM in underperformed scenes.\\rSpecifically, our framework abstracts the general modules of different methods\\rinto basic design elements, and we design four variants based on a shared\\rtheoretical framework. SimAda is simple yet effective, which removes all\\rdataset-specific designs and focuses solely on general optimization, ensuring\\rthat SimAda can be applied to all SAM-based and even Transformer-based models.\\rWe conduct extensive experiments on nine datasets of six downstream tasks. The\\rresults demonstrate that SimAda significantly improves the performance of SAM\\ron multiple downstream tasks and achieves state-of-the-art performance on most\\rof them, without requiring task-specific designs. Code is available at:\\rhttps://github.com/zongzi13545329/SimAda\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17803 ,  17318kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17807\\rDate: Wed, 31 Jan 2024 13:06:48 GMT   (12589kb,D)\\r\\rTitle: Advances in 3D Generation: A Survey\\rAuthors: Xiaoyu Li, Qi Zhang, Di Kang, Weihao Cheng, Yiming Gao, Jingbo Zhang,\\r  Zhihao Liang, Jing Liao, Yan-Pei Cao, Ying Shan\\rCategories: cs.CV cs.GR\\rComments: 33 pages, 12 figures\\r\\\\\\\\\\r  Generating 3D models lies at the core of computer graphics and has been the\\rfocus of decades of research. With the emergence of advanced neural\\rrepresentations and generative models, the field of 3D content generation is\\rdeveloping rapidly, enabling the creation of increasingly high-quality and\\rdiverse 3D models. The rapid growth of this field makes it difficult to stay\\rabreast of all recent developments. In this survey, we aim to introduce the\\rfundamental methodologies of 3D generation methods and establish a structured\\rroadmap, encompassing 3D representation, generation methods, datasets, and\\rcorresponding applications. Specifically, we introduce the 3D representations\\rthat serve as the backbone for 3D generation. Furthermore, we provide a\\rcomprehensive overview of the rapidly growing literature on generation methods,\\rcategorized by the type of algorithmic paradigms, including feedforward\\rgeneration, optimization-based generation, procedural generation, and\\rgenerative novel view synthesis. Lastly, we discuss available datasets,\\rapplications, and open challenges. We hope this survey will help readers\\rexplore this exciting topic and foster further advancements in the field of 3D\\rcontent generation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17807 ,  12589kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17821\\rDate: Wed, 31 Jan 2024 13:24:51 GMT   (13493kb,D)\\r\\rTitle: Do Object Detection Localization Errors Affect Human Performance and\\r  Trust?\\rAuthors: Sven de Witte, Ombretta Strafforello, Jan van Gemert\\rCategories: cs.CV cs.HC\\r\\\\\\\\\\r  Bounding boxes are often used to communicate automatic object detection\\rresults to humans, aiding humans in a multitude of tasks. We investigate the\\rrelationship between bounding box localization errors and human task\\rperformance. We use observer performance studies on a visual multi-object\\rcounting task to measure both human trust and performance with different levels\\rof bounding box accuracy. The results show that localization errors have no\\rsignificant impact on human accuracy or trust in the system. Recall and\\rprecision errors impact both human performance and trust, suggesting that\\roptimizing algorithms based on the F1 score is more beneficial in\\rhuman-computer tasks. Lastly, the paper offers an improvement on bounding boxes\\rin multi-object counting tasks with center dots, showing improved performance\\rand better resilience to localization inaccuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17821 ,  13493kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17828\\rDate: Wed, 31 Jan 2024 13:41:17 GMT   (20667kb,D)\\r\\rTitle: Leveraging Swin Transformer for Local-to-Global Weakly Supervised\\r  Semantic Segmentation\\rAuthors: Rozhan Ahmadi, Shohreh Kasaei\\rCategories: cs.CV cs.AI\\rComments: 7 pages, 4 figures, 3 tables\\r\\\\\\\\\\r  In recent years, weakly supervised semantic segmentation using image-level\\rlabels as supervision has received significant attention in the field of\\rcomputer vision. Most existing methods have addressed the challenges arising\\rfrom the lack of spatial information in these labels by focusing on\\rfacilitating supervised learning through the generation of pseudo-labels from\\rclass activation maps (CAMs). Due to the localized pattern detection of\\rConvolutional Neural Networks (CNNs), CAMs often emphasize only the most\\rdiscriminative parts of an object, making it challenging to accurately\\rdistinguish foreground objects from each other and the background. Recent\\rstudies have shown that Vision Transformer (ViT) features, due to their global\\rview, are more effective in capturing the scene layout than CNNs. However, the\\ruse of hierarchical ViTs has not been extensively explored in this field. This\\rwork explores the use of Swin Transformer by proposing SWTformer to enhance\\rthe accuracy of the initial seed CAMs by bringing local and global views\\rtogether. SWTformer-V1 generates class probabilities and CAMs using only the\\rpatch tokens as features. SWTformer-V2 incorporates a multi-scale feature\\rfusion mechanism to extract additional information and utilizes a\\rbackground-aware mechanism to generate more accurate localization maps with\\rimproved cross-object discrimination. Based on experiments on the PascalVOC\\r2012 dataset, SWTformer-V1 achieves a 0.98% mAP higher localization accuracy,\\routperforming state-of-the-art models. It also yields comparable performance by\\r0.82% mIoU on average higher than other methods in generating initial\\rlocalization maps, depending only on the classification network. SWTformer-V2\\rfurther improves the accuracy of the generated seed CAMs by 5.32% mIoU, further\\rproving the effectiveness of the local-to-global view provided by the Swin\\rtransformer.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17828 ,  20667kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17851\\rDate: Wed, 31 Jan 2024 14:13:01 GMT   (1053kb,D)\\r\\rTitle: Instruction-Guided Scene Text Recognition\\rAuthors: Yongkun Du and Zhineng Chen and Yuchen Su and Caiyan Jia and Yu-Gang\\r  Jiang\\rCategories: cs.CV\\r\\\\\\\\\\r  Multi-modal models have shown appealing performance in visual tasks recently,\\ras instruction-guided training has evoked the ability to understand\\rfine-grained visual content. However, current methods cannot be trivially\\rapplied to scene text recognition (STR) due to the gap between natural and text\\rimages. In this paper, we introduce a novel paradigm that formulates STR as an\\rinstruction learning problem, and propose instruction-guided scene text\\rrecognition (IGTR) to achieve effective cross-modal learning. IGTR first\\rgenerates rich and diverse instruction triplets of ,\\rserving as guidance for nuanced text image understanding. Then, we devise an\\rarchitecture with dedicated cross-modal feature fusion module, and multi-task\\ranswer head to effectively fuse the required instruction and image features for\\ranswering questions. Built upon these designs, IGTR facilitates accurate text\\rrecognition by comprehending character attributes. Experiments on English and\\rChinese benchmarks show that IGTR outperforms existing models by significant\\rmargins. Furthermore, by adjusting the instructions, IGTR enables various\\rrecognition schemes. These include zero-shot prediction, where the model is\\rtrained based on instructions not explicitly targeting character recognition,\\rand the recognition of rarely appearing and morphologically similar characters,\\rwhich were previous challenges for existing models.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17851 ,  1053kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17857\\rDate: Wed, 31 Jan 2024 14:19:03 GMT   (30010kb,D)\\r\\rTitle: Semantic Anything in 3D Gaussians\\rAuthors: Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing\\r  Li, Zhaoxiang Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  3D Gaussian Splatting has emerged as an alternative 3D representation of\\rNeural Radiance Fields (NeRFs), benefiting from its high-quality rendering\\rresults and real-time rendering speed. Considering the 3D Gaussian\\rrepresentation remains unparsed, it is necessary first to execute object\\rsegmentation within this domain. Subsequently, scene editing and collision\\rdetection can be performed, proving vital to a multitude of applications, such\\ras virtual reality (VR), augmented reality (AR), game/movie production, etc. In\\rthis paper, we propose a novel approach to achieve object segmentation in 3D\\rGaussian via an interactive procedure without any training process and learned\\rparameters. We refer to the proposed method as SA-GS, for Segment Anything in\\r3D Gaussians. Given a set of clicked points in a single input view, SA-GS can\\rgeneralize SAM to achieve 3D consistent segmentation via the proposed\\rmulti-view mask generation and view-wise label assignment methods. We also\\rpropose a cross-view label-voting approach to assign labels from different\\rviews. In addition, in order to address the boundary roughness issue of\\rsegmented objects resulting from the non-negligible spatial sizes of 3D\\rGaussian located at the boundary, SA-GS incorporates the simple but effective\\rGaussian Decomposition scheme. Extensive experiments demonstrate that SA-GS\\rachieves high-quality 3D segmentation results, which can also be easily applied\\rfor scene editing and collision detection tasks. Codes will be released soon.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17857 ,  30010kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17862\\rDate: Wed, 31 Jan 2024 14:21:49 GMT   (1871kb,D)\\r\\rTitle: Proximity QA: Unleashing the Power of Multi-Modal Large Language Models\\r  for Spatial Proximity Analysis\\rAuthors: Jianing Li, Xi Nan, Ming Lu, Li Du, Shanghang Zhang\\rCategories: cs.CV\\rComments: 15 pages,version 1\\rACM-class: I.5.4; I.2.7\\r\\\\\\\\\\r  Multi-modal large language models (MLLMs) have demonstrated remarkable\\rvision-language capabilities, primarily due to the exceptional in-context\\runderstanding and multi-task learning strengths of large language models\\r(LLMs). The advent of visual instruction tuning has further enhanced MLLMs'\\rperformance in vision-language understanding. However, while existing MLLMs\\radeptly recognize \\\\textit{what} objects are in an image, they still face\\rchallenges in effectively discerning \\\\textit{where} these objects are,\\rparticularly along the distance (scene depth) axis. To overcome this limitation\\rin MLLMs, we introduce Proximity Question Answering (Proximity QA), a novel\\rframework designed to enable MLLMs to infer the proximity relationship between\\robjects in images. The framework operates in two phases: the first phase\\rfocuses on guiding the models to understand the relative depth of objects, and\\rthe second phase further encourages the models to infer the proximity\\rrelationships between objects based on their depth perceptions. We also propose\\ra VQA dataset called Proximity-110K, containing additional instructions that\\rincorporate depth information and the proximity relationships of objects. We\\rhave conducted extensive experiments to validate Proximity QA's superior\\rability in depth perception and proximity analysis, outperforming other\\rstate-of-the-art MLLMs. Code and dataset will be released at\\r\\\\textcolor{magenta}{https://github.com/NorthSummer/ProximityQA.git}.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17862 ,  1871kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17868\\rDate: Wed, 31 Jan 2024 14:27:07 GMT   (15145kb,D)\\r\\rTitle: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment\\r  Anything Model\\rAuthors: Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, Chun Yuan\\rCategories: cs.CV cs.LG\\rComments: Accepted at ICLR 2024 Conference\\r\\\\\\\\\\r  The Segment Anything Model (SAM) stands as a foundational framework for image\\rsegmentation. While it exhibits remarkable zero-shot generalization in typical\\rscenarios, its advantage diminishes when applied to specialized domains like\\rmedical imagery and remote sensing. To address this limitation, this paper\\rintroduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning\\rapproach. By integrating ultra-lightweight convolutional parameters into\\rLow-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases\\rinto the plain ViT encoder, further reinforcing SAM's local prior assumption.\\rNotably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge\\rbut also revives its capacity of learning high-level image semantics, which is\\rconstrained by SAM's foreground-background segmentation pretraining.\\rComprehensive experimentation across diverse benchmarks spanning multiple\\rdomains underscores Conv-LoRA's superiority in adapting SAM to real-world\\rsemantic segmentation tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17868 ,  15145kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17874\\rDate: Wed, 31 Jan 2024 14:32:56 GMT   (4010kb,D)\\r\\rTitle: VR-based generation of photorealistic synthetic data for training\\r  hand-object tracking models\\rAuthors: Chengyan Zhang, Rahul Chaudhari\\rCategories: cs.CV\\r\\\\\\\\\\r  Supervised learning models for precise tracking of hand-object interactions\\r(HOI) in 3D require large amounts of annotated data for training. Moreover, it\\ris not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object\\rpose) on 2D images. To address these issues, we present blender-hoisynth, an\\rinteractive synthetic data generator based on the Blender software.\\rBlender-hoisynth can scalably generate and automatically annotate visual HOI\\rtraining data. Other competing approaches usually generate synthetic HOI data\\rcompeletely without human input. While this may be beneficial in some\\rscenarios, HOI applications inherently necessitate direct control over the HOIs\\ras an expression of human intent. With blender-hoisynth, it is possible for\\rusers to interact with objects via virtual hands using standard Virtual Reality\\rhardware. The synthetically generated data are characterized by a high degree\\rof photorealism and contain visually plausible and physically realistic videos\\rof hands grasping objects and moving them around in 3D. To demonstrate the\\refficacy of our data generation, we replace large parts of the training data in\\rthe well-known DexYCB dataset with hoisynth data and train a state-of-the-art\\rHOI reconstruction model with it. We show that there is no significant\\rdegradation in the model performance despite the data replacement.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17874 ,  4010kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17879\\rDate: Wed, 31 Jan 2024 14:36:49 GMT   (22119kb,D)\\r\\rTitle: AEROBLADE: Training-Free Detection of Latent Diffusion Images Using\\r  Autoencoder Reconstruction Error\\rAuthors: Jonas Ricker, Denis Lukovnikov, Asja Fischer\\rCategories: cs.CV\\r\\\\\\\\\\r  With recent text-to-image models, anyone can generate deceptively realistic\\rimages with arbitrary contents, fueling the growing threat of visual\\rdisinformation. A key enabler for generating high-resolution images with low\\rcomputational cost has been the development of latent diffusion models (LDMs).\\rIn contrast to conventional diffusion models, LDMs perform the denoising\\rprocess in the low-dimensional latent space of a pre-trained autoencoder (AE)\\rinstead of the high-dimensional image space. Despite their relevance, the\\rforensic analysis of LDMs is still in its infancy. In this work we propose\\rAEROBLADE, a novel detection method which exploits an inherent component of\\rLDMs: the AE used to transform images between image and latent space. We find\\rthat generated images can be more accurately reconstructed by the AE than real\\rimages, allowing for a simple detection approach based on the reconstruction\\rerror. Most importantly, our method is easy to implement and does not require\\rany training, yet nearly matches the performance of detectors that rely on\\rextensive training. We empirically demonstrate that AEROBLADE is effective\\ragainst state-of-the-art LDMs including Stable Diffusion and Midjourney. Beyond\\rdetection, our approach allows for the qualitative analysis of images, which\\rcan be leveraged for identifying inpainted regions.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17879 ,  22119kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17881\\rDate: Wed, 31 Jan 2024 14:39:11 GMT   (6896kb,D)\\r\\rTitle: PVLR: Prompt-driven Visual-Linguistic Representation Learning for\\r  Multi-Label Image Recognition\\rAuthors: Hao Tan, Zichang Tan, Jun Li, Jun Wan, Zhen Lei\\rCategories: cs.CV\\rComments: 15 pages, 8 figures\\r\\\\\\\\\\r  Multi-label image recognition is a fundamental task in computer vision.\\rRecently, vision-language models have made notable advancements in this area.\\rHowever, previous methods often failed to effectively leverage the rich\\rknowledge within language models and instead incorporated label semantics into\\rvisual features in a unidirectional manner. In this paper, we propose a\\rPrompt-driven Visual-Linguistic Representation Learning (PVLR) framework to\\rbetter leverage the capabilities of the linguistic modality. In PVLR, we first\\rintroduce a dual-prompting strategy comprising Knowledge-Aware Prompting (KAP)\\rand Context-Aware Prompting (CAP). KAP utilizes fixed prompts to capture the\\rintrinsic semantic knowledge and relationships across all labels, while CAP\\remploys learnable prompts to capture context-aware label semantics and\\rrelationships. Later, we propose an Interaction and Fusion Module (IFM) to\\rinteract and fuse the representations obtained from KAP and CAP. In contrast to\\rthe unidirectional fusion in previous works, we introduce a Dual-Modal\\rAttention (DMA) that enables bidirectional interaction between textual and\\rvisual features, yielding context-aware label representations and\\rsemantic-related visual representations, which are subsequently used to\\rcalculate similarities and generate final predictions for all labels. Extensive\\rexperiments on three popular datasets including MS-COCO, Pascal VOC 2007, and\\rNUS-WIDE demonstrate the superiority of PVLR.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17881 ,  6896kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17883\\rDate: Wed, 31 Jan 2024 14:41:40 GMT   (61685kb,D)\\r\\rTitle: Reimagining Reality: A Comprehensive Survey of Video Inpainting\\r  Techniques\\rAuthors: Shreyank N Gowda, Yash Thakre, Shashank Narayana Gowda, Xiaobo Jin\\rCategories: cs.CV\\r\\\\\\\\\\r  This paper offers a comprehensive analysis of recent advancements in video\\rinpainting techniques, a critical subset of computer vision and artificial\\rintelligence. As a process that restores or fills in missing or corrupted\\rportions of video sequences with plausible content, video inpainting has\\revolved significantly with the advent of deep learning methodologies. Despite\\rthe plethora of existing methods and their swift development, the landscape\\rremains complex, posing challenges to both novices and established researchers.\\rOur study deconstructs major techniques, their underpinning theories, and their\\reffective applications. Moreover, we conduct an exhaustive comparative study,\\rcentering on two often-overlooked dimensions: visual quality and computational\\refficiency. We adopt a human-centric approach to assess visual quality,\\renlisting a panel of annotators to evaluate the output of different video\\rinpainting techniques. This provides a nuanced qualitative understanding that\\rcomplements traditional quantitative metrics. Concurrently, we delve into the\\rcomputational aspects, comparing inference times and memory demands across a\\rstandardized hardware setup. This analysis underscores the balance between\\rquality and efficiency: a critical consideration for practical applications\\rwhere resources may be constrained. By integrating human validation and\\rcomputational resource comparison, this survey not only clarifies the present\\rlandscape of video inpainting techniques but also charts a course for future\\rexplorations in this vibrant and evolving field.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17883 ,  61685kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17895\\rDate: Wed, 31 Jan 2024 15:02:26 GMT   (42283kb,D)\\r\\rTitle: ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural\\r  Radiance Fields\\rAuthors: Edward Bartrum and Thu Nguyen-Phuoc and Chris Xie and Zhengqin Li and\\r  Numair Khan and Armen Avetisyan and Douglas Lanman and Lei Xiao\\rCategories: cs.CV cs.AI cs.GR\\rComments: For our project page, see https://replaceanything3d.github.io/\\r\\\\\\\\\\r  We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene\\rediting method that enables the replacement of specific objects within a scene.\\rGiven multi-view images of a scene, a text prompt describing the object to\\rreplace, and a text prompt describing the new object, our Erase-and-Replace\\rapproach can effectively swap objects in the scene with newly generated content\\rwhile maintaining 3D consistency across multiple viewpoints. We demonstrate the\\rversatility of ReplaceAnything3D by applying it to various realistic 3D scenes,\\rshowcasing results of modified foreground objects that are well-integrated with\\rthe rest of the scene without affecting its overall integrity.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17895 ,  42283kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17904\\rDate: Wed, 31 Jan 2024 15:10:29 GMT   (5906kb,D)\\r\\rTitle: Hi-SAM: Marrying Segment Anything Model for Hierarchical Text\\r  Segmentation\\rAuthors: Maoyuan Ye, Jing Zhang, Juhua Liu, Chenyu Liu, Baocai Yin, Cong Liu,\\r  Bo Du, Dacheng Tao\\rCategories: cs.CV\\rComments: GitHub repository: https://github.com/ymy-k/Hi-SAM\\r\\\\\\\\\\r  The Segment Anything Model (SAM), a profound vision foundation model\\rpre-trained on a large-scale dataset, breaks the boundaries of general\\rsegmentation and sparks various downstream applications. This paper introduces\\rHi-SAM, a unified model leveraging SAM for hierarchical text segmentation.\\rHi-SAM excels in text segmentation across four hierarchies, including stroke,\\rword, text-line, and paragraph, while realizing layout analysis as well.\\rSpecifically, we first turn SAM into a high-quality text stroke segmentation\\r(TSS) model through a parameter-efficient fine-tuning approach. We use this TSS\\rmodel to iteratively generate the text stroke labels in a semi-automatical\\rmanner, unifying labels across the four text hierarchies in the HierText\\rdataset. Subsequently, with these complete labels, we launch the end-to-end\\rtrainable Hi-SAM based on the TSS architecture with a customized hierarchical\\rmask decoder. During inference, Hi-SAM offers both automatic mask generation\\r(AMG) mode and promptable segmentation mode. In terms of the AMG mode, Hi-SAM\\rsegments text stroke foreground masks initially, then samples foreground points\\rfor hierarchical text mask generation and achieves layout analysis in passing.\\rAs for the promptable mode, Hi-SAM provides word, text-line, and paragraph\\rmasks with a single point click. Experimental results show the state-of-the-art\\rperformance of our TSS model: 84.86% fgIOU on Total-Text and 88.96% fgIOU on\\rTextSeg for text stroke segmentation. Moreover, compared to the previous\\rspecialist for joint hierarchical detection and layout analysis on HierText,\\rHi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on the\\rtext-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout analysis,\\rrequiring 20x fewer training epochs. The code is available at\\rhttps://github.com/ymy-k/Hi-SAM.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17904 ,  5906kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17910\\rDate: Wed, 31 Jan 2024 15:15:41 GMT   (18122kb,D)\\r\\rTitle: Controllable Dense Captioner with Multimodal Embedding Bridging\\rAuthors: Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Qixiang Ye,\\r  Fang Wan\\rCategories: cs.CV\\rComments: https://github.com/callsys/ControlCap\\r\\\\\\\\\\r  In this paper, we propose a controllable dense captioner (ControlCap), which\\raccommodates user's intention to dense captioning by introducing linguistic\\rguidance. ControlCap is defined as a multimodal embedding bridging\\rarchitecture, which comprises multimodal embedding generation (MEG) module and\\rbi-directional embedding bridging (BEB) module. While MEG module represents\\robjects/regions by combining embeddings of detailed information with\\rcontext-aware ones, it also endows ControlCap the adaptability to specialized\\rcontrols by utilizing them as linguistic guidance. BEB module aligns the\\rlinguistic guidance with visual embeddings through borrowing/returning features\\rfrom/to the visual domain and gathering such features to predict text\\rdescriptions. Experiments on Visual Genome and VG-COCO datasets show that\\rControlCap respectively outperforms the state-of-the-art methods by 1.5% and\\r3.7% (mAP). Last but not least, with the capability of converting\\rregion-category pairs to region-text pairs, ControlCap is able to act as a\\rpowerful data engine for dense captioning. Code is available at\\rhttps://github.com/callsys/ControlCap.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17910 ,  18122kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17916\\rDate: Wed, 31 Jan 2024 15:32:44 GMT   (2324kb,D)\\r\\rTitle: Source-free Domain Adaptive Object Detection in Remote Sensing Images\\rAuthors: Weixing Liu, Jun Liu, Xin Su, Han Nie, Bin Luo\\rCategories: cs.CV\\rComments: 14 pages, 11 figures\\r\\\\\\\\\\r  Recent studies have used unsupervised domain adaptive object detection\\r(UDAOD) methods to bridge the domain gap in remote sensing (RS) images.\\rHowever, UDAOD methods typically assume that the source domain data can be\\raccessed during the domain adaptation process. This setting is often\\rimpractical in the real world due to RS data privacy and transmission\\rdifficulty. To address this challenge, we propose a practical source-free\\robject detection (SFOD) setting for RS images, which aims to perform target\\rdomain adaptation using only the source pre-trained model. We propose a new\\rSFOD method for RS images consisting of two parts: perturbed domain generation\\rand alignment. The proposed multilevel perturbation constructs the perturbed\\rdomain in a simple yet efficient form by perturbing the domain-variant features\\rat the image level and feature level according to the color and style bias. The\\rproposed multilevel alignment calculates feature and label consistency between\\rthe perturbed domain and the target domain across the teacher-student network,\\rand introduces the distillation of feature prototype to mitigate the noise of\\rpseudo-labels. By requiring the detector to be consistent in the perturbed\\rdomain and the target domain, the detector is forced to focus on\\rdomaininvariant features. Extensive results of three synthetic-to-real\\rexperiments and three cross-sensor experiments have validated the effectiveness\\rof our method which does not require access to source domain RS images.\\rFurthermore, experiments on computer vision datasets show that our method can\\rbe extended to other fields as well. Our code will be available at:\\rhttps://weixliu.github.io/ .\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17916 ,  2324kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17948\\rDate: Wed, 31 Jan 2024 15:57:21 GMT   (556kb,D)\\r\\rTitle: HyperZ$\\\\cdot$Z$\\\\cdot$W Operator Connects Slow-Fast Networks for Full\\r  Context Interaction\\rAuthors: Harvie Zhang\\rCategories: cs.CV\\rComments: 10 pages, 6 figures, 5 tables\\r\\\\\\\\\\r  The self-attention mechanism utilizes large implicit weight matrices,\\rprogrammed through dot product-based activations with very few trainable\\rparameters, to enable long sequence modeling. In this paper, we investigate the\\rpossibility of discarding residual learning by employing large implicit kernels\\rto achieve full context interaction at each layer of the network. To accomplish\\rit, we introduce coordinate-based implicit MLPs as a slow network to generate\\rhyper-kernels for another fast convolutional network. To get context-varying\\rweights for fast dynamic encoding, we propose a\\r$\\\\mathrm{Hyper}\\\\mathcal{Z{\\\\cdot}Z{\\\\cdot}W}$ operator that connects\\rhyper-kernels ($\\\\mathcal{W}$) and hidden activations ($\\\\mathcal{Z}$) through\\rsimple elementwise multiplication, followed by convolution of $\\\\mathcal{Z}$\\rusing the context-dependent $\\\\mathcal{W}$. Based on this design, we present a\\rnovel Terminator architecture that integrates hyper-kernels of different sizes\\rto produce multi-branch hidden representations for enhancing the feature\\rextraction capability of each layer. Additionally, a bottleneck layer is\\remployed to compress the concatenated channels, allowing only valuable\\rinformation to propagate to the subsequent layers. Notably, our model\\rincorporates several innovative components and exhibits excellent properties,\\rsuch as introducing local feedback error for updating the slow network, stable\\rzero-mean features, faster training convergence, and fewer model parameters.\\rExtensive experimental results on pixel-level 1D and 2D image classification\\rbenchmarks demonstrate the superior performance of our architecture.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17948 ,  556kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17972\\rDate: Wed, 31 Jan 2024 16:27:47 GMT   (9867kb,D)\\r\\rTitle: MelNet: A Real-Time Deep Learning Algorithm for Object Detection\\rAuthors: Yashar Azadvatan and Murat Kurt\\rCategories: cs.CV cs.AI cs.LG\\rComments: 11 pages, 9 figures, 5 tables\\r\\\\\\\\\\r  In this study, a novel deep learning algorithm for object detection, named\\rMelNet, was introduced. MelNet underwent training utilizing the KITTI dataset\\rfor object detection. Following 300 training epochs, MelNet attained an mAP\\r(mean average precision) score of 0.732. Additionally, three alternative models\\r-YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI\\rdataset and juxtaposed with MelNet for object detection.\\r  The outcomes underscore the efficacy of employing transfer learning in\\rcertain instances. Notably, preexisting models trained on prominent datasets\\r(e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding\\runderscores the viability of creating a new model tailored to a specific\\rscenario and training it on a specific dataset. This investigation demonstrates\\rthat training MelNet exclusively on the KITTI dataset also surpasses\\rEfficientDet after 150 epochs. Consequently, post-training, MelNet's\\rperformance closely aligns with that of other pre-trained models.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17972 ,  9867kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17981\\rDate: Wed, 31 Jan 2024 16:38:32 GMT   (2381kb,D)\\r\\rTitle: Enhancing Multimodal Large Language Models with Vision Detection Models:\\r  An Empirical Study\\rAuthors: Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Despite the impressive capabilities of Multimodal Large Language Models\\r(MLLMs) in integrating text and image modalities, challenges remain in\\raccurately interpreting detailed visual elements. This paper presents an\\rempirical study on enhancing MLLMs with state-of-the-art (SOTA) object\\rdetection and Optical Character Recognition models to improve fine-grained\\rimage understanding and reduce hallucination in responses. Our research\\rinvestigates the embedding-based infusion of detection information, the impact\\rof such infusion on the MLLMs' original abilities, and the interchangeability\\rof detection models. We conduct systematic experiments with models such as\\rLLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines\\rMLLMs' performance in specific visual tasks but also maintains their original\\rstrengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10\\rbenchmarks, achieving an improvement of up to 12.99% on the normalized average\\rscore, marking a notable advancement in multimodal understanding. We release\\rour codes to facilitate further exploration into the fine-grained multimodal\\rdialogue capabilities of MLLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17981 ,  2381kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17985\\rDate: Wed, 31 Jan 2024 16:44:20 GMT   (31719kb,D)\\r\\rTitle: Shrub of a thousand faces: an individual segmentation from satellite\\r  images using deep learning\\rAuthors: Rohaifa Khaldi, Siham Tabik, Sergio Puertas-Ruiz, Julio Pe\\\\~nas de\\r  Giles, Jos\\\\'e Antonio H\\\\'odar Correa, Regino Zamora, Domingo Alcaraz Segura\\rCategories: cs.CV cs.AI\\rComments: 39 pages, 20 figures\\r\\\\\\\\\\r  Monitoring the distribution and size structure of long-living shrubs, such as\\rJuniperus communis, can be used to estimate the long-term effects of climate\\rchange on high-mountain and high latitude ecosystems. Historical aerial\\rvery-high resolution imagery offers a retrospective tool to monitor shrub\\rgrowth and distribution at high precision. Currently, deep learning models\\rprovide impressive results for detecting and delineating the contour of objects\\rwith defined shapes. However, adapting these models to detect natural objects\\rthat express complex growth patterns, such as junipers, is still a challenging\\rtask.\\r  This research presents a novel approach that leverages remotely sensed RGB\\rimagery in conjunction with Mask R-CNN-based instance segmentation models to\\rindividually delineate Juniperus shrubs above the treeline in Sierra Nevada\\r(Spain). In this study, we propose a new data construction design that consists\\rin using photo interpreted (PI) and field work (FW) data to respectively\\rdevelop and externally validate the model. We also propose a new shrub-tailored\\revaluation algorithm based on a new metric called Multiple Intersections over\\rGround Truth Area (MIoGTA) to assess and optimize the model shrub delineation\\rperformance. Finally, we deploy the developed model for the first time to\\rgenerate a wall-to-wall map of Juniperus individuals.\\r  The experimental results demonstrate the efficiency of our dual data\\rconstruction approach in overcoming the limitations associated with traditional\\rfield survey methods. They also highlight the robustness of MIoGTA metric in\\revaluating instance segmentation models on species with complex growth patterns\\rshowing more resilience against data annotation uncertainty. Furthermore, they\\rshow the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone in\\rdelineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%,\\rrespectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17985 ,  31719kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17992\\rDate: Wed, 31 Jan 2024 16:52:19 GMT   (6064kb,D)\\r\\rTitle: Multilinear Operator Networks\\rAuthors: Yixin Cheng, Grigorios G. Chrysos, Markos Georgopoulos, Volkan Cevher\\rCategories: cs.CV cs.LG\\rComments: International Conference on Learning Representations Poster(2024)\\r\\\\\\\\\\r  Despite the remarkable capabilities of deep neural networks in image\\rrecognition, the dependence on activation functions remains a largely\\runexplored area and has yet to be eliminated. On the other hand, Polynomial\\rNetworks is a class of models that does not require activation functions, but\\rhave yet to perform on par with modern architectures. In this work, we aim\\rclose this gap and propose MONet, which relies solely on multilinear operators.\\rThe core layer of MONet, called Mu-Layer, captures multiplicative interactions\\rof the elements of the input token. MONet captures high-degree interactions of\\rthe input elements and we demonstrate the efficacy of our approach on a series\\rof image recognition and scientific computing benchmarks. The proposed model\\routperforms prior polynomial networks and performs on par with modern\\rarchitectures. We believe that MONet can inspire further research on models\\rthat use entirely multilinear operations.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17992 ,  6064kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18032\\rDate: Wed, 31 Jan 2024 17:54:43 GMT   (19496kb,D)\\r\\rTitle: DROP: Decouple Re-Identification and Human Parsing with Task-specific\\r  Features for Occluded Person Re-identification\\rAuthors: Shuguang Dou, Xiangyang Jiang, Yuanpeng Tu, Junyao Gao, Zefan Qu,\\r  Qingsong Zhao, Cairong Zhao\\rCategories: cs.CV\\r\\\\\\\\\\r  The paper introduces the Decouple Re-identificatiOn and human Parsing (DROP)\\rmethod for occluded person re-identification (ReID). Unlike mainstream\\rapproaches using global features for simultaneous multi-task learning of ReID\\rand human parsing, or relying on semantic information for attention guidance,\\rDROP argues that the inferior performance of the former is due to distinct\\rgranularity requirements for ReID and human parsing features. ReID focuses on\\rinstance part-level differences between pedestrian parts, while human parsing\\rcenters on semantic spatial context, reflecting the internal structure of the\\rhuman body. To address this, DROP decouples features for ReID and human\\rparsing, proposing detail-preserving upsampling to combine varying resolution\\rfeature maps. Parsing-specific features for human parsing are decoupled, and\\rhuman position information is exclusively added to the human parsing branch. In\\rthe ReID branch, a part-aware compactness loss is introduced to enhance\\rinstance-level part differences. Experimental results highlight the efficacy of\\rDROP, especially achieving a Rank-1 accuracy of 76.8% on Occluded-Duke,\\rsurpassing two mainstream methods. The codebase is accessible at\\rhttps://github.com/shuguang-52/DROP.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18032 ,  19496kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18054\\rDate: Wed, 31 Jan 2024 18:20:42 GMT   (4238kb,D)\\r\\rTitle: Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based\\r  Action Recognition\\rAuthors: Wei Wei, Tom De Schepper, Kevin Mets\\rCategories: cs.CV cs.LG\\rComments: This work is accepted at VISAPP 2024 as a short paper\\r\\\\\\\\\\r  Continual learning (CL) is the research field that aims to build machine\\rlearning models that can accumulate knowledge continuously over different tasks\\rwithout retraining from scratch. Previous studies have shown that pre-training\\rgraph neural networks (GNN) may lead to negative transfer (Hu et al., 2020)\\rafter fine-tuning, a setting which is closely related to CL. Thus, we focus on\\rstudying GNN in the continual graph learning (CGL) setting. We propose the\\rfirst continual graph learning benchmark for spatio-temporal graphs and use it\\rto benchmark well-known CGL methods in this novel setting. The benchmark is\\rbased on the N-UCLA and NTU-RGB+D datasets for skeleton-based action\\rrecognition. Beyond benchmarking for standard performance metrics, we study the\\rclass and task-order sensitivity of CGL methods, i.e., the impact of learning\\rorder on each class/task's performance, and the architectural sensitivity of\\rCGL methods with backbone GNN at various widths and depths. We reveal that\\rtask-order robust methods can still be class-order sensitive and observe\\rresults that contradict previous empirical observations on architectural\\rsensitivity in CL.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18054 ,  4238kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18075\\rDate: Wed, 31 Jan 2024 18:56:09 GMT   (31120kb,D)\\r\\rTitle: CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting\\rAuthors: Jiezhi Yang, Khushi Desai, Charles Packer, Harshil Bhatia, Nicholas\\r  Rhinehart, Rowan McAllister, Joseph Gonzalez\\rCategories: cs.CV\\r\\\\\\\\\\r  We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene\\rForecasting, a method for predicting future 3D scenes given past observations,\\rsuch as 2D ego-centric images. Our method maps an image to a distribution over\\rplausible 3D latent scene configurations using a probabilistic encoder, and\\rpredicts the evolution of the hypothesized scenes through time. Our latent\\rscene representation conditions a global Neural Radiance Field (NeRF) to\\rrepresent a 3D scene model, which enables explainable predictions and\\rstraightforward downstream applications. This approach extends beyond previous\\rneural rendering work by considering complex scenarios of uncertainty in\\renvironmental states and dynamics. We employ a two-stage training of\\rPose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we\\rauto-regressively predict latent scene representations as a partially\\robservable Markov decision process, utilizing a mixture density network. We\\rdemonstrate the utility of our method in realistic scenarios using the CARLA\\rdriving simulator, where CARFF can be used to enable efficient trajectory and\\rcontingency planning in complex multi-agent autonomous driving scenarios\\rinvolving visual occlusions.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18075 ,  31120kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18083\\rDate: Wed, 31 Jan 2024 18:59:12 GMT   (3110kb,D)\\r\\rTitle: Improved Scene Landmark Detection for Camera Localization\\rAuthors: Tien Do and Sudipta N. Sinha\\rCategories: cs.CV cs.RO\\rComments: To be presented at 3DV 2024\\r\\\\\\\\\\r  Camera localization methods based on retrieval, local feature matching, and\\r3D structure-based pose estimation are accurate but require high storage, are\\rslow, and are not privacy-preserving. A method based on scene landmark\\rdetection (SLD) was recently proposed to address these limitations. It involves\\rtraining a convolutional neural network (CNN) to detect a few predetermined,\\rsalient, scene-specific 3D points or landmarks and computing camera pose from\\rthe associated 2D-3D correspondences. Although SLD outperformed existing\\rlearning-based approaches, it was notably less accurate than 3D structure-based\\rmethods. In this paper, we show that the accuracy gap was due to insufficient\\rmodel capacity and noisy labels during training. To mitigate the capacity\\rissue, we propose to split the landmarks into subgroups and train a separate\\rnetwork for each subgroup. To generate better training labels, we propose using\\rdense reconstructions to estimate visibility of scene landmarks. Finally, we\\rpresent a compact architecture to improve memory efficiency. Accuracy wise, our\\rapproach is on par with state of the art structure based methods on the\\rINDOOR-6 dataset but runs significantly faster and uses less storage. Code and\\rmodels can be found at https://github.com/microsoft/SceneLandmarkLocalization.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18083 ,  3110kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18084\\rDate: Wed, 31 Jan 2024 18:59:57 GMT   (16225kb,D)\\r\\rTitle: Binding Touch to Everything: Learning Unified Multimodal Tactile\\r  Representations\\rAuthors: Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang,\\r  Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, Alex Wong\\rCategories: cs.CV cs.RO\\r\\\\\\\\\\r  The ability to associate touch with other modalities has huge implications\\rfor humans and computational systems. However, multimodal learning with touch\\rremains challenging due to the expensive data collection process and\\rnon-standardized sensor outputs. We introduce UniTouch, a unified tactile model\\rfor vision-based touch sensors connected to multiple modalities, including\\rvision, language, and sound. We achieve this by aligning our UniTouch\\rembeddings to pretrained image embeddings already associated with a variety of\\rother modalities. We further propose learnable sensor-specific tokens, allowing\\rthe model to learn from a set of heterogeneous tactile sensors, all at the same\\rtime. UniTouch is capable of conducting various touch sensing tasks in the\\rzero-shot setting, from robot grasping prediction to touch image question\\ranswering. To the best of our knowledge, UniTouch is the first to demonstrate\\rsuch capabilities. Project page: https://cfeng16.github.io/UniTouch/\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18084 ,  16225kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18085\\rDate: Wed, 31 Jan 2024 18:59:59 GMT   (14588kb,D)\\r\\rTitle: Motion Guidance: Diffusion-Based Image Editing with Differentiable\\r  Motion Estimators\\rAuthors: Daniel Geng, Andrew Owens\\rCategories: cs.CV\\r\\\\\\\\\\r  Diffusion models are capable of generating impressive images conditioned on\\rtext descriptions, and extensions of these models allow users to edit images at\\ra relatively coarse scale. However, the ability to precisely edit the layout,\\rposition, pose, and shape of objects in images with diffusion models is still\\rdifficult. To this end, we propose motion guidance, a zero-shot technique that\\rallows a user to specify dense, complex motion fields that indicate where each\\rpixel in an image should move. Motion guidance works by steering the diffusion\\rsampling process with the gradients through an off-the-shelf optical flow\\rnetwork. Specifically, we design a guidance loss that encourages the sample to\\rhave the desired motion, as estimated by a flow network, while also being\\rvisually similar to the source image. By simultaneously sampling from a\\rdiffusion model and guiding the sample to have low guidance loss, we can obtain\\ra motion-edited image. We demonstrate that our technique works on complex\\rmotions and produces high quality edits of real and generated images.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18085 ,  14588kb)\\r%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17435 (*cross-listing*)\\rDate: Tue, 30 Jan 2024 20:49:47 GMT   (587kb,D)\\r\\rTitle: Can Large Language Models Replace Economic Choice Prediction Labs?\\rAuthors: Eilam Shapira, Omer Madmon, Roi Reichart, Moshe Tennenholtz\\rCategories: cs.LG cs.AI cs.CL cs.GT cs.HC\\r\\\\\\\\\\r  Economic choice prediction is an essential challenging task, often\\rconstrained by the difficulties in acquiring human choice data. Indeed,\\rexperimental economics studies had focused mostly on simple choice settings.\\rThe AI community has recently contributed to that effort in two ways:\\rconsidering whether LLMs can substitute for humans in the above-mentioned\\rsimple choice prediction settings, and the study through ML lens of more\\relaborated but still rigorous experimental economics settings, employing\\rincomplete information, repetitive play, and natural language communication,\\rnotably language-based persuasion games. This leaves us with a major\\rinspiration: can LLMs be used to fully simulate the economic environment and\\rgenerate data for efficient human choice prediction, substituting for the\\relaborated economic lab studies? We pioneer the study of this subject,\\rdemonstrating its feasibility. In particular, we show that a model trained\\rsolely on LLM-generated data can effectively predict human behavior in a\\rlanguage-based persuasion game, and can even outperform models trained on\\ractual human data.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17435 ,  587kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17505 (*cross-listing*)\\rDate: Tue, 30 Jan 2024 23:46:35 GMT   (1422kb,D)\\r\\rTitle: Arrows of Time for Large Language Models\\rAuthors: Vassilis Papadopoulos, J\\\\'er\\\\'emie Wenger, Cl\\\\'ement Hongler\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\\\r  We study the probabilistic modeling performed by Autoregressive Large\\rLanguage Models through the angle of time directionality. We empirically find a\\rtime asymmetry exhibited by such models in their ability to model natural\\rlanguage: a difference in the average log-perplexity when trying to predict the\\rnext token versus when trying to predict the previous one. This difference is\\rat the same time subtle and very consistent across various modalities\\r(language, model size, training time, ...). Theoretically, this is surprising:\\rfrom an information-theoretic point of view, there should be no such\\rdifference. We provide a theoretical framework to explain how such an asymmetry\\rcan appear from sparsity and computational complexity considerations, and\\routline a number of perspectives opened by our results.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17505 ,  1422kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17511 (*cross-listing*)\\rDate: Wed, 31 Jan 2024 00:08:44 GMT   (525kb,D)\\r\\rTitle: Linguistically Communicating Uncertainty in Patient-Facing Risk\\r  Prediction Models\\rAuthors: Adarsa Sivaprasad and Ehud Reiter\\rCategories: cs.AI cs.CL\\r\\\\\\\\\\r  This paper addresses the unique challenges associated with uncertainty\\rquantification in AI models when applied to patient-facing contexts within\\rhealthcare. Unlike traditional eXplainable Artificial Intelligence (XAI)\\rmethods tailored for model developers or domain experts, additional\\rconsiderations of communicating in natural language, its presentation and\\revaluating understandability are necessary. We identify the challenges in\\rcommunication model performance, confidence, reasoning and unknown knowns using\\rnatural language in the context of risk prediction. We propose a design aimed\\rat addressing these challenges, focusing on the specific application of\\rin-vitro fertilisation outcome prediction.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17511 ,  525kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17902 (*cross-listing*)\\rDate: Wed, 31 Jan 2024 15:06:34 GMT   (115kb,D)\\r\\rTitle: Revisiting speech segmentation and lexicon learning with better features\\rAuthors: Herman Kamper, Benjamin van Niekerk\\rCategories: eess.AS cs.CL cs.SD\\rComments: 2 pages\\r\\\\\\\\\\r  We revisit a self-supervised method that segments unlabelled speech into\\rword-like segments. We start from the two-stage duration-penalised dynamic\\rprogramming method that performs zero-resource segmentation without learning an\\rexplicit lexicon. In the first acoustic unit discovery stage, we replace\\rcontrastive predictive coding features with HuBERT. After word segmentation in\\rthe second stage, we get an acoustic word embedding for each segment by\\raveraging HuBERT features. These embeddings are clustered using K-means to get\\ra lexicon. The result is good full-coverage segmentation with a lexicon that\\rachieves state-of-the-art performance on the ZeroSpeech benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17902 ,  115kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18018 (*cross-listing*)\\rDate: Wed, 31 Jan 2024 17:28:24 GMT   (3000kb,D)\\r\\rTitle: Prompt-Driven LLM Safeguarding via Directed Representation Optimization\\rAuthors: Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei\\r  Chang, Minlie Huang, Nanyun Peng\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\\\r  Prepending model inputs with safety prompts is a common practice of\\rsafeguarding large language models (LLMs) from complying with queries that\\rcontain harmful intents. However, the working mechanisms of safety prompts have\\rnot yet been fully understood, which hinders the potential for automatically\\roptimizing them for improved LLM safety. Motivated by this problem, we\\rinvestigate the impact of safety prompts from the perspective of model\\rrepresentations. We find that in models' representation space, harmful and\\rharmless queries can be largely distinguished, but this is not noticeably\\renhanced by safety prompts. Instead, the queries' representations are moved by\\rdifferent safety prompts in similar directions, where models become more prone\\rto refusal (i.e., refusing to provide assistance) even when the queries are\\rharmless. Inspired by these findings, we propose a method called DRO (Directed\\rRepresentation Optimization) for automatic safety prompt optimization. DRO\\rtreats safety prompts as continuous, trainable embeddings and learns to move\\rthe representations of harmful/harmless queries along/opposite the direction in\\rwhich the model's refusal probability increases. We demonstrate that DRO\\rremarkably improves the safeguarding performance of human-crafted safety\\rprompts and outperforms strong baselines, as evaluated on out-of-domain\\rbenchmarks, without compromising the general model capability.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18018 ,  3000kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17317 (*cross-listing*)\\rDate: Sun, 21 Jan 2024 14:29:33 GMT   (883kb)\\r\\rTitle: Detection of Auditory Brainstem Response Peaks Using Image Processing\\r  Techniques in Infants with Normal Hearing Sensitivity\\rAuthors: Amir Majidpour, Samer Kais Jameel, Jafar Majidpour, Houra Bagheri,\\r  Tarik A.Rashid, Ahmadreza Nazeri, Mahshid Moheb Aleaba\\rCategories: q-bio.NC cs.CV cs.SD eess.AS eess.IV\\r\\\\\\\\\\r  Introduction: The auditory brainstem response (ABR) is measured to find the\\rbrainstem-level peripheral auditory nerve system integrity in children having\\rnormal hearing. The Auditory Evoked Potential (AEP) is generated using acoustic\\rstimuli. Interpreting these waves requires competence to avoid misdiagnosing\\rhearing problems. Automating ABR test labeling with computer vision may reduce\\rhuman error. Method: The ABR test results of 26 children aged 1 to 20 months\\rwith normal hearing in both ears were used. A new approach is suggested for\\rautomatically calculating the peaks of waves of different intensities (in\\rdecibels). The procedure entails acquiring wave images from an Audera device\\rusing the Color Thresholder method, segmenting each wave as a single wave image\\rusing the Image Region Analyzer application, converting all wave images into\\rwaves using Image Processing (IP) techniques, and finally calculating the\\rlatency of the peaks for each wave to be used by an audiologist for diagnosing\\rthe disease. Findings: Image processing techniques were able to detect 1, 3,\\rand 5 waves in the diagnosis field with accuracy (0.82), (0.98), and (0.98),\\rrespectively, and its precision for waves 1, 3, and 5, were respectively\\r(0.32), (0.97) and (0.87). This evaluation also worked well in the thresholding\\rpart and 82.7 % correctly detected the ABR waves. Conclusion: Our findings\\rindicate that the audiology test battery suite can be made more accurate,\\rquick, and error-free by using technology to automatically detect and label ABR\\rwaves.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17317 ,  883kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17320 (*cross-listing*)\\rDate: Sat, 27 Jan 2024 18:00:20 GMT   (2295kb)\\r\\rTitle: Sigma-lognormal modeling of speech\\rAuthors: C. Carmona-Duarte, M.A.Ferrer, R. Plamondon, A. Gomez-Rodellar, P.\\r  Gomez-Vilda\\rCategories: q-bio.NC cs.CV cs.SD eess.AS\\rComments: Published in Open Acces\\rJournal-ref: Cognitive Computation, 13(2). pp. 488-503, 2021\\rDOI: 10.1007/s12559-020-09803-8\\r\\\\\\\\\\r  Human movement studies and analyses have been fundamental in many scientific\\rdomains, ranging from neuroscience to education, pattern recognition to\\rrobotics, health care to sports, and beyond. Previous speech motor models were\\rproposed to understand how speech movement is produced and how the resulting\\rspeech varies when some parameters are changed. However, the inverse approach,\\rin which the muscular response parameters and the subject's age are derived\\rfrom real continuous speech, is not possible with such models. Instead, in the\\rhandwriting field, the kinematic theory of rapid human movements and its\\rassociated Sigma-lognormal model have been applied successfully to obtain the\\rmuscular response parameters. This work presents a speech kinematics based\\rmodel that can be used to study, analyze, and reconstruct complex speech\\rkinematics in a simplified manner. A method based on the kinematic theory of\\rrapid human movements and its associated Sigma lognormal model are applied to\\rdescribe and to parameterize the asymptotic impulse response of the\\rneuromuscular networks involved in speech as a response to a neuromotor\\rcommand. The method used to carry out transformations from formants to a\\rmovement observation is also presented. Experiments carried out with the\\r(English) VTR TIMIT database and the (German) Saarbrucken Voice Database,\\rincluding people of different ages, with and without laryngeal pathologies,\\rcorroborate the link between the extracted parameters and aging, on the one\\rhand, and the proportion between the first and second formants required in\\rapplying the kinematic theory of rapid human movements, on the other. The\\rresults should drive innovative developments in the modeling and understanding\\rof speech kinematics.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17320 ,  2295kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17484 (*cross-listing*)\\rDate: Tue, 30 Jan 2024 22:37:24 GMT   (7802kb,D)\\r\\rTitle: Pixel to Elevation: Learning to Predict Elevation Maps at Long Range\\r  using Images for Autonomous Offroad Navigation\\rAuthors: Chanyoung Chung, Georgios Georgakis, Patrick Spieler, Curtis Padgett,\\r  Shehryar Khattak\\rCategories: cs.RO cs.CV cs.LG\\rComments: 8 pages, 6 figures, Under review\\r\\\\\\\\\\r  Understanding terrain topology at long-range is crucial for the success of\\roff-road robotic missions, especially when navigating at high-speeds. LiDAR\\rsensors, which are currently heavily relied upon for geometric mapping, provide\\rsparse measurements when mapping at greater distances. To address this\\rchallenge, we present a novel learning-based approach capable of predicting\\rterrain elevation maps at long-range using only onboard egocentric images in\\rreal-time. Our proposed method is comprised of three main elements. First, a\\rtransformer-based encoder is introduced that learns cross-view associations\\rbetween the egocentric views and prior bird-eye-view elevation map predictions.\\rSecond, an orientation-aware positional encoding is proposed to incorporate the\\r3D vehicle pose information over complex unstructured terrain with multi-view\\rvisual image features. Lastly, a history-augmented learn-able map embedding is\\rproposed to achieve better temporal consistency between elevation map\\rpredictions to facilitate the downstream navigational tasks. We experimentally\\rvalidate the applicability of our proposed approach for autonomous offroad\\rrobotic navigation in complex and unstructured terrain using real-world offroad\\rdriving data. Furthermore, the method is qualitatively and quantitatively\\rcompared against the current state-of-the-art methods. Extensive field\\rexperiments demonstrate that our method surpasses baseline models in accurately\\rpredicting terrain elevation while effectively capturing the overall terrain\\rtopology at long-ranges. Finally, ablation studies are conducted to highlight\\rand understand the effect of key components of the proposed approach and\\rvalidate their suitability to improve offroad robotic navigation capabilities.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17484 ,  7802kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17542 (*cross-listing*)\\rDate: Wed, 31 Jan 2024 02:09:21 GMT   (2045kb,D)\\r\\rTitle: Data-Effective Learning: A Comprehensive Medical Benchmark\\rAuthors: Wenxuan Yang, Weimin Tan, Yuqi Sun, Bo Yan\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\\\r  Data-effective learning aims to use data in the most impactful way to train\\rAI models, which involves strategies that focus on data quality rather than\\rquantity, ensuring the data used for training has high informational value.\\rData-effective learning plays a profound role in accelerating AI training,\\rreducing computational costs, and saving data storage, which is very important\\ras the volume of medical data in recent years has grown beyond many people's\\rexpectations. However, due to the lack of standards and comprehensive\\rbenchmark, research on medical data-effective learning is poorly studied. To\\raddress this gap, our paper introduces a comprehensive benchmark specifically\\rfor evaluating data-effective learning in the medical field. This benchmark\\rincludes a dataset with millions of data samples from 31 medical centers\\r(DataDEL), a baseline method for comparison (MedDEL), and a new evaluation\\rmetric (NormDEL) to objectively measure data-effective learning performance.\\rOur extensive experimental results show the baseline MedDEL can achieve\\rperformance comparable to the original large dataset with only 5% of the data.\\rEstablishing such an open data-effective learning benchmark is crucial for the\\rmedical AI research community because it facilitates efficient data use,\\rpromotes collaborative breakthroughs, and fosters the development of\\rcost-effective, scalable, and impactful healthcare solutions. The project can\\rbe accessed at\\rhttps://github.com/shadow2469/Data-Effective-Learning-A-Comprehensive-Medical-Benchmark.git.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17542 ,  2045kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17544 (*cross-listing*)\\rDate: Wed, 31 Jan 2024 02:18:27 GMT   (438kb,D)\\r\\rTitle: Trainable Fixed-Point Quantization for Deep Learning Acceleration on\\r  FPGAs\\rAuthors: Dingyi Dai, Yichi Zhang, Jiahao Zhang, Zhanqiu Hu, Yaohui Cai, Qi Sun,\\r  Zhiru Zhang\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Quantization is a crucial technique for deploying deep learning models on\\rresource-constrained devices, such as embedded FPGAs. Prior efforts mostly\\rfocus on quantizing matrix multiplications, leaving other layers like BatchNorm\\ror shortcuts in floating-point form, even though fixed-point arithmetic is more\\refficient on FPGAs. A common practice is to fine-tune a pre-trained model to\\rfixed-point for FPGA deployment, but potentially degrading accuracy.\\r  This work presents QFX, a novel trainable fixed-point quantization approach\\rthat automatically learns the binary-point position during model training.\\rAdditionally, we introduce a multiplier-free quantization strategy within QFX\\rto minimize DSP usage. QFX is implemented as a PyTorch-based library that\\refficiently emulates fixed-point arithmetic, supported by FPGA HLS, in a\\rdifferentiable manner during backpropagation. With minimal effort, models\\rtrained with QFX can readily be deployed through HLS, producing the same\\rnumerical results as their software counterparts. Our evaluation shows that\\rcompared to post-training quantization, QFX can quantize models trained with\\relement-wise layers quantized to fewer bits and achieve higher accuracy on both\\rCIFAR-10 and ImageNet datasets. We further demonstrate the efficacy of\\rmultiplier-free quantization using a state-of-the-art binarized neural network\\raccelerator designed for an embedded FPGA (AMD Xilinx Ultra96 v2). We plan to\\rrelease QFX in open-source format.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17544 ,  438kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17571 (*cross-listing*)\\rDate: Wed, 31 Jan 2024 03:28:11 GMT   (15310kb,D)\\r\\rTitle: Is Registering Raw Tagged-MR Enough for Strain Estimation in the Era of\\r  Deep Learning?\\rAuthors: Zhangxing Bian, Ahmed Alshareef, Shuwen Wei, Junyu Chen, Yuli Wang,\\r  Jonghye Woo, Dzung L. Pham, Jiachen Zhuo, Aaron Carass, Jerry L. Prince\\rCategories: eess.IV cs.CV\\rComments: Accepted to SPIE Medical Imaging 2024 (oral)\\r\\\\\\\\\\r  Magnetic Resonance Imaging with tagging (tMRI) has long been utilized for\\rquantifying tissue motion and strain during deformation. However, a phenomenon\\rknown as tag fading, a gradual decrease in tag visibility over time, often\\rcomplicates post-processing. The first contribution of this study is to model\\rtag fading by considering the interplay between $T_1$ relaxation and the\\rrepeated application of radio frequency (RF) pulses during serial imaging\\rsequences. This is a factor that has been overlooked in prior research on tMRI\\rpost-processing. Further, we have observed an emerging trend of utilizing raw\\rtagged MRI within a deep learning-based (DL) registration framework for motion\\restimation. In this work, we evaluate and analyze the impact of commonly used\\rimage similarity objectives in training DL registrations on raw tMRI. This is\\rthen compared with the Harmonic Phase-based approach, a traditional approach\\rwhich is claimed to be robust to tag fading. Our findings, derived from both\\rsimulated images and an actual phantom scan, reveal the limitations of various\\rsimilarity losses in raw tMRI and emphasize caution in registration tasks where\\rimage intensity changes over time.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17571 ,  15310kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17583 (*cross-listing*)\\rDate: Wed, 31 Jan 2024 03:58:28 GMT   (12475kb,D)\\r\\rTitle: Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion\\rAuthors: Tairan He, Chong Zhang, Wenli Xiao, Guanqi He, Changliu Liu, Guanya\\r  Shi\\rCategories: cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY\\rComments: Project website: https://agile-but-safe.github.io/\\r\\\\\\\\\\r  Legged robots navigating cluttered environments must be jointly agile for\\refficient task execution and safe to avoid collisions with obstacles or humans.\\rExisting studies either develop conservative controllers (< 1.0 m/s) to ensure\\rsafety, or focus on agility without considering potentially fatal collisions.\\rThis paper introduces Agile But Safe (ABS), a learning-based control framework\\rthat enables agile and collision-free locomotion for quadrupedal robots. ABS\\rinvolves an agile policy to execute agile motor skills amidst obstacles and a\\rrecovery policy to prevent failures, collaboratively achieving high-speed and\\rcollision-free navigation. The policy switch in ABS is governed by a learned\\rcontrol-theoretic reach-avoid value network, which also guides the recovery\\rpolicy as an objective function, thereby safeguarding the robot in a closed\\rloop. The training process involves the learning of the agile policy, the\\rreach-avoid value network, the recovery policy, and an exteroception\\rrepresentation network, all in simulation. These trained modules can be\\rdirectly deployed in the real world with onboard sensing and computation,\\rleading to high-speed and collision-free navigation in confined indoor and\\routdoor spaces with both static and dynamic obstacles.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17583 ,  12475kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17593 (*cross-listing*)\\rDate: Wed, 31 Jan 2024 04:34:31 GMT   (497kb,D)\\r\\rTitle: Head and Neck Tumor Segmentation from [18F]F-FDG PET/CT Images Based on\\r  3D Diffusion Model\\rAuthors: Yafei Dong and Kuang Gong\\rCategories: eess.IV cs.CV physics.med-ph\\rComments: 28 pages, 5 figures\\r\\\\\\\\\\r  Head and neck (H&N) cancers are among the most prevalent types of cancer\\rworldwide, and [18F]F-FDG PET/CT is widely used for H&N cancer management.\\rRecently, the diffusion model has demonstrated remarkable performance in\\rvarious image-generation tasks. In this work, we proposed a 3D diffusion model\\rto accurately perform H&N tumor segmentation from 3D PET and CT volumes. The 3D\\rdiffusion model was developed considering the 3D nature of PET and CT images\\racquired. During the reverse process, the model utilized a 3D UNet structure\\rand took the concatenation of PET, CT, and Gaussian noise volumes as the\\rnetwork input to generate the tumor mask. Experiments based on the HECKTOR\\rchallenge dataset were conducted to evaluate the effectiveness of the proposed\\rdiffusion model. Several state-of-the-art techniques based on U-Net and\\rTransformer structures were adopted as the reference methods. Benefits of\\remploying both PET and CT as the network input as well as further extending the\\rdiffusion model from 2D to 3D were investigated based on various quantitative\\rmetrics and the uncertainty maps generated. Results showed that the proposed 3D\\rdiffusion model could generate more accurate segmentation results compared with\\rother methods. Compared to the diffusion model in 2D format, the proposed 3D\\rmodel yielded superior results. Our experiments also highlighted the advantage\\rof utilizing dual-modality PET and CT data over only single-modality data for\\rH&N tumor segmentation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17593 ,  497kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17695 (*cross-listing*)\\rDate: Wed, 31 Jan 2024 09:31:28 GMT   (10121kb,D)\\r\\rTitle: Datacube segmentation via Deep Spectral Clustering\\rAuthors: Alessandro Bombini and Fernando Garc\\\\'ia-Avello Bof\\\\'ias and Caterina\\r  Bracci and Michele Ginolfi and Chiara Ruberto\\rCategories: cs.LG cs.CV physics.app-ph\\rComments: 20 pages, 10 figures, doi for code repository, dataset and trained\\r  model available and reported in the paper\\r\\\\\\\\\\r  Extended Vision techniques are ubiquitous in physics. However, the data cubes\\rsteaming from such analysis often pose a challenge in their interpretation, due\\rto the intrinsic difficulty in discerning the relevant information from the\\rspectra composing the data cube.\\r  Furthermore, the huge dimensionality of data cube spectra poses a complex\\rtask in its statistical interpretation; nevertheless, this complexity contains\\ra massive amount of statistical information that can be exploited in an\\runsupervised manner to outline some essential properties of the case study at\\rhand, e.g.~it is possible to obtain an image segmentation via (deep) clustering\\rof data-cube's spectra, performed in a suitably defined low-dimensional\\rembedding space.\\r  To tackle this topic, we explore the possibility of applying unsupervised\\rclustering methods in encoded space, i.e. perform deep clustering on the\\rspectral properties of datacube pixels. A statistical dimensional reduction is\\rperformed by an ad hoc trained (Variational) AutoEncoder, in charge of mapping\\rspectra into lower dimensional metric spaces, while the clustering process is\\rperformed by a (learnable) iterative K-Means clustering algorithm.\\r  We apply this technique to two different use cases, of different physical\\rorigins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data on\\rpictorial artworks, and a dataset of simulated astrophysical observations.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17695 ,  10121kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17790 (*cross-listing*)\\rDate: Wed, 31 Jan 2024 12:32:18 GMT   (503kb,D)\\r\\rTitle: RADIN: Souping on a Budget\\rAuthors: Thibaut Menes and Olivier Risser-Maroix\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Model Soups, extending Stochastic Weights Averaging (SWA), combine models\\rfine-tuned with different hyperparameters. Yet, their adoption is hindered by\\rcomputational challenges due to subset selection issues. In this paper, we\\rpropose to speed up model soups by approximating soups performance using\\raveraged ensemble logits performances. Theoretical insights validate the\\rcongruence between ensemble logits and weight averaging soups across any mixing\\rratios. Our Resource ADjusted soups craftINg (RADIN) procedure stands out by\\rallowing flexible evaluation budgets, enabling users to adjust his budget of\\rexploration adapted to his resources while increasing performance at lower\\rbudget compared to previous greedy approach (up to 4% on ImageNet).\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17790 ,  503kb)\\r%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.10558\\rreplaced with revised version Wed, 31 Jan 2024 13:14:02 GMT   (508kb,D)\\r\\rTitle: On-the-fly Denoising for Data Augmentation in Natural Language\\r  Understanding\\rAuthors: Tianqing Fang, Wenxuan Zhou, Fangyu Liu, Hongming Zhang, Yangqiu Song,\\r  Muhao Chen\\rCategories: cs.CL cs.AI\\rComments: Findings of EACL 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2212.10558 ,  508kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.10767\\rreplaced with revised version Wed, 31 Jan 2024 04:10:30 GMT   (41kb,D)\\r\\rTitle: How Does Beam Search improve Span-Level Confidence Estimation in\\r  Generative Sequence Labeling?\\rAuthors: Kazuma Hashimoto and Iftekhar Naim and Karthik Raman\\rCategories: cs.CL\\rComments: UncertaiNLP 2024 (an EACL 2024 workshop:\\r  https://uncertainlp.github.io/)\\r\\\\\\\\ ( https://arxiv.org/abs/2212.10767 ,  41kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14205\\rreplaced with revised version Wed, 31 Jan 2024 13:28:58 GMT   (8368kb,D)\\r\\rTitle: $\\\\mu$PLAN: Summarizing using a Content Plan as Cross-Lingual Bridge\\rAuthors: Fantine Huot, Joshua Maynez, Chris Alberti, Reinald Kim Amplayo,\\r  Priyanka Agrawal, Constanza Fierro, Shashi Narayan, Mirella Lapata\\rCategories: cs.CL\\rComments: EACL 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14205 ,  8368kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14341\\rreplaced with revised version Wed, 31 Jan 2024 02:32:19 GMT   (5960kb,D)\\r\\rTitle: APPLS: Evaluating Evaluation Metrics for Plain Language Summarization\\rAuthors: Yue Guo, Tal August, Gondy Leroy, Trevor Cohen, Lucy Lu Wang\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14341 ,  5960kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.15002\\rreplaced with revised version Wed, 31 Jan 2024 03:56:22 GMT   (7690kb,D)\\r\\rTitle: A RelEntLess Benchmark for Modelling Graded Relations between Named\\r  Entities\\rAuthors: Asahi Ushio and Jose Camacho Collados and Steven Schockaert\\rCategories: cs.CL cs.LG\\rComments: EACL 2024 main conference\\r\\\\\\\\ ( https://arxiv.org/abs/2305.15002 ,  7690kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.00162\\rreplaced with revised version Wed, 31 Jan 2024 05:00:25 GMT   (13661kb,D)\\r\\rTitle: What Do Self-Supervised Speech Models Know About Words?\\rAuthors: Ankita Pasad, Chung-Ming Chien, Shane Settle, Karen Livescu\\rCategories: cs.CL cs.LG eess.AS\\rComments: Pre-MIT Press publication version\\r\\\\\\\\ ( https://arxiv.org/abs/2307.00162 ,  13661kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.08648\\rreplaced with revised version Wed, 31 Jan 2024 02:36:48 GMT   (5633kb,D)\\r\\rTitle: MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings\\rAuthors: Yonchanok Khaokaew, Hao Xue, Flora D. Salim\\rCategories: cs.CL cs.AI\\rDOI: 10.1145/3643514\\r\\\\\\\\ ( https://arxiv.org/abs/2309.08648 ,  5633kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.08929\\rreplaced with revised version Wed, 31 Jan 2024 14:25:15 GMT   (7366kb,D)\\r\\rTitle: Leveraging Multi-lingual Positive Instances in Contrastive Learning to\\r  Improve Sentence Embedding\\rAuthors: Kaiyan Zhao, Qiyu Wu, Xin-Qiang Cai, Yoshimasa Tsuruoka\\rCategories: cs.CL\\rComments: Accepted to EACL 2024, main conference\\r\\\\\\\\ ( https://arxiv.org/abs/2309.08929 ,  7366kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.08958\\rreplaced with revised version Wed, 31 Jan 2024 03:42:04 GMT   (39kb,D)\\r\\rTitle: Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\r  Alpaca\\rAuthors: Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Andrey Kutuzov, Barry\\r  Haddow, Kenneth Heafield\\rCategories: cs.CL cs.AI\\rComments: Accepted to Findings of ACL: EACL 2024. Added human evaluation and\\r  shortened writing\\r\\\\\\\\ ( https://arxiv.org/abs/2309.08958 ,  39kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09048\\rreplaced with revised version Tue, 30 Jan 2024 20:21:00 GMT   (14458kb,D)\\r\\rTitle: GRASP: A novel benchmark for evaluating language GRounding And Situated\\r  Physics understanding in multimodal language models\\rAuthors: Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia\\r  Ohmer, Elia Bruni\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09048 ,  14458kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.15623\\rreplaced with revised version Wed, 31 Jan 2024 06:58:51 GMT   (2808kb,D)\\r\\rTitle: Injecting linguistic knowledge into BERT for Dialogue State Tracking\\rAuthors: Xiaohan Feng, Xixin Wu, Helen Meng\\rCategories: cs.CL cs.AI cs.LG\\rComments: This work has been submitted to the IEEE for possible publication.\\r  Copyright may be transferred without notice, after which this version may no\\r  longer be accessible\\r\\\\\\\\ ( https://arxiv.org/abs/2311.15623 ,  2808kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.00949\\rreplaced with revised version Tue, 30 Jan 2024 21:32:31 GMT   (4105kb,D)\\r\\rTitle: Hyperparameter Optimization for Large Language Model Instruction-Tuning\\rAuthors: Christophe Tribes, Sacha Benarroch-Lelong, Peng Lu, Ivan Kobyzev\\rCategories: cs.CL math.OC\\r\\\\\\\\ ( https://arxiv.org/abs/2312.00949 ,  4105kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.03863\\rreplaced with revised version Wed, 31 Jan 2024 11:29:40 GMT   (816kb,D)\\r\\rTitle: Efficient Large Language Models: A Survey\\rAuthors: Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu,\\r  Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang\\rCategories: cs.CL cs.AI\\rComments: Version 3: Added more latest papers\\r\\\\\\\\ ( https://arxiv.org/abs/2312.03863 ,  816kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.07518\\rreplaced with revised version Wed, 31 Jan 2024 06:20:32 GMT   (2768kb,D)\\r\\rTitle: Survey of Natural Language Processing for Education: Taxonomy,\\r  Systematic Review, and Future Trends\\rAuthors: Yunshi Lan, Xinyuan Li, Hanyue Du, Xuesong Lu, Ming Gao, Weining Qian,\\r  Aoying Zhou\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2401.07518 ,  2768kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.08694\\rreplaced with revised version Tue, 30 Jan 2024 21:59:08 GMT   (327kb,D)\\r\\rTitle: Combining Confidence Elicitation and Sample-based Methods for\\r  Uncertainty Quantification in Misinformation Mitigation\\rAuthors: Mauricio Rivera, Jean-Fran\\\\c{c}ois Godbout, Reihaneh Rabbany, Kellin\\r  Pelrine\\rCategories: cs.CL cs.AI\\rComments: 12 pages, 11 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2401.08694 ,  327kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.11864\\rreplaced with revised version Wed, 31 Jan 2024 03:50:07 GMT   (8638kb,D)\\r\\rTitle: Improving Small Language Models' Mathematical Reasoning via\\r  Equation-of-Thought Distillation\\rAuthors: Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2401.11864 ,  8638kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.12292\\rreplaced with revised version Wed, 31 Jan 2024 06:44:42 GMT   (14979kb,D)\\r\\rTitle: GRATH: Gradual Self-Truthifying for Large Language Models\\rAuthors: Weixin Chen, Dawn Song, Bo Li\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2401.12292 ,  14979kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.14440\\rreplaced with revised version Wed, 31 Jan 2024 10:52:52 GMT   (7082kb,D)\\r\\rTitle: Semantic Sensitivities and Inconsistent Predictions: Measuring the\\r  Fragility of NLI Models\\rAuthors: Erik Arakelyan, Zhaoqi Liu, Isabelle Augenstein\\rCategories: cs.CL cs.AI cs.CY cs.LG\\rComments: EACL 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2401.14440 ,  7082kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.15378\\rreplaced with revised version Wed, 31 Jan 2024 12:39:06 GMT   (875kb)\\r\\rTitle: A RAG-based Question Answering System Proposal for Understanding Islam:\\r  MufassirQAS LLM\\rAuthors: Ahmet Yusuf Alan, Enis Karaarslan, \\\\Omer Aydin\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2401.15378 ,  875kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.15496\\rreplaced with revised version Wed, 31 Jan 2024 17:36:29 GMT   (2176kb,D)\\r\\rTitle: Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue\\r  Summarization\\rAuthors: Jianfei Xiao, Yancan Chen, Yimin Ou, Hanyi Yu, Yiyong Xiao\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2401.15496 ,  2176kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.16092\\rreplaced with revised version Wed, 31 Jan 2024 08:33:37 GMT   (19141kb,D)\\r\\rTitle: Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and\\r  Prompt Engineering May Not Help You\\rAuthors: Felix Friedrich, Katharina H\\\\ammerl, Patrick Schramowski, Jindrich\\r  Libovicky, Kristian Kersting, Alexander Fraser\\rCategories: cs.CL cs.CY cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2401.16092 ,  19141kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.16403\\rreplaced with revised version Wed, 31 Jan 2024 07:59:16 GMT   (7914kb,D)\\r\\rTitle: ViLexNorm: A Lexical Normalization Corpus for Vietnamese Social Media\\r  Text\\rAuthors: Thanh-Nhi Nguyen, Thanh-Phong Le, Kiet Van Nguyen\\rCategories: cs.CL\\rComments: Accepted at the EACL 2024 Main Conference\\r\\\\\\\\ ( https://arxiv.org/abs/2401.16403 ,  7914kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2209.09641\\rreplaced with revised version Wed, 31 Jan 2024 01:33:16 GMT   (3303kb,D)\\r\\rTitle: Calibrating Segmentation Networks with Margin-based Label Smoothing\\rAuthors: Balamurali Murugesan, Bingyuan Liu, Adrian Galdran, Ismail Ben Ayed,\\r  Jose Dolz\\rCategories: cs.CV\\rComments: MedIA 2023. The code is available at\\r  https://github.com/Bala93/MarginLoss. arXiv admin note: substantial text\\r  overlap with arXiv:2111.15430\\r\\\\\\\\ ( https://arxiv.org/abs/2209.09641 ,  3303kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.06432\\rreplaced with revised version Wed, 31 Jan 2024 16:50:58 GMT   (813kb,D)\\r\\rTitle: A Deep Learning-based Global and Segmentation-based Semantic Feature\\r  Fusion Approach for Indoor Scene Classification\\rAuthors: Ricardo Pereira, Tiago Barros, Luis Garrote, Ana Lopes, Urbano J.\\r  Nunes\\rCategories: cs.CV\\rComments: Published at Pattern Recognition Letters 2024 (DOI:\\r  10.1016/j.patrec.2024.01.022)\\rJournal-ref: Pattern Recognition Letters, vol 179, pp. 24-30, 2024\\rDOI: 10.1016/j.patrec.2024.01.022\\r\\\\\\\\ ( https://arxiv.org/abs/2302.06432 ,  813kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.09171\\rreplaced with revised version Wed, 31 Jan 2024 06:27:21 GMT   (1522kb,D)\\r\\rTitle: Empowering CAM-Based Methods with Capability to Generate Fine-Grained\\r  and High-Faithfulness Explanations\\rAuthors: Changqing Qiu, Fusheng Jin, Yining Zhang\\rCategories: cs.CV\\rComments: This paper has been accepted by AAAI2024\\r\\\\\\\\ ( https://arxiv.org/abs/2303.09171 ,  1522kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.14346\\rreplaced with revised version Wed, 31 Jan 2024 16:00:54 GMT   (22541kb,D)\\r\\rTitle: Collaborative Multi-Object Tracking with Conformal Uncertainty\\r  Propagation\\rAuthors: Sanbao Su, Songyang Han, Yiming Li, Zhili Zhang, Chen Feng, Caiwen\\r  Ding, Fei Miao\\rCategories: cs.CV\\rComments: This paper has been accepted by IEEE Robotics and Automation Letters\\r\\\\\\\\ ( https://arxiv.org/abs/2303.14346 ,  22541kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.11588\\rreplaced with revised version Wed, 31 Jan 2024 10:15:08 GMT   (41965kb,D)\\r\\rTitle: Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields\\rAuthors: Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao\\rCategories: cs.CV cs.GR\\rComments: Accepted by TVCG; Homepage:\\r  https://eckertzhang.github.io/Text2NeRF.github.io/\\r  Code:https://github.com/eckertzhang/Text2NeRF\\r\\\\\\\\ ( https://arxiv.org/abs/2305.11588 ,  41965kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.05658\\rreplaced with revised version Wed, 31 Jan 2024 09:30:09 GMT   (4757kb,D)\\r\\rTitle: GMS-3DQA: Projection-based Grid Mini-patch Sampling for 3D Model Quality\\r  Assessment\\rAuthors: Zicheng Zhang, Wei Sun, Houning Wu, Yingjie Zhou, Chunyi Li, Xiongkuo\\r  Min, Guangtao Zhai, Weisi Lin\\rCategories: cs.CV eess.IV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.05658 ,  4757kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.10895\\rreplaced with revised version Wed, 31 Jan 2024 12:40:51 GMT   (27595kb,D)\\r\\rTitle: Variational Autoencoding of Dental Point Clouds\\rAuthors: Johan Ziruo Ye, Thomas {\\\\O}rkild, Peter Lempel S{\\\\o}ndergaard,\\r  S{\\\\o}ren Hauberg\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2307.10895 ,  27595kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.07490\\rreplaced with revised version Wed, 31 Jan 2024 03:07:35 GMT   (2646kb,D)\\r\\rTitle: BSED: Baseline Shapley-Based Explainable Detector\\rAuthors: Michihiro Kuroki, Toshihiko Yamasaki\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2308.07490 ,  2646kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.11199\\rreplaced with revised version Wed, 31 Jan 2024 14:11:56 GMT   (785kb,D)\\r\\rTitle: ConcatPlexer: Additional Dim1 Batching for Faster ViTs\\rAuthors: Donghoon Han, Seunghyeon Seo, Donghyeon Jeon, Jiho Jang, Chaerin Kong\\r  and Nojun Kwak\\rCategories: cs.CV cs.AI cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2308.11199 ,  785kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.02169\\rreplaced with revised version Wed, 31 Jan 2024 06:18:14 GMT   (0kb,I)\\r\\rTitle: Dual Relation Alignment for Composed Image Retrieval\\rAuthors: Xintong Jiang, Yaxiong Wang, Yujiao Wu, Meng Wang, Xueming Qian\\rCategories: cs.CV cs.AI\\rComments: The architecture of our model changes, hence methodolgy and\\r  experiments changes a lot, We have significantly revised the original\\r  manuscript of the paper, so a withdraw of our original script is needed\\r\\\\\\\\ ( https://arxiv.org/abs/2309.02169 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.03160\\rreplaced with revised version Wed, 31 Jan 2024 10:02:49 GMT   (5921kb,D)\\r\\rTitle: ResFields: Residual Neural Fields for Spatiotemporal Signals\\rAuthors: Marko Mihajlovic, Sergey Prokudin, Marc Pollefeys, Siyu Tang\\rCategories: cs.CV\\rComments: [ICLR 2024 Spotlight] Project and code at:\\r  https://markomih.github.io/ResFields/\\r\\\\\\\\ ( https://arxiv.org/abs/2309.03160 ,  5921kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.05930\\rreplaced with revised version Wed, 31 Jan 2024 16:11:27 GMT   (2160kb,D)\\r\\rTitle: Combining Deep Learning and Street View Imagery to Map Smallholder Crop\\r  Types\\rAuthors: Jordi Laguarta Soler, Thomas Friedel, Sherrie Wang\\rCategories: cs.CV cs.AI\\rComments: Accepted to AAAI-24: Special Track on AI for Social Impact\\r\\\\\\\\ ( https://arxiv.org/abs/2309.05930 ,  2160kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.10896\\rreplaced with revised version Wed, 31 Jan 2024 11:47:24 GMT   (5478kb,D)\\r\\rTitle: PLVS: A SLAM System with Points, Lines, Volumetric Mapping, and 3D\\r  Incremental Segmentation\\rAuthors: Luigi Freda\\rCategories: cs.CV cs.RO\\r\\\\\\\\ ( https://arxiv.org/abs/2309.10896 ,  5478kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.15589\\rreplaced with revised version Wed, 31 Jan 2024 13:19:38 GMT   (6660kb,D)\\r\\rTitle: Domain generalization across tumor types, laboratories, and species --\\r  insights from the 2022 edition of the Mitosis Domain Generalization Challenge\\rAuthors: Marc Aubreville, Nikolas Stathonikos, Taryn A. Donovan, Robert\\r  Klopfleisch, Jonathan Ganz, Jonas Ammeling, Frauke Wilm, Mitko Veta, Samir\\r  Jabari, Markus Eckstein, Jonas Annuscheit, Christian Krumnow, Engin Bozaba,\\r  Sercan Cayir, Hongyan Gu, Xiang 'Anthony' Chen, Mostafa Jahanifar, Adam\\r  Shephard, Satoshi Kondo, Satoshi Kasai, Sujatha Kotte, VG Saipradeep, Maxime\\r  W. Lafarge, Viktor H. Koelzer, Ziyue Wang, Yongbing Zhang, Sen Yang, Xiyue\\r  Wang, Katharina Breininger, Christof A. Bertram\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.15589 ,  6660kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.19413\\rreplaced with revised version Wed, 31 Jan 2024 14:31:23 GMT   (8014kb,D)\\r\\rTitle: CARPE-ID: Continuously Adaptable Re-identification for Personalized\\r  Robot Assistance\\rAuthors: Federico Rollo, Andrea Zunino, Nikolaos Tsagarakis, Enrico Mingo\\r  Hoffman, Arash Ajoudani\\rCategories: cs.CV cs.RO\\rComments: Accepted to the International Conference on Robotics and Automation\\r  (ICRA) 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.19413 ,  8014kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.20381\\rreplaced with revised version Tue, 30 Jan 2024 19:32:19 GMT   (36565kb,D)\\r\\rTitle: A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical\\r  Image Analysis\\rAuthors: Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lei Wang, Lingqiao\\r  Liu, Leyang Cui, Zhaopeng Tu, Longyue Wang, Luping Zhou\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2310.20381 ,  36565kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.01886\\rreplaced with revised version Wed, 31 Jan 2024 12:13:49 GMT   (16022kb,D)\\r\\rTitle: Bridging the Gap between Multi-focus and Multi-modal: A Focused\\r  Integration Framework for Multi-modal Image Fusion\\rAuthors: Xilai Li, Xiaosong Li, Tao Ye, Xiaoqi Cheng, Wuyang Liu, Haishu Tan\\rCategories: cs.CV\\rComments: Accepted to IEEE/CVF Winter Conference on Applications of Computer\\r  Vision (WACV) 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.01886 ,  16022kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.11319\\rreplaced with revised version Tue, 30 Jan 2024 22:51:22 GMT   (5339kb,D)\\r\\rTitle: GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for\\r  Automated Segmentation of Mobility Infrastructure\\rAuthors: Rafi Ibn Sultan, Chengyin Li, Hui Zhu, Prashant Khanduri, Marco\\r  Brocanelli, Dongxiao Zhu\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2311.11319 ,  5339kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.12076\\rreplaced with revised version Wed, 31 Jan 2024 02:01:11 GMT   (11738kb,D)\\r\\rTitle: Towards Few-shot Out-of-Distribution Detection\\rAuthors: Jiuqing Dong, Yongbin Gao, Heng Zhou, Jun Cen, Yifan Yao, Sook Yoon,\\r  Park Dong Sun\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2311.12076 ,  11738kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.12831\\rreplaced with revised version Wed, 31 Jan 2024 03:53:31 GMT   (33179kb,D)\\r\\rTitle: ECNR: Efficient Compressive Neural Representation of Time-Varying\\r  Volumetric Datasets\\rAuthors: Kaiyuan Tang and Chaoli Wang\\rCategories: cs.CV cs.GR cs.LG\\rComments: Accepted by IEEE PacificVis 2024 (conference papers track)\\r\\\\\\\\ ( https://arxiv.org/abs/2311.12831 ,  33179kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.14097\\rreplaced with revised version Tue, 30 Jan 2024 19:32:59 GMT   (18258kb,D)\\r\\rTitle: ACT: Adversarial Consistency Models\\rAuthors: Fei Kong, Jinhao Duan, Lichao Sun, Hao Cheng, Renjing Xu, Hengtao\\r  Shen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2311.14097 ,  18258kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.06712\\rreplaced with revised version Wed, 31 Jan 2024 18:44:22 GMT   (41767kb,D)\\r\\rTitle: Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion\\r  Models\\rAuthors: Zhipeng Bao and Yijun Li and Krishna Kumar Singh and Yu-Xiong Wang and\\r  Martial Hebert\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2312.06712 ,  41767kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12431\\rreplaced with revised version Wed, 31 Jan 2024 10:57:40 GMT   (15400kb,D)\\r\\rTitle: On Inference Stability for Diffusion Models\\rAuthors: Viet Nguyen, Giang Vu, Tung Nguyen Thanh, Khoat Than, Toan Tran\\rCategories: cs.CV\\rComments: Oral presentation at AAAI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12431 ,  15400kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00028\\rreplaced with revised version Wed, 31 Jan 2024 12:34:48 GMT   (2432kb,D)\\r\\rTitle: An Empirical Study of Scaling Law for OCR\\rAuthors: Miao Rang, Zhenni Bi, Chuanjian Liu, Yunhe Wang, Kai Han\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00028 ,  2432kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.06550\\rreplaced with revised version Wed, 31 Jan 2024 18:13:53 GMT   (10625kb)\\r\\rTitle: Multimodal Urban Areas of Interest Generation via Remote Sensing Imagery\\r  and Geographical Prior\\rAuthors: Chuanji Shi, Yingying Zhang, Jiaotuan Wang, Xin Guo and Qiqi Zhu\\rCategories: cs.CV cs.AI\\rComments: 9 pages, 9 figures\\rMSC-class: 68T99\\rACM-class: I.4.9\\r\\\\\\\\ ( https://arxiv.org/abs/2401.06550 ,  10625kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.08357\\rreplaced with revised version Wed, 31 Jan 2024 12:18:10 GMT   (9647kb,D)\\r\\rTitle: SAMF: Small-Area-Aware Multi-focus Image Fusion for Object Detection\\rAuthors: Xilai Li, Xiaosong Li, Haishu Tan, Jinyang Li\\rCategories: cs.CV\\rComments: Accepted to International Conference on Acoustics, Speech and Signal\\r  Processing (ICASSP) 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2401.08357 ,  9647kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.09742\\rreplaced with revised version Tue, 30 Jan 2024 22:49:18 GMT   (13754kb,D)\\r\\rTitle: Image Translation as Diffusion Visual Programmers\\rAuthors: Cheng Han, James C. Liang, Qifan Wang, Majid Rabbani, Sohail Dianat,\\r  Raghuveer Rao, Ying Nian Wu, Dongfang Liu\\rCategories: cs.CV\\rComments: 25 pages, 20 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2401.09742 ,  13754kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.11395\\rreplaced with revised version Wed, 31 Jan 2024 06:31:59 GMT   (1159kb,D)\\r\\rTitle: UniM-OV3D: Uni-Modality Open-Vocabulary 3D Scene Understanding with\\r  Fine-Grained Feature Representation\\rAuthors: Qingdong He, Jinlong Peng, Zhengkai Jiang, Kai Wu, Xiaozhong Ji,\\r  Jiangning Zhang, Yabiao Wang, Chengjie Wang, Mingang Chen, Yunsheng Wu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.11395 ,  1159kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.13554\\rreplaced with revised version Wed, 31 Jan 2024 15:54:10 GMT   (30498kb,D)\\r\\rTitle: PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour\\r  Recognition\\rAuthors: Otto Brookes, Majid Mirmehdi, Colleen Stephens, Samuel Angedakin,\\r  Katherine Corogenes, Dervla Dowd, Paula Dieguez, Thurston C. Hicks, Sorrel\\r  Jones, Kevin Lee, Vera Leinert, Juan Lapuente, Maureen S. McCarthy, Amelia\\r  Meier, Mizuki Murai, Emmanuelle Normand, Virginie Vergnes, Erin G. Wessling,\\r  Roman M. Wittig, Kevin Langergraber, Nuria Maldonado, Xinyu Yang, Klaus\\r  Zuberbuhler, Christophe Boesch, Mimi Arandjelovic, Hjalmar Kuhl, Tilo\\r  Burghardt\\rCategories: cs.CV\\rComments: Accepted at IJCV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.13554 ,  30498kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.14718\\rreplaced with revised version Wed, 31 Jan 2024 05:09:44 GMT   (175kb,D)\\r\\rTitle: A Survey on Video Prediction: From Deterministic to Generative\\r  Approaches\\rAuthors: Ruibo Ming, Zhewei Huang, Zhuoxuan Ju, Jianming Hu, Lihui Peng,\\r  Shuchang Zhou\\rCategories: cs.CV\\rComments: under review\\r\\\\\\\\ ( https://arxiv.org/abs/2401.14718 ,  175kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.15885\\rreplaced with revised version Wed, 31 Jan 2024 12:41:05 GMT   (5755kb,D)\\r\\rTitle: Rectify the Regression Bias in Long-Tailed Object Detection\\rAuthors: Ke Zhu, Minghao Fu, Jie Shao, Tianyu Liu, Jianxin Wu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.15885 ,  5755kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.15893\\rreplaced with revised version Wed, 31 Jan 2024 03:53:05 GMT   (2888kb,D)\\r\\rTitle: Arbitrary-Scale Downscaling of Tidal Current Data Using Implicit\\r  Continuous Representation\\rAuthors: Dongheon Lee, Seungmyong Jeong, Youngmin Ro\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.15893 ,  2888kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.15977\\rreplaced with revised version Wed, 31 Jan 2024 07:41:04 GMT   (32102kb,D)\\r\\rTitle: Motion-I2V: Consistent and Controllable Image-to-Video Generation with\\r  Explicit Motion Modeling\\rAuthors: Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi\\r  Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai,\\r  Hongsheng Li\\rCategories: cs.CV\\rComments: Project page: https://xiaoyushi97.github.io/Motion-I2V/\\r\\\\\\\\ ( https://arxiv.org/abs/2401.15977 ,  32102kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.16416\\rreplaced with revised version Wed, 31 Jan 2024 15:51:45 GMT   (497kb,D)\\r\\rTitle: Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian\\r  Splatting\\rAuthors: Yiming Huang and Beilei Cui and Long Bai and Ziqi Guo and Mengya Xu\\r  and Hongliang Ren\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.16416 ,  497kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.16468\\rreplaced with revised version Wed, 31 Jan 2024 18:54:15 GMT   (42471kb,D)\\r\\rTitle: High-Quality Image Restoration Following Human Instructions\\rAuthors: Marcos V. Conde, Gregor Geigle, Radu Timofte\\rCategories: cs.CV cs.LG eess.IV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.16468 ,  42471kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17053\\rreplaced with revised version Wed, 31 Jan 2024 14:53:22 GMT   (7793kb,D)\\r\\rTitle: BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane\\r  Extrapolation\\rAuthors: Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang,\\r  Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, and Pan Ji\\rCategories: cs.CV cs.AI cs.GR\\rComments: Video: https://www.youtube.com/watch?v=PxIBtd6G0mA\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17053 ,  7793kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2209.08316\\rreplaced with revised version Wed, 31 Jan 2024 15:49:34 GMT   (1161kb,D)\\r\\rTitle: An Empathetic AI Coach for Self-Attachment Therapy\\rAuthors: Lisa Alazraki, Ali Ghachem, Neophytos Polydorou, Foaad Khosmood and\\r  Abbas Edalat\\rCategories: cs.AI cs.CL cs.LG\\rJournal-ref: 2021 IEEE Third International Conference on Cognitive Machine\\r  Intelligence (CogMI), 2021, pp. 78-87\\rDOI: 10.1109/CogMI52975.2021.00019\\r\\\\\\\\ ( https://arxiv.org/abs/2209.08316 ,  1161kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.15176\\rreplaced with revised version Wed, 31 Jan 2024 12:40:40 GMT   (1642kb,D)\\r\\rTitle: RCT Rejection Sampling for Causal Estimation Evaluation\\rAuthors: Katherine A. Keith, Sergey Feldman, David Jurgens, Jonathan Bragg,\\r  Rohit Bhattacharya\\rCategories: cs.AI cs.CL cs.LG stat.ME\\rComments: Code and data at https://github.com/kakeith/rct_rejection_sampling\\rJournal-ref: Transactions on Machine Learning Research (TMLR) 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2307.15176 ,  1642kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.04645 (*cross-listing*)\\rreplaced with revised version Wed, 31 Jan 2024 09:54:43 GMT   (5096kb,D)\\r\\rTitle: Do self-supervised speech and language models extract similar\\r  representations as human brain?\\rAuthors: Peili Chen, Linyang He, Li Fu, Lu Fan, Edward F. Chang, Yuanning Li\\rCategories: q-bio.NC cs.AI cs.CL eess.AS\\rComments: To appear in 2024 IEEE International Conference on Acoustics, Speech\\r  and Signal Processing\\r\\\\\\\\ ( https://arxiv.org/abs/2310.04645 ,  5096kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.20703\\rreplaced with revised version Wed, 31 Jan 2024 12:39:06 GMT   (16827kb,D)\\r\\rTitle: Vanishing Gradients in Reinforcement Finetuning of Language Models\\rAuthors: Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley,\\r  Preetum Nakkiran, Joshua Susskind, Etai Littwin\\rCategories: cs.LG cs.AI cs.CL stat.ML\\rComments: Accepted to ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.20703 ,  16827kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.11143\\rreplaced with revised version Wed, 31 Jan 2024 01:22:43 GMT   (4157kb,D)\\r\\rTitle: Gaussian Adaptive Attention is All You Need: Robust Contextual\\r  Representations Across Multiple Modalities\\rAuthors: Georgios Ioannides, Aman Chadha, Aaron Elkins\\rCategories: cs.LG cs.AI cs.CL cs.CV cs.SD eess.AS eess.SP\\r\\\\\\\\ ( https://arxiv.org/abs/2401.11143 ,  4157kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2006.02570 (*cross-listing*)\\rreplaced with revised version Wed, 24 Jan 2024 21:39:38 GMT   (81104kb,D)\\r\\rTitle: Exploration of Interpretability Techniques for Deep COVID-19\\r  Classification using Chest X-ray Images\\rAuthors: Soumick Chatterjee, Fatima Saad, Chompunuch Sarasaen, Suhita Ghosh,\\r  Valerie Krug, Rupali Khatun, Rahul Mishra, Nirja Desai, Petia Radeva, Georg\\r  Rose, Sebastian Stober, Oliver Speck, Andreas N\\\\urnberger\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2006.02570 ,  81104kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.09196\\rreplaced with revised version Wed, 31 Jan 2024 05:30:08 GMT   (44172kb,D)\\r\\rTitle: Learning to Predict Gradients for Semi-Supervised Continual Learning\\rAuthors: Yan Luo, Yongkang Wong, Mohan Kankanhalli, Qi Zhao\\rCategories: cs.LG cs.CV\\rComments: Accepted by IEEE Transactions on Neural Networks and Learning Systems\\r  (TNNLS)\\rJournal-ref: IEEE Transactions on Neural Networks and Learning Systems, 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2201.09196 ,  44172kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2203.01327 (*cross-listing*)\\rreplaced with revised version Wed, 31 Jan 2024 00:40:51 GMT   (3024kb,D)\\r\\rTitle: Hyperspectral Pixel Unmixing with Latent Dirichlet Variational\\r  Autoencoder\\rAuthors: Kiran Mantripragada and Faisal Z. Qureshi\\rCategories: eess.IV cs.CV cs.LG\\rDOI: 10.1109/TGRS.2024.3357589\\r\\\\\\\\ ( https://arxiv.org/abs/2203.01327 ,  3024kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2205.15523\\rreplaced with revised version Wed, 31 Jan 2024 05:30:22 GMT   (15355kb,D)\\r\\rTitle: Variational Transfer Learning using Cross-Domain Latent Modulation\\rAuthors: Jinyong Hou, Jeremiah D. Deng, Stephen Cranefield, Xuejie Din\\rCategories: cs.LG cs.AI cs.CV\\rComments: Under review. Extended version of a previous WACV paper\\r  (arXiv:2012.11727). 13 pages, 8 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2205.15523 ,  15355kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.06225\\rreplaced with revised version Wed, 31 Jan 2024 15:50:17 GMT   (116kb,D)\\r\\rTitle: On the Generalizability of ECG-based Stress Detection Models\\rAuthors: Pooja Prajod, Elisabeth Andr\\\\'e\\rCategories: cs.LG cs.AI cs.CV\\rComments: Published in Proceedings of 2022 21st IEEE International Conference\\r  on Machine Learning and Applications (ICMLA)\\rDOI: 10.1109/ICMLA55696.2022.00090\\r\\\\\\\\ ( https://arxiv.org/abs/2210.06225 ,  116kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.13530\\rreplaced with revised version Wed, 31 Jan 2024 17:29:26 GMT   (8201kb,D)\\r\\rTitle: Domain-Generalizable Multiple-Domain Clustering\\rAuthors: Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum\\rCategories: cs.LG cs.CV\\rComments: 13 pages, 3 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2301.13530 ,  8201kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.12862\\rreplaced with revised version Wed, 31 Jan 2024 01:05:14 GMT   (14170kb,D)\\r\\rTitle: Associative Transformer\\rAuthors: Yuwei Sun, Hideya Ochiai, Zhirong Wu, Stephen Lin, Ryota Kanai\\rCategories: cs.LG cs.CV cs.NE\\r\\\\\\\\ ( https://arxiv.org/abs/2309.12862 ,  14170kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.11050 (*cross-listing*)\\rreplaced with revised version Wed, 31 Jan 2024 06:00:39 GMT   (6205kb,D)\\r\\rTitle: $k$-$t$ CLAIR: Self-Consistency Guided Multi-Prior Learning for Dynamic\\r  Parallel MR Image Reconstruction\\rAuthors: Liping Zhang and Weitian Chen\\rCategories: eess.IV cs.CV physics.med-ph\\rComments: 12 pages, 3 figures, 4 tables. CMRxRecon Challenge, MICCAI 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2310.11050 ,  6205kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.19620\\rreplaced with revised version Wed, 31 Jan 2024 11:22:46 GMT   (4800kb,D)\\r\\rTitle: Large Trajectory Models are Scalable Motion Predictors and Planners\\rAuthors: Qiao Sun, Shiduo Zhang, Danjiao Ma, Jingzhe Shi, Derun Li, Simian Luo,\\r  Yu Wang, Ningyi Xu, Guangzhi Cao, Hang Zhao\\rCategories: cs.RO cs.AI cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2310.19620 ,  4800kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.08404 (*cross-listing*)\\rreplaced with revised version Tue, 30 Jan 2024 20:56:07 GMT   (1534kb)\\r\\rTitle: Training and Comparison of nnU-Net and DeepMedic Methods for\\r  Autosegmentation of Pediatric Brain Tumors\\rAuthors: Arastoo Vossough, Nastaran Khalili, Ariana M. Familiar, Deep Gandhi,\\r  Karthik Viswanathan, Wenxin Tu, Debanjan Haldar, Sina Bagheri, Hannah\\r  Anderson, Shuvanjan Haldar, Phillip B. Storm, Adam Resnick, Jeffrey B. Ware,\\r  Ali Nabavizadeh, Anahita Fathi Kazerooni\\rCategories: eess.IV cs.CV cs.LG physics.med-ph\\r\\\\\\\\ ( https://arxiv.org/abs/2401.08404 ,  1534kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.17217\\rreplaced with revised version Wed, 31 Jan 2024 05:21:13 GMT   (2702kb,D)\\r\\rTitle: GazeGPT: Augmenting Human Capabilities using Gaze-contingent Contextual\\r  AI for Smart Eyewear\\rAuthors: Robert Konrad, Nitish Padmanaban, J. Gabriel Buckmaster, Kevin C.\\r  Boyle, Gordon Wetzstein\\rCategories: cs.HC cs.CV\\rComments: Project video: https://youtu.be/AuDFHHTK_m8\\r\\\\\\\\ ( https://arxiv.org/abs/2401.17217 ,  2702kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.14383\\rreplaced with revised version Wed, 31 Jan 2024 16:59:03 GMT   (4573kb,D)\\r\\rTitle: A Large-Scale Feasibility Study of Screen-based 3D Visualization and\\r  Augmented Reality Tools for Human Anatomy Education: Exploring Gender\\r  Perspectives in Learning Experience\\rAuthors: Roghayeh Leila Barmaki, Kangsoo Kim, Zhang Guo, Qile Wang, Kevin Yu,\\r  Rebecca Pearlman, and Nassir Navab\\rCategories: cs.HC cs.GR\\rComments: This work is accepted and presented at IEEE International Conference\\r  on Artificial Intelligence & extended and Virtual Reality (AIxVR 2024)\\r\\\\\\\\ ( https://arxiv.org/abs/2307.14383 ,  4573kb)\\r%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%--- ------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\rSend any comments regarding submissions directly to submitter.\\r------------------------------------------------------------------------------\\rArchives at http://arxiv.org/\\rTo unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel\\r------------------------------------------------------------------------------\\r Submissions to:\\rComputation and Language\\rComputer Vision and Pattern Recognition\\rGraphics\\r received from  Tue 14 Nov 23 19:00:00 GMT  to  Wed 15 Nov 23 19:00:00 GMT\\r------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08469\\rDate: Tue, 14 Nov 2023 19:00:55 GMT   (2829kb,D)\\r\\rTitle: UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations\\rAuthors: Wenting Zhao, Justin T Chiu, Jena D. Hwang, Faeze Brahman, Jack\\r  Hessel, Sanjiban Choudhury, Yejin Choi, Xiang Lorraine Li, Alane Suhr\\rCategories: cs.CL\\r\\\\\\\\\\r  Language technologies that accurately model the dynamics of events must\\rperform commonsense reasoning. Existing work evaluating commonsense reasoning\\rfocuses on making inferences about common, everyday situations. To instead\\rinvestigate the ability to model unusual, unexpected, and unlikely situations,\\rwe explore the task of uncommonsense abductive reasoning. Given a piece of\\rcontext with an unexpected outcome, this task requires reasoning abductively to\\rgenerate a natural language explanation that makes the unexpected outcome more\\rlikely in the context. To this end, we curate and release a new English\\rlanguage corpus called UNcommonsense. We characterize the differences between\\rthe performance of human explainers and the best performing large language\\rmodels, finding that model-enhanced human-written explanations achieve the\\rhighest quality by trading off between specificity and diversity. Finally, we\\rexperiment with several online imitation learning algorithms to train open and\\raccessible language models on this task. When compared with the vanilla\\rsupervised fine-tuning approach, these methods consistently reduce lose rates\\ron both common and uncommonsense abductive reasoning judged by human\\revaluators.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08469 ,  2829kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08472\\rDate: Tue, 14 Nov 2023 19:02:03 GMT   (257kb,D)\\r\\rTitle: Selecting Shots for Demographic Fairness in Few-Shot Learning with Large\\r  Language Models\\rAuthors: Carlos Aguirre, Kuleen Sasse, Isabel Cachola and Mark Dredze\\rCategories: cs.CL\\r\\\\\\\\\\r  Recently, work in NLP has shifted to few-shot (in-context) learning, with\\rlarge language models (LLMs) performing well across a range of tasks. However,\\rwhile fairness evaluations have become a standard for supervised methods,\\rlittle is known about the fairness of LLMs as prediction systems. Further,\\rcommon standard methods for fairness involve access to models weights or are\\rapplied during finetuning, which are not applicable in few-shot learning. Do\\rLLMs exhibit prediction biases when used for standard NLP tasks? In this work,\\rwe explore the effect of shots, which directly affect the performance of\\rmodels, on the fairness of LLMs as NLP classification systems. We consider how\\rdifferent shot selection strategies, both existing and new demographically\\rsensitive methods, affect model fairness across three standard fairness\\rdatasets. We discuss how future work can include LLM fairness evaluations.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08472 ,  257kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08481\\rDate: Tue, 14 Nov 2023 19:15:55 GMT   (281kb,D)\\r\\rTitle: Functionality learning through specification instructions\\rAuthors: Pedro Henrique Luz de Araujo and Benjamin Roth\\rCategories: cs.CL\\rComments: 33 pages, 8 figures\\r\\\\\\\\\\r  Test suites assess natural language processing models' performance on\\rspecific functionalities: cases of interest involving model robustness,\\rfairness, or particular linguistic capabilities. They enable fine-grained\\revaluations of model aspects that would otherwise go unnoticed in standard\\revaluation datasets, but they do not address the problem of how to fix the\\rfailure cases. Previous work has explored functionality learning by fine-tuning\\rmodels on suite data. While this improves performance on seen functionalities,\\rit often does not generalize to unseen ones and can harm general performance.\\r  This paper analyses a fine-tuning-free approach to functionality learning.\\rFor each functionality in a suite, we generate a specification instruction that\\rencodes it. We combine the obtained specification instructions to create\\rspecification-augmented prompts, which we feed to language models pre-trained\\ron natural instruction data to generate suite predictions. A core aspect of our\\ranalysis is to measure the effect that including a set of specifications has on\\ra held-out set of unseen, qualitatively different specifications. Our\\rexperiments across four tasks and models ranging from 80M to 175B parameters\\rshow that smaller models struggle to follow specification instructions.\\rHowever, larger models (> 3B params.) can benefit from specifications and even\\rgeneralize desirable behaviors across functionalities.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08481 ,  281kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08487\\rDate: Tue, 14 Nov 2023 19:28:51 GMT   (467kb,D)\\r\\rTitle: Alignment is not sufficient to prevent large language models from\\r  generating harmful information: A psychoanalytic perspective\\rAuthors: Zi Yin, Wei Ding, Jia Liu\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Large Language Models (LLMs) are central to a multitude of applications but\\rstruggle with significant risks, notably in generating harmful content and\\rbiases. Drawing an analogy to the human psyche's conflict between evolutionary\\rsurvival instincts and societal norm adherence elucidated in Freud's\\rpsychoanalysis theory, we argue that LLMs suffer a similar fundamental\\rconflict, arising between their inherent desire for syntactic and semantic\\rcontinuity, established during the pre-training phase, and the post-training\\ralignment with human values. This conflict renders LLMs vulnerable to\\radversarial attacks, wherein intensifying the models' desire for continuity can\\rcircumvent alignment efforts, resulting in the generation of harmful\\rinformation. Through a series of experiments, we first validated the existence\\rof the desire for continuity in LLMs, and further devised a straightforward yet\\rpowerful technique, such as incomplete sentences, negative priming, and\\rcognitive dissonance scenarios, to demonstrate that even advanced LLMs struggle\\rto prevent the generation of harmful information. In summary, our study\\runcovers the root of LLMs' vulnerabilities to adversarial attacks, hereby\\rquestioning the efficacy of solely relying on sophisticated alignment methods,\\rand further advocates for a new training idea that integrates modal concepts\\ralongside traditional amodal concepts, aiming to endow LLMs with a more nuanced\\runderstanding of real-world contexts and ethical considerations.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08487 ,  467kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08505\\rDate: Tue, 14 Nov 2023 19:53:53 GMT   (8092kb,D)\\r\\rTitle: Semi-Structured Chain-of-Thought: Integrating Multiple Sources of\\r  Knowledge for Improved Language Model Reasoning\\rAuthors: Xin Su, Tiep Le, Steven Bethard, Phillip Howard\\rCategories: cs.CL\\r\\\\\\\\\\r  An important open question pertaining to the use of large language models for\\rknowledge-intensive tasks is how to effectively integrate knowledge from three\\rsources: the model's parametric memory, external structured knowledge, and\\rexternal unstructured knowledge. Most existing prompting methods either rely\\rsolely on one or two of these sources, or require repeatedly invoking large\\rlanguage models to generate similar or identical content. In this work, we\\rovercome these limitations by introducing a novel semi-structured prompting\\rapproach that seamlessly integrates the model's parametric memory with\\runstructured knowledge from text documents and structured knowledge from\\rknowledge graphs. Experimental results on open-domain multi-hop question\\ranswering datasets demonstrate that our prompting method significantly\\rsurpasses existing techniques, even exceeding those which require fine-tuning.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08505 ,  8092kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08511\\rDate: Tue, 14 Nov 2023 20:07:34 GMT   (9435kb,D)\\r\\rTitle: CoRE-CoG: Conversational Recommendation of Entities using Constrained\\r  Generation\\rAuthors: Harshvardhan Srivastava and Kanav Pruthi and Soumen Chakrabarti and\\r  Mausam\\rCategories: cs.CL\\rComments: 12 Pages\\r\\\\\\\\\\r  End-to-end conversational recommendation systems (CRS) generate responses by\\rleveraging both dialog history and a knowledge base (KB). A CRS mainly faces\\rthree key challenges: (1) at each turn, it must decide if recommending a KB\\rentity is appropriate; if so, it must identify the most relevant KB entity to\\rrecommend; and finally, it must recommend the entity in a fluent utterance that\\ris consistent with the conversation history. Recent CRSs do not pay sufficient\\rattention to these desiderata, often generating unfluent responses or not\\rrecommending (relevant) entities at the right turn. We introduce a new CRS we\\rcall CoRE-CoG. CoRE-CoG addresses the limitations in prior systems by\\rimplementing (1) a recommendation trigger that decides if the system utterance\\rshould include an entity, (2) a type pruning module that improves the relevance\\rof recommended entities, and (3) a novel constrained response generator to make\\rrecommendations while maintaining fluency. Together, these modules ensure\\rsimultaneous accurate recommendation decisions and fluent system utterances.\\rExperiments with recent benchmarks show the superiority particularly on\\rconditional generation sub-tasks with close to 10 F1 and 4 Recall@1 percent\\rpoints gain over baselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08511 ,  9435kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08526\\rDate: Tue, 14 Nov 2023 20:39:12 GMT   (1283kb,D)\\r\\rTitle: GLiNER: Generalist Model for Named Entity Recognition using\\r  Bidirectional Transformer\\rAuthors: Urchade Zaratiana, Nadi Tomeh, Pierre Holat, Thierry Charnois\\rCategories: cs.CL cs.AI cs.LG\\rComments: Work in progress\\r\\\\\\\\\\r  Named Entity Recognition (NER) is essential in various Natural Language\\rProcessing (NLP) applications. Traditional NER models are effective but limited\\rto a set of predefined entity types. In contrast, Large Language Models (LLMs)\\rcan extract arbitrary entities through natural language instructions, offering\\rgreater flexibility. However, their size and cost, particularly for those\\raccessed via APIs like ChatGPT, make them impractical in resource-limited\\rscenarios. In this paper, we introduce a compact NER model trained to identify\\rany type of entity. Leveraging a bidirectional transformer encoder, our model,\\rGLiNER, facilitates parallel entity extraction, an advantage over the slow\\rsequential token generation of LLMs. Through comprehensive testing, GLiNER\\rdemonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs\\rin zero-shot evaluations on various NER benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08526 ,  1283kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08533\\rDate: Tue, 14 Nov 2023 20:58:21 GMT   (1019kb,D)\\r\\rTitle: Natural Language Processing for Financial Regulation\\rAuthors: Ixandra Achitouv, Dragos Gorduza and Antoine Jacquier\\rCategories: cs.CL cs.LG q-fin.CP\\rComments: 20 pages, 3 figures\\r\\\\\\\\\\r  This article provides an understanding of Natural Language Processing\\rtechniques in the framework of financial regulation, more specifically in order\\rto perform semantic matching search between rules and policy when no dataset is\\ravailable for supervised learning. We outline how to outperform simple\\rpre-trained sentences-transformer models using freely available resources and\\rexplain the mathematical concepts behind the key building blocks of Natural\\rLanguage Processing.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08533 ,  1019kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08538\\rDate: Tue, 14 Nov 2023 21:04:03 GMT   (199kb,D)\\r\\rTitle: Extending Multilingual Machine Translation through Imitation Learning\\rAuthors: Wen Lai, Viktor Hangya, Alexander Fraser\\rCategories: cs.CL\\r\\\\\\\\\\r  Despite the growing variety of languages supported by existing multilingual\\rneural machine translation (MNMT) models, most of the world's languages are\\rstill being left behind. We aim to extend large-scale MNMT models to a new\\rlanguage, allowing for translation between the newly added and all of the\\ralready supported languages in a challenging scenario: using only a parallel\\rcorpus between the new language and English. Previous approaches, such as\\rcontinued training on parallel data including the new language, suffer from\\rcatastrophic forgetting (i.e., performance on other languages is reduced). Our\\rnovel approach Imit-MNMT treats the task as an imitation learning process,\\rwhich mimicks the behavior of an expert, a technique widely used in the\\rcomputer vision area, but not well explored in NLP. More specifically, we\\rconstruct a pseudo multi-parallel corpus of the new and the original languages\\rby pivoting through English, and imitate the output distribution of the\\roriginal MNMT model. Extensive experiments show that our approach significantly\\rimproves the translation performance between the new and the original\\rlanguages, without severe catastrophic forgetting. We also demonstrate that our\\rapproach is capable of solving copy and off-target problems, which are two\\rcommon issues existence in current large-scale MNMT models.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08538 ,  199kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08545\\rDate: Tue, 14 Nov 2023 21:19:14 GMT   (487kb,D)\\r\\rTitle: Efficient Continual Pre-training for Building Domain Specific Large\\r  Language Models\\rAuthors: Yong Xie, Karan Aggarwal, Aitzaz Ahmad\\rCategories: cs.CL\\r\\\\\\\\\\r  Large language models (LLMs) have demonstrated remarkable open-domain\\rcapabilities. Traditionally, LLMs tailored for a domain are trained from\\rscratch to excel at handling domain-specific tasks. In this work, we explore an\\ralternative strategy of continual pre-training as a means to develop\\rdomain-specific LLMs. We introduce FinPythia-6.9B, developed through\\rdomain-adaptive continual pre-training on the financial domain. Continual\\rpre-trained FinPythia showcases consistent improvements on financial tasks over\\rthe original foundational model. We further explore simple but effective data\\rselection strategies for continual pre-training. Our data selection strategies\\routperforms vanilla continual pre-training's performance with just 10% of\\rcorpus size and cost, without any degradation on open-domain standard tasks.\\rOur work proposes an alternative solution to building domain-specific LLMs from\\rscratch in a cost-effective manner.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08545 ,  487kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08552\\rDate: Tue, 14 Nov 2023 21:28:10 GMT   (72kb,D)\\r\\rTitle: UT5: Pretraining Non autoregressive T5 with unrolled denoising\\rAuthors: Mahmoud G. Salem, Jiayu Ye, Chu-Cheng Lin, Frederick Liu\\rCategories: cs.CL\\r\\\\\\\\\\r  Recent advances in Transformer-based Large Language Models have made great\\rstrides in natural language generation. However, to decode K tokens, an\\rautoregressive model needs K sequential forward passes, which may be a\\rperformance bottleneck for large language models. Many non-autoregressive (NAR)\\rresearch are aiming to address this sequentiality bottleneck, albeit many have\\rfocused on a dedicated architecture in supervised benchmarks. In this work, we\\rstudied unsupervised pretraining for non auto-regressive T5 models via unrolled\\rdenoising and shown its SoTA results in downstream generation tasks such as\\rSQuAD question generation and XSum.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08552 ,  72kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08562\\rDate: Tue, 14 Nov 2023 21:46:27 GMT   (9175kb,D)\\r\\rTitle: MAgIC: Benchmarking Large Language Model Powered Multi-Agent in\\r  Cognition, Adaptability, Rationality and Collaboration\\rAuthors: Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer,\\r  See Kiong Ng, Jiashi Feng\\rCategories: cs.CL\\rComments: work in progress\\r\\\\\\\\\\r  Large Language Models (LLMs) have marked a significant advancement in the\\rfield of natural language processing, demonstrating exceptional capabilities in\\rreasoning, tool usage, and memory. As their applications extend into\\rmulti-agent environments, a need has arisen for a comprehensive evaluation\\rframework that captures their abilities in reasoning, planning, collaboration,\\rand more. This work introduces a novel benchmarking framework specifically\\rtailored to assess LLMs within multi-agent settings, providing quantitative\\rmetrics to evaluate their judgment, reasoning, deception, self-awareness,\\rcollaboration, coordination, and rationality. We utilize games such as\\rChameleon and Undercover, alongside game theory scenarios like Cost Sharing,\\rMulti-player Prisoner's Dilemma, and Public Good, to create diverse testing\\renvironments. Our framework is fortified with the Probabilistic Graphical\\rModeling (PGM) method, enhancing the LLMs' capabilities in navigating complex\\rsocial and cognitive dimensions. The benchmark evaluates seven multi-agent\\rsystems powered by different LLMs, quantitatively highlighting a significant\\rcapability gap over threefold between the strongest, GPT-4, and the weakest,\\rLlama-2-70B. It also confirms that our PGM enhancement boosts the inherent\\rabilities of all selected models by 50% on average. Our codes are released here\\rhttps://github.com/cathyxl/MAgIC.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08562 ,  9175kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08572\\rDate: Tue, 14 Nov 2023 22:32:39 GMT   (268kb,D)\\r\\rTitle: Parameter-Efficient Multilingual Summarisation: An Empirical Study\\rAuthors: Chenxi Whitehouse, Fantine Huot, Jasmijn Bastings, Mostafa Dehghani,\\r  Chu-Cheng Lin, Mirella Lapata\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  With the increasing prevalence of Large Language Models, traditional full\\rfine-tuning approaches face growing challenges, especially in memory-intensive\\rtasks. This paper investigates the potential of Parameter-Efficient\\rFine-Tuning, focusing on Low-Rank Adaptation (LoRA), for complex and\\runder-explored multilingual summarisation tasks. We conduct an extensive study\\racross different data availability scenarios, including full-data, low-data,\\rand cross-lingual transfer, leveraging models of different sizes. Our findings\\rreveal that LoRA lags behind full fine-tuning when trained with full data,\\rhowever, it excels in low-data scenarios and cross-lingual transfer.\\rInterestingly, as models scale up, the performance gap between LoRA and full\\rfine-tuning diminishes. Additionally, we investigate effective strategies for\\rfew-shot cross-lingual transfer, finding that continued LoRA tuning achieves\\rthe best performance compared to both full fine-tuning and dynamic composition\\rof language-specific LoRA modules.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08572 ,  268kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08579\\rDate: Tue, 14 Nov 2023 22:47:23 GMT   (22714kb,D)\\r\\rTitle: Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational\\r  AutoEncoders\\rAuthors: Yingji Zhang, Marco Valentino, Danilo S. Carvalho, Ian Pratt-Hartmann,\\r  Andr\\\\'e Freitas\\rCategories: cs.CL\\r\\\\\\\\\\r  The injection of syntactic information in Variational AutoEncoders (VAEs) has\\rbeen shown to result in an overall improvement of performances and\\rgeneralisation. An effective strategy to achieve such a goal is to separate the\\rencoding of distributional semantic features and syntactic structures into\\rheterogeneous latent spaces via multi-task learning or dual encoder\\rarchitectures. However, existing works employing such techniques are limited to\\rLSTM-based VAEs. In this paper, we investigate latent space separation methods\\rfor structural syntactic injection in Transformer-based VAE architectures\\r(i.e., Optimus). Specifically, we explore how syntactic structures can be\\rleveraged in the encoding stage through the integration of graph-based and\\rsequential models, and how multiple, specialised latent representations can be\\rinjected into the decoder's attention mechanism via low-rank operators. Our\\rempirical evaluation, carried out on natural language sentences and\\rmathematical expressions, reveals that the proposed end-to-end VAE architecture\\rcan result in a better overall organisation of the latent space, alleviating\\rthe information loss occurring in standard VAE setups, resulting in enhanced\\rperformances on language modelling and downstream generation tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08579 ,  22714kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08584\\rDate: Tue, 14 Nov 2023 23:13:27 GMT   (13592kb,D)\\r\\rTitle: Asking More Informative Questions for Grounded Retrieval\\rAuthors: Sedrick Keh, Justin T. Chiu, Daniel Fried\\rCategories: cs.CL\\r\\\\\\\\\\r  When a model is trying to gather information in an interactive setting, it\\rbenefits from asking informative questions. However, in the case of a grounded\\rmulti-turn image identification task, previous studies have been constrained to\\rpolar yes/no questions, limiting how much information the model can gain in a\\rsingle turn. We present an approach that formulates more informative,\\ropen-ended questions. In doing so, we discover that off-the-shelf visual\\rquestion answering (VQA) models often make presupposition errors, which\\rstandard information gain question selection methods fail to account for. To\\raddress this issue, we propose a method that can incorporate presupposition\\rhandling into both question selection and belief updates. Specifically, we use\\ra two-stage process, where the model first filters out images which are\\rirrelevant to a given question, then updates its beliefs about which image the\\ruser intends. Through self-play and human evaluations, we show that our method\\ris successful in asking informative open-ended questions, increasing accuracy\\rover the past state-of-the-art by 14%, while resulting in 48% more efficient\\rgames in human evaluations.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08584 ,  13592kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08588\\rDate: Tue, 14 Nov 2023 23:18:52 GMT   (254kb,D)\\r\\rTitle: CodeScope: An Execution-based Multilingual Multitask Multidimensional\\r  Benchmark for Evaluating LLMs on Code Understanding and Generation\\rAuthors: Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen\\r  Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Shuiguang Deng, Hari Sundaram\\rCategories: cs.CL cs.AI cs.SE\\r\\\\\\\\\\r  Large Language Models (LLMs) have demonstrated remarkable performance on\\rcoding related tasks, particularly on assisting humans in programming and\\rfacilitating programming automation. However, existing benchmarks for\\revaluating the code understanding and generation capacities of LLMs suffer from\\rsevere limitations. First, most benchmarks are deficient as they focus on a\\rnarrow range of popular programming languages and specific tasks, whereas the\\rreal-world software development scenarios show dire need to implement systems\\rwith multilingual programming environments to satisfy diverse requirements.\\rPractical programming practices also strongly expect multi-task settings for\\rtesting coding capabilities of LLMs comprehensively and robustly. Second, most\\rbenchmarks also fail to consider the actual executability and the consistency\\rof execution results of the generated code. To bridge these gaps between\\rexisting benchmarks and expectations from practical applications, we introduce\\rCodeScope, an execution-based, multilingual, multi-task, multi-dimensional\\revaluation benchmark for comprehensively gauging LLM capabilities on coding\\rtasks. CodeScope covers 43 programming languages and 8 coding tasks. It\\revaluates the coding performance of LLMs from three dimensions (perspectives):\\rdifficulty, efficiency, and length. To facilitate execution-based evaluations\\rof code generation, we develop MultiCodeEngine, an automated code execution\\rengine that supports 14 programming languages. Finally, we systematically\\revaluate and analyze 8 mainstream LLMs on CodeScope tasks and demonstrate the\\rsuperior breadth and challenges of CodeScope for evaluating LLMs on code\\runderstanding and generation tasks compared to other benchmarks. The CodeScope\\rbenchmark and datasets are publicly available at\\rhttps://github.com/WeixiangYAN/CodeScope.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08588 ,  254kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08590\\rDate: Tue, 14 Nov 2023 23:20:51 GMT   (3914kb,D)\\r\\rTitle: PEMA: Plug-in External Memory Adaptation for Language Models\\rAuthors: HyunJin Kim, Young Jin Kim, JinYeong Bak\\rCategories: cs.CL\\r\\\\\\\\\\r  Pre-trained language models (PLMs) have demonstrated impressive performance\\racross various downstream NLP tasks. Nevertheless, the resource requirements of\\rpre-training large language models in terms of memory and training compute pose\\rsignificant challenges. Furthermore, due to the substantial resources required,\\rmany PLM weights are confidential. Consequently, users are compelled to share\\rtheir data with model owners for fine-tuning on specific tasks. To overcome the\\rlimitations, we introduce Plug-in External Memory Adaptation (PEMA), a\\rParameter-Efficient Fine-Tuning (PEFT) approach designed for fine-tuning PLMs\\rwithout the need for all weights. PEMA can be integrated into the context\\rrepresentation of test data during inference to execute downstream tasks. It\\rleverages an external memory to store context representations generated by a\\rPLM, mapped with the desired target word. Our method entails training\\rLoRA-based weight matrices within the final layer of the PLM for enhanced\\refficiency. The probability is then interpolated with the next-word\\rdistribution from the PLM to perform downstream tasks. To improve the\\rgeneration quality, we propose a novel interpolation strategy named Gradual\\rUnrolling. To demonstrate the effectiveness of our proposed method, we conduct\\rexperiments to demonstrate the efficacy of PEMA with a syntactic dataset and\\rassess its performance on machine translation and style transfer tasks using\\rreal datasets. PEMA outperforms other PEFT methods in terms of memory and\\rlatency efficiency for training and inference. Furthermore, it outperforms\\rother baselines in preserving the meaning of sentences while generating\\rappropriate language and styles.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08590 ,  3914kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08593\\rDate: Tue, 14 Nov 2023 23:28:36 GMT   (2525kb,D)\\r\\rTitle: ACID: Abstractive, Content-Based IDs for Document Retrieval with\\r  Language Models\\rAuthors: Haoxin Li, Phillip Keung, Daniel Cheng, Jungo Kasai, Noah A. Smith\\rCategories: cs.CL cs.IR\\r\\\\\\\\\\r  Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a new approach\\rfor end-to-end document retrieval that directly generates document identifiers\\rgiven an input query. Techniques for designing effective, high-quality document\\rIDs remain largely unexplored. We introduce ACID, in which each document's ID\\ris composed of abstractive keyphrases generated by a large language model,\\rrather than an integer ID sequence as done in past work. We compare our method\\rwith the current state-of-the-art technique for ID generation, which produces\\rIDs through hierarchical clustering of document embeddings. We also examine\\rsimpler methods to generate natural-language document IDs, including the naive\\rapproach of using the first k words of each document as its ID or words with\\rhigh BM25 scores in that document. We show that using ACID improves top-10 and\\rtop-20 accuracy by 15.6% and 14.4% (relative) respectively versus the\\rstate-of-the-art baseline on the MSMARCO 100k retrieval task, and 4.4% and 4.0%\\rrespectively on the Natural Questions 100k retrieval task. Our results\\rdemonstrate the effectiveness of human-readable, natural-language IDs in\\rgenerative retrieval with LMs. The code for reproducing our results and the\\rkeyword-augmented datasets will be released on formal publication.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08593 ,  2525kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08596\\rDate: Tue, 14 Nov 2023 23:40:22 GMT   (352kb,D)\\r\\rTitle: Are You Sure? Challenging LLMs Leads to Performance Drops in The\\r  FlipFlop Experiment\\rAuthors: Philippe Laban and Lidiya Murakhovs'ka and Caiming Xiong and\\r  Chien-Sheng Wu\\rCategories: cs.CL\\r\\\\\\\\\\r  The interactive nature of Large Language Models (LLMs) theoretically allows\\rmodels to refine and improve their answers, yet systematic analysis of the\\rmulti-turn behavior of LLMs remains limited. In this paper, we propose the\\rFlipFlop experiment: in the first round of the conversation, an LLM responds to\\ra prompt containing a classification task. In a second round, the LLM is\\rchallenged with a follow-up phrase like Are you sure?, offering an\\ropportunity for the model to reflect on its initial answer, and decide whether\\rto confirm or flip its answer. A systematic study of nine LLMs on seven\\rclassification tasks reveals that models flip their answers on average 46% of\\rthe time and that all models see a deterioration of accuracy between their\\rfirst and final prediction, with an average drop of 17%. The FlipFlop\\rexperiment illustrates the universality of sycophantic behavior in LLMs and\\rprovides a robust framework to analyze model behavior and evaluate potential\\rsolutions.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08596 ,  352kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08598\\rDate: Tue, 14 Nov 2023 23:43:47 GMT   (7491kb,D)\\r\\rTitle: DALA: A Distribution-Aware LoRA-Based Adversarial Attack against\\r  Pre-trained Language Models\\rAuthors: Yibo Wang, Xiangjue Dong, James Caverlee, Philip S. Yu\\rCategories: cs.CL\\rComments: First two authors contribute equally\\r\\\\\\\\\\r  Pre-trained language models (PLMs) that achieve success in applications are\\rsusceptible to adversarial attack methods that are capable of generating\\radversarial examples with minor perturbations. Although recent attack methods\\rcan achieve a relatively high attack success rate (ASR), our observation shows\\rthat the generated adversarial examples have a different data distribution\\rcompared with the original examples. Specifically, these adversarial examples\\rexhibit lower confidence levels and higher distance to the training data\\rdistribution. As a result, they are easy to detect using very simple detection\\rmethods, diminishing the actual effectiveness of these attack methods. To solve\\rthis problem, we propose a Distribution-Aware LoRA-based Adversarial Attack\\r(DALA) method, which considers the distribution shift of adversarial examples\\rto improve attack effectiveness under detection methods. We further design a\\rnew evaluation metric NASR combining ASR and detection for the attack task. We\\rconduct experiments on four widely-used datasets and validate the attack\\reffectiveness on ASR and NASR of the adversarial examples generated by DALA on\\rthe BERT-base model and the black-box LLaMA2-7b model.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08598 ,  7491kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08605\\rDate: Wed, 15 Nov 2023 00:02:25 GMT   (15338kb,D)\\r\\rTitle: Navigating the Ocean of Biases: Political Bias Attribution in Language\\r  Models via Causal Structures\\rAuthors: David F. Jenny, Yann Billeter, Mrinmaya Sachan, Bernhard Sch\\\\olkopf\\r  and Zhijing Jin\\rCategories: cs.CL cs.AI cs.CY cs.SI\\r\\\\\\\\\\r  The rapid advancement of Large Language Models (LLMs) has sparked intense\\rdebate regarding their ability to perceive and interpret complex\\rsocio-political landscapes. In this study, we undertake an exploration of\\rdecision-making processes and inherent biases within LLMs, exemplified by\\rChatGPT, specifically contextualizing our analysis within political debates. We\\raim not to critique or validate LLMs' values, but rather to discern how they\\rinterpret and adjudicate good arguments. By applying Activity Dependency\\rNetworks (ADNs), we extract the LLMs' implicit criteria for such assessments\\rand illustrate how normative values influence these perceptions. We discuss the\\rconsequences of our findings for human-AI alignment and bias mitigation. Our\\rcode and data at https://github.com/david-jenny/LLM-Political-Study.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08605 ,  15338kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08607\\rDate: Wed, 15 Nov 2023 00:09:21 GMT   (868kb,D)\\r\\rTitle: Towards Generalizable SER: Soft Labeling and Data Augmentation for\\r  Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech\\rAuthors: Mohamed Osman, Tamer Nadeem, Ghada Khoriba\\rCategories: cs.CL cs.LG eess.AS\\rComments: Accepted as talk at NeurIPS ML for Audio workshop\\r\\\\\\\\\\r  Recognizing emotions in spoken communication is crucial for advanced\\rhuman-machine interaction. Current emotion detection methodologies often\\rdisplay biases when applied cross-corpus. To address this, our study\\ramalgamates 16 diverse datasets, resulting in 375 hours of data across\\rlanguages like English, Chinese, and Japanese. We propose a soft labeling\\rsystem to capture gradational emotional intensities. Using the Whisper encoder\\rand data augmentation methods inspired by contrastive learning, our method\\remphasizes the temporal dynamics of emotions. Our validation on four\\rmultilingual datasets demonstrates notable zero-shot generalization. We publish\\rour open source model weights and initial promising results after fine-tuning\\ron Hume-Prosody.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08607 ,  868kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08614\\rDate: Wed, 15 Nov 2023 00:34:28 GMT   (1424kb,D)\\r\\rTitle: XplainLLM: A QA Explanation Dataset for Understanding LLM\\r  Decision-Making\\rAuthors: Zichen Chen, Jianda Chen, Mitali Gaidhani, Ambuj Singh, Misha Sra\\rCategories: cs.CL cs.AI\\rComments: 17 pages, 6 figures, 7 tables. Our dataset is available at:\\r  https://github.com/chen-zichen/XplainLLM_dataset.git\\r\\\\\\\\\\r  Large Language Models (LLMs) have recently made impressive strides in natural\\rlanguage understanding tasks. Despite their remarkable performance,\\runderstanding their decision-making process remains a big challenge. In this\\rpaper, we look into bringing some transparency to this process by introducing a\\rnew explanation dataset for question answering (QA) tasks that integrates\\rknowledge graphs (KGs) in a novel way. Our dataset includes 12,102\\rquestion-answer-explanation (QAE) triples. Each explanation in the dataset\\rlinks the LLM's reasoning to entities and relations in the KGs. The explanation\\rcomponent includes a why-choose explanation, a why-not-choose explanation, and\\ra set of reason-elements that underlie the LLM's decision. We leverage KGs and\\rgraph attention networks (GAT) to find the reason-elements and transform them\\rinto why-choose and why-not-choose explanations that are comprehensible to\\rhumans. Through quantitative and qualitative evaluations, we demonstrate the\\rpotential of our dataset to improve the in-context learning of LLMs, and\\renhance their interpretability and explainability. Our work contributes to the\\rfield of explainable AI by enabling a deeper understanding of the LLMs\\rdecision-making process to make them more transparent and thereby, potentially\\rmore reliable, to researchers and practitioners alike. Our dataset is available\\rat: https://github.com/chen-zichen/XplainLLM_dataset.git\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08614 ,  1424kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08620\\rDate: Wed, 15 Nov 2023 00:57:51 GMT   (7787kb,D)\\r\\rTitle: Toucan: Token-Aware Character Level Language Modeling\\rAuthors: William Fleshman and Benjamin Van Durme\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  Character-level language models obviate the need for separately trained\\rtokenizers, but efficiency suffers from longer sequence lengths. Learning to\\rcombine character representations into tokens has made training these models\\rmore efficient, but they still require decoding characters individually. We\\rpropose Toucan, an augmentation to character-level models to make them\\rtoken-aware. Comparing our method to prior work, we demonstrate significant\\rspeed-ups in character generation without a loss in language modeling\\rperformance. We then explore differences between our learned dynamic\\rtokenization of character sequences with popular fixed vocabulary solutions\\rsuch as Byte-Pair Encoding and WordPiece, finding our approach leads to a\\rgreater amount of longer sequences tokenized as single items. Our project and\\rcode are available at https://nlp.jhu.edu/nuggets/.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08620 ,  7787kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08637\\rDate: Wed, 15 Nov 2023 01:24:09 GMT   (60kb,D)\\r\\rTitle: Formal Proofs as Structured Explanations: Proposing Several Tasks on\\r  Explainable Natural Language Inference\\rAuthors: Lasha Abzianidze\\rCategories: cs.CL\\rComments: 7 pages, 2 figures\\rMSC-class: 68T50\\rACM-class: I.2.7\\r\\\\\\\\\\r  In this position paper, we propose a way of exploiting formal proofs to put\\rforward several explainable natural language inference (NLI) tasks. The formal\\rproofs will be produced by a reliable and high-performing logic-based NLI\\rsystem. Taking advantage of the in-depth information available in the generated\\rformal proofs, we show how it can be used to define NLI tasks with structured\\rexplanations. The proposed tasks can be ordered according to difficulty defined\\rin terms of the granularity of explanations. We argue that the tasks will\\rsuffer with substantially fewer shortcomings than the existing explainable NLI\\rtasks (or datasets).\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08637 ,  60kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08640\\rDate: Wed, 15 Nov 2023 01:28:28 GMT   (675kb,D)\\r\\rTitle: Multistage Collaborative Knowledge Distillation from Large Language\\r  Models\\rAuthors: Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer, Md\\r  Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, Andrew McCallum\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  We study semi-supervised sequence prediction tasks where labeled data are too\\rscarce to effectively finetune a model and at the same time few-shot prompting\\rof a large language model (LLM) has suboptimal performance. This happens when a\\rtask, such as parsing, is expensive to annotate and also unfamiliar to a\\rpretrained LLM. In this paper, we present a discovery that student models\\rdistilled from a prompted LLM can often generalize better than their teacher on\\rsuch tasks. Leveraging this finding, we propose a new distillation method,\\rmultistage collaborative knowledge distillation from an LLM (MCKD), for such\\rtasks. MCKD first prompts an LLM using few-shot in-context learning to produce\\rpseudolabels for unlabeled data. Then, at each stage of distillation, a pair of\\rstudents are trained on disjoint partitions of the pseudolabeled data. Each\\rstudent subsequently produces new and improved pseudolabels for the unseen\\rpartition to supervise the next round of student(s) with. We show the benefit\\rof multistage cross-partition labeling on two constituency parsing tasks. On\\rCRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the\\rperformance of supervised finetuning with 500 examples and outperforms the\\rprompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08640 ,  675kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08648\\rDate: Wed, 15 Nov 2023 01:58:54 GMT   (7347kb,D)\\r\\rTitle: Explore Spurious Correlations at the Concept Level in Language Models\\r  for Text Classification\\rAuthors: Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, Furong Huang\\rCategories: cs.CL cs.AI\\rComments: 14 pages, 3 page appendix\\r\\\\\\\\\\r  Language models (LMs) have gained great achievement in various NLP tasks for\\rboth fine-tuning and in-context learning (ICL) methods. Despite its outstanding\\rperformance, evidence shows that spurious correlations caused by imbalanced\\rlabel distributions in training data (or exemplars in ICL) lead to robustness\\rissues. However, previous studies mostly focus on word- and phrase-level\\rfeatures and fail to tackle it from the concept level, partly due to the lack\\rof concept labels and subtle and diverse expressions of concepts in text. In\\rthis paper, we first use the LLM to label the concept for each text and then\\rmeasure the concept bias of models for fine-tuning or ICL on the test data.\\rSecond, we propose a data rebalancing method to mitigate the spurious\\rcorrelations by adding the LLM-generated counterfactual data to make a balanced\\rlabel distribution for each concept. We verify the effectiveness of our\\rmitigation method and show its superiority over the token removal method.\\rOverall, our results show that there exist label distribution biases in\\rconcepts across multiple text classification datasets, and LMs will utilize\\rthese shortcuts to make predictions in both fine-tuning and ICL methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08648 ,  7347kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08662\\rDate: Wed, 15 Nov 2023 02:59:10 GMT   (8229kb,D)\\r\\rTitle: Multi-Set Inoculation: Assessing Model Robustness Across Multiple\\r  Challenge Sets\\rAuthors: Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, Dan Roth\\rCategories: cs.CL cs.AI cs.IR\\rComments: 13 pages, 2 Figure, 12 Tables\\r\\\\\\\\\\r  Language models, given their black-box nature, often exhibit sensitivity to\\rinput perturbations, leading to trust issues due to hallucinations. To bolster\\rtrust, it's essential to understand these models' failure modes and devise\\rstrategies to enhance their performance. In this study, we propose a framework\\rto study the effect of input perturbations on language models of different\\rscales, from pre-trained models to large language models (LLMs). We use\\rfine-tuning to train a robust model to perturbations, and we investigate\\rwhether exposure to one perturbation improves or degrades the model's\\rperformance on other perturbations. To address multi-perturbation robustness,\\rwe suggest three distinct training strategies. We also extend the framework to\\rLLMs via a chain of thought(COT) prompting with exemplars. We instantiate our\\rframework for the Tabular-NLI task and show that the proposed strategies train\\rthe model robust to different perturbations without losing accuracy on a given\\rdataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08662 ,  8229kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08666\\rDate: Wed, 15 Nov 2023 03:21:04 GMT   (1086kb)\\r\\rTitle: It Takes Two to Negotiate: Modeling Social Exchange in Online\\r  Multiplayer Games\\rAuthors: Kokil Jaidka and Hansin Ahuja and Lynnette Ng\\rCategories: cs.CL cs.GT cs.LG\\rComments: 28 pages, 11 figures. Accepted to CSCW '24 and forthcoming the\\r  Proceedings of ACM HCI '24\\r\\\\\\\\\\r  Online games are dynamic environments where players interact with each other,\\rwhich offers a rich setting for understanding how players negotiate their way\\rthrough the game to an ultimate victory. This work studies online player\\rinteractions during the turn-based strategy game, Diplomacy. We annotated a\\rdataset of over 10,000 chat messages for different negotiation strategies and\\rempirically examined their importance in predicting long- and short-term game\\routcomes. Although negotiation strategies can be predicted reasonably\\raccurately through the linguistic modeling of the chat messages, more is needed\\rfor predicting short-term outcomes such as trustworthiness. On the other hand,\\rthey are essential in graph-aware reinforcement learning approaches to predict\\rlong-term outcomes, such as a player's success, based on their prior\\rnegotiation history. We close with a discussion of the implications and impact\\rof our work. The dataset is available at\\rhttps://github.com/kj2013/claff-diplomacy.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08666 ,  1086kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08669\\rDate: Wed, 15 Nov 2023 03:29:02 GMT   (345kb,D)\\r\\rTitle: Understanding Calibration for Multilingual Question Answering Models\\rAuthors: Yahan Yang, Soham Dan, Dan Roth, Insup Lee\\rCategories: cs.CL cs.LG\\rComments: Preprint. Under Submission\\r\\\\\\\\\\r  Multilingual pre-trained language models are incredibly effective at Question\\rAnswering (QA), a core task in Natural Language Understanding, achieving high\\raccuracies on several multilingual benchmarks. However, little is known about\\rhow well they are calibrated. In this paper, we study the calibration\\rproperties of several pre-trained multilingual large language models (LLMs) on\\ra variety of question-answering tasks. We perform extensive experiments,\\rspanning both extractive and generative QA model designs and diverse languages,\\rspanning both high-resource and low-resource ones. We study different\\rdimensions of calibration in in-distribution, out-of-distribution, and\\rcross-lingual transfer settings, and investigate strategies to improve it,\\rincluding post-hoc methods and regularized fine-tuning. We demonstrate\\rautomatically translated data augmentation as a highly effective technique to\\rimprove model calibration. We also conduct a number of ablation experiments to\\rstudy the effect of model size on calibration and how multilingual models\\rcompare with their monolingual counterparts for diverse tasks and languages.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08669 ,  345kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08685\\rDate: Wed, 15 Nov 2023 04:22:22 GMT   (130kb,D)\\r\\rTitle: Safer-Instruct: Aligning Language Models with Automated Preference Data\\rAuthors: Taiwei Shi, Kai Chen, Jieyu Zhao\\rCategories: cs.CL cs.AI\\rComments: 11 pages\\r\\\\\\\\\\r  Reinforcement Learning from Human Feedback (RLHF) is a vital strategy for\\renhancing model safety in language models. However, annotating preference data\\rfor RLHF is a resource-intensive and creativity-demanding process, while\\rautomatic generation methods face limitations in data diversity and quality. In\\rresponse, we present Safer-Instruct, a novel pipeline for semi-automatically\\rconstructing large-scale preference datasets. Our approach leverages reversed\\rinstruction tuning, instruction induction, and expert model evaluation to\\refficiently generate high-quality preference data without human annotators. We\\revaluate Safer-Instruct using LLaMA for instruction induction and GPT-4 as an\\rexpert model, generating approximately 10K preference samples. Finetuning an\\rAlpaca model on this dataset demonstrates improved harmlessness while\\rmaintaining competitive performance on conversation and downstream tasks.\\rSafer-Instruct addresses the challenges in preference data acquisition,\\radvancing the development of safer and more responsible AI systems. Our code\\rand data are available at https://github.com/uscnlp-lime/safer-instruct\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08685 ,  130kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08687\\rDate: Wed, 15 Nov 2023 04:30:20 GMT   (217kb,D)\\r\\rTitle: An Eye on Clinical BERT: Investigating Language Model Generalization for\\r  Diabetic Eye Disease Phenotyping\\rAuthors: Keith Harrigian, Tina Tang, Anthony Gonzales, Cindy X. Cai, Mark\\r  Dredze\\rCategories: cs.CL cs.AI cs.LG\\rComments: Extended Abstract presented at Machine Learning for Health (ML4H)\\r  symposium 2023, December 10th, 2023, New Orleans, United States, 24 pages\\r\\\\\\\\\\r  Diabetic eye disease is a major cause of blindness worldwide. The ability to\\rmonitor relevant clinical trajectories and detect lapses in care is critical to\\rmanaging the disease and preventing blindness. Alas, much of the information\\rnecessary to support these goals is found only in the free text of the\\relectronic medical record. To fill this information gap, we introduce a system\\rfor extracting evidence from clinical text of 19 clinical concepts related to\\rdiabetic eye disease and inferring relevant attributes for each. In developing\\rthis ophthalmology phenotyping system, we are also afforded a unique\\ropportunity to evaluate the effectiveness of clinical language models at\\radapting to new clinical domains. Across multiple training paradigms, we find\\rthat BERT language models pretrained on out-of-distribution clinical data offer\\rno significant improvement over BERT language models pretrained on non-clinical\\rdata for our domain. Our study tempers recent claims that language models\\rpretrained on clinical data are necessary for clinical NLP tasks and highlights\\rthe importance of not treating clinical language data as a single homogeneous\\rdomain.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08687 ,  217kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08692\\rDate: Wed, 15 Nov 2023 04:40:43 GMT   (637kb,D)\\r\\rTitle: Routing to the Expert: Efficient Reward-guided Ensemble of Large\\r  Language Models\\rAuthors: Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang\\r  Zhou, Jingren Zhou\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  The complementary potential of Large Language Models (LLM) assumes\\roff-the-shelf LLMs have heterogeneous expertise in a wide range of domains and\\rtasks so that an ensemble of LLMs can achieve consistently better performance.\\rExisting ensemble methods for LLMs mainly focus on reward model ranking of\\routputs, leading to significant computation overhead. To combat this issue, we\\rrevisit the complementary potential of LLMs and further elaborate it by mining\\rlatent expertise with off-the-shelf reward models. We propose Zooter, a\\rreward-guided routing method distilling rewards on training queries to train a\\rrouting function, which can precisely distribute each query to the LLM with\\rexpertise about it. We also integrate a tag-based label enhancement to mitigate\\rnoise from uncertainty when using rewards as silver supervision. Zooter shows\\rcomputation efficiency in inference as it introduces only a minor computation\\roverhead of a routing function compared with reward model ranking methods. We\\revaluate Zooter on a comprehensive benchmark collection with 26 subsets on\\rdifferent domains and tasks. Zooter outperforms the best single model on\\raverage and ranks first on 44% of tasks, even surpassing multiple reward model\\rranking methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08692 ,  637kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08704\\rDate: Wed, 15 Nov 2023 05:11:26 GMT   (240kb,D)\\r\\rTitle: Can Large Language Models Follow Concept Annotation Guidelines? A Case\\r  Study on Scientific and Financial Domains\\rAuthors: Marcio Fonseca and Shay B. Cohen\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Although large language models (LLMs) exhibit remarkable capacity to leverage\\rin-context demonstrations, it is still unclear to what extent they can learn\\rnew concepts or facts from ground-truth labels. To address this question, we\\rexamine the capacity of instruction-tuned LLMs to follow in-context concept\\rguidelines for sentence labeling tasks. We design guidelines that present\\rdifferent types of factual and counterfactual concept definitions, which are\\rused as prompts for zero-shot sentence classification tasks. Our results show\\rthat although concept definitions consistently help in task performance, only\\rthe larger models (with 70B parameters or more) have limited ability to work\\runder counterfactual contexts. Importantly, only proprietary models such as\\rGPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is\\rdue to more sophisticated alignment methods. Finally, we find that\\rFalcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which\\rindicates that careful fine-tuning is more effective than increasing model\\rscale. Altogether, our simple evaluation method reveals significant gaps in\\rconcept understanding between the most capable open-source language models and\\rthe leading proprietary APIs.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08704 ,  240kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08705\\rDate: Wed, 15 Nov 2023 05:11:43 GMT   (6958kb,D)\\r\\rTitle: Evaluating Robustness of Dialogue Summarization Models in the Presence\\r  of Naturally Occurring Variations\\rAuthors: Ankita Gupta, Chulaka Gunasekara, Hui Wan, Jatin Ganhotra, Sachindra\\r  Joshi, Marina Danilevsky\\rCategories: cs.CL\\r\\\\\\\\\\r  Dialogue summarization task involves summarizing long conversations while\\rpreserving the most salient information. Real-life dialogues often involve\\rnaturally occurring variations (e.g., repetitions, hesitations) and existing\\rdialogue summarization models suffer from performance drop on such\\rconversations. In this study, we systematically investigate the impact of such\\rvariations on state-of-the-art dialogue summarization models using publicly\\ravailable datasets. To simulate real-life variations, we introduce two types of\\rperturbations: utterance-level perturbations that modify individual utterances\\rwith errors and language variations, and dialogue-level perturbations that add\\rnon-informative exchanges (e.g., repetitions, greetings). We conduct our\\ranalysis along three dimensions of robustness: consistency, saliency, and\\rfaithfulness, which capture different aspects of the summarization model's\\rperformance. We find that both fine-tuned and instruction-tuned models are\\raffected by input variations, with the latter being more susceptible,\\rparticularly to dialogue-level perturbations. We also validate our findings via\\rhuman evaluation. Finally, we investigate if the robustness of fine-tuned\\rmodels can be improved by training them with a fraction of perturbed data and\\robserve that this approach is insufficient to address robustness challenges\\rwith current models and thus warrants a more thorough investigation to identify\\rbetter solutions. Overall, our work highlights robustness challenges in\\rdialogue summarization and provides insights for future research.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08705 ,  6958kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08711\\rDate: Wed, 15 Nov 2023 05:28:07 GMT   (878kb,D)\\r\\rTitle: PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning\\rAuthors: Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng\\r  Jiang, Francesco Barbieri\\rCategories: cs.CL\\r\\\\\\\\\\r  Instruction tuning has remarkably advanced large language models (LLMs) in\\runderstanding and responding to diverse human instructions. Despite the success\\rin high-resource languages, its application in lower-resource ones faces\\rchallenges due to the imbalanced foundational abilities of LLMs across\\rdifferent languages, stemming from the uneven language distribution in their\\rpre-training data. To tackle this issue, we propose pivot language guided\\rgeneration (PLUG), an approach that utilizes a high-resource language,\\rprimarily English, as the pivot to enhance instruction tuning in lower-resource\\rlanguages. It trains the model to first process instructions in the pivot\\rlanguage, and then produce responses in the target language. To evaluate our\\rapproach, we introduce a benchmark, X-AlpacaEval, of instructions in 4\\rlanguages (Chinese, Korean, Italian, and Spanish), each annotated by\\rprofessional translators. Our approach demonstrates a significant improvement\\rin the instruction-following abilities of LLMs by 29% on average, compared to\\rdirectly responding in the target language alone. Further experiments validate\\rthe versatility of our approach by employing alternative pivot languages beyond\\rEnglish to assist languages where LLMs exhibit lower proficiency.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08711 ,  878kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08718\\rDate: Wed, 15 Nov 2023 05:58:35 GMT   (155kb,D)\\r\\rTitle: Decomposing Uncertainty for Large Language Models through Input\\r  Clarification Ensembling\\rAuthors: Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, Yang\\r  Zhang\\rCategories: cs.CL\\rComments: 15 pages, 3 figures\\r\\\\\\\\\\r  Uncertainty decomposition refers to the task of decomposing the total\\runcertainty of a model into data (aleatoric) uncertainty, resulting from the\\rinherent complexity or ambiguity of the data, and model (epistemic)\\runcertainty, resulting from the lack of knowledge in the model. Performing\\runcertainty decomposition for large language models (LLMs) is an important step\\rtoward improving the reliability, trustworthiness, and interpretability of\\rLLMs, but this research task is very challenging and remains unresolved. The\\rexisting canonical method, Bayesian Neural Network (BNN), cannot be applied to\\rLLMs, because BNN requires training and ensembling multiple variants of models,\\rwhich is infeasible or prohibitively expensive for LLMs. In this paper, we\\rintroduce an uncertainty decomposition framework for LLMs, called input\\rclarifications ensemble, which bypasses the need to train new models. Rather\\rthan ensembling models with different parameters, our approach generates a set\\rof clarifications for the input, feeds them into the fixed LLMs, and ensembles\\rthe corresponding predictions. We show that our framework shares a symmetric\\rdecomposition structure with BNN. Empirical evaluations demonstrate that the\\rproposed framework provides accurate and reliable uncertainty quantification on\\rvarious tasks. Code will be made publicly available at\\rhttps://github.com/UCSB-NLP-Chang/llm_uncertainty .\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08718 ,  155kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08719\\rDate: Wed, 15 Nov 2023 06:08:35 GMT   (917kb,D)\\r\\rTitle: Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term\\r  Memory\\rAuthors: Lei Liu and Xiaoyan Yang and Yue Shen and Binbin Hu and Zhiqiang Zhang\\r  and Jinjie Gu and Guannan Zhang\\rCategories: cs.CL\\r\\\\\\\\\\r  Memory-augmented Large Language Models (LLMs) have demonstrated remarkable\\rperformance in long-term human-machine interactions, which basically relies on\\riterative recalling and reasoning of history to generate high-quality\\rresponses. However, such repeated recall-reason steps easily produce biased\\rthoughts, \\\\textit{i.e.}, inconsistent reasoning results when recalling the same\\rhistory for different questions. On the contrary, humans can keep thoughts in\\rthe memory and recall them without repeated reasoning. Motivated by this human\\rcapability, we propose a novel memory mechanism called TiM (Think-in-Memory)\\rthat enables LLMs to maintain an evolved memory for storing historical thoughts\\ralong the conversation stream. The TiM framework consists of two crucial\\rstages: (1) before generating a response, a LLM agent recalls relevant thoughts\\rfrom memory, and (2) after generating a response, the LLM agent post-thinks and\\rincorporates both historical and new thoughts to update the memory. Thus, TiM\\rcan eliminate the issue of repeated reasoning by saving the post-thinking\\rthoughts as the history. Besides, we formulate the basic principles to organize\\rthe thoughts in memory based on the well-established operations,\\r(\\\\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic\\rupdates and evolution of the thoughts. Furthermore, we introduce\\rLocality-Sensitive Hashing into TiM to achieve efficient retrieval for the\\rlong-term conversations. We conduct qualitative and quantitative experiments on\\rreal-world and simulated dialogues covering a wide range of topics,\\rdemonstrating that equipping existing LLMs with TiM significantly enhances\\rtheir performance in generating responses for long-term interactions.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08719 ,  917kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08723\\rDate: Wed, 15 Nov 2023 06:33:52 GMT   (4634kb,D)\\r\\rTitle: Token Prediction as Implicit Classification to Identify LLM-Generated\\r  Text\\rAuthors: Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha\\r  Raj\\rCategories: cs.CL\\rComments: EMNLP 2023, Main Conference\\r\\\\\\\\\\r  This paper introduces a novel approach for identifying the possible large\\rlanguage models (LLMs) involved in text generation. Instead of adding an\\radditional classification layer to a base LM, we reframe the classification\\rtask as a next-token prediction task and directly fine-tune the base LM to\\rperform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the\\rbackbone for our experiments. We compared our approach to the more direct\\rapproach of utilizing hidden states for classification. Evaluation shows the\\rexceptional performance of our method in the text classification task,\\rhighlighting its simplicity and efficiency. Furthermore, interpretability\\rstudies on the features extracted by our model reveal its ability to\\rdifferentiate distinctive writing styles among various LLMs even in the absence\\rof an explicit classifier. We also collected a dataset named OpenLLMText,\\rcontaining approximately 340k text samples from human and LLMs, including\\rGPT3.5, PaLM, LLaMA, and GPT2.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08723 ,  4634kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08724\\rDate: Wed, 15 Nov 2023 06:35:01 GMT   (359kb)\\r\\rTitle: Method for Text Entity Linking in Power Distribution Scheduling Oriented\\r  to Power Distribution Network Knowledge Graph\\rAuthors: Xiang Li, Che Wang, Bing Li, Hao Chen, Sizhe Li\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  The proposed method for linking entities in power distribution dispatch texts\\rto a power distribution network knowledge graph is based on a deep\\runderstanding of these networks. This method leverages the unique features of\\rentities in both the power distribution network's knowledge graph and the\\rdispatch texts, focusing on their semantic, phonetic, and syntactic\\rcharacteristics. An enhanced model, the Lexical Semantic Feature-based Skip\\rConvolutional Neural Network (LSF-SCNN), is utilized for effectively matching\\rdispatch text entities with those in the knowledge graph. The efficacy of this\\rmodel, compared to a control model, is evaluated through cross-validation\\rmethods in real-world power distribution dispatch scenarios. The results\\rindicate that the LSF-SCNN model excels in accurately linking a variety of\\rentity types, demonstrating high overall accuracy in entity linking when the\\rprocess is conducted in English.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08724 ,  359kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08726\\rDate: Wed, 15 Nov 2023 06:36:29 GMT   (1156kb,D)\\r\\rTitle: Uncertainty Estimation on Sequential Labeling via Uncertainty\\r  Transmission\\rAuthors: Jianfeng He, Linlin Yu, Shuo Lei, Chang-Tien Lu, Feng Chen\\rCategories: cs.CL\\rComments: 11 pages, 2 figures\\r\\\\\\\\\\r  Sequential labeling is a task predicting labels for each token in a sequence,\\rsuch as Named Entity Recognition (NER). NER tasks aim to extract entities and\\rpredict their labels given a text, which is important in information\\rextraction. Although previous works have shown great progress in improving NER\\rperformance, uncertainty estimation on NER (UE-NER) is still underexplored but\\ressential. This work focuses on UE-NER, which aims to estimate uncertainty\\rscores for the NER predictions. Previous uncertainty estimation models often\\roverlook two unique characteristics of NER: the connection between entities\\r(i.e., one entity embedding is learned based on the other ones) and wrong span\\rcases in the entity extraction subtask. Therefore, we propose a Sequential\\rLabeling Posterior Network (SLPN) to estimate uncertainty scores for the\\rextracted entities, considering uncertainty transmitted from other tokens.\\rMoreover, we have defined an evaluation strategy to address the specificity of\\rwrong-span cases. Our SLPN has achieved significant improvements on two\\rdatasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant\\rdataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08726 ,  1156kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08732\\rDate: Wed, 15 Nov 2023 06:48:50 GMT   (16266kb,D)\\r\\rTitle: Enhancing Emergency Decision-making with Knowledge Graphs and Large\\r  Language Models\\rAuthors: Minze Chen, Zhenxiang Tao, Weitong Tang, Tingxin Qin, Rui Yang, Chunli\\r  Zhu\\rCategories: cs.CL\\rComments: 26 pages, 6 figures\\r\\\\\\\\\\r  Emergency management urgently requires comprehensive knowledge while having a\\rhigh possibility to go beyond individuals' cognitive scope. Therefore,\\rartificial intelligence(AI) supported decision-making under that circumstance\\ris of vital importance. Recent emerging large language models (LLM) provide a\\rnew direction for enhancing targeted machine intelligence. However, the\\rutilization of LLM directly would inevitably introduce unreliable output for\\rits inherent issue of hallucination and poor reasoning skills. In this work, we\\rdevelop a system called Enhancing Emergency decision-making with Knowledge\\rGraph and LLM (E-KELL), which provides evidence-based decision-making in\\rvarious emergency stages. The study constructs a structured emergency knowledge\\rgraph and guides LLMs to reason over it via a prompt chain. In real-world\\revaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in\\rcomprehensibility, accuracy, conciseness, and instructiveness from a group of\\remergency commanders and firefighters, demonstrating a significant improvement\\racross various situations compared to baseline models. This work introduces a\\rnovel approach to providing reliable emergency decision support.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08732 ,  16266kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08734\\rDate: Wed, 15 Nov 2023 06:54:44 GMT   (8569kb,D)\\r\\rTitle: Thread of Thought Unraveling Chaotic Contexts\\rAuthors: Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long,\\r  Jian-Guang Lou, Jianbing Shen\\rCategories: cs.CL\\rComments: 11 pages, 7 figures, 5 tables\\r\\\\\\\\\\r  Large Language Models (LLMs) have ushered in a transformative era in the\\rfield of natural language processing, excelling in tasks related to text\\rcomprehension and generation. Nevertheless, they encounter difficulties when\\rconfronted with chaotic contexts (e.g., distractors rather than long irrelevant\\rcontext), leading to the inadvertent omission of certain details within the\\rchaotic context. In response to these challenges, we introduce the Thread of\\rThought (ThoT) strategy, which draws inspiration from human cognitive\\rprocesses. ThoT systematically segments and analyzes extended contexts while\\radeptly selecting pertinent information. This strategy serves as a versatile\\rplug-and-play module, seamlessly integrating with various LLMs and prompting\\rtechniques. In the experiments, we utilize the PopQA and EntityQ datasets, as\\rwell as a Multi-Turn Conversation Response dataset (MTCR) we collected, to\\rillustrate that ThoT significantly improves reasoning performance compared to\\rother prompting techniques.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08734 ,  8569kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08756\\rDate: Wed, 15 Nov 2023 07:50:57 GMT   (162kb,D)\\r\\rTitle: Accelerating Toeplitz Neural Network with Constant-time Inference\\r  Complexity\\rAuthors: Zhen Qin, Yiran Zhong\\rCategories: cs.CL\\rComments: Accepted to EMNLP 2023. Yiran Zhong is the corresponding author. The\\r  source code is available at\\r  https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion\\r\\\\\\\\\\r  Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in\\rvarious sequence modeling tasks. They outperform commonly used\\rTransformer-based models while benefiting from log-linear space-time\\rcomplexities. On the other hand, State Space Models (SSMs) achieve lower\\rperformance than TNNs in language modeling but offer the advantage of constant\\rinference complexity. In this paper, we aim to combine the strengths of TNNs\\rand SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to\\rachieve the same constant inference complexities as SSMs. To accomplish this,\\rwe formulate the conversion process as an optimization problem and provide a\\rclosed-form solution. We demonstrate how to transform the target equation into\\ra Vandermonde linear system problem, which can be efficiently solved using the\\rDiscrete Fourier Transform (DFT). Notably, our method requires no training and\\rmaintains numerical stability. It can be also applied to any LongConv-based\\rmodel. To assess its effectiveness, we conduct extensive experiments on\\rlanguage modeling tasks across various settings. Additionally, we compare our\\rmethod to other gradient-descent solutions, highlighting the superior numerical\\rstability of our approach. The source code is available at\\rhttps://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08756 ,  162kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08788\\rDate: Wed, 15 Nov 2023 09:01:55 GMT   (1096kb,D)\\r\\rTitle: X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented\\r  Instruction Tuning with Auxiliary Evaluation Aspects\\rAuthors: Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav\\r  Kumar, Reza Ghanadan, Lifu Huang\\rCategories: cs.CL cs.AI cs.LG\\rComments: 17 pages, 5 figures, 14 tables\\r\\\\\\\\\\r  Natural Language Generation (NLG) typically involves evaluating the generated\\rtext in various aspects (e.g., consistency and naturalness) to obtain a\\rcomprehensive assessment. However, multi-aspect evaluation remains challenging\\ras it may require the evaluator to generalize to any given evaluation aspect\\reven if it's absent during training. In this paper, we introduce X-Eval, a\\rtwo-stage instruction tuning framework to evaluate the text in both seen and\\runseen aspects customized by end users. X-Eval consists of two learning stages:\\rthe vanilla instruction tuning stage that improves the model's ability to\\rfollow evaluation instructions, and an enhanced instruction tuning stage that\\rexploits the connections between fine-grained evaluation aspects to better\\rassess text quality. To support the training of X-Eval, we collect\\rAspectInstruct, the first instruction tuning dataset tailored for multi-aspect\\rNLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance\\rtask diversity, we devise an augmentation strategy that converts human rating\\rannotations into diverse forms of NLG evaluation tasks, including scoring,\\rcomparison, ranking, and Boolean question answering. Extensive experiments\\racross three essential categories of NLG tasks: dialogue generation,\\rsummarization, and data-to-text coupled with 21 aspects in meta-evaluation,\\rdemonstrate that our X-Eval enables even a lightweight language model to\\rachieve a comparable if not higher correlation with human judgments compared to\\rthe state-of-the-art NLG evaluators, such as GPT-4.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08788 ,  1096kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08793\\rDate: Wed, 15 Nov 2023 09:07:29 GMT   (192kb,D)\\r\\rTitle: German FinBERT: A German Pre-trained Language Model\\rAuthors: Moritz Scherrmann\\rCategories: cs.CL stat.ML\\r\\\\\\\\\\r  This study presents German FinBERT, a novel pre-trained German language model\\rtailored for financial textual data. The model is trained through a\\rcomprehensive pre-training process, leveraging a substantial corpus comprising\\rfinancial reports, ad-hoc announcements and news related to German companies.\\rThe corpus size is comparable to the data sets commonly used for training\\rstandard BERT models. I evaluate the performance of German FinBERT on\\rdownstream tasks, specifically sentiment prediction, topic recognition and\\rquestion answering against generic German language models. My results\\rdemonstrate improved performance on finance-specific data, indicating the\\refficacy of German FinBERT in capturing domain-specific nuances. The presented\\rfindings suggest that German FinBERT holds promise as a valuable tool for\\rfinancial text analysis, potentially benefiting various applications in the\\rfinancial domain.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08793 ,  192kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08803\\rDate: Wed, 15 Nov 2023 09:18:09 GMT   (266kb,D)\\r\\rTitle: StrategyLLM: Large Language Models as Strategy Generators, Executors,\\r  Optimizers, and Evaluators for Problem Solving\\rAuthors: Chang Gao, Haiyun Jiang, Deng Cai, Shuming Shi, Wai Lam\\rCategories: cs.CL\\r\\\\\\\\\\r  Most existing chain-of-thought (CoT) prompting methods suffer from the issues\\rof generalizability and consistency, as they often rely on instance-specific\\rsolutions that may not be applicable to other cases and lack task-level\\rconsistency in their reasoning steps. To address these limitations, we propose\\ra comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to\\rtackle various tasks. The framework improves generalizability by formulating\\rgeneral problem-solving strategies and enhances consistency by producing\\rconsistent solutions using these strategies. StrategyLLM employs four LLM-based\\ragents: strategy generator, executor, optimizer, and evaluator, working\\rtogether to generate, evaluate, and select promising strategies for a given\\rtask automatically. The experimental results demonstrate that StrategyLLM\\routperforms the competitive baseline CoT-SC that requires human-annotated\\rsolutions on 13 datasets across 4 challenging tasks without human involvement,\\rincluding math reasoning (39.2% $\\\\rightarrow$ 43.3%), commonsense reasoning\\r(70.3% $\\\\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\\\\rightarrow$ 62.0%),\\rand symbolic reasoning (30.0% $\\\\rightarrow$ 79.2%).\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08803 ,  266kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08817\\rDate: Wed, 15 Nov 2023 09:38:53 GMT   (145kb,D)\\r\\rTitle: MAP's not dead yet: Uncovering true language model modes by conditioning\\r  away degeneracy\\rAuthors: Davis Yoshida, Kartik Goyal, Kevin Gimpel\\rCategories: cs.CL cs.AI cs.LG\\rComments: 49 pages, 3 figures\\r\\\\\\\\\\r  It has been widely observed that exact or approximate MAP (mode-seeking)\\rdecoding from natural language generation (NLG) models consistently leads to\\rdegenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has\\rgenerally been attributed to either a fundamental inadequacy of modes in models\\ror weaknesses in language modeling. Contrastingly in this work, we emphasize\\rthat degenerate modes can even occur in the absence of any model error, due to\\rcontamination of the training data. Specifically, we show that mixing even a\\rtiny amount of low-entropy noise with a population text distribution can cause\\rthe data distribution's mode to become degenerate, implying that any models\\rtrained on it will be as well. As the unconditional mode of NLG models will\\roften be degenerate, we therefore propose to apply MAP decoding to the model's\\rdistribution conditional on avoiding specific degeneracies. Using exact-search,\\rwe empirically verify that the length-conditional modes of machine translation\\rmodels and language models are indeed more fluent and topical than their\\runconditional modes. For the first time, we also share many examples of exact\\rmodal sequences from these models, and from several variants of the LLaMA-7B\\rmodel. Notably, the modes of the LLaMA models are still degenerate, showing\\rthat improvements in modeling have not fixed this issue. Because of the cost of\\rexact mode finding algorithms, we develop an approximate mode finding approach,\\rACBS, which finds sequences that are both high-likelihood and high-quality. We\\rapply this approach to LLaMA-7B, a model which was not trained for instruction\\rfollowing, and find that we are able to elicit reasonable outputs without any\\rfinetuning.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08817 ,  145kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08836\\rDate: Wed, 15 Nov 2023 10:25:14 GMT   (79kb,D)\\r\\rTitle: Evaluating Gender Bias in the Translation of Gender-Neutral Languages\\r  into English\\rAuthors: Spencer Rarrick, Ranjita Naik, Sundar Poudel, Vishal Chowdhary\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Machine Translation (MT) continues to improve in quality and adoption, yet\\rthe inadvertent perpetuation of gender bias remains a significant concern.\\rDespite numerous studies into gender bias in translations from gender-neutral\\rlanguages such as Turkish into more strongly gendered languages like English,\\rthere are no benchmarks for evaluating this phenomenon or for assessing\\rmitigation strategies. To address this gap, we introduce GATE X-E, an extension\\rto the GATE (Rarrick et al., 2023) corpus, that consists of human translations\\rfrom Turkish, Hungarian, Finnish, and Persian into English. Each translation is\\raccompanied by feminine, masculine, and neutral variants for each possible\\rgender interpretation. The dataset, which contains between 1250 and 1850\\rinstances for each of the four language pairs, features natural sentences with\\ra wide range of sentence lengths and domains, challenging translation rewriters\\ron various linguistic phenomena. Additionally, we present an English gender\\rrewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We\\ropen source our contributions to encourage further research on gender\\rdebiasing.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08836 ,  79kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08838\\rDate: Wed, 15 Nov 2023 10:25:30 GMT   (8469kb,D)\\r\\rTitle: Disinformation Capabilities of Large Language Models\\rAuthors: Ivan Vykopal, Mat\\\\'u\\\\v{s} Pikuliak, Ivan Srba, Robert Moro, Dominik\\r  Macko, Maria Bielikova\\rCategories: cs.CL\\r\\\\\\\\\\r  Automated disinformation generation is often listed as one of the risks of\\rlarge language models (LLMs). The theoretical ability to flood the information\\rspace with disinformation content might have dramatic consequences for\\rdemocratic societies around the world. This paper presents a comprehensive\\rstudy of the disinformation capabilities of the current generation of LLMs to\\rgenerate false news articles in English language. In our study, we evaluated\\rthe capabilities of 10 LLMs using 20 disinformation narratives. We evaluated\\rseveral aspects of the LLMs: how well they are at generating news articles, how\\rstrongly they tend to agree or disagree with the disinformation narratives, how\\roften they generate safety warnings, etc. We also evaluated the abilities of\\rdetection models to detect these articles as LLM-generated. We conclude that\\rLLMs are able to generate convincing news articles that agree with dangerous\\rdisinformation narratives.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08838 ,  8469kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08849\\rDate: Wed, 15 Nov 2023 10:40:45 GMT   (288kb,D)\\r\\rTitle: OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient\\r  Large-scale Multilingual Continued Pretraining\\rAuthors: Yihong Liu, Peiqin Lin, Mingyang Wang, Hinrich Sch\\\\utze\\rCategories: cs.CL\\r\\\\\\\\\\r  Pretraining multilingual language models from scratch requires considerable\\rcomputational resources and substantial training data. Therefore, a more\\refficient method is to adapt existing pretrained language models (PLMs) to new\\rlanguages via vocabulary extension and continued pretraining. However, this\\rmethod usually randomly initializes the embeddings of new subwords and\\rintroduces substantially more embedding parameters to the language model, thus\\rweakening the efficiency. To address these issues, we propose a novel\\rframework: \\\\textbf{O}ne \\\\textbf{F}or \\\\textbf{A}ll (\\\\textbf{\\\\textsc{Ofa}}),\\rwhich wisely initializes the embeddings of unseen subwords from target\\rlanguages and thus can adapt a PLM to multiple languages efficiently and\\reffectively. \\\\textsc{Ofa} takes advantage of external well-aligned multilingual\\rword embeddings and injects the alignment knowledge into the new embeddings. In\\raddition, \\\\textsc{Ofa} applies matrix factorization and replaces the cumbersome\\rembeddings with two lower-dimensional matrices, which significantly reduces the\\rnumber of parameters while not sacrificing the performance. Through extensive\\rexperiments, we show models initialized by \\\\textsc{Ofa} are efficient and\\routperform several baselines. \\\\textsc{Ofa} not only accelerates the convergence\\rof continued pretraining, which is friendly to a limited computation budget,\\rbut also improves the zero-shot crosslingual transfer on a wide range of\\rdownstream tasks. We make our code and models publicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08849 ,  288kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08877\\rDate: Wed, 15 Nov 2023 11:27:44 GMT   (2287kb,D)\\r\\rTitle: Llamas Know What GPTs Don't Show: Surrogate Models for Confidence\\r  Estimation\\rAuthors: Vaishnavi Shrivastava, Percy Liang, Ananya Kumar\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  To maintain user trust, large language models (LLMs) should signal low\\rconfidence on examples where they are incorrect, instead of misleading the\\ruser. The standard approach of estimating confidence is to use the softmax\\rprobabilities of these models, but as of November 2023, state-of-the-art LLMs\\rsuch as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We\\rfirst study eliciting confidence linguistically -- asking an LLM for its\\rconfidence in its answer -- which performs reasonably (80.5% AUC on GPT-4\\raveraged across 12 question-answering datasets -- 7% above a random baseline)\\rbut leaves room for improvement. We then explore using a surrogate confidence\\rmodel -- using a model where we do have probabilities to evaluate the original\\rmodel's confidence in a given question. Surprisingly, even though these\\rprobabilities come from a different and often weaker model, this method leads\\rto higher AUC than linguistic confidences on 9 out of 12 datasets. Our best\\rmethod composing linguistic confidences and surrogate model probabilities gives\\rstate-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on\\rGPT-4).\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08877 ,  2287kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08883\\rDate: Wed, 15 Nov 2023 11:42:41 GMT   (7882kb,D)\\r\\rTitle: Enabling Large Language Models to Learn from Rules\\rAuthors: Wenkai Yang, Yankai Lin, Jie Zhou, Jirong Wen\\rCategories: cs.CL\\rComments: In progress\\r\\\\\\\\\\r  Large language models (LLMs) have shown incredible performance in completing\\rvarious real-world tasks. The current knowledge learning paradigm of LLMs is\\rmainly based on learning from examples, in which LLMs learn the internal rule\\rimplicitly from a certain number of supervised examples. However, the learning\\rparadigm may not well learn those complicated rules, especially when the\\rtraining examples are limited. We are inspired that humans can learn the new\\rtasks or knowledge in another way by learning from rules. That is, humans can\\rgrasp the new tasks or knowledge quickly and generalize well given only a\\rdetailed rule and a few optional examples. Therefore, in this paper, we aim to\\rexplore the feasibility of this new learning paradigm, which encodes the\\rrule-based knowledge into LLMs. We propose rule distillation, which first uses\\rthe strong in-context abilities of LLMs to extract the knowledge from the\\rtextual rules and then explicitly encode the knowledge into LLMs' parameters by\\rlearning from the above in-context signals produced inside the model. Our\\rexperiments show that making LLMs learn from rules by our method is much more\\refficient than example-based learning in both the sample size and\\rgeneralization ability.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08883 ,  7882kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08886\\rDate: Wed, 15 Nov 2023 11:48:16 GMT   (8480kb,D)\\r\\rTitle: CLIMB: Curriculum Learning for Infant-inspired Model Building\\rAuthors: Richard Diehl Martinez, Zebulon Goriely, Hope McGovern, Christopher\\r  Davis, Andrew Caines, Paula Buttery, Lisa Beinborn\\rCategories: cs.CL\\r\\\\\\\\\\r  We describe our team's contribution to the STRICT-SMALL track of the BabyLM\\rChallenge. The challenge requires training a language model from scratch using\\ronly a relatively small training dataset of ten million words. We experiment\\rwith three variants of cognitively-motivated curriculum learning and analyze\\rtheir effect on the performance of the model on linguistic evaluation tasks. In\\rthe vocabulary curriculum, we analyze methods for constraining the vocabulary\\rin the early stages of training to simulate cognitively more plausible learning\\rcurves. In the data curriculum experiments, we vary the order of the training\\rinstances based on i) infant-inspired expectations and ii) the learning\\rbehavior of the model. In the objective curriculum, we explore different\\rvariations of combining the conventional masked language modeling task with a\\rmore coarse-grained word class prediction task to reinforce linguistic\\rgeneralization capabilities. Our results did not yield consistent improvements\\rover our own non-curriculum learning baseline across a range of linguistic\\rbenchmarks; however, we do find marginal gains on select tasks. Our analysis\\rhighlights key takeaways for specific combinations of tasks and settings which\\rbenefit from our proposed curricula. We moreover determine that careful\\rselection of model architecture, and training hyper-parameters yield\\rsubstantial improvements over the default baselines provided by the BabyLM\\rchallenge.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08886 ,  8480kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08890\\rDate: Wed, 15 Nov 2023 11:50:10 GMT   (7770kb,D)\\r\\rTitle: Large Language Models are legal but they are not: Making the case for a\\r  powerful LegalLLM\\rAuthors: Thanmay Jayakumar, Fauzan Farooqui, Luqman Farooqui\\rCategories: cs.CL\\rComments: 7 pages, Accepted at Natural Legal Language Processing Workshop,\\r  EMNLP 2023\\r\\\\\\\\\\r  Realizing the recent advances in Natural Language Processing (NLP) to the\\rlegal sector poses challenging problems such as extremely long sequence\\rlengths, specialized vocabulary that is usually only understood by legal\\rprofessionals, and high amounts of data imbalance. The recent surge of Large\\rLanguage Models (LLMs) has begun to provide new opportunities to apply NLP in\\rthe legal domain due to their ability to handle lengthy, complex sequences.\\rMoreover, the emergence of domain-specific LLMs has displayed extremely\\rpromising results on various tasks. In this study, we aim to quantify how\\rgeneral LLMs perform in comparison to legal-domain models (be it an LLM or\\rotherwise). Specifically, we compare the zero-shot performance of three\\rgeneral-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR\\rsubset of the LexGLUE benchmark for contract provision classification. Although\\rthe LLMs were not explicitly trained on legal data, we observe that they are\\rstill able to classify the theme correctly in most cases. However, we find that\\rtheir mic-F1/mac-F1 performance is up to 19.2/26.8\\\\% lesser than smaller models\\rfine-tuned on the legal domain, thus underscoring the need for more powerful\\rlegal-domain LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08890 ,  7770kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08894\\rDate: Wed, 15 Nov 2023 11:56:56 GMT   (39kb,D)\\r\\rTitle: Combining Transfer Learning with In-context Learning using Blackbox LLMs\\r  for Zero-shot Knowledge Base Question Answering\\rAuthors: Mayur Patidar, Avinash Singh, Riya Sawhney, Indrajit Bhattacharya,\\r  Mausam\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  We address the zero-shot transfer learning setting for the knowledge base\\rquestion answering (KBQA) problem, where a large volume of labeled training\\rdata is available for the source domain, but no such labeled examples are\\ravailable for the target domain. Transfer learning for KBQA makes use of large\\rvolumes of unlabeled data in the target in addition to the labeled data in the\\rsource. More recently, few-shot in-context learning using Black-box Large\\rLanguage Models (BLLMs) has been adapted for KBQA without considering any\\rsource domain data. In this work, we show how to meaningfully combine these two\\rparadigms for KBQA so that their benefits add up. Specifically, we preserve the\\rtwo stage retrieve-then-generate pipeline of supervised KBQA and introduce\\rinteraction between in-context learning using BLLMs and transfer learning from\\rthe source for both stages. In addition, we propose execution-guided\\rself-refinement using BLLMs, decoupled from the transfer setting. With the help\\rof experiments using benchmark datasets GrailQA as the source and WebQSP as the\\rtarget, we show that the proposed combination brings significant improvements\\rto both stages and also outperforms by a large margin state-of-the-art\\rsupervised KBQA models trained on the source. We also show that in the\\rin-domain setting, the proposed BLLM augmentation significantly outperforms\\rstate-of-the-art supervised models, when the volume of labeled data is limited,\\rand also outperforms these marginally even when using the entire large training\\rdataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08894 ,  39kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08896\\rDate: Wed, 15 Nov 2023 12:02:52 GMT   (161kb)\\r\\rTitle: HELLaMA: LLaMA-based Table to Text Generation by Highlighting the\\r  Important Evidence\\rAuthors: Junyi Bian, Xiaolei Qin, Wuhe Zou, Mengzuo Huang, Weidong Zhang\\rCategories: cs.CL\\r\\\\\\\\\\r  Large models have demonstrated significant progress across various domains,\\rparticularly in tasks related to text generation. In the domain of Table to\\rText, many Large Language Model (LLM)-based methods currently resort to\\rmodifying prompts to invoke public APIs, incurring potential costs and\\rinformation leaks. With the advent of open-source large models, fine-tuning\\rLLMs has become feasible. In this study, we conducted parameter-efficient\\rfine-tuning on the LLaMA2 model. Distinguishing itself from previous\\rfine-tuning-based table-to-text methods, our approach involves injecting\\rreasoning information into the input by emphasizing table-specific row data.\\rOur model consists of two modules: 1) a table reasoner that identifies relevant\\rrow evidence, and 2) a table summarizer that generates sentences based on the\\rhighlighted table. To facilitate this, we propose a search strategy to\\rconstruct reasoning labels for training the table reasoner. On both the FetaQA\\rand QTSumm datasets, our approach achieved state-of-the-art results.\\rAdditionally, we observed that highlighting input tables significantly enhances\\rthe model's performance and provides valuable interpretability.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08896 ,  161kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08921\\rDate: Wed, 15 Nov 2023 12:47:52 GMT   (863kb,D)\\r\\rTitle: Self-Improving for Zero-Shot Named Entity Recognition with Large\\r  Language Models\\rAuthors: Tingyu Xie, Qi Li, Yan Zhang, Zuozhu Liu, Hongwei Wang\\rCategories: cs.CL\\r\\\\\\\\\\r  Exploring the application of powerful large language models (LLMs) on the\\rfundamental named entity recognition (NER) task has drawn much attention\\rrecently. This work aims to investigate the possibilities of pushing the\\rboundary of zero-shot NER with LLM via a training-free self-improving strategy.\\rWe propose a self-improving framework, which utilize an unlabeled corpus to\\rstimulate the self-learning ability of LLMs on NER. First, we use LLM to make\\rpredictions on the unlabeled corpus and obtain the self-annotated data. Second,\\rwe explore various strategies to select reliable samples from the\\rself-annotated dataset as demonstrations, considering the similarity, diversity\\rand reliability of demonstrations. Finally, we conduct inference for the test\\rquery via in-context learning with the selected self-annotated demonstrations.\\rThrough comprehensive experimental analysis, our study yielded the following\\rfindings: (1) The self-improving framework further pushes the boundary of\\rzero-shot NER with LLMs, and achieves an obvious performance improvement; (2)\\rIterative self-improving or naively increasing the size of unlabeled corpus\\rdoes not guarantee improvements; (3) There might still be space for improvement\\rvia more advanced strategy for reliable entity selection.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08921 ,  863kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08941\\rDate: Wed, 15 Nov 2023 13:23:24 GMT   (880kb,D)\\r\\rTitle: Reasoning over Description Logic-based Contexts with Transformers\\rAuthors: Angelos Poulis, Eleni Tsalapati, Manolis Koubarakis\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  One way that the current state of the art measures the reasoning ability of\\rtransformer-based models is by evaluating accuracy in downstream tasks like\\rlogical question answering or proof generation over synthetic contexts\\rexpressed in natural language. However, most of the contexts used are in\\rpractice very simple; in most cases, they are generated from short first-order\\rlogic sentences with only a few logical operators and quantifiers. In this\\rwork, we seek to answer the question how well a transformer-based model will\\rperform reasoning over expressive contexts. For this purpose, we construct a\\rsynthetic natural language question-answering dataset, generated by description\\rlogic knowledge bases. For the generation of the knowledge bases, we use the\\rexpressive language $\\\\mathcal{ALCQ}$. The resulting dataset contains 384K\\rexamples, and increases in two dimensions: i) reasoning depth, and ii) length\\rof sentences. We show that the performance of our DeBERTa-based model,\\rDELTA$_M$, is marginally affected when the reasoning depth is increased and it\\ris not affected at all when the length of the sentences is increasing. We also\\revaluate the generalization ability of the model on reasoning depths unseen at\\rtraining, both increasing and decreasing, revealing interesting insights into\\rthe model's adaptive generalization abilities.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08941 ,  880kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08966\\rDate: Wed, 15 Nov 2023 13:53:28 GMT   (533kb,D)\\r\\rTitle: Improving Large-scale Deep Biasing with Phoneme Features and Text-only\\r  Data in Streaming Transducer\\rAuthors: Jin Qiu, Lu Huang, Boyu Li, Jun Zhang, Lu Lu, Zejun Ma\\rCategories: cs.CL cs.SD eess.AS\\rComments: Submitted to ASRU 2023\\r\\\\\\\\\\r  Deep biasing for the Transducer can improve the recognition performance of\\rrare words or contextual entities, which is essential in practical\\rapplications, especially for streaming Automatic Speech Recognition (ASR).\\rHowever, deep biasing with large-scale rare words remains challenging, as the\\rperformance drops significantly when more distractors exist and there are words\\rwith similar grapheme sequences in the bias list. In this paper, we combine the\\rphoneme and textual information of rare words in Transducers to distinguish\\rwords with similar pronunciation or spelling. Moreover, the introduction of\\rtraining with text-only data containing more rare words benefits large-scale\\rdeep biasing. The experiments on the LibriSpeech corpus demonstrate that the\\rproposed method achieves state-of-the-art performance on rare word error rate\\rfor different scales and levels of bias lists.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08966 ,  533kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08968\\rDate: Wed, 15 Nov 2023 14:01:41 GMT   (9253kb,D)\\r\\rTitle: Identifying Linear Relational Concepts in Large Language Models\\rAuthors: David Chanin, Anthony Hunter, Oana-Maria Camburu\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Transformer language models (LMs) have been shown to represent concepts as\\rdirections in the latent space of hidden activations. However, for any given\\rhuman-interpretable concept, how can we find its direction in the latent space?\\rWe present a technique called linear relational concepts (LRC) for finding\\rconcept directions corresponding to human-interpretable concepts at a given\\rhidden layer in a transformer LM by first modeling the relation between subject\\rand object as a linear relational embedding (LRE). While the LRE work was\\rmainly presented as an exercise in understanding model representations, we find\\rthat inverting the LRE while using earlier object layers results in a powerful\\rtechnique to find concept directions that both work well as a classifier and\\rcausally influence model outputs.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08968 ,  9253kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08981\\rDate: Wed, 15 Nov 2023 14:15:30 GMT   (1323kb,D)\\r\\rTitle: Speculative Contrastive Decoding\\rAuthors: Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou\\rCategories: cs.CL\\rComments: Working in Progress\\r\\\\\\\\\\r  Large language models (LLMs) have shown extraordinary performance in various\\rlanguage tasks, but high computational requirements hinder their widespread\\rdeployment. Speculative decoding, which uses amateur models to predict the\\rgeneration of expert models, has been proposed as a way to accelerate LLM\\rinference. However, speculative decoding focuses on acceleration instead of\\rmaking the best use of the token distribution from amateur models. We proposed\\rSpeculative Contrastive Decoding (SCD), an accelerated decoding method\\rleveraging the natural contrast between expert and amateur models in\\rspeculative decoding. Comprehensive evaluations on four benchmarks show that\\rSCD can achieve similar acceleration factors as speculative decoding while\\rfurther improving the generation quality as the contrastive decoding. The\\ranalysis of token probabilities further demonstrates the compatibility between\\rspeculative and contrastive decoding. Overall, SCD provides an effective\\rapproach to enhance the decoding quality of LLMs while saving computational\\rresources.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08981 ,  1323kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08982\\rDate: Wed, 15 Nov 2023 14:15:41 GMT   (1222kb,D)\\r\\rTitle: SentAlign: Accurate and Scalable Sentence Alignment\\rAuthors: Stein{\\\\th}\\\\'or Steingr\\\\'imsson, Hrafn Loftsson, Andy Way\\rCategories: cs.CL\\rComments: EMNLP 2023 System Demonstration paper\\r\\\\\\\\\\r  We present SentAlign, an accurate sentence alignment tool designed to handle\\rvery large parallel document pairs. Given user-defined parameters, the\\ralignment algorithm evaluates all possible alignment paths in fairly large\\rdocuments of thousands of sentences and uses a divide-and-conquer approach to\\ralign documents containing tens of thousands of sentences. The scoring function\\ris based on LaBSE bilingual sentence representations. SentAlign outperforms\\rfive other sentence alignment tools when evaluated on two different evaluation\\rsets, German-French and English-Icelandic, and on a downstream machine\\rtranslation task.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08982 ,  1222kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08993\\rDate: Wed, 15 Nov 2023 14:26:30 GMT   (370kb,D)\\r\\rTitle: When does In-context Learning Fall Short and Why? A Study on\\r  Specification-Heavy Tasks\\rAuthors: Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yunjia Qi, Zimu Wang,\\r  Zhili Wu, Kaisheng Zeng, Bin Xu, Lei Hou, Juanzi Li\\rCategories: cs.CL cs.AI\\rComments: Under review\\r\\\\\\\\\\r  In-context learning (ICL) has become the default method for using large\\rlanguage models (LLMs), making the exploration of its limitations and\\runderstanding the underlying causes crucial. In this paper, we find that ICL\\rfalls short of handling specification-heavy tasks, which are tasks with\\rcomplicated and extensive task specifications, requiring several hours for\\rordinary humans to master, such as traditional information extraction tasks.\\rThe performance of ICL on these tasks mostly cannot reach half of the\\rstate-of-the-art results. To explore the reasons behind this failure, we\\rconduct comprehensive experiments on 18 specification-heavy tasks with various\\rLLMs and identify three primary reasons: inability to specifically understand\\rcontext, misalignment in task schema comprehension with humans, and inadequate\\rlong-text understanding ability. Furthermore, we demonstrate that through\\rfine-tuning, LLMs can achieve decent performance on these tasks, indicating\\rthat the failure of ICL is not an inherent flaw of LLMs, but rather a drawback\\rof existing alignment methods that renders LLMs incapable of handling\\rcomplicated specification-heavy tasks via ICL. To substantiate this, we perform\\rdedicated instruction tuning on LLMs for these tasks and observe a notable\\rimprovement. We hope the analyses in this paper could facilitate advancements\\rin alignment methods enabling LLMs to meet more sophisticated human demands.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08993 ,  370kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09000\\rDate: Wed, 15 Nov 2023 14:41:57 GMT   (1021kb,D)\\r\\rTitle: Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and\\r  Correction of LLM Output\\rAuthors: Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora,\\r  Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan,\\r  Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, Preslav\\r  Nakov\\rCategories: cs.CL\\rComments: 29 pages, 11 figures\\r\\\\\\\\\\r  The increased use of large language models (LLMs) across a variety of\\rreal-world applications calls for mechanisms to verify the factual accuracy of\\rtheir outputs. In this work, we present a holistic end-to-end solution for\\rannotating the factuality of LLM-generated responses, which encompasses a\\rmulti-stage annotation scheme designed to yield detailed labels concerning the\\rverifiability and factual inconsistencies found in LLM outputs. We design and\\rbuild an annotation tool to speed up the labelling procedure and ease the\\rworkload of raters. It allows flexible incorporation of automatic results in\\rany stage, e.g. automatically-retrieved evidence. We further construct an\\ropen-domain document-level factuality benchmark in three-level granularity:\\rclaim, sentence and document. Preliminary experiments show that FacTool,\\rFactScore and Perplexity.ai are struggling to identify false claims with the\\rbest F1=0.53. Annotation tool, benchmark and code are available at\\rhttps://github.com/yuxiaw/Factcheck-GPT.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09000 ,  1021kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09006\\rDate: Wed, 15 Nov 2023 14:48:08 GMT   (398kb,D)\\r\\rTitle: Data Similarity is Not Enough to Explain Language Model Performance\\rAuthors: Gregory Yauney and Emily Reif and David Mimno\\rCategories: cs.CL cs.LG\\rJournal-ref: Published in EMNLP 2023\\r\\\\\\\\\\r  Large language models achieve high performance on many but not all downstream\\rtasks. The interaction between pretraining data and task data is commonly\\rassumed to determine this variance: a task with data that is more similar to a\\rmodel's pretraining data is assumed to be easier for that model. We test\\rwhether distributional and example-specific similarity measures (embedding-,\\rtoken- and model-based) correlate with language model performance through a\\rlarge-scale comparison of the Pile and C4 pretraining datasets with downstream\\rbenchmarks. Similarity correlates with performance for multilingual datasets,\\rbut in other benchmarks, we surprisingly find that similarity metrics are not\\rcorrelated with accuracy or even each other. This suggests that the\\rrelationship between pretraining data and downstream tasks is more complex than\\roften assumed.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09006 ,  398kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09008\\rDate: Wed, 15 Nov 2023 14:50:16 GMT   (483kb,D)\\r\\rTitle: End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and\\r  Future Directions\\rAuthors: Libo Qin, Wenbo Pan, Qiguang Chen, Lizi Liao, Zhou Yu, Yue Zhang,\\r  Wanxiang Che, Min Li\\rCategories: cs.CL\\rComments: Accepted at EMNLP2023\\r\\\\\\\\\\r  End-to-end task-oriented dialogue (EToD) can directly generate responses in\\ran end-to-end fashion without modular training, which attracts escalating\\rpopularity. The advancement of deep neural networks, especially the successful\\ruse of large pre-trained models, has further led to significant progress in\\rEToD research in recent years. In this paper, we present a thorough review and\\rprovide a unified perspective to summarize existing approaches as well as\\rrecent trends to advance the development of EToD research. The contributions of\\rthis paper can be summarized: (1) \\\\textbf{\\\\textit{First survey}}: to our\\rknowledge, we take the first step to present a thorough survey of this research\\rfield; (2) \\\\textbf{\\\\textit{New taxonomy}}: we first introduce a unified\\rperspective for EToD, including (i) \\\\textit{Modularly EToD} and (ii)\\r\\\\textit{Fully EToD}; (3) \\\\textbf{\\\\textit{New Frontiers}}: we discuss some\\rpotential frontier areas as well as the corresponding challenges, hoping to\\rspur breakthrough research in EToD field; (4) \\\\textbf{\\\\textit{Abundant\\rresources}}: we build a public website\\\\footnote{We collect the related papers,\\rbaseline projects, and leaderboards for the community at\\r\\\\url{https://etods.net/}.}, where EToD researchers could directly access the\\rrecent progress. We hope this work can serve as a thorough reference for the\\rEToD research community.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09008 ,  483kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09022\\rDate: Wed, 15 Nov 2023 15:12:15 GMT   (651kb,D)\\r\\rTitle: Exploring the Potential of Large Language Models in Computational\\r  Argumentation\\rAuthors: Guizhen Chen, Liying Cheng, Luu Anh Tuan, Lidong Bing\\rCategories: cs.CL\\r\\\\\\\\\\r  Computational argumentation has become an essential tool in various fields,\\rincluding artificial intelligence, law, and public policy. It is an emerging\\rresearch field in natural language processing (NLP) that attracts increasing\\rattention. Research on computational argumentation mainly involves two types of\\rtasks: argument mining and argument generation. As large language models (LLMs)\\rhave demonstrated strong abilities in understanding context and generating\\rnatural language, it is worthwhile to evaluate the performance of LLMs on\\rvarious computational argumentation tasks. This work aims to embark on an\\rassessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under\\rzero-shot and few-shot settings within the realm of computational\\rargumentation. We organize existing tasks into 6 main classes and standardise\\rthe format of 14 open-sourced datasets. In addition, we present a new benchmark\\rdataset on counter speech generation, that aims to holistically evaluate the\\rend-to-end performance of LLMs on argument mining and argument generation.\\rExtensive experiments show that LLMs exhibit commendable performance across\\rmost of these datasets, demonstrating their capabilities in the field of\\rargumentation. We also highlight the limitations in evaluating computational\\rargumentation and provide suggestions for future research directions in this\\rfield.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09022 ,  651kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09033\\rDate: Wed, 15 Nov 2023 15:25:28 GMT   (472kb,D)\\r\\rTitle: MELA: Multilingual Evaluation of Linguistic Acceptability\\rAuthors: Ziyin Zhang and Yikang Liu and Weifang Huang and Junyu Mao and Rui\\r  Wang and Hai Hu\\rCategories: cs.CL cs.AI\\rComments: Work in progress\\r\\\\\\\\\\r  Recent benchmarks for Large Language Models (LLMs) have mostly focused on\\rapplication-driven tasks such as complex reasoning and code generation, and\\rthis has led to a scarcity in purely linguistic evaluation of LLMs. Against\\rthis background, we introduce Multilingual Evaluation of Linguistic\\rAcceptability -- MELA, the first multilingual benchmark on linguistic\\racceptability with 48K samples covering 10 languages from a diverse set of\\rlanguage families. We establish baselines of commonly used LLMs along with\\rsupervised models, and conduct cross-lingual transfer and multi-task learning\\rexperiments with XLM-R. In pursuit of multilingual interpretability, we analyze\\rthe weights of fine-tuned XLM-R to explore the possibility of identifying\\rtransfer difficulty between languages. Our results show that ChatGPT benefits\\rmuch from in-context examples but still lags behind fine-tuned XLM-R, while the\\rperformance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting.\\rCross-lingual and multi-task learning experiments show that unlike semantic\\rtasks, in-language training data is crucial in acceptability judgements.\\rResults in layerwise probing indicate that the upper layers of XLM-R become a\\rtask-specific but language-agnostic region for multilingual acceptability\\rjudgment. We also introduce the concept of conflicting weight, which could be a\\rpotential indicator for the difficulty of cross-lingual transfer between\\rlanguages. Our data will be available at https://github.com/sjtu-compling/MELA.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09033 ,  472kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09048\\rDate: Wed, 15 Nov 2023 15:38:28 GMT   (8807kb,D)\\r\\rTitle: GRASP: A novel benchmark for evaluating language GRounding And Situated\\r  Physics understanding in multimodal language models\\rAuthors: Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia\\r  Ohmer, Elia Bruni\\rCategories: cs.CL\\r\\\\\\\\\\r  This paper presents GRASP, a novel benchmark to evaluate the language\\rgrounding and physical understanding capabilities of video-based multimodal\\rlarge language models (LLMs). This evaluation is accomplished via a two-tier\\rapproach leveraging Unity simulations. The initial level tests for language\\rgrounding by assessing a model's ability to relate simple textual descriptions\\rwith visual information. The second level evaluates the model's understanding\\rof 'Intuitive Physics' principles, such as object permanence and continuity. In\\raddition to releasing the benchmark, we use it to evaluate several\\rstate-of-the-art multimodal LLMs. Our evaluation reveals significant\\rshortcomings in current models' language grounding and intuitive physics. These\\ridentified limitations underline the importance of benchmarks like GRASP to\\rmonitor the progress of future models in developing these competencies.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09048 ,  8807kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09053\\rDate: Wed, 15 Nov 2023 15:44:42 GMT   (8673kb,D)\\r\\rTitle: Assessing Knowledge Editing in Language Models via Relation Perspective\\rAuthors: Yifan Wei, Xiaoyan Yu, Huanhuan Ma, Fangyu Lei, Yixuan Weng, Ran Song,\\r  Kang Liu\\rCategories: cs.CL cs.AI\\rComments: Work in progress\\r\\\\\\\\\\r  Knowledge Editing (KE) for modifying factual knowledge in Large Language\\rModels (LLMs) has been receiving increasing attention. However, existing\\rknowledge editing methods are entity-centric, and it is unclear whether this\\rapproach is suitable for a relation-centric perspective. To address this gap,\\rthis paper constructs a new benchmark named RaKE, which focuses on Relation\\rbased Knowledge Editing. In this paper, we establish a suite of innovative\\rmetrics for evaluation and conduct comprehensive experiments involving various\\rknowledge editing baselines. We notice that existing knowledge editing methods\\rexhibit the potential difficulty in their ability to edit relations. Therefore,\\rwe further explore the role of relations in factual triplets within the\\rtransformer. Our research results confirm that knowledge related to relations\\ris not only stored in the FFN network but also in the attention layers. This\\rprovides experimental support for future relation-based knowledge editing\\rmethods.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09053 ,  8673kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09060\\rDate: Wed, 15 Nov 2023 15:52:40 GMT   (9036kb,D)\\r\\rTitle: Do Localization Methods Actually Localize Memorized Data in LLMs?\\rAuthors: Ting-Yun Chang, Jesse Thomason, and Robin Jia\\rCategories: cs.CL\\r\\\\\\\\\\r  Large language models (LLMs) can memorize many pretrained sequences verbatim.\\rThis paper studies if we can locate a small set of neurons in LLMs responsible\\rfor memorizing a given sequence. While the concept of localization is often\\rmentioned in prior work, methods for localization have never been\\rsystematically and directly evaluated; we address this with two benchmarking\\rapproaches. In our INJ Benchmark, we actively inject a piece of new information\\rinto a small subset of LLM weights and measure whether localization methods can\\ridentify these ground truth weights. In the DEL Benchmark, we study\\rlocalization of pretrained data that LLMs have already memorized; while this\\rsetting lacks ground truth, we can still evaluate localization by measuring\\rwhether dropping out located neurons erases a memorized sequence from the\\rmodel. We evaluate five localization methods on our two benchmarks, and both\\rshow similar rankings. All methods exhibit promising localization ability,\\respecially for pruning-based methods, though the neurons they identify are not\\rnecessarily specific to a single memorized sequence.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09060 ,  9036kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09066\\rDate: Wed, 15 Nov 2023 16:05:55 GMT   (61kb,D)\\r\\rTitle: Identifying Self-Disclosures of Use, Misuse and Addiction in\\r  Community-based Social Media Posts\\rAuthors: Chenghao Yang, Tuhin Chakrabarty, Karli R Hochstatter, Melissa N\\r  Slavin, Nabila El-Bassel, Smaranda Muresan\\rCategories: cs.CL\\rComments: Work in progress\\r\\\\\\\\\\r  In the last decade, the United States has lost more than 500,000 people from\\ran overdose involving prescription and illicit opioids\\r(https://www.cdc.gov/drugoverdose/epidemic/index.html) making it a national\\rpublic health emergency (USDHHS, 2017). To more effectively prevent\\runintentional opioid overdoses, medical practitioners require robust and timely\\rtools that can effectively identify at-risk patients. Community-based social\\rmedia platforms such as Reddit allow self-disclosure for users to discuss\\rotherwise sensitive drug-related behaviors, often acting as indicators for\\ropioid use disorder. Towards this, we present a moderate size corpus of 2500\\ropioid-related posts from various subreddits spanning 6 different phases of\\ropioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For\\revery post, we annotate span-level extractive explanations and crucially study\\rtheir role both in annotation quality and model development. We evaluate\\rseveral state-of-the-art models in a supervised, few-shot, or zero-shot\\rsetting. Experimental results and error analysis show that identifying the\\rphases of opioid use disorder is highly contextual and challenging. However, we\\rfind that using explanations during modeling leads to a significant boost in\\rclassification accuracy demonstrating their beneficial role in a high-stakes\\rdomain such as studying the opioid use disorder continuum. The dataset will be\\rmade available for research on Github in the formal version.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09066 ,  61kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09069\\rDate: Wed, 15 Nov 2023 16:11:27 GMT   (3858kb,D)\\r\\rTitle: How Well Do Large Language Models Truly Ground?\\rAuthors: Hyunji Lee, Sejune Joo, Chaeeun Kim, Joel Jang, Doyoung Kim,\\r  Kyoung-Woon On, Minjoon Seo\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Reliance on the inherent knowledge of Large Language Models (LLMs) can cause\\rissues such as hallucinations, lack of control, and difficulties in integrating\\rvariable knowledge. To mitigate this, LLMs can be probed to generate responses\\rby grounding on external context, often given as input (knowledge-augmented\\rmodels). Yet, previous research is often confined to a narrow view of the term\\rgrounding, often only focusing on whether the response contains the correct\\ranswer or not, which does not ensure the reliability of the entire response. To\\raddress this limitation, we introduce a strict definition of grounding: a model\\ris considered truly grounded when its responses (1) fully utilize necessary\\rknowledge from the provided context, and (2) don't exceed the knowledge within\\rthe contexts. We introduce a new dataset and a grounding metric to assess this\\rnew definition and perform experiments across 13 LLMs of different sizes and\\rtraining methods to provide insights into the factors that influence grounding\\rperformance. Our findings contribute to a better understanding of how to\\rimprove grounding capabilities and suggest an area of improvement toward more\\rreliable and controllable LLM applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09069 ,  3858kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09071\\rDate: Wed, 15 Nov 2023 16:13:14 GMT   (7482kb,D)\\r\\rTitle: How Multilingual is Multilingual LLM?\\rAuthors: Fei Yuan, Shuai Yuan, Zhiyong Wu, Lei Li\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Large Language Models (LLMs), trained predominantly on extensive English\\rdata, often exhibit limitations when applied to other languages. Current\\rresearch is primarily focused on enhancing the multilingual capabilities of\\rthese models by employing various tuning strategies. Despite their\\reffectiveness in certain languages, the understanding of the multilingual\\rabilities of LLMs remains incomplete. This study endeavors to evaluate the\\rmultilingual capacity of LLMs by conducting an exhaustive analysis across 101\\rlanguages, and classifies languages with similar characteristics into four\\rdistinct quadrants. By delving into each quadrant, we shed light on the\\rrationale behind their categorization and offer actionable guidelines for\\rtuning these languages. Extensive experiments reveal that existing LLMs possess\\rmultilingual capabilities that surpass our expectations, and we can\\rsignificantly improve the multilingual performance of LLMs by focusing on these\\rdistinct attributes present in each quadrant.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09071 ,  7482kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09086\\rDate: Wed, 15 Nov 2023 16:30:44 GMT   (126kb,D)\\r\\rTitle: The Uli Dataset: An Exercise in Experience Led Annotation of oGBV\\rAuthors: Arnav Arora, Maha Jinadoss, Cheshta Arora, Denny George,\\r  Brindaalakshmi, Haseena Dawood Khan, Kirti Rawat, Div, Ritash, Seema Mathur,\\r  Shivani Yadav, Shehla Rashid Shora, Rie Raut, Sumit Pawar, Apurva Paithane,\\r  Sonia, Vivek, Dharini Priscilla, Khairunnisha, Grace Banu, Ambika Tandon,\\r  Rishav Thakker, Rahul Dev Korra, Aatman Vaidya, Tarunima Prabhakar\\rCategories: cs.CL cs.AI cs.SI\\r\\\\\\\\\\r  Online gender based violence has grown concomitantly with adoption of the\\rinternet and social media. Its effects are worse in the Global majority where\\rmany users use social media in languages other than English. The scale and\\rvolume of conversations on the internet has necessitated the need for automated\\rdetection of hate speech, and more specifically gendered abuse. There is,\\rhowever, a lack of language specific and contextual data to build such\\rautomated tools. In this paper we present a dataset on gendered abuse in three\\rlanguages- Hindi, Tamil and Indian English. The dataset comprises of tweets\\rannotated along three questions pertaining to the experience of gender abuse,\\rby experts who identify as women or a member of the LGBTQIA community in South\\rAsia. Through this dataset we demonstrate a participatory approach to creating\\rdatasets that drive AI systems.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09086 ,  126kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09090\\rDate: Wed, 15 Nov 2023 16:35:59 GMT   (8269kb,D)\\r\\rTitle: Social Bias Probing: Fairness Benchmarking for Language Models\\rAuthors: Marta Marchiori Manerba, Karolina Sta\\\\'nczak, Riccardo Guidotti,\\r  Isabelle Augenstein\\rCategories: cs.CL\\r\\\\\\\\\\r  Large language models have been shown to encode a variety of social biases,\\rwhich carries the risk of downstream harms. While the impact of these biases\\rhas been recognized, prior methods for bias evaluation have been limited to\\rbinary association tests on small datasets, offering a constrained view of the\\rnature of societal biases within language models. In this paper, we propose an\\roriginal framework for probing language models for societal biases. We collect\\ra probing dataset to analyze language models' general associations, as well as\\ralong the axes of societal categories, identities, and stereotypes. To this\\rend, we leverage a novel perplexity-based fairness score. We curate a\\rlarge-scale benchmarking dataset addressing drawbacks and limitations of\\rexisting fairness collections, expanding to a variety of different identities\\rand stereotypes. When comparing our methodology with prior work, we demonstrate\\rthat biases within language models are more nuanced than previously\\racknowledged. In agreement with recent findings, we find that larger model\\rvariants exhibit a higher degree of bias. Moreover, we expose how identities\\rexpressing different religions lead to the most pronounced disparate treatments\\racross all models.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09090 ,  8269kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09096\\rDate: Wed, 15 Nov 2023 16:42:29 GMT   (9084kb,D)\\r\\rTitle: Defending Large Language Models Against Jailbreaking Attacks Through\\r  Goal Prioritization\\rAuthors: Zhexin Zhang, Junxiao Yang, Pei Ke, Minlie Huang\\rCategories: cs.CL\\rComments: 14 pages\\r\\\\\\\\\\r  Large Language Models (LLMs) continue to advance in their capabilities, yet\\rthis progress is accompanied by a growing array of safety risks. While\\rsignificant attention has been dedicated to exploiting weaknesses in LLMs\\rthrough jailbreaking attacks, there remains a paucity of exploration into\\rdefending against these attacks. We point out a pivotal factor contributing to\\rthe success of jailbreaks: the inherent conflict between the goals of being\\rhelpful and ensuring safety. To counter jailbreaking attacks, we propose to\\rintegrate goal prioritization at both training and inference stages.\\rImplementing goal prioritization during inference substantially diminishes the\\rAttack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to\\r2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising\\rgeneral performance. Furthermore, integrating the concept of goal\\rprioritization into the training phase reduces the ASR from 71.0% to 6.6% for\\rLLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are\\rincluded during training, our approach slashes the ASR by half, decreasing it\\rfrom 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs\\rface greater safety risks, they also possess a greater capacity to be steered\\rtowards defending against such attacks. We hope our work could contribute to\\rthe comprehension of jailbreaking attacks and defenses, and shed light on the\\rrelationship between LLMs' capability and safety. Our code will be available at\\r\\\\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09096 ,  9084kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09101\\rDate: Wed, 15 Nov 2023 16:47:57 GMT   (497kb,D)\\r\\rTitle: Towards A Unified View of Answer Calibration for Multi-Step Reasoning\\rAuthors: Shumin Deng, Ningyu Zhang, Nay Oo, Bryan Hooi\\rCategories: cs.CL cs.AI cs.IR cs.LG\\rComments: Working in Progress\\r\\\\\\\\\\r  Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have\\rbroadened the scope for improving multi-step reasoning capabilities. Usually,\\ranswer calibration strategies such as step-level or path-level calibration play\\ra vital role in multi-step reasoning. While effective, there remains a\\rsignificant gap in our understanding of the key factors that drive their\\rsuccess. In this paper, we break down the design of recent answer calibration\\rstrategies and present a unified view which establishes connections between\\rthem. We then conduct a thorough evaluation on these strategies from a unified\\rview, systematically scrutinizing step-level and path-level answer calibration\\racross multiple paths. Our study holds the potential to illuminate key insights\\rfor optimizing multi-step reasoning with answer calibration.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09101 ,  497kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09105\\rDate: Wed, 15 Nov 2023 16:52:14 GMT   (171kb,D)\\r\\rTitle: MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding\\r  Dataset with Event Argument Annotation\\rAuthors: Xiaozhi Wang, Hao Peng, Yong Guan, Kaisheng Zeng, Jianhui Chen, Lei\\r  Hou, Xu Han, Yankai Lin, Zhiyuan Liu, Ruobing Xie, Jie Zhou, Juanzi Li\\rCategories: cs.CL\\rComments: Working in progress\\r\\\\\\\\\\r  Understanding events in texts is a core objective of natural language\\runderstanding, which requires detecting event occurrences, extracting event\\rarguments, and analyzing inter-event relationships. However, due to the\\rannotation challenges brought by task complexity, a large-scale dataset\\rcovering the full process of event understanding has long been absent. In this\\rpaper, we introduce MAVEN-Arg, which augments MAVEN datasets with event\\rargument annotations, making the first all-in-one dataset supporting event\\rdetection, event argument extraction (EAE), and event relation extraction. As\\ran EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive\\rschema covering 162 event types and 612 argument roles, all with expert-written\\rdefinitions and examples; (2) a large data scale, containing 98,591 events and\\r290,613 arguments obtained with laborious human annotation; (3) the exhaustive\\rannotation supporting all task variants of EAE, which annotates both entity and\\rnon-entity event arguments in document level. Experiments indicate that\\rMAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary\\rlarge language models (LLMs). Furthermore, to demonstrate the benefits of an\\rall-in-one dataset, we preliminarily explore a potential application, future\\revent prediction, with LLMs. MAVEN-Arg and our code can be obtained from\\rhttps://github.com/THU-KEG/MAVEN-Argument.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09105 ,  171kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09106\\rDate: Wed, 15 Nov 2023 16:53:35 GMT   (1012kb,D)\\r\\rTitle: We Demand Justice!: Towards Grounding Political Text in Social Context\\rAuthors: Rajkumar Pujari and Chengfei Wu and Dan Goldwasser\\rCategories: cs.CL\\rComments: Was accepted to and withdrawn from Findings of EMNLP 2023\\r\\\\\\\\\\r  Social media discourse from US politicians frequently consists of 'seemingly\\rsimilar language used by opposing sides of the political spectrum'. But often,\\rit translates to starkly contrasting real-world actions. For instance, We need\\rto keep our students safe from mass shootings may signal either arming\\rteachers to stop the shooter or banning guns to reduce mass shootings\\rdepending on who says it and their political stance on the issue. In this\\rpaper, we define and characterize the context that is required to fully\\runderstand such ambiguous statements in a computational setting and ground them\\rin real-world entities, actions, and attitudes. To that end, we propose two\\rchallenging datasets that require an understanding of the real-world context of\\rthe text to be solved effectively. We benchmark these datasets against\\rbaselines built upon large pre-trained models such as BERT, RoBERTa, GPT-3,\\retc. Additionally, we develop and benchmark more structured baselines building\\rupon existing 'Discourse Contextualization Framework' and 'Political Actor\\rRepresentation' models. We perform analysis of the datasets and baseline\\rpredictions to obtain further insights into the pragmatic language\\runderstanding challenges posed by the proposed social grounding tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09106 ,  1012kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09109\\rDate: Wed, 15 Nov 2023 16:56:49 GMT   (9314kb,D)\\r\\rTitle: Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge\\r  Graph Completion?\\rAuthors: Yusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe\\rCategories: cs.CL cs.AI cs.LG\\rComments: 15 pages, 10 figures\\r\\\\\\\\\\r  Knowledge graphs (KGs) consist of links that describe relationships between\\rentities. Due to the difficulty of manually enumerating all relationships\\rbetween entities, automatically completing them is essential for KGs. Knowledge\\rGraph Completion (KGC) is a task that infers unseen relationships between\\rentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\\rTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\\ronly the knowledge from training data. In contrast, the recent Pre-trained\\rLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\\rTherefore, PLM-based KGC can estimate missing links between entities by reusing\\rmemorized knowledge from pre-training without inference. This approach is\\rproblematic because building KGC models aims to infer unseen links between\\rentities. However, conventional evaluations in KGC do not consider inference\\rand memorization abilities separately. Thus, a PLM-based KGC method, which\\rachieves high performance in current KGC evaluations, may be ineffective in\\rpractical applications. To address this issue, we analyze whether PLM-based KGC\\rmethods make inferences or merely access memorized knowledge. For this purpose,\\rwe propose a method for constructing synthetic datasets specified in this\\ranalysis and conclude that PLMs acquire the inference abilities required for\\rKGC through pre-training, even though the performance improvements mostly come\\rfrom textual information of entities and relations.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09109 ,  9314kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09114\\rDate: Wed, 15 Nov 2023 17:04:56 GMT   (8042kb,D)\\r\\rTitle: Ever: Mitigating Hallucination in Large Language Models through\\r  Real-Time Verification and Rectification\\rAuthors: Haoqiang Kang, Juntong Ni, Huaxiu Yao\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  Large Language Models (LLMs) have demonstrated remarkable proficiency in\\rgenerating fluent text. However, they often encounter the challenge of\\rgenerating inaccurate or hallucinated content. This issue is common in both\\rnon-retrieval-based generation and retrieval-augmented generation approaches,\\rand existing post-hoc rectification methods may not address the accumulated\\rhallucination errors that may be caused by the snowballing issue, especially\\rin reasoning tasks. To tackle these challenges, we introduce a novel approach\\rcalled Real-time Verification and Rectification (Ever). Instead of waiting\\runtil the end of the generation process to rectify hallucinations, Ever employs\\ra real-time, step-wise generation and hallucination rectification strategy. The\\rprimary objective is to detect and rectify hallucinations as they occur during\\rthe text generation process. When compared to both retrieval-based and\\rnon-retrieval-based baselines, Ever demonstrates a significant improvement in\\rgenerating trustworthy and factually accurate text across a diverse range of\\rtasks, including short-form QA, biography generation, and multi-hop reasoning.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09114 ,  8042kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09117\\rDate: Wed, 15 Nov 2023 17:07:44 GMT   (1743kb,D)\\r\\rTitle: R-Spin: Efficient Speaker and Noise-invariant Representation Learning\\r  with Acoustic Pieces\\rAuthors: Heng-Jui Chang, James Glass\\rCategories: cs.CL cs.SD eess.AS\\rComments: Preprint, work in progress\\r\\\\\\\\\\r  This paper introduces Robust Spin (R-Spin), a data-efficient self-supervised\\rfine-tuning framework for speaker and noise-invariant speech representations by\\rlearning discrete acoustic units with speaker-invariant clustering (Spin).\\rR-Spin resolves Spin's issues and enhances content representations by learning\\rto predict acoustic pieces. R-Spin offers a 12X reduction in computational\\rresources compared to previous state-of-the-art methods while outperforming\\rthem in severely distorted speech scenarios. This paper provides detailed\\ranalyses to show how discrete units contribute to speech encoder training and\\rimproving robustness in diverse acoustic environments.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09117 ,  1743kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09122\\rDate: Wed, 15 Nov 2023 17:09:54 GMT   (254kb,D)\\r\\rTitle: Universal NER: A Gold-Standard Multilingual Named Entity Recognition\\r  Benchmark\\rAuthors: Stephen Mayhew, Terra Blevins, Shuheng Liu, Marek \\\\v{S}uppa, Hila\\r  Gonen, Joseph Marvin Imperial, B\\\\orje F. Karlsson, Peiqin Lin, Nikola\\r  Ljube\\\\v{s}i\\\\'c, LJ Miranda, Barbara Plank, Arij Riabi, Yuval Pinter\\rCategories: cs.CL\\r\\\\\\\\\\r  We introduce Universal NER (UNER), an open, community-driven project to\\rdevelop gold-standard NER benchmarks in many languages. The overarching goal of\\rUNER is to provide high-quality, cross-lingually consistent annotations to\\rfacilitate and standardize multilingual NER research. UNER v1 contains 18\\rdatasets annotated with named entities in a cross-lingual consistent schema\\racross 12 diverse languages. In this paper, we detail the dataset creation and\\rcomposition of UNER; we also provide initial modeling baselines on both\\rin-language and cross-lingual learning settings. We release the data, code, and\\rfitted models to the public.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09122 ,  254kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09130\\rDate: Wed, 15 Nov 2023 17:20:20 GMT   (10780kb,D)\\r\\rTitle: Social Meme-ing: Measuring Linguistic Variation in Memes\\rAuthors: Naitian Zhou, David Jurgens and David Bamman\\rCategories: cs.CL\\r\\\\\\\\\\r  Much work in the space of NLP has used computational methods to explore\\rsociolinguistic variation in text. In this paper, we argue that memes, as\\rmultimodal forms of language comprised of visual templates and text, also\\rexhibit meaningful social variation. We construct a computational pipeline to\\rcluster individual instances of memes into templates and semantic variables,\\rtaking advantage of their multimodal structure in doing so. We apply this\\rmethod to a large collection of meme images from Reddit and make available the\\rresulting \\\\textsc{SemanticMemes} dataset of 3.8M images clustered by their\\rsemantic function. We use these clusters to analyze linguistic variation in\\rmemes, discovering not only that socially meaningful variation in meme usage\\rexists between subreddits, but that patterns of meme innovation and\\racculturation within these communities align with previous findings on written\\rlanguage.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09130 ,  10780kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09132\\rDate: Wed, 15 Nov 2023 17:21:58 GMT   (236kb,D)\\r\\rTitle: Aligning Neural Machine Translation Models: Human Feedback in Training\\r  and Inference\\rAuthors: Miguel Moura Ramos, Patrick Fernandes, Ant\\\\'onio Farinhas, Andr\\\\'e F.\\r  T. Martins\\rCategories: cs.CL\\rComments: 14 pages, work-in-progress\\r\\\\\\\\\\r  Reinforcement learning from human feedback (RLHF) is a recent technique to\\rimprove the quality of the text generated by a language model, making it closer\\rto what humans would generate. A core ingredient in RLHF's success in aligning\\rand improving large language models (LLMs) is its reward model, trained using\\rhuman feedback on model outputs. In machine translation (MT), where metrics\\rtrained from human annotations can readily be used as reward models, recent\\rmethods using minimum Bayes risk decoding and reranking have succeeded in\\rimproving the final quality of translation. In this study, we comprehensively\\rexplore and compare techniques for integrating quality metrics as reward models\\rinto the MT pipeline. This includes using the reward model for data filtering,\\rduring the training phase through RL, and at inference time by employing\\rreranking techniques, and we assess the effects of combining these in a unified\\rapproach. Our experimental results, conducted across multiple translation\\rtasks, underscore the crucial role of effective data filtering, based on\\restimated quality, in harnessing the full potential of RL in enhancing MT\\rquality. Furthermore, our findings demonstrate the effectiveness of combining\\rRL training with reranking techniques, showcasing substantial improvements in\\rtranslation quality.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09132 ,  236kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09136\\rDate: Wed, 15 Nov 2023 17:27:14 GMT   (7982kb,D)\\r\\rTitle: RRescue: Ranking LLM Responses to Enhance Reasoning Over Context\\rAuthors: Yikun Wang and Rui Zheng and Haoming Li and Qi Zhang and Tao Gui and\\r  Fei Liu\\rCategories: cs.CL\\r\\\\\\\\\\r  Effectively using a given context is paramount for large language models. A\\rcontext window can include task specifications, retrieved documents, previous\\rconversations, and even model self-reflections, functioning similarly to\\repisodic memory. While efforts are being made to expand the context window,\\rstudies indicate that LLMs do not use their context optimally for response\\rgeneration. In this paper, we present a novel approach to optimize LLMs using\\rranking metrics, which teaches LLMs to rank a collection of\\rcontextually-grounded candidate responses. Rather than a traditional full\\rordering, we advocate for a partial ordering. This is because achieving\\rconsensus on the perfect order for system responses can be challenging. Our\\rpartial ordering is more robust, less sensitive to noise, and can be acquired\\rthrough human labelers, heuristic functions, or model distillation. We test our\\rsystem's improved contextual understanding using the latest benchmarks,\\rincluding a new multi-document question answering dataset. We conduct ablation\\rstudies to understand crucial factors, such as how to gather candidate\\rresponses, determine their most suitable order, and balance supervised\\rfine-tuning with ranking metrics. Our approach, named RRescue, suggests a\\rpromising avenue for enhancing LLMs' contextual understanding via response\\rranking.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09136 ,  7982kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09144\\rDate: Wed, 15 Nov 2023 17:40:27 GMT   (314kb,D)\\r\\rTitle: Grounding or Guesswork? Large Language Models are Presumptive Grounders\\rAuthors: Omar Shaikh, Kristina Gligori\\\\'c, Ashna Khetan, Matthias Gerstgrasser,\\r  Diyi Yang, Dan Jurafsky\\rCategories: cs.CL cs.HC\\rComments: 16 pages, 2 figures\\r\\\\\\\\\\r  Effective conversation requires common ground: a shared understanding between\\rthe participants. Common ground, however, does not emerge spontaneously in\\rconversation. Speakers and listeners work together to both identify and\\rconstruct a shared basis while avoiding misunderstanding. To accomplish\\rgrounding, humans rely on a range of dialogue acts, like clarification (What do\\ryou mean?) and acknowledgment (I understand.). In domains like teaching and\\remotional support, carefully constructing grounding prevents misunderstanding.\\rHowever, it is unclear whether large language models (LLMs) leverage these\\rdialogue acts in constructing common ground. To this end, we curate a set of\\rgrounding acts and propose corresponding metrics that quantify attempted\\rgrounding. We study whether LLMs use these grounding acts, simulating them\\rtaking turns from several dialogue datasets, and comparing the results to\\rhumans. We find that current LLMs are presumptive grounders, biased towards\\rassuming common ground without using grounding acts. To understand the roots of\\rthis behavior, we examine the role of instruction tuning and reinforcement\\rlearning with human feedback (RLHF), finding that RLHF leads to less grounding.\\rAltogether, our work highlights the need for more research investigating\\rgrounding in human-AI interaction.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09144 ,  314kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09149\\rDate: Wed, 15 Nov 2023 17:46:39 GMT   (1561kb,D)\\r\\rTitle: Temporal Knowledge Question Answering via Abstract Reasoning Induction\\rAuthors: Ziyang Chen, Dongfang Li, Xiang Zhao, Baotian Hu, Min Zhang\\rCategories: cs.CL cs.AI\\rComments: 17 pages, 10 figures\\rACM-class: I.2.7\\r\\\\\\\\\\r  In this paper, we tackle the significant challenge of temporal knowledge\\rreasoning in Large Language Models (LLMs), an area where such models frequently\\rencounter difficulties. These difficulties often result in the generation of\\rmisleading or incorrect information, primarily due to their limited capacity to\\rprocess evolving factual knowledge and complex temporal logic. In response, we\\rpropose a novel, constructivism-based approach that advocates for a paradigm\\rshift in LLM learning towards an active, ongoing process of knowledge synthesis\\rand customization. At the heart of our proposal is the Abstract Reasoning\\rInduction ARI framework, which divides temporal reasoning into two distinct\\rphases: Knowledge-agnostic and Knowledge-based. This division aims to reduce\\rinstances of hallucinations and improve LLMs' capacity for integrating abstract\\rmethodologies derived from historical data. Our approach achieves remarkable\\rimprovements, with relative gains of 29.7\\\\% and 9.27\\\\% on two temporal QA\\rdatasets, underscoring its efficacy in advancing temporal reasoning in LLMs.\\rThe code will be released at https://github.com/czy1999/ARI.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09149 ,  1561kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09154\\rDate: Wed, 15 Nov 2023 17:50:30 GMT   (1793kb,D)\\r\\rTitle: CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models\\rAuthors: Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu\\r  Hu, Yiran Wei, Rui Wang, Hongyuan Lu\\rCategories: cs.CL\\r\\\\\\\\\\r  We are currently in an era of fierce competition among various large language\\rmodels (LLMs) continuously pushing the boundaries of benchmark performance.\\rHowever, genuinely assessing the capabilities of these LLMs has become a\\rchallenging and critical issue due to potential data contamination, and it\\rwastes dozens of time and effort for researchers and engineers to download and\\rtry those contaminated models. To save our precious time, we propose a novel\\rand useful method, Clean-Eval, which mitigates the issue of data contamination\\rand evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to\\rparaphrase and back-translate the contaminated data into a candidate set,\\rgenerating expressions with the same meaning but in different surface forms. A\\rsemantic detector is then used to filter the generated low-quality samples to\\rnarrow down this candidate set. The best candidate is finally selected from\\rthis set based on the BLEURT score. According to human assessment, this best\\rcandidate is semantically similar to the original contamination data but\\rexpressed differently. All candidates can form a new benchmark to evaluate the\\rmodel. Our experiments illustrate that Clean-Eval substantially restores the\\ractual evaluation results on contaminated LLMs under both few-shot learning and\\rfine-tuning scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09154 ,  1793kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09174\\rDate: Wed, 15 Nov 2023 18:11:23 GMT   (530kb,D)\\r\\rTitle: AbsPyramid: Benchmarking the Abstraction Ability of Language Models with\\r  a Unified Entailment Graph\\rAuthors: Zhaowei Wang, Haochen Shi, Weiqi Wang, Tianqing Fang, Hongming Zhang,\\r  Sehyun Choi, Xin Liu, Yangqiu Song\\rCategories: cs.CL cs.AI\\rComments: Work in progress\\r\\\\\\\\\\r  Cognitive research indicates that abstraction ability is essential in human\\rintelligence, which remains under-explored in language models. In this paper,\\rwe present AbsPyramid, a unified entailment graph of 221K textual descriptions\\rof abstraction knowledge. While existing resources only touch nouns or verbs\\rwithin simplified events or specific domains, AbsPyramid collects abstract\\rknowledge for three components of diverse events to comprehensively evaluate\\rthe abstraction ability of language models in the open domain. Experimental\\rresults demonstrate that current LLMs face challenges comprehending abstraction\\rknowledge in zero-shot and few-shot settings. By training on our rich\\rabstraction knowledge, we find LLMs can acquire basic abstraction abilities and\\rgeneralize to unseen events. In the meantime, we empirically show that our\\rbenchmark is comprehensive to enhance LLMs across two previous abstraction\\rtasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09174 ,  530kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09179\\rDate: Wed, 15 Nov 2023 18:15:37 GMT   (96kb,D)\\r\\rTitle: SiRA: Sparse Mixture of Low Rank Adaptation\\rAuthors: Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei\\r  Shu, Han Lu, Canoee Liu, Liangchen Luo, Jindong Chen, Lei Meng\\rCategories: cs.CL\\r\\\\\\\\\\r  Parameter Efficient Tuning has been an prominent approach to adapt the Large\\rLanguage Model to downstream tasks. Most previous works considers adding the\\rdense trainable parameters, where all parameters are used to adapt certain\\rtask. We found this less effective empirically using the example of LoRA that\\rintroducing more trainable parameters does not help. Motivated by this we\\rinvestigate the importance of leveraging sparse computation and propose SiRA:\\rsparse mixture of low rank adaption. SiRA leverages the Sparse Mixture of\\rExpert(SMoE) to boost the performance of LoRA. Specifically it enforces the top\\r$k$ experts routing with a capacity limit restricting the maximum number of\\rtokens each expert can process. We propose a novel and simple expert dropout on\\rtop of gating network to reduce the over-fitting issue. Through extensive\\rexperiments, we verify SiRA performs better than LoRA and other mixture of\\rexpert approaches across different single tasks and multitask settings.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09179 ,  96kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09180\\rDate: Wed, 15 Nov 2023 18:19:58 GMT   (1283kb,D)\\r\\rTitle: PEARL: Personalizing Large Language Model Writing Assistants with\\r  Generation-Calibrated Retrievers\\rAuthors: Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes,\\r  Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi\\rCategories: cs.CL cs.HC cs.IR\\rComments: Pre-print, work in progress\\r\\\\\\\\\\r  Powerful large language models have facilitated the development of writing\\rassistants that promise to significantly improve the quality and efficiency of\\rcomposition and communication. However, a barrier to effective assistance is\\rthe lack of personalization in LLM outputs to the author's communication style\\rand specialized knowledge. In this paper, we address this challenge by\\rproposing PEARL, a retrieval-augmented LLM writing assistant personalized with\\ra generation-calibrated retriever. Our retriever is trained to select historic\\ruser-authored documents for prompt augmentation, such that they are likely to\\rbest personalize LLM generations for a user request. We propose two key\\rnovelties for training our retriever: 1) A training data selection method that\\ridentifies user requests likely to benefit from personalization and documents\\rthat provide that benefit; and 2) A scale-calibrating KL-divergence objective\\rthat ensures that our retriever closely tracks the benefit of a document for\\rpersonalized generation. We demonstrate the effectiveness of PEARL in\\rgenerating personalized workplace social media posts and Reddit comments.\\rFinally, we showcase the potential of a generation-calibrated retriever to\\rdouble as a performance predictor and further improve low-quality generations\\rvia LLM chaining.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09180 ,  1283kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09182\\rDate: Wed, 15 Nov 2023 18:23:17 GMT   (8911kb,D)\\r\\rTitle: ContraDoc: Understanding Self-Contradictions in Documents with Large\\r  Language Models\\rAuthors: Jierui Li, Vipul Raheja, Dhruv Kumar\\rCategories: cs.CL\\r\\\\\\\\\\r  In recent times, large language models (LLMs) have shown impressive\\rperformance on various document-level tasks such as document classification,\\rsummarization, and question-answering. However, research on understanding their\\rcapabilities on the task of self-contradictions in long documents has been very\\rlimited. In this work, we introduce ContraDoc, the first human-annotated\\rdataset to study self-contradictions in long documents across multiple domains,\\rvarying document lengths, self-contradictions types, and scope. We then analyze\\rthe current capabilities of four state-of-the-art open-source and commercially\\ravailable LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4\\rperforms the best and can outperform humans on this task, we find that it is\\rstill unreliable and struggles with self-contradictions that require more\\rnuance and context. We release the dataset and all the code associated with the\\rexperiments.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09182 ,  8911kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09184\\rDate: Wed, 15 Nov 2023 18:25:26 GMT   (8331kb,D)\\r\\rTitle: Benchmarking Generation and Evaluation Capabilities of Large Language\\r  Models for Instruction Controllable Summarization\\rAuthors: Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han,\\r  Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan\\rCategories: cs.CL cs.LG\\rComments: GitHub Repo: https://github.com/yale-nlp/InstruSum\\r\\\\\\\\\\r  While large language models (LLMs) already achieve strong performance on\\rstandard generic summarization benchmarks, their performance on more complex\\rsummarization task settings is less studied. Therefore, we benchmark LLMs on\\rinstruction controllable text summarization, where the model input consists of\\rboth a source article and a natural language requirement for the desired\\rsummary characteristics. To this end, we curate an evaluation-only dataset for\\rthis task setting and conduct human evaluation on 5 LLM-based summarization\\rsystems. We then benchmark LLM-based automatic evaluation for this task with 4\\rdifferent evaluation protocols and 11 LLMs, resulting in 40 evaluation methods\\rin total. Our study reveals that instruction controllable text summarization\\rremains a challenging task for LLMs, since (1) all LLMs evaluated still make\\rfactual and other types of errors in their summaries; (2) all LLM-based\\revaluation methods cannot achieve a strong alignment with human annotators when\\rjudging the quality of candidate summaries; (3) different LLMs show large\\rperformance gaps in summary generation and evaluation. We make our collected\\rbenchmark, InstruSum, publicly available to facilitate future research in this\\rdirection.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09184 ,  8331kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09188\\rDate: Wed, 15 Nov 2023 18:28:29 GMT   (2406kb,D)\\r\\rTitle: Towards Verifiable Text Generation with Symbolic References\\rAuthors: Lucas Torroba Hennigen, Shannon Shen, Aniruddha Nrusimha, Bernhard\\r  Gapp, David Sontag, Yoon Kim\\rCategories: cs.CL cs.AI cs.LG\\rComments: 46 pages, 4 figures, 6 tables\\r\\\\\\\\\\r  Large language models (LLMs) have demonstrated an impressive ability to\\rsynthesize plausible and fluent text. However they remain vulnerable to\\rhallucinations, and thus their outputs generally require manual human\\rverification for high-stakes applications, which can be time-consuming and\\rdifficult. This paper proposes symbolically grounded generation (SymGen) as a\\rsimple approach for enabling easier validation of an LLM's output. SymGen\\rprompts an LLM to interleave its regular output text with explicit symbolic\\rreferences to fields present in some conditioning data (e.g., a table in JSON\\rformat). The references can be used to display the provenance of different\\rspans of text in the generation, reducing the effort required for manual\\rverification. Across data-to-text and question answering experiments, we find\\rthat LLMs are able to directly output text that makes use of symbolic\\rreferences while maintaining fluency and accuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09188 ,  2406kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09189\\rDate: Wed, 15 Nov 2023 18:32:27 GMT   (674kb,D)\\r\\rTitle: PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for\\r  Mental Health\\rAuthors: Haoan Jin, Siyuan Chen, Mengyue Wu, Kenny Q. Zhu\\rCategories: cs.CL\\r\\\\\\\\\\r  Recently, there has been a growing interest in utilizing large language\\rmodels (LLMs) in mental health research, with studies showcasing their\\rremarkable capabilities, such as disease detection. However, there is currently\\ra lack of a comprehensive benchmark for evaluating the capability of LLMs in\\rthis domain. Therefore, we address this gap by introducing the first\\rcomprehensive benchmark tailored to the unique characteristics of the mental\\rhealth domain. This benchmark encompasses a total of six sub-tasks, covering\\rthree dimensions, to systematically assess the capabilities of LLMs in the\\rrealm of mental health. We have designed corresponding concise prompts for each\\rsub-task. And we comprehensively evaluate a total of eight advanced LLMs using\\rour benchmark. Experiment results not only demonstrate significant room for\\rimprovement in current LLMs concerning mental health but also unveil potential\\rdirections for future model optimization.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09189 ,  674kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09193\\rDate: Wed, 15 Nov 2023 18:39:21 GMT   (5968kb,D)\\r\\rTitle: The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task\\rAuthors: Yifan Wu, Pengchuan Zhang, Wenhan Xiong, Barlas Oguz, James C. Gee,\\r  Yixin Nie\\rCategories: cs.CL cs.AI cs.CV\\r\\\\\\\\\\r  The study explores the effectiveness of the Chain-of-Thought approach, known\\rfor its proficiency in language tasks by breaking them down into sub-tasks and\\rintermediate steps, in improving vision-language tasks that demand\\rsophisticated perception and reasoning. We present the Description then\\rDecision strategy, which is inspired by how humans process signals. This\\rstrategy significantly improves probing task performance by 50%, establishing\\rthe groundwork for future research on reasoning paradigms in complex\\rvision-language tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09193 ,  5968kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09194\\rDate: Wed, 15 Nov 2023 18:39:56 GMT   (119kb,D)\\r\\rTitle: Structural Priming Demonstrates Abstract Grammatical Representations in\\r  Multilingual Language Models\\rAuthors: James A. Michaelov, Catherine Arnett, Tyler A. Chang, Benjamin K.\\r  Bergen\\rCategories: cs.CL\\rComments: Accepted at EMNLP 2023\\r\\\\\\\\\\r  Abstract grammatical knowledge - of parts of speech and grammatical patterns\\r- is key to the capacity for linguistic generalization in humans. But how\\rabstract is grammatical knowledge in large language models? In the human\\rliterature, compelling evidence for grammatical abstraction comes from\\rstructural priming. A sentence that shares the same grammatical structure as a\\rpreceding sentence is processed and produced more readily. Because confounds\\rexist when using stimuli in a single language, evidence of abstraction is even\\rmore compelling from crosslingual structural priming, where use of a syntactic\\rstructure in one language primes an analogous structure in another language. We\\rmeasure crosslingual structural priming in large language models, comparing\\rmodel behavior to human experimental results from eight crosslingual\\rexperiments covering six languages, and four monolingual structural priming\\rexperiments in three non-English languages. We find evidence for abstract\\rmonolingual and crosslingual grammatical representations in the models that\\rfunction similarly to those found in humans. These results demonstrate that\\rgrammatical representations in multilingual language models are not only\\rsimilar across languages, but they can causally influence text produced in\\rdifferent languages.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09194 ,  119kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09198\\rDate: Wed, 15 Nov 2023 18:42:44 GMT   (8647kb,D)\\r\\rTitle: Never Lost in the Middle: Improving Large Language Models via Attention\\r  Strengthening Question Answering\\rAuthors: He Junqing, Pan Kunhao, Dong Xiaoqun, Song Zhuoyang, Liu Yibo, Liang\\r  Yuxin, Wang Hao, Sun Qianguo, Zhang Songxin, Xie Zejian, Zhang Jiaxing\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  While large language models (LLMs) are equipped with longer text input\\rcapabilities than before, they are struggling to seek correct information in\\rlong contexts. The lost in the middle problem challenges most LLMs, referring\\rto the dramatic decline in accuracy when correct information is located in the\\rmiddle. To overcome this crucial issue, this paper proposes to enhance the\\rinformation searching and reflection ability of LLMs in long contexts via\\rspecially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).\\rFollowing these tasks, our model excels in focusing more precisely on the\\rdesired information. Experimental results show substantial improvement in\\rMulti-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%\\rabsolute gain in shuffled settings, by 21.5% in passage retrieval task. We\\rrelease our model, Ziya-Reader to promote related research in the community.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09198 ,  8647kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09204\\rDate: Wed, 15 Nov 2023 18:46:56 GMT   (29kb,D)\\r\\rTitle: Fusion-Eval: Integrating Evaluators with LLMs\\rAuthors: Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong\\r  Chen, Lei Meng\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Evaluating Large Language Models (LLMs) is a complex task, especially\\rconsidering the intricacies of natural language understanding and the\\rexpectations for high-level reasoning. Traditional evaluations typically lean\\ron human-based, model-based, or automatic-metrics-based paradigms, each with\\rits own advantages and shortcomings. We introduce Fusion-Eval, a system that\\remploys LLMs not solely for direct evaluations, but to skillfully integrate\\rinsights from diverse evaluators. This gives Fusion-Eval flexibility, enabling\\rit to work effectively across diverse tasks and make optimal use of multiple\\rreferences. In testing on the SummEval dataset, Fusion-Eval achieved a Spearman\\rcorrelation of 0.96, outperforming other evaluators. The success of Fusion-Eval\\runderscores the potential of LLMs to produce evaluations that closely align\\rhuman perspectives, setting a new standard in the field of LLM evaluation.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09204 ,  29kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09205\\rDate: Wed, 15 Nov 2023 18:47:42 GMT   (804kb,D)\\r\\rTitle: When Is Multilinguality a Curse? Language Modeling for 250 High- and\\r  Low-Resource Languages\\rAuthors: Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen\\rCategories: cs.CL\\r\\\\\\\\\\r  Multilingual language models are widely used to extend NLP systems to\\rlow-resource languages. However, concrete evidence for the effects of\\rmultilinguality on language modeling performance in individual languages\\rremains scarce. Here, we pre-train over 10,000 monolingual and multilingual\\rlanguage models for over 250 languages, including multiple language families\\rthat are under-studied in NLP. We assess how language modeling performance in\\reach language varies as a function of (1) monolingual dataset size, (2) added\\rmultilingual dataset size, (3) linguistic similarity of the added languages,\\rand (4) model size (up to 45M parameters). We find that in moderation, adding\\rmultilingual data improves low-resource language modeling performance, similar\\rto increasing low-resource dataset sizes by up to 33%. Improvements depend on\\rthe syntactic similarity of the added multilingual data, with marginal\\radditional effects of vocabulary overlap. However, high-resource languages\\rconsistently perform worse in multilingual pre-training scenarios. As dataset\\rsizes increase, adding multilingual data begins to hurt performance for both\\rlow-resource and high-resource languages, likely due to limited model capacity\\r(the curse of multilinguality). These results suggest that massively\\rmultilingual pre-training may not be optimal for any languages involved, but\\rthat more targeted models can significantly improve performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09205 ,  804kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09206\\rDate: Wed, 15 Nov 2023 18:47:52 GMT   (959kb,D)\\r\\rTitle: TableLlama: Towards Open Large Generalist Models for Tables\\rAuthors: Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun\\rCategories: cs.CL\\r\\\\\\\\\\r  Semi-structured tables are ubiquitous. There has been a variety of tasks that\\raim to automatically interpret, augment, and query tables. Current methods\\roften require pretraining on tables or special model architecture design, are\\rrestricted to specific table types, or have simplifying assumptions about\\rtables and tasks. This paper makes the first step towards developing\\ropen-source large language models (LLMs) as generalists for a diversity of\\rtable-based tasks. Towards that end, we construct TableInstruct, a new dataset\\rwith a variety of realistic tables and tasks, for instruction tuning and\\revaluating LLMs. We further develop the first open-source generalist model for\\rtables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the\\rlong context challenge. We experiment under both in-domain setting and\\rout-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves\\rcomparable or better performance than the SOTA for each task, despite the\\rlatter often has task-specific design. On 6 out-of-domain datasets, it achieves\\r6-48 absolute point gains compared with the base model, showing that training\\ron TableInstruct enhances the model's generalizability. We will open-source our\\rdataset and trained model to boost future work on developing open generalist\\rmodels for tables.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09206 ,  959kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09210\\rDate: Wed, 15 Nov 2023 18:54:53 GMT   (1043kb,D)\\r\\rTitle: Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language\\r  Models\\rAuthors: Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong\\r  Yu\\rCategories: cs.CL cs.AI\\rComments: Preprint\\r\\\\\\\\\\r  Retrieval-augmented language models (RALMs) represent a substantial\\radvancement in the capabilities of large language models, notably in reducing\\rfactual hallucination by leveraging external knowledge sources. However, the\\rreliability of the retrieved information is not always guaranteed. The\\rretrieval of irrelevant data can lead to misguided responses, and potentially\\rcausing the model to overlook its inherent knowledge, even when it possesses\\radequate information to address the query. Moreover, standard RALMs often\\rstruggle to assess whether they possess adequate knowledge, both intrinsic and\\rretrieved, to provide an accurate answer. In situations where knowledge is\\rlacking, these systems should ideally respond with unknown when the answer is\\runattainable. In response to these challenges, we introduces Chain-of-Noting\\r(CoN), a novel approach aimed at improving the robustness of RALMs in facing\\rnoisy, irrelevant documents and in handling unknown scenarios. The core idea of\\rCoN is to generate sequential reading notes for retrieved documents, enabling a\\rthorough evaluation of their relevance to the given question and integrating\\rthis information to formulate the final answer. We employed ChatGPT to create\\rtraining data for CoN, which was subsequently trained on an LLaMa-2 7B model.\\rOur experiments across four open-domain QA benchmarks show that RALMs equipped\\rwith CoN significantly outperform standard RALMs. Notably, CoN achieves an\\raverage improvement of +7.9 in EM score given entirely noisy retrieved\\rdocuments and +10.5 in rejection rates for real-time questions that fall\\routside the pre-training knowledge scope.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09210 ,  1043kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09212\\rDate: Wed, 15 Nov 2023 18:55:43 GMT   (7260kb,D)\\r\\rTitle: Controllable Text Summarization: Unraveling Challenges, Approaches, and\\r  Prospects - A Survey\\rAuthors: Ashok Urlana, Pruthwik Mishra, Tathagato Roy, Rahul Mishra\\rCategories: cs.CL cs.AI\\rComments: 19 pages, 1 figure\\rACM-class: I.2.7\\r\\\\\\\\\\r  Generic text summarization approaches often fail to address the specific\\rintent and needs of individual users. Recently, scholarly attention has turned\\rto the development of summarization methods that are more closely tailored and\\rcontrolled to align with specific objectives and user needs. While a growing\\rcorpus of research is devoted towards a more controllable summarization, there\\ris no comprehensive survey available that thoroughly explores the diverse\\rcontrollable aspects or attributes employed in this context, delves into the\\rassociated challenges, and investigates the existing solutions. In this survey,\\rwe formalize the Controllable Text Summarization (CTS) task, categorize\\rcontrollable aspects according to their shared characteristics and objectives,\\rand present a thorough examination of existing methods and datasets within each\\rcategory. Moreover, based on our findings, we uncover limitations and research\\rgaps, while also delving into potential solutions and future directions for\\rCTS.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09212 ,  7260kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09213\\rDate: Wed, 15 Nov 2023 18:55:45 GMT   (7969kb,D)\\r\\rTitle: GRIM: GRaph-based Interactive narrative visualization for gaMes\\rAuthors: Jorge Leandro, Sudha Rao, Michael Xu, Weijia Xu, Nebosja Jojic, Chris\\r  Brockett, and Bill Dolan\\rCategories: cs.CL\\r\\\\\\\\\\r  Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The\\rnarratives of these may take years to write and typically involve a large\\rcreative team. In this work, we demonstrate the potential of large generative\\rtext models to assist this process. \\\\textbf{GRIM}, a prototype\\r\\\\textbf{GR}aph-based \\\\textbf{I}nteractive narrative visualization system for\\rga\\\\textbf{M}es, generates a rich narrative graph with branching storylines that\\rmatch a high-level narrative description and constraints provided by the\\rdesigner. Game designers can interactively edit the graph by automatically\\rgenerating new sub-graphs that fit the edits within the original narrative and\\rconstraints. We illustrate the use of \\\\textbf{GRIM} in conjunction with GPT-4,\\rgenerating branching narratives for four well-known stories with different\\rcontextual constraints.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09213 ,  7969kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09214\\rDate: Wed, 15 Nov 2023 18:56:23 GMT   (7964kb,D)\\r\\rTitle: Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive\\r  Thinking from Large Language Models\\rAuthors: Weize Liu, Guocong Li, Kai Zhang, Bang Du, Qiyuan Chen, Xuming Hu,\\r  Hongxia Xu, Jintai Chen, Jian Wu\\rCategories: cs.CL\\rComments: 13 pages, 5 figures\\r\\\\\\\\\\r  Large language models (LLMs) have achieved remarkable advancements in the\\rfield of natural language processing. However, the sheer scale and\\rcomputational demands of these models present formidable challenges when\\rconsidering their practical deployment in resource-constrained contexts. While\\rtechniques such as chain-of-thought (CoT) distillation have displayed promise\\rin distilling LLMs into small language models (SLMs), there is a risk that\\rdistilled SLMs may still carry over flawed reasoning or hallucinations\\rinherited from their LLM counterparts. To address these issues, we propose a\\rtwofold methodology: First, we introduce a novel method for distilling the\\rself-evaluation capability inherent in LLMs into SLMs, which aims to mitigate\\rthe adverse effects of erroneous reasoning and reduce hallucinations. Second,\\rwe advocate for a comprehensive distillation process that incorporates multiple\\rdistinct chain-of-thought and self-evaluation paradigms and ensures a more\\rholistic and robust knowledge transfer into SLMs. Experiments on three NLP\\rbenchmarks demonstrate that our method significantly improves the performance\\rof distilled SLMs and sheds light on the path towards developing smaller models\\rclosely aligned with human cognition.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09214 ,  7964kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09216\\rDate: Wed, 15 Nov 2023 18:58:19 GMT   (2688kb,D)\\r\\rTitle: Assessing Translation capabilities of Large Language Models involving\\r  English and Indian Languages\\rAuthors: Vandan Mujadia, Ashok Urlana, Yash Bhaskar, Penumalla Aditya Pavani,\\r  Kukkapalli Shravya, Parameswari Krishnamurthy and Dipti Misra Sharma\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Generative Large Language Models (LLMs) have achieved remarkable advancements\\rin various NLP tasks. In this work, our aim is to explore the multilingual\\rcapabilities of large language models by using machine translation as a task\\rinvolving English and 22 Indian languages. We first investigate the translation\\rcapabilities of raw large language models, followed by exploring the in-context\\rlearning capabilities of the same raw models. We fine-tune these large language\\rmodels using parameter efficient fine-tuning methods such as LoRA and\\radditionally with full fine-tuning. Through our study, we have identified the\\rbest performing large language model for the translation task involving LLMs,\\rwhich is based on LLaMA.\\r  Our results demonstrate significant progress, with average BLEU scores of\\r13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99,\\r42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for\\rEnglish to Indian languages on IN22 (conversational), IN22 (general),\\rflores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, for\\rIndian languages to English, we achieved average BLEU scores of 14.03, 16.65,\\r16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51,\\rand 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational),\\rIN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets.\\rOverall, our findings highlight the potential and strength of large language\\rmodels for machine translation capabilities, including for languages that are\\rcurrently underrepresented in LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09216 ,  2688kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08438\\rDate: Tue, 14 Nov 2023 14:27:53 GMT   (2757kb,D)\\r\\rTitle: LocaliseBot: Multi-view 3D object localisation with differentiable\\r  rendering for robot grasping\\rAuthors: Sujal Vijayaraghavan and Redwan Alqasemi and Rajiv Dubey and Sudeep\\r  Sarkar\\rCategories: cs.CV cs.LG\\rDOI: 10.1007/978-3-031-25075-0_47\\r\\\\\\\\\\r  Robot grasp typically follows five stages: object detection, object\\rlocalisation, object pose estimation, grasp pose estimation, and grasp\\rplanning. We focus on object pose estimation. Our approach relies on three\\rpieces of information: multiple views of the object, the camera's extrinsic\\rparameters at those viewpoints, and 3D CAD models of objects. The first step\\rinvolves a standard deep learning backbone (FCN ResNet) to estimate the object\\rlabel, semantic segmentation, and a coarse estimate of the object pose with\\rrespect to the camera. Our novelty is using a refinement module that starts\\rfrom the coarse pose estimate and refines it by optimisation through\\rdifferentiable rendering. This is a purely vision-based approach that avoids\\rthe need for other information such as point cloud or depth images. We evaluate\\rour object pose estimation approach on the ShapeNet dataset and show\\rimprovements over the state of the art. We also show that the estimated object\\rpose results in 99.65% grasp accuracy with the ground truth grasp candidates on\\rthe Object Clutter Indoor Dataset (OCID) Grasp dataset, as computed using\\rstandard practice.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08438 ,  2757kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08488\\rDate: Tue, 14 Nov 2023 19:31:19 GMT   (6136kb,D)\\r\\rTitle: MUDD: A New Re-Identification Dataset with Efficient Annotation for\\r  Off-Road Racers in Extreme Conditions\\rAuthors: Jacob Tyo, Motolani Olarinre, Youngseog Chung, Zachary C. Lipton\\rCategories: cs.CV\\r\\\\\\\\\\r  Re-identifying individuals in unconstrained environments remains an open\\rchallenge in computer vision. We introduce the Muddy Racer re-IDentification\\rDataset (MUDD), the first large-scale benchmark for matching identities of\\rmotorcycle racers during off-road competitions. MUDD exhibits heavy mud\\rocclusion, motion blurring, complex poses, and extreme lighting conditions\\rpreviously unseen in existing re-id datasets. We present an annotation\\rmethodology incorporating auxiliary information that reduced labeling time by\\rover 65%. We establish benchmark performance using state-of-the-art re-id\\rmodels including OSNet and ResNet-50. Without fine-tuning, the best models\\rachieve only 33% Rank-1 accuracy. Fine-tuning on MUDD boosts results to 79%\\rRank-1, but significant room for improvement remains. We analyze the impact of\\rreal-world factors including mud, pose, lighting, and more. Our work exposes\\ropen problems in re-identifying individuals under extreme conditions. We hope\\rMUDD serves as a diverse and challenging benchmark to spur progress in robust\\rre-id, especially for computer vision applications in emerging sports\\ranalytics. All code and data can be found at https://github.com/JacobTyo/MUDD.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08488 ,  6136kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08503\\rDate: Tue, 14 Nov 2023 19:53:09 GMT   (687kb,D)\\r\\rTitle: MADG: Margin-based Adversarial Learning for Domain Generalization\\rAuthors: Aveen Dayal, Vimal K. B., Linga Reddy Cenkeramaddi, C. Krishna Mohan,\\r  Abhinav Kumar and Vineeth N Balasubramanian\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Domain Generalization (DG) techniques have emerged as a popular approach to\\raddress the challenges of domain shift in Deep Learning (DL), with the goal of\\rgeneralizing well to the target domain unseen during the training. In recent\\ryears, numerous methods have been proposed to address the DG setting, among\\rwhich one popular approach is the adversarial learning-based methodology. The\\rmain idea behind adversarial DG methods is to learn domain-invariant features\\rby minimizing a discrepancy metric. However, most adversarial DG methods use\\r0-1 loss based $\\\\mathcal{H}\\\\Delta\\\\mathcal{H}$ divergence metric. In contrast,\\rthe margin loss-based discrepancy metric has the following advantages: more\\rinformative, tighter, practical, and efficiently optimizable. To mitigate this\\rgap, this work proposes a novel adversarial learning DG algorithm, MADG,\\rmotivated by a margin loss-based discrepancy metric. The proposed MADG model\\rlearns domain-invariant features across all source domains and uses adversarial\\rtraining to generalize well to the unseen target domain. We also provide a\\rtheoretical analysis of the proposed MADG model based on the unseen target\\rerror bound. Specifically, we construct the link between the source and unseen\\rdomains in the real-valued hypothesis space and derive the generalization bound\\rusing margin loss and Rademacher complexity. We extensively experiment with the\\rMADG model on popular real-world DG datasets, VLCS, PACS, OfficeHome,\\rDomainNet, and TerraIncognita. We evaluate the proposed algorithm on\\rDomainBed's benchmark and observe consistent performance across all the\\rdatasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08503 ,  687kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08525\\rDate: Tue, 14 Nov 2023 20:37:54 GMT   (34749kb,D)\\r\\rTitle: Efficient Rotation Invariance in Deep Neural Networks through Artificial\\r  Mental Rotation\\rAuthors: Lukas Tuggener, Thilo Stadelmann, J\\\\urgen Schmidhuber\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Humans and animals recognize objects irrespective of the beholder's point of\\rview, which may drastically change their appearances. Artificial pattern\\rrecognizers also strive to achieve this, e.g., through translational invariance\\rin convolutional neural networks (CNNs). However, both CNNs and vision\\rtransformers (ViTs) perform very poorly on rotated inputs. Here we present\\rartificial mental rotation (AMR), a novel deep learning paradigm for dealing\\rwith in-plane rotations inspired by the neuro-psychological concept of mental\\rrotation. Our simple AMR implementation works with all common CNN and ViT\\rarchitectures. We test it on ImageNet, Stanford Cars, and Oxford Pet. With a\\rtop-1 error (averaged across datasets and architectures) of $0.743$, AMR\\routperforms the current state of the art (rotational data augmentation, average\\rtop-1 error of $0.626$) by $19\\\\%$. We also easily transfer a trained AMR module\\rto a downstream task to improve the performance of a pre-trained semantic\\rsegmentation model on rotated CoCo from $32.7$ to $55.2$ IoU.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08525 ,  34749kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08539\\rDate: Tue, 14 Nov 2023 21:04:49 GMT   (6632kb,D)\\r\\rTitle: Physical Adversarial Examples for Multi-Camera Systems\\rAuthors: Ana R\\\\u{a}du\\\\c{t}oiu and Jan-Philipp Schulze and Philip Sperl and\\r  Konstantin B\\\\ottinger\\rCategories: cs.CV cs.CR cs.LG\\r\\\\\\\\\\r  Neural networks build the foundation of several intelligent systems, which,\\rhowever, are known to be easily fooled by adversarial examples. Recent advances\\rmade these attacks possible even in air-gapped scenarios, where the autonomous\\rsystem observes its surroundings by, e.g., a camera. We extend these ideas in\\rour research and evaluate the robustness of multi-camera setups against such\\rphysical adversarial examples. This scenario becomes ever more important with\\rthe rise in popularity of autonomous vehicles, which fuse the information of\\rseveral cameras for their driving decision. While we find that multi-camera\\rsetups provide some robustness towards past attack methods, we see that this\\radvantage reduces when optimizing on multiple perspectives at once. We propose\\ra novel attack method that we call Transcender-MC, where we incorporate online\\r3D renderings and perspective projections in the training process. Moreover, we\\rmotivate that certain data augmentation techniques can facilitate the\\rgeneration of successful adversarial examples even further. Transcender-MC is\\r11% more effective in successfully attacking multi-camera setups than\\rstate-of-the-art methods. Our findings offer valuable insights regarding the\\rresilience of object detection in a setup with multiple cameras and motivate\\rthe need of developing adequate defense mechanisms against them.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08539 ,  6632kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08557\\rDate: Tue, 14 Nov 2023 21:39:15 GMT   (13148kb,D)\\r\\rTitle: Low-light Pedestrian Detection in Visible and Infrared Image Feeds:\\r  Issues and Challenges\\rAuthors: Hrishikesh Vachhani, Thangarajah Akilan, Yash Devmurari, Nisharaff\\r  Shaik, Dhruvisha Patel\\rCategories: cs.CV cs.AI cs.LG\\r\\\\\\\\\\r  Pedestrian detection has become a cornerstone for several high-level tasks,\\rincluding autonomous driving, intelligent transportation, and traffic\\rsurveillance. There are several works focussed on pedestrian detection using\\rvisible images, mainly in the daytime. However, this task is very intriguing\\rwhen the environmental conditions change to poor lighting or nighttime.\\rRecently, new ideas have been spurred to use alternative sources, such as Far\\rInfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light\\rconditions. This study comprehensively reviews recent developments in low-light\\rpedestrian detection approaches. It systematically categorizes and analyses\\rvarious algorithms from region-based to non-region-based and graph-based\\rlearning methodologies by highlighting their methodologies, implementation\\rissues, and challenges. It also outlines the key benchmark datasets that can be\\rused for research and development of advanced pedestrian detection algorithms,\\rparticularly in low-light situations\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08557 ,  13148kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08577\\rDate: Tue, 14 Nov 2023 22:46:01 GMT   (33611kb,D)\\r\\rTitle: Finding AI-Generated Faces in the Wild\\rAuthors: Gonzalo J. Aniano Porcile, Jack Gindi, Shivansh Mundra, James R.\\r  Verbus, Hany Farid\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  AI-based image generation has continued to rapidly improve, producing\\rincreasingly more realistic images with fewer obvious visual flaws.\\rAI-generated images are being used to create fake online profiles which in turn\\rare being used for spam, fraud, and disinformation campaigns. As the general\\rproblem of detecting any type of manipulated or synthesized content is\\rreceiving increasing attention, here we focus on a more narrow task of\\rdistinguishing a real face from an AI-generated face. This is particularly\\rapplicable when tackling inauthentic online accounts with a fake user profile\\rphoto. We show that by focusing on only faces, a more resilient and\\rgeneral-purpose artifact can be detected that allows for the detection of\\rAI-generated faces from a variety of GAN- and diffusion-based synthesis\\rengines, and across image resolutions (as low as 128 x 128 pixels) and\\rqualities.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08577 ,  33611kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08581\\rDate: Tue, 14 Nov 2023 22:54:29 GMT   (15201kb,D)\\r\\rTitle: Drivable 3D Gaussian Avatars\\rAuthors: Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael\\r  Zollh\\\\ofer, Justus Thies, Javier Romero\\rCategories: cs.CV\\rComments: Website: https://zielon.github.io/d3ga/\\r\\\\\\\\\\r  We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable\\rmodel for human bodies rendered with Gaussian splats. Current photorealistic\\rdrivable avatars require either accurate 3D registrations during training,\\rdense input images during testing, or both. The ones based on neural radiance\\rfields also tend to be prohibitively slow for telepresence applications. This\\rwork uses the recently presented 3D Gaussian Splatting (3DGS) technique to\\rrender realistic humans at real-time framerates, using dense calibrated\\rmulti-view videos as input. To deform those primitives, we depart from the\\rcommonly used point deformation method of linear blend skinning (LBS) and use a\\rclassic volumetric deformation method: cage deformations. Given their smaller\\rsize, we drive these deformations with joint angles and keypoints, which are\\rmore suitable for communication applications. Our experiments on nine subjects\\rwith varied body shapes, clothes, and motions obtain higher-quality results\\rthan state-of-the-art methods when using the same training and test data.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08581 ,  15201kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08622\\rDate: Wed, 15 Nov 2023 01:00:02 GMT   (7418kb,D)\\r\\rTitle: Multiple-Question Multiple-Answer Text-VQA\\rAuthors: Peng Tang, Srikar Appalaraju, R. Manmatha, Yusheng Xie, Vijay\\r  Mahadevan\\rCategories: cs.CV cs.CL cs.LG\\r\\\\\\\\\\r  We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do\\rtext-VQA in encoder-decoder transformer models. The text-VQA task requires a\\rmodel to answer a question by understanding multi-modal content: text\\r(typically from OCR) and an associated image. To the best of our knowledge,\\ralmost all previous approaches for text-VQA process a single question and its\\rassociated content to predict a single answer. In order to answer multiple\\rquestions from the same image, each question and content are fed into the model\\rmultiple times. In contrast, our proposed MQMA approach takes multiple\\rquestions and content as input at the encoder and predicts multiple answers at\\rthe decoder in an auto-regressive manner at the same time. We make several\\rnovel architectural modifications to standard encoder-decoder transformers to\\rsupport MQMA. We also propose a novel MQMA denoising pre-training task which is\\rdesigned to teach the model to align and delineate multiple questions and\\rcontent with associated answers. MQMA pre-trained model achieves\\rstate-of-the-art results on multiple text-VQA datasets, each with strong\\rbaselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%),\\rDocVQA (+1.1%) absolute improvements over the previous state-of-the-art\\rapproaches.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08622 ,  7418kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08623\\rDate: Wed, 15 Nov 2023 01:01:02 GMT   (649kb,D)\\r\\rTitle: DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder\\r  Transformer Models\\rAuthors: Peng Tang, Pengkai Zhu, Tian Li, Srikar Appalaraju, Vijay Mahadevan,\\r  R. Manmatha\\rCategories: cs.CV cs.CL cs.LG\\r\\\\\\\\\\r  Encoder-decoder transformer models have achieved great success on various\\rvision-language (VL) tasks, but they suffer from high inference latency.\\rTypically, the decoder takes up most of the latency because of the\\rauto-regressive decoding. To accelerate the inference, we propose an approach\\rof performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit\\rencoder-decoder transformer model which is trained with deep supervision so\\rthat each of its decoder layers is capable of generating plausible predictions.\\rIn addition, we leverage simple yet practical techniques, including shared\\rgeneration head and adaptation modules, to keep accuracy when exiting at\\rshallow decoder layers. Based on the multi-exit model, we perform step-level\\rdynamic early exit during inference, where the model may decide to use fewer\\rdecoder layers based on its confidence of the current layer at each individual\\rdecoding step. Considering different number of decoder layers may be used at\\rdifferent decoding steps, we compute deeper-layer decoder features of previous\\rdecoding steps just-in-time, which ensures the features from different decoding\\rsteps are semantically aligned. We evaluate our approach with two\\rstate-of-the-art encoder-decoder transformer models on various VL tasks. We\\rshow our approach can reduce overall inference latency by 30%-60% with\\rcomparable or even higher accuracy compared to baselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08623 ,  649kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08646\\rDate: Wed, 15 Nov 2023 01:53:46 GMT   (9441kb,D)\\r\\rTitle: Painterly Image Harmonization via Adversarial Residual Learning\\rAuthors: Xudong Wang, Li Niu, Junyan Cao, Yan Hong, Liqing Zhang\\rCategories: cs.CV\\rComments: Accepted by WACV2024\\r\\\\\\\\\\r  Image compositing plays a vital role in photo editing. After inserting a\\rforeground object into another background image, the composite image may look\\runnatural and inharmonious. When the foreground is photorealistic and the\\rbackground is an artistic painting, painterly image harmonization aims to\\rtransfer the style of background painting to the foreground object, which is a\\rchallenging task due to the large domain gap between foreground and background.\\rIn this work, we employ adversarial learning to bridge the domain gap between\\rforeground feature map and background feature map. Specifically, we design a\\rdual-encoder generator, in which the residual encoder produces the residual\\rfeatures added to the foreground feature map from main encoder. Then, a\\rpixel-wise discriminator plays against the generator, encouraging the refined\\rforeground feature map to be indistinguishable from background feature map.\\rExtensive experiments demonstrate that our method could achieve more harmonious\\rand visually appealing results than previous methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08646 ,  9441kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08655\\rDate: Wed, 15 Nov 2023 02:28:52 GMT   (705kb)\\r\\rTitle: Review of AlexNet for Medical Image Classification\\rAuthors: Wenhao Tang, Junding Sun, Shuihua Wang, Yudong Zhang\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  In recent years, the rapid development of deep learning has led to a wide\\rrange of applications in the field of medical image classification. The\\rvariants of neural network models with ever-increasing performance share some\\rcommonalities: to try to mitigate overfitting, improve generalization, avoid\\rgradient vanishing and exploding, etc. AlexNet first utilizes the dropout\\rtechnique to mitigate overfitting and the ReLU activation function to avoid\\rgradient vanishing. Therefore, we focus our discussion on AlexNet, which has\\rcontributed greatly to the development of CNNs in 2012. After reviewing over 40\\rpapers, including journal papers and conference papers, we give a narrative on\\rthe technical details, advantages, and application areas of AlexNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08655 ,  705kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08657\\rDate: Wed, 15 Nov 2023 02:33:08 GMT   (14688kb,D)\\r\\rTitle: ConeQuest: A Benchmark for Cone Segmentation on Mars\\rAuthors: Mirali Purohit, Jacob Adler, Hannah Kerner\\rCategories: cs.CV cs.LG\\rComments: Accepted at WACV 2024\\r\\\\\\\\\\r  Over the years, space scientists have collected terabytes of Mars data from\\rsatellites and rovers. One important set of features identified in Mars orbital\\rimages is pitted cones, which are interpreted to be mud volcanoes believed to\\rform in regions that were once saturated in water (i.e., a lake or ocean).\\rIdentifying pitted cones globally on Mars would be of great importance, but\\rexpert geologists are unable to sort through the massive orbital image archives\\rto identify all examples. However, this task is well suited for computer\\rvision. Although several computer vision datasets exist for various\\rMars-related tasks, there is currently no open-source dataset available for\\rcone detection/segmentation. Furthermore, previous studies trained models using\\rdata from a single region, which limits their applicability for global\\rdetection and mapping. Motivated by this, we introduce ConeQuest, the first\\rexpert-annotated public dataset to identify cones on Mars. ConeQuest consists\\rof >13k samples from 3 different regions of Mars. We propose two benchmark\\rtasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size\\rGeneralization. We finetune and evaluate widely-used segmentation models on\\rboth benchmark tasks. Results indicate that cone segmentation is a challenging\\ropen problem not solved by existing segmentation models, which achieve an\\raverage IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and\\r(ii), respectively. We believe this new benchmark dataset will facilitate the\\rdevelopment of more accurate and robust models for cone segmentation. Data and\\rcode are available at https://github.com/kerner-lab/ConeQuest.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08657 ,  14688kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08673\\rDate: Wed, 15 Nov 2023 03:37:41 GMT   (2538kb,D)\\r\\rTitle: CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking\\r  Embedding\\rAuthors: Jianzong Wang, Yimin Deng, Ziqi Liang, Xulong Zhang, Ning Cheng, Jing\\r  Xiao\\rCategories: cs.CV\\rComments: Accepted by the 21st IEEE International Symposium on Parallel and\\r  Distributed Processing with Applications (IEEE ISPA 2023)\\r\\\\\\\\\\r  This paper proposes a talking face generation method named CP-EB that takes\\ran audio signal as input and a person image as reference, to synthesize a\\rphoto-realistic people talking video with head poses controlled by a short\\rvideo clip and proper eye blinking embedding. It's noted that not only the head\\rpose but also eye blinking are both important aspects for deep fake detection.\\rThe implicit control of poses by video has already achieved by the state-of-art\\rwork. According to recent research, eye blinking has weak correlation with\\rinput audio which means eye blinks extraction from audio and generation are\\rpossible. Hence, we propose a GAN-based architecture to extract eye blink\\rfeature from input audio and reference video respectively and employ\\rcontrastive training between them, then embed it into the concatenated features\\rof identity and poses to generate talking face images. Experimental results\\rshow that the proposed method can generate photo-realistic talking face with\\rsynchronous lips motions, natural head poses and blinking eyes.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08673 ,  2538kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08747\\rDate: Wed, 15 Nov 2023 07:29:24 GMT   (4221kb,D)\\r\\rTitle: Improved Dense Nested Attention Network Based on Transformer for\\r  Infrared Small Target Detection\\rAuthors: Chun Bao, Jie Cao, Yaqian Ning, Tianhua Zhao, Zhijun Li, Zechen Wang,\\r  Li Zhang, and Qun Hao\\rCategories: cs.CV\\r\\\\\\\\\\r  Infrared small target detection based on deep learning offers unique\\radvantages in separating small targets from complex and dynamic backgrounds.\\rHowever, the features of infrared small targets gradually weaken as the depth\\rof convolutional neural network (CNN) increases. To address this issue, we\\rpropose a novel method for detecting infrared small targets called improved\\rdense nested attention network (IDNANet), which is based on the transformer\\rarchitecture. We preserve the dense nested structure of dense nested attention\\rnetwork (DNANet) and introduce the Swin-transformer during feature extraction\\rstage to enhance the continuity of features. Furthermore, we integrate the\\rACmix attention structure into the dense nested structure to enhance the\\rfeatures of intermediate layers. Additionally, we design a weighted dice binary\\rcross-entropy (WD-BCE) loss function to mitigate the negative impact of\\rforeground-background imbalance in the samples. Moreover, we develop a dataset\\rspecifically for infrared small targets, called BIT-SIRST. The dataset\\rcomprises a significant amount of real-world targets and manually annotated\\rlabels, as well as synthetic data and corresponding labels. We have evaluated\\rthe effectiveness of our method through experiments conducted on public\\rdatasets. In comparison to other state-of-the-art methods, our approach\\routperforms in terms of probability of detection (P_d), false-alarm rate (F_a),\\rand mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89 on the\\rNUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08747 ,  4221kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08759\\rDate: Wed, 15 Nov 2023 08:01:12 GMT   (7446kb,D)\\r\\rTitle: 4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters\\rAuthors: Yijie Zhou, Chao Li, Jin Liang, Tianyi Xu, Xin Liu, Jun Xu\\rCategories: cs.CV\\rComments: WACV2024\\r\\\\\\\\\\r  The illumination of improperly exposed photographs has been widely corrected\\rusing deep convolutional neural networks or Transformers. Despite with\\rpromising performance, these methods usually suffer from large parameter\\ramounts and heavy computational FLOPs on high-resolution photographs. In this\\rpaper, we propose extremely light-weight (with only ~8K parameters) Multi-Scale\\rLinear Transformation (MSLT) networks under the multi-layer perception\\rarchitecture, which can process 4K-resolution sRGB images at 125\\rFrame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT\\rnetworks first decompose an input image into high and low frequency layers by\\rLaplacian pyramid techniques, and then sequentially correct different layers by\\rpixel-adaptive linear transformation, which is implemented by efficient\\rbilateral grid learning or 1x1 convolutions. Experiments on two benchmark\\rdatasets demonstrate the efficiency of our MSLTs against the state-of-the-arts\\ron photo exposure correction. Extensive ablation studies validate the\\reffectiveness of our contributions. The code is available at\\rhttps://github.com/Zhou-Yijie/MSLTNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08759 ,  7446kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08764\\rDate: Wed, 15 Nov 2023 08:13:52 GMT   (829kb,D)\\r\\rTitle: Combining Past, Present and Future: A Self-Supervised Approach for Class\\r  Incremental Learning\\rAuthors: Xiaoshuang Chen, Zhongyi Sun, Ke Yan, Shouhong Ding, Hongtao Lu\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Class Incremental Learning (CIL) aims to handle the scenario where data of\\rnovel classes occur continuously and sequentially. The model should recognize\\rthe sequential novel classes while alleviating the catastrophic forgetting. In\\rthe self-supervised manner, it becomes more challenging to avoid the conflict\\rbetween the feature embedding spaces of novel classes and old ones without any\\rclass labels. To address the problem, we propose a self-supervised CIL\\rframework CPPF, meaning Combining Past, Present and Future. In detail, CPPF\\rconsists of a prototype clustering module (PC), an embedding space reserving\\rmodule (ESR) and a multi-teacher distillation module (MTD). 1) The PC and the\\rESR modules reserve embedding space for subsequent phases at the prototype\\rlevel and the feature level respectively to prepare for knowledge learned in\\rthe future. 2) The MTD module maintains the representations of the current\\rphase without the interference of past knowledge. One of the teacher networks\\rretains the representations of the past phases, and the other teacher network\\rdistills relation information of the current phase to the student network.\\rExtensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our\\rproposed method boosts the performance of self-supervised class incremental\\rlearning. We will release code in the near future.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08764 ,  829kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08782\\rDate: Wed, 15 Nov 2023 08:54:57 GMT   (4514kb,D)\\r\\rTitle: Language Semantic Graph Guided Data-Efficient Learning\\rAuthors: Wenxuan Ma and Shuang Li and Lincan Cai and Jingxuan Kang\\rCategories: cs.CV cs.MM\\rComments: Accepted by NeurIPS 2023\\r\\\\\\\\\\r  Developing generalizable models that can effectively learn from limited data\\rand with minimal reliance on human supervision is a significant objective\\rwithin the machine learning community, particularly in the era of deep neural\\rnetworks. Therefore, to achieve data-efficient learning, researchers typically\\rexplore approaches that can leverage more related or unlabeled data without\\rnecessitating additional manual labeling efforts, such as Semi-Supervised\\rLearning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL\\rleverages unlabeled data in the training process, while TL enables the transfer\\rof expertise from related data distributions. DA broadens the dataset by\\rsynthesizing new data from existing examples. However, the significance of\\radditional knowledge contained within labels has been largely overlooked in\\rresearch. In this paper, we propose a novel perspective on data efficiency that\\rinvolves exploiting the semantic information contained in the labels of the\\ravailable data. Specifically, we introduce a Language Semantic Graph (LSG)\\rwhich is constructed from labels manifest as natural language descriptions.\\rUpon this graph, an auxiliary graph neural network is trained to extract\\rhigh-level semantic relations and then used to guide the training of the\\rprimary model, enabling more adequate utilization of label knowledge. Across\\rimage, video, and audio modalities, we utilize the LSG method in both TL and\\rSSL scenarios and illustrate its versatility in significantly enhancing\\rperformance compared to other data-efficient learning approaches. Additionally,\\rour in-depth analysis shows that the LSG method also expedites the training\\rprocess.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08782 ,  4514kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08786\\rDate: Wed, 15 Nov 2023 08:59:02 GMT   (4791kb,D)\\r\\rTitle: HFORD: High-Fidelity and Occlusion-Robust De-identification for Face\\r  Privacy Protection\\rAuthors: Dongxin Chen, Mingrui Zhu, Nannan Wang, Xinbo Gao\\rCategories: cs.CV\\r\\\\\\\\\\r  With the popularity of smart devices and the development of computer vision\\rtechnology, concerns about face privacy protection are growing. The face\\rde-identification technique is a practical way to solve the identity protection\\rproblem. The existing facial de-identification methods have revealed several\\rproblems, including the impact on the realism of anonymized results when faced\\rwith occlusions and the inability to maintain identity-irrelevant details in\\ranonymized results. We present a High-Fidelity and Occlusion-Robust\\rDe-identification (HFORD) method to deal with these issues. This approach can\\rdisentangle identities and attributes while preserving image-specific details\\rsuch as background, facial features (e.g., wrinkles), and lighting, even in\\roccluded scenes. To disentangle the latent codes in the GAN inversion space, we\\rintroduce an Identity Disentanglement Module (IDM). This module selects the\\rlatent codes that are closely related to the identity. It further separates the\\rlatent codes into identity-related codes and attribute-related codes, enabling\\rthe network to preserve attributes while only modifying the identity. To ensure\\rthe preservation of image details and enhance the network's robustness to\\rocclusions, we propose an Attribute Retention Module (ARM). This module\\radaptively preserves identity-irrelevant details and facial occlusions and\\rblends them into the generated results in a modulated manner. Extensive\\rexperiments show that our method has higher quality, better detail fidelity,\\rand stronger occlusion robustness than other face de-identification methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08786 ,  4791kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08806\\rDate: Wed, 15 Nov 2023 09:22:52 GMT   (5430kb,D)\\r\\rTitle: SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in\\r  Spiking Transformer\\rAuthors: Yue Liu, Shanlin Xiao, Bo Li, Zhiyi Yu\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  As the third-generation neural network, the Spiking Neural Network (SNN) has\\rthe advantages of low power consumption and high energy efficiency, making it\\rsuitable for implementation on edge devices. More recently, the most advanced\\rSNN, Spikformer, combines the self-attention module from Transformer with SNN\\rto achieve remarkable performance. However, it adopts larger channel dimensions\\rin MLP layers, leading to an increased number of redundant model parameters. To\\reffectively decrease the computational complexity and weight parameters of the\\rmodel, we explore the Lottery Ticket Hypothesis (LTH) and discover a very\\rsparse ($\\\\ge$90%) subnetwork that achieves comparable performance to the\\roriginal network. Furthermore, we also design a lightweight token selector\\rmodule, which can remove unimportant background information from images based\\ron the average spike firing rate of neurons, selecting only essential\\rforeground image tokens to participate in attention calculation. Based on that,\\rwe present SparseSpikformer, a co-design framework aimed at achieving sparsity\\rin Spikformer through token and weight pruning techniques. Experimental results\\rdemonstrate that our framework can significantly reduce 90% model parameters\\rand cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining\\rthe accuracy of the original model.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08806 ,  5430kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08811\\rDate: Wed, 15 Nov 2023 09:30:52 GMT   (11736kb,D)\\r\\rTitle: Correlation-aware active learning for surgery video segmentation\\rAuthors: Fei Wu, Pablo Marquez-Neila, Mingyi Zheng, Hedyeh Rafii-Tari, Raphael\\r  Sznitman\\rCategories: cs.CV\\rComments: WACV 2024, 8 pages\\r\\\\\\\\\\r  Semantic segmentation is a complex task that relies heavily on large amounts\\rof annotated image data. However, annotating such data can be time-consuming\\rand resource-intensive, especially in the medical domain. Active Learning (AL)\\ris a popular approach that can help to reduce this burden by iteratively\\rselecting images for annotation to improve the model performance. In the case\\rof video data, it is important to consider the model uncertainty and the\\rtemporal nature of the sequences when selecting images for annotation. This\\rwork proposes a novel AL strategy for surgery video segmentation, \\\\COALSamp{},\\rCOrrelation-aWare Active Learning. Our approach involves projecting images into\\ra latent space that has been fine-tuned using contrastive learning and then\\rselecting a fixed number of representative images from local clusters of video\\rframes. We demonstrate the effectiveness of this approach on two video datasets\\rof surgical instruments and three real-world video datasets. The datasets and\\rcode will be made publicly available upon receiving necessary approvals.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08811 ,  11736kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08835\\rDate: Wed, 15 Nov 2023 10:22:35 GMT   (8196kb,D)\\r\\rTitle: Correlation-guided Query-Dependency Calibration in Video Representation\\r  Learning for Temporal Grounding\\rAuthors: WonJun Moon, Sangeek Hyun, SuBeen Lee, Jae-Pil Heo\\rCategories: cs.CV\\rComments: 20 pages, 14 figures, 14 tables, Code is available at\\r  https://github.com/wjun0830/CGDETR\\r\\\\\\\\\\r  Recent endeavors in video temporal grounding enforce strong cross-modal\\rinteractions through attention mechanisms to overcome the modality gap between\\rvideo and text query. However, previous works treat all video clips equally\\rregardless of their semantic relevance with the text query in attention\\rmodules. In this paper, our goal is to provide clues for query-associated video\\rclips within the crossmodal encoding process. With our Correlation-Guided\\rDetection Transformer~(CG-DETR), we explore the appropriate clip-wise degree of\\rcross-modal interactions and how to exploit such degrees for prediction. First,\\rwe design an adaptive cross-attention layer with dummy tokens. Dummy tokens\\rconditioned by text query take a portion of the attention weights, preventing\\rirrelevant video clips from being represented by the text query. Yet, not all\\rword tokens equally inherit the text query's correlation to video clips. Thus,\\rwe further guide the cross-attention map by inferring the fine-grained\\rcorrelation between video clips and words. We enable this by learning a joint\\rembedding space for high-level concepts, i.e., moment and sentence level, and\\rinferring the clip-word correlation. Lastly, we use a moment-adaptive saliency\\rdetector to exploit each video clip's degrees of text engagement. We validate\\rthe superiority of CG-DETR with the state-of-the-art results on various\\rbenchmarks for both moment retrieval and highlight detection. Codes are\\ravailable at https://github.com/wjun0830/CGDETR.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08835 ,  8196kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08843\\rDate: Wed, 15 Nov 2023 10:33:20 GMT   (12748kb,D)\\r\\rTitle: Personalized Video Relighting Using Casual Light Stage\\rAuthors: Jun Myeong Choi, Max Christman, Roni Sengupta\\rCategories: cs.CV cs.GR\\r\\\\\\\\\\r  In this paper, we develop a personalized video relighting algorithm that\\rproduces high-quality and temporally consistent relit video under any pose,\\rexpression, and lighting conditions in real-time. Existing relighting\\ralgorithms typically rely either on publicly available synthetic data, which\\ryields poor relighting results, or instead on Light Stage data which is\\rinaccessible and is not publicly available. We show that by casually capturing\\rvideo of a user watching YouTube videos on a monitor we can train a\\rpersonalized algorithm capable of producing high-quality relighting under any\\rcondition. Our key contribution is a novel neural relighting architecture that\\reffectively separates the intrinsic appearance features, geometry and\\rreflectance, from the source lighting and then combines it with the target\\rlighting to generate a relit image. This neural architecture enables smoothing\\rof intrinsic appearance features leading to temporally stable video relighting.\\rBoth qualitative and quantitative evaluations show that our relighting\\rarchitecture improves portrait image relighting quality and temporal\\rconsistency over state-of-the-art approaches on both casually captured Light\\rStage at Your Desk (LSYD) data and Light Stage captured One Light At a Time\\r(OLAT) datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08843 ,  12748kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08844\\rDate: Wed, 15 Nov 2023 10:34:14 GMT   (7248kb,D)\\r\\rTitle: Violet: A Vision-Language Model for Arabic Image Captioning with Gemini\\r  Decoder\\rAuthors: Abdelrahman Mohamed, Fakhraddin Alwajih, El Moatez Billah Nagoudi,\\r  Alcides Alcoba Inciarte, Muhammad Abdul-Mageed\\rCategories: cs.CV cs.CL\\rComments: Accepted in ArabicNLP Conference\\r\\\\\\\\\\r  Although image captioning has a vast array of applications, it has not\\rreached its full potential in languages other than English. Arabic, for\\rinstance, although the native language of more than 400 million people, remains\\rlargely underrepresented in this area. This is due to the lack of labeled data\\rand powerful Arabic generative models. We alleviate this issue by presenting a\\rnovel vision-language model dedicated to Arabic, dubbed \\\\textit{Violet}. Our\\rmodel is based on a vision encoder and a Gemini text decoder that maintains\\rgeneration fluency while allowing fusion between the vision and language\\rcomponents. To train our model, we introduce a new method for automatically\\racquiring data from available English datasets. We also manually prepare a new\\rdataset for evaluation. \\\\textit{Violet} performs sizeably better than our\\rbaselines on all of our evaluation datasets. For example, it reaches a CIDEr\\rscore of $61.2$ on our manually annotated dataset and achieves an improvement\\rof $13$ points on Flickr8k.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08844 ,  7248kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08850\\rDate: Wed, 15 Nov 2023 10:42:06 GMT   (13068kb,D)\\r\\rTitle: Controlling the Output of a Generative Model by Latent Feature Vector\\r  Shifting\\rAuthors: R\\\\'obert Belanec, Peter Lacko, Krist\\\\'ina Malinovsk\\\\'a\\rCategories: cs.CV\\rComments: 7 pages, presented on DISA2023 conference in Ko\\\\v{s}ice\\rJournal-ref: R. Belanec, P. Lacko and K. Malinovsk\\\\'a, Controlling the Output\\r  of a Generative Model by Latent Feature Vector Shifting, 2023 World\\r  Symposium on Digital Intelligence for Systems and Machines (DISA),\\r  Ko\\\\v{s}ice, Slovakia, 2023, pp. 24-30\\rDOI: 10.1109/DISA59116.2023.10308936\\r\\\\\\\\\\r  State-of-the-art generative models (e.g. StyleGAN3 \\\\cite{karras2021alias})\\roften generate photorealistic images based on vectors sampled from their latent\\rspace. However, the ability to control the output is limited. Here we present\\rour novel method for latent vector shifting for controlled output image\\rmodification utilizing semantic features of the generated images. In our\\rapproach we use a pre-trained model of StyleGAN3 that generates images of\\rrealistic human faces in relatively high resolution. We complement the\\rgenerative model with a convolutional neural network classifier, namely\\rResNet34, trained to classify the generated images with binary facial features\\rfrom the CelebA dataset. Our latent feature shifter is a neural network model\\rwith a task to shift the latent vectors of a generative model into a specified\\rfeature direction. We have trained latent feature shifter for multiple facial\\rfeatures, and outperformed our baseline method in the number of generated\\rimages with the desired feature. To train our latent feature shifter neural\\rnetwork, we have designed a dataset of pairs of latent vectors with and without\\ra certain feature. Based on the evaluation, we conclude that our latent feature\\rshifter approach was successful in the controlled generation of the StyleGAN3\\rgenerator.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08850 ,  13068kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08863\\rDate: Wed, 15 Nov 2023 10:49:15 GMT   (4905kb,D)\\r\\rTitle: Toulouse Hyperspectral Data Set: a benchmark data set to assess\\r  semi-supervised spectral representation learning and pixel-wise\\r  classification techniques\\rAuthors: Romain Thoreau, Laurent Risser, V\\\\'eronique Achard, B\\\\'eatrice\\r  Berthelot, Xavier Briottet\\rCategories: cs.CV\\rComments: 17 pages, 13 figures\\r\\\\\\\\\\r  Airborne hyperspectral images can be used to map the land cover in large\\rurban areas, thanks to their very high spatial and spectral resolutions on a\\rwide spectral domain. While the spectral dimension of hyperspectral images is\\rhighly informative of the chemical composition of the land surface, the use of\\rstate-of-the-art machine learning algorithms to map the land cover has been\\rdramatically limited by the availability of training data. To cope with the\\rscarcity of annotations, semi-supervised and self-supervised techniques have\\rlately raised a lot of interest in the community. Yet, the publicly available\\rhyperspectral data sets commonly used to benchmark machine learning models are\\rnot totally suited to evaluate their generalization performances due to one or\\rseveral of the following properties: a limited geographical coverage (which\\rdoes not reflect the spectral diversity in metropolitan areas), a small number\\rof land cover classes and a lack of appropriate standard train / test splits\\rfor semi-supervised and self-supervised learning. Therefore, we release in this\\rpaper the Toulouse Hyperspectral Data Set that stands out from other data sets\\rin the above-mentioned respects in order to meet key issues in spectral\\rrepresentation learning and classification over large-scale hyperspectral\\rimages with very few labeled pixels. Besides, we discuss and experiment the\\rself-supervised task of Masked Autoencoders and establish a baseline for\\rpixel-wise classification based on a conventional autoencoder combined with a\\rRandom Forest classifier achieving 82% overall accuracy and 74% F1 score. The\\rToulouse Hyperspectral Data Set and our code are publicly available at\\rhttps://www.toulouse-hyperspectral-data-set.com and\\rhttps://www.github.com/Romain3Ch216/tlse-experiments, respectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08863 ,  4905kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08870\\rDate: Wed, 15 Nov 2023 11:11:25 GMT   (24721kb,D)\\r\\rTitle: One-Shot Federated Learning with Classifier-Guided Diffusion Models\\rAuthors: Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  One-shot federated learning (OSFL) has gained attention in recent years due\\rto its low communication cost. However, most of the existing methods require\\rauxiliary datasets or training generators, which hinders their practicality in\\rreal-world scenarios. In this paper, we explore the novel opportunities that\\rdiffusion models bring to OSFL and propose FedCADO, utilizing guidance from\\rclient classifiers to generate data that complies with clients' distributions\\rand subsequently training the aggregated model on the server. Specifically, our\\rmethod involves targeted optimizations in two aspects. On one hand, we\\rconditionally edit the randomly sampled initial noises, embedding them with\\rspecified semantics and distributions, resulting in a significant improvement\\rin both the quality and stability of generation. On the other hand, we employ\\rthe BN statistics from the classifiers to provide detailed guidance during\\rgeneration. These tailored optimizations enable us to limitlessly generate\\rdatasets, which closely resemble the distribution and quality of the original\\rclient dataset. Our method effectively handles the heterogeneous client models\\rand the problems of non-IID features or labels. In terms of privacy protection,\\rour method avoids training any generator or transferring any auxiliary\\rinformation on clients, eliminating any additional privacy leakage risks.\\rLeveraging the extensive knowledge stored in the pre-trained diffusion model,\\rthe synthetic datasets can assist us in surpassing the knowledge limitations of\\rthe client samples, resulting in aggregation models that even outperform the\\rperformance ceiling of centralized training in some cases, which is\\rconvincingly demonstrated in the sufficient quantification and visualization\\rexperiments conducted on three large-scale multi-domain image datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08870 ,  24721kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08891\\rDate: Wed, 15 Nov 2023 11:51:10 GMT   (10864kb,D)\\r\\rTitle: AdapterShadow: Adapting Segment Anything Model for Shadow Detection\\rAuthors: Leiping Jie and Hui Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  Segment anything model (SAM) has shown its spectacular performance in\\rsegmenting universal objects, especially when elaborate prompts are provided.\\rHowever, the drawback of SAM is twofold. On the first hand, it fails to segment\\rspecific targets, e.g., shadow images or lesions in medical images. On the\\rother hand, manually specifying prompts is extremely time-consuming. To\\rovercome the problems, we propose AdapterShadow, which adapts SAM model for\\rshadow detection. To adapt SAM for shadow images, trainable adapters are\\rinserted into the frozen image encoder of SAM, since the training of the full\\rSAM model is both time and memory consuming. Moreover, we introduce a novel\\rgrid sampling method to generate dense point prompts, which helps to\\rautomatically segment shadows without any manual interventions. Extensive\\rexperiments are conducted on four widely used benchmark datasets to demonstrate\\rthe superior performance of our proposed method. Codes will are publicly\\ravailable at https://github.com/LeipingJie/AdapterShadow.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08891 ,  10864kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08910\\rDate: Wed, 15 Nov 2023 12:31:43 GMT   (16524kb,D)\\r\\rTitle: Progressive Feedback-Enhanced Transformer for Image Forgery Localization\\rAuthors: Haochen Zhu, Gang Cao, Xianglin Huang\\rCategories: cs.CV cs.CR\\r\\\\\\\\\\r  Blind detection of the forged regions in digital images is an effective\\rauthentication means to counter the malicious use of local image editing\\rtechniques. Existing encoder-decoder forensic networks overlook the fact that\\rdetecting complex and subtle tampered regions typically requires more feedback\\rinformation. In this paper, we propose a Progressive FeedbACk-enhanced\\rTransformer (ProFact) network to achieve coarse-to-fine image forgery\\rlocalization. Specifically, the coarse localization map generated by an initial\\rbranch network is adaptively fed back to the early transformer encoder layers\\rfor enhancing the representation of positive features while suppressing\\rinterference factors. The cascaded transformer network, combined with a\\rcontextual spatial pyramid module, is designed to refine discriminative\\rforensic features for improving the forgery localization accuracy and\\rreliability. Furthermore, we present an effective strategy to automatically\\rgenerate large-scale forged image samples close to real-world forensic\\rscenarios, especially in realistic and coherent processing. Leveraging on such\\rsamples, a progressive and cost-effective two-stage training protocol is\\rapplied to the ProFact network. The extensive experimental results on nine\\rpublic forensic datasets show that our proposed localizer greatly outperforms\\rthe state-of-the-art on the generalization ability and robustness of image\\rforgery localization. Code will be publicly available at\\rhttps://github.com/multimediaFor/ProFact.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08910 ,  16524kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08923\\rDate: Wed, 15 Nov 2023 12:55:19 GMT   (5872kb,D)\\r\\rTitle: Leveraging Activation Maximization and Generative Adversarial Training\\r  to Recognize and Explain Patterns in Natural Areas in Satellite Imagery\\rAuthors: Ahmed Emam, Timo T. Stomberg, Ribana Roscher\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Natural protected areas are vital for biodiversity, climate change\\rmitigation, and supporting ecological processes. Despite their significance,\\rcomprehensive mapping is hindered by a lack of understanding of their\\rcharacteristics and a missing land cover class definition. This paper aims to\\radvance the explanation of the designating patterns forming protected and wild\\rareas. To this end, we propose a novel framework that uses activation\\rmaximization and a generative adversarial model. With this, we aim to generate\\rsatellite images that, in combination with domain knowledge, are capable of\\roffering complete and valid explanations for the spatial and spectral patterns\\rthat define the natural authenticity of these regions. Our proposed framework\\rproduces more precise attribution maps pinpointing the designating patterns\\rforming the natural authenticity of protected areas. Our approach fosters our\\runderstanding of the ecological integrity of the protected natural areas and\\rmay contribute to future monitoring and preservation efforts.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08923 ,  5872kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08931\\rDate: Wed, 15 Nov 2023 13:04:57 GMT   (3187kb,D)\\r\\rTitle: Structural-Based Uncertainty in Deep Learning Across Anatomical Scales:\\r  Analysis in White Matter Lesion Segmentation\\rAuthors: Nataliia Molchanova, Vatsal Raina, Andrey Malinin, Francesco La Rosa,\\r  Adrien Depeursinge, Mark Gales, Cristina Granziera, Henning Muller, Mara\\r  Graziani, Meritxell Bach Cuadra\\rCategories: cs.CV\\rComments: Preprint submitted to the journal\\r\\\\\\\\\\r  This paper explores uncertainty quantification (UQ) as an indicator of the\\rtrustworthiness of automated deep-learning (DL) tools in the context of white\\rmatter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of\\rmultiple sclerosis (MS) patients. Our study focuses on two principal aspects of\\runcertainty in structured output segmentation tasks. Firstly, we postulate that\\ra good uncertainty measure should indicate predictions likely to be incorrect\\rwith high uncertainty values. Second, we investigate the merit of quantifying\\runcertainty at different anatomical scales (voxel, lesion, or patient). We\\rhypothesize that uncertainty at each scale is related to specific types of\\rerrors. Our study aims to confirm this relationship by conducting separate\\ranalyses for in-domain and out-of-domain settings. Our primary methodological\\rcontributions are (i) the development of novel measures for quantifying\\runcertainty at lesion and patient scales, derived from structural prediction\\rdiscrepancies, and (ii) the extension of an error retention curve analysis\\rframework to facilitate the evaluation of UQ performance at both lesion and\\rpatient scales. The results from a multi-centric MRI dataset of 172 patients\\rdemonstrate that our proposed measures more effectively capture model errors at\\rthe lesion and patient scales compared to measures that average voxel-scale\\runcertainty values. We provide the UQ protocols code at\\rhttps://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08931 ,  3187kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08955\\rDate: Wed, 15 Nov 2023 13:40:58 GMT   (11928kb)\\r\\rTitle: A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution\\rAuthors: Jianjun Liu, Zebin Wu, Liang Xiao\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  Fusion-based hyperspectral image (HSI) super-resolution aims to produce a\\rhigh-spatial-resolution HSI by fusing a low-spatial-resolution HSI and a\\rhigh-spatial-resolution multispectral image. Such a HSI super-resolution\\rprocess can be modeled as an inverse problem, where the prior knowledge is\\ressential for obtaining the desired solution. Motivated by the success of\\rdiffusion models, we propose a novel spectral diffusion prior for fusion-based\\rHSI super-resolution. Specifically, we first investigate the spectrum\\rgeneration problem and design a spectral diffusion model to model the spectral\\rdata distribution. Then, in the framework of maximum a posteriori, we keep the\\rtransition information between every two neighboring states during the reverse\\rgenerative process, and thereby embed the knowledge of trained spectral\\rdiffusion model into the fusion problem in the form of a regularization term.\\rAt last, we treat each generation step of the final optimization problem as its\\rsubproblem, and employ the Adam to solve these subproblems in a reverse\\rsequence. Experimental results conducted on both synthetic and real datasets\\rdemonstrate the effectiveness of the proposed approach. The code of the\\rproposed approach will be available on https://github.com/liuofficial/SDP.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08955 ,  11928kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08972\\rDate: Wed, 15 Nov 2023 14:04:37 GMT   (5654kb,D)\\r\\rTitle: Unsupervised approaches based on optimal transport and convex analysis\\r  for inverse problems in imaging\\rAuthors: Marcello Carioni, Subhadip Mukherjee, Hong Ye Tan, Junqi Tang\\rCategories: cs.CV cs.LG math.OC\\r\\\\\\\\\\r  Unsupervised deep learning approaches have recently become one of the crucial\\rresearch areas in imaging owing to their ability to learn expressive and\\rpowerful reconstruction operators even when paired high-quality training data\\ris scarcely available. In this chapter, we review theoretically principled\\runsupervised learning schemes for solving imaging inverse problems, with a\\rparticular focus on methods rooted in optimal transport and convex analysis. We\\rbegin by reviewing the optimal transport-based unsupervised approaches such as\\rthe cycle-consistency-based models and learned adversarial regularization\\rmethods, which have clear probabilistic interpretations. Subsequently, we give\\ran overview of a recent line of works on provably convergent learned\\roptimization algorithms applied to accelerate the solution of imaging inverse\\rproblems, alongside their dedicated unsupervised training schemes. We also\\rsurvey a number of provably convergent plug-and-play algorithms (based on\\rgradient-step deep denoisers), which are among the most important and widely\\rapplied unsupervised approaches for imaging problems. At the end of this\\rsurvey, we provide an overview of a few related unsupervised learning\\rframeworks that complement our focused schemes. Together with a detailed\\rsurvey, we provide an overview of the key mathematical results that underlie\\rthe methods reviewed in the chapter to keep our discussion self-contained.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08972 ,  5654kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08995\\rDate: Wed, 15 Nov 2023 14:33:22 GMT   (1563kb)\\r\\rTitle: Simple but Effective Unsupervised Classification for Specified Domain\\r  Images: A Case Study on Fungi Images\\rAuthors: Zhaocong liu, Fa Zhang, Lin Cheng, Huanxi Deng, Xiaoyan Yang, Zhenyu\\r  Zhang, and Chichun Zhou\\rCategories: cs.CV\\r\\\\\\\\\\r  High-quality labeled datasets are essential for deep learning. Traditional\\rmanual annotation methods are not only costly and inefficient but also pose\\rchallenges in specialized domains where expert knowledge is needed.\\rSelf-supervised methods, despite leveraging unlabeled data for feature\\rextraction, still require hundreds or thousands of labeled instances to guide\\rthe model for effective specialized image classification. Current unsupervised\\rlearning methods offer automatic classification without prior annotation but\\roften compromise on accuracy. As a result, efficiently procuring high-quality\\rlabeled datasets remains a pressing challenge for specialized domain images\\rdevoid of annotated data. Addressing this, an unsupervised classification\\rmethod with three key ideas is introduced: 1) dual-step feature dimensionality\\rreduction using a pre-trained model and manifold learning, 2) a voting\\rmechanism from multiple clustering algorithms, and 3) post-hoc instead of prior\\rmanual annotation. This approach outperforms supervised methods in\\rclassification accuracy, as demonstrated with fungal image data, achieving\\r94.1% and 96.7% on public and private datasets respectively. The proposed\\runsupervised classification method reduces dependency on pre-annotated\\rdatasets, enabling a closed-loop for data classification. The simplicity and\\rease of use of this method will also bring convenience to researchers in\\rvarious fields in building datasets, promoting AI applications for images in\\rspecialized domains.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08995 ,  1563kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09004\\rDate: Wed, 15 Nov 2023 14:46:20 GMT   (21500kb,D)\\r\\rTitle: Incremental Object-Based Novelty Detection with Feedback Loop\\rAuthors: Simone Caldarella, Elisa Ricci, Rahaf Aljundi\\rCategories: cs.CV\\r\\\\\\\\\\r  Object-based Novelty Detection (ND) aims to identify unknown objects that do\\rnot belong to classes seen during training by an object detection model. The\\rtask is particularly crucial in real-world applications, as it allows to avoid\\rpotentially harmful behaviours, e.g. as in the case of object detection models\\radopted in a self-driving car or in an autonomous robot. Traditional approaches\\rto ND focus on one time offline post processing of the pretrained object\\rdetection output, leaving no possibility to improve the model robustness after\\rtraining and discarding the abundant amount of out-of-distribution data\\rencountered during deployment.\\r  In this work, we propose a novel framework for object-based ND, assuming that\\rhuman feedback can be requested on the predicted output and later incorporated\\rto refine the ND model without negatively affecting the main object detection\\rperformance. This refinement operation is repeated whenever new feedback is\\ravailable. To tackle this new formulation of the problem for object detection,\\rwe propose a lightweight ND module attached on top of a pre-trained object\\rdetection model, which is incrementally updated through a feedback loop. We\\ralso propose a new benchmark to evaluate methods on this new setting and test\\rextensively our ND approach against baselines, showing increased robustness and\\ra successful incorporation of the received feedback.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09004 ,  21500kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09024\\rDate: Wed, 15 Nov 2023 15:14:16 GMT   (1894kb,D)\\r\\rTitle: Fast Certification of Vision-Language Models Using Incremental\\r  Randomized Smoothing\\rAuthors: A K Nirala (1), A Joshi (2), C Hegde (2), S Sarkar (1) ((1) Iowa State\\r  University, (2) New York University)\\rCategories: cs.CV\\r\\\\\\\\\\r  A key benefit of deep vision-language models such as CLIP is that they enable\\rzero-shot open vocabulary classification; the user has the ability to define\\rnovel class labels via natural language prompts at inference time. However,\\rwhile CLIP-based zero-shot classifiers have demonstrated competitive\\rperformance across a range of domain shifts, they remain highly vulnerable to\\radversarial attacks. Therefore, ensuring the robustness of such models is\\rcrucial for their reliable deployment in the wild.\\r  In this work, we introduce Open Vocabulary Certification (OVC), a fast\\rcertification method designed for open-vocabulary models like CLIP via\\rrandomized smoothing techniques. Given a base training set of prompts and\\rtheir corresponding certified CLIP classifiers, OVC relies on the observation\\rthat a classifier with a novel prompt can be viewed as a perturbed version of\\rnearby classifiers in the base training set. Therefore, OVC can rapidly certify\\rthe novel classifier using a variation of incremental randomized smoothing. By\\rusing a caching trick, we achieve approximately two orders of magnitude\\racceleration in the certification process for novel prompts. To achieve further\\r(heuristic) speedups, OVC approximates the embedding space at a given input\\rusing a multivariate normal distribution bypassing the need for sampling via\\rforward passes through the vision backbone. We demonstrate the effectiveness of\\rOVC on through experimental evaluation using multiple vision-language backbones\\ron the CIFAR-10 and ImageNet test datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09024 ,  1894kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09029\\rDate: Wed, 15 Nov 2023 15:20:24 GMT   (11772kb,D)\\r\\rTitle: Self-Annotated 3D Geometric Learning for Smeared Points Removal\\rAuthors: Miaowei Wang and Daniel Morris\\rCategories: cs.CV\\rComments: The paper is accepted at WACV2024(https://wacv2024.thecvf.com/)\\r\\\\\\\\\\r  There has been significant progress in improving the accuracy and quality of\\rconsumer-level dense depth sensors. Nevertheless, there remains a common depth\\rpixel artifact which we call smeared points. These are points not on any 3D\\rsurface and typically occur as interpolations between foreground and background\\robjects. As they cause fictitious surfaces, these points have the potential to\\rharm applications dependent on the depth maps. Statistical outlier removal\\rmethods fare poorly in removing these points as they tend also to remove actual\\rsurface points. Trained network-based point removal faces difficulty in\\robtaining sufficient annotated data. To address this, we propose a fully\\rself-annotated method to train a smeared point removal classifier. Our approach\\rrelies on gathering 3D geometric evidence from multiple perspectives to\\rautomatically detect and annotate smeared points and valid points. To validate\\rthe effectiveness of our method, we present a new benchmark dataset: the Real\\rAzure-Kinect dataset. Experimental results and ablation studies show that our\\rmethod outperforms traditional filters and other self-annotated methods. Our\\rwork is publicly available at\\rhttps://github.com/wangmiaowei/wacv2024_smearedremover.git.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09029 ,  11772kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09050\\rDate: Wed, 15 Nov 2023 15:40:46 GMT   (2480kb,D)\\r\\rTitle: Improving Zero-shot Visual Question Answering via Large Language Models\\r  with Reasoning Question Prompts\\rAuthors: Yunshi Lan, Xiang Li, Xin Liu, Yang Li, Wei Qin and Weining Qian\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Zero-shot Visual Question Answering (VQA) is a prominent vision-language task\\rthat examines both the visual and textual understanding capability of systems\\rin the absence of training data. Recently, by converting the images into\\rcaptions, information across multi-modalities is bridged and Large Language\\rModels (LLMs) can apply their strong zero-shot generalization capability to\\runseen questions. To design ideal prompts for solving VQA via LLMs, several\\rstudies have explored different strategies to select or generate\\rquestion-answer pairs as the exemplar prompts, which guide LLMs to answer the\\rcurrent questions effectively. However, they totally ignore the role of\\rquestion prompts. The original questions in VQA tasks usually encounter\\rellipses and ambiguity which require intermediate reasoning. To this end, we\\rpresent Reasoning Question Prompts for VQA tasks, which can further activate\\rthe potential of LLMs in zero-shot scenarios. Specifically, for each question,\\rwe first generate self-contained questions as reasoning question prompts via an\\runsupervised question edition module considering sentence fluency, semantic\\rintegrity and syntactic invariance. Each reasoning question prompt clearly\\rindicates the intent of the original question. This results in a set of\\rcandidate answers. Then, the candidate answers associated with their confidence\\rscores acting as answer heuristics are fed into LLMs and produce the final\\ranswer. We evaluate reasoning question prompts on three VQA challenges,\\rexperimental results demonstrate that they can significantly improve the\\rresults of LLMs on zero-shot setting and outperform existing state-of-the-art\\rzero-shot methods on three out of four data sets. Our source code is publicly\\rreleased at \\\\url{https://github.com/ECNU-DASE-NLP/RQP}.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09050 ,  2480kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09064\\rDate: Wed, 15 Nov 2023 16:02:13 GMT   (6175kb,D)\\r\\rTitle: Imagine the Unseen World: A Benchmark for Systematic Generalization in\\r  Visual World Models\\rAuthors: Yeongbin Kim, Gautam Singh, Junyeong Park, Caglar Gulcehre, Sungjin\\r  Ahn\\rCategories: cs.CV cs.LG\\rComments: Published as a conference paper at NeurIPS 2023. The first two\\r  authors contributed equally. To download the benchmark, visit\\r  https://systematic-visual-imagination.github.io\\r\\\\\\\\\\r  Systematic compositionality, or the ability to adapt to novel situations by\\rcreating a mental model of the world using reusable pieces of knowledge,\\rremains a significant challenge in machine learning. While there has been\\rconsiderable progress in the language domain, efforts towards systematic visual\\rimagination, or envisioning the dynamical implications of a visual observation,\\rare in their infancy. We introduce the Systematic Visual Imagination Benchmark\\r(SVIB), the first benchmark designed to address this problem head-on. SVIB\\roffers a novel framework for a minimal world modeling problem, where models are\\revaluated based on their ability to generate one-step image-to-image\\rtransformations under a latent world dynamics. The framework provides benefits\\rsuch as the possibility to jointly optimize for systematic perception and\\rimagination, a range of difficulty levels, and the ability to control the\\rfraction of possible factor combinations used during training. We provide a\\rcomprehensive evaluation of various baseline models on SVIB, offering insight\\rinto the current state-of-the-art in systematic visual imagination. We hope\\rthat this benchmark will help advance visual systematic compositionality.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09064 ,  6175kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09077\\rDate: Wed, 15 Nov 2023 16:19:13 GMT   (3917kb,D)\\r\\rTitle: Spiking NeRF: Representing the Real-World Geometry by a Discontinuous\\r  Representation\\rAuthors: Zhanfeng Liao, Qian Zheng, Yan Liu, Gang Pan\\rCategories: cs.CV\\r\\\\\\\\\\r  A crucial reason for the success of existing NeRF-based methods is to build a\\rneural density field for the geometry representation via multiple perceptron\\rlayers (MLPs). MLPs are continuous functions, however, real geometry or density\\rfield is frequently discontinuous at the interface between the air and the\\rsurface. Such a contrary brings the problem of unfaithful geometry\\rrepresentation. To this end, this paper proposes spiking NeRF, which leverages\\rspiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking Neural\\rNetwork (SNN) framework to build a discontinuous density field for faithful\\rgeometry representation. Specifically, we first demonstrate the reason why\\rcontinuous density fields will bring inaccuracy. Then, we propose to use the\\rspiking neurons to build a discontinuous density field. We conduct\\rcomprehensive analysis for the problem of existing spiking neuron models and\\rthen provide the numerical relationship between the parameter of spiking neuron\\rand the theoretical accuracy of geometry, Based on this, we propose a bounded\\rspiking neuron to build the discontinuous density field. Our results achieve\\rSOTA performance. Our code and data will be released to the public.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09077 ,  3917kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09084\\rDate: Wed, 15 Nov 2023 16:26:49 GMT   (1634kb,D)\\r\\rTitle: Contrastive Transformer Learning with Proximity Data Generation for\\r  Text-Based Person Search\\rAuthors: Hefeng Wu, Weifeng Chen, Zhibin Liu, Tianshui Chen, Zhiguang Chen,\\r  Liang Lin\\rCategories: cs.CV\\rComments: Accepted by IEEE T-CSVT\\r\\\\\\\\\\r  Given a descriptive text query, text-based person search (TBPS) aims to\\rretrieve the best-matched target person from an image gallery. Such a\\rcross-modal retrieval task is quite challenging due to significant modality\\rgap, fine-grained differences and insufficiency of annotated data. To better\\ralign the two modalities, most existing works focus on introducing\\rsophisticated network structures and auxiliary tasks, which are complex and\\rhard to implement. In this paper, we propose a simple yet effective dual\\rTransformer model for text-based person search. By exploiting a hardness-aware\\rcontrastive learning strategy, our model achieves state-of-the-art performance\\rwithout any special design for local feature alignment or side information.\\rMoreover, we propose a proximity data generation (PDG) module to automatically\\rproduce more diverse data for cross-modal training. The PDG module first\\rintroduces an automatic generation algorithm based on a text-to-image diffusion\\rmodel, which generates new text-image pair samples in the proximity space of\\roriginal ones. Then it combines approximate text generation and feature-level\\rmixup during training to further strengthen the data diversity. The PDG module\\rcan largely guarantee the reasonability of the generated samples that are\\rdirectly used for training without any human inspection for noise rejection. It\\rimproves the performance of our model significantly, providing a feasible\\rsolution to the data insufficiency problem faced by such fine-grained\\rvisual-linguistic tasks. Extensive experiments on two popular datasets of the\\rTBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approach\\routperforms state-of-the-art approaches evidently, e.g., improving by 3.88%,\\r4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will be\\ravailable at https://github.com/HCPLab-SYSU/PersonSearch-CTLG\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09084 ,  1634kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09093\\rDate: Wed, 15 Nov 2023 16:41:18 GMT   (214kb,D)\\r\\rTitle: Applications of Computer Vision in Autonomous Vehicles: Methods,\\r  Challenges and Future Directions\\rAuthors: Xingshuai Dong and Massimiliano L. Cappuccio\\rCategories: cs.CV cs.RO\\r\\\\\\\\\\r  Autonomous vehicle refers to a vehicle capable of perceiving its surrounding\\renvironment and driving with little or no human driver input. The perception\\rsystem is a fundamental component which enables the autonomous vehicle to\\rcollect data and extract relevant information from the environment to drive\\rsafely. Benefit from the recent advances in computer vision, the perception\\rtask can be achieved by using sensors, such as camera, LiDAR, radar, and\\rultrasonic sensor. This paper reviews publications on computer vision and\\rautonomous driving that are published during the last ten years. In particular,\\rwe first investigate the development of autonomous driving systems and\\rsummarize these systems that are developed by the major automotive\\rmanufacturers from different countries. Second, we investigate the sensors and\\rbenchmark data sets that are commonly utilized for autonomous driving. Then, a\\rcomprehensive overview of computer vision applications for autonomous driving\\rsuch as depth estimation, object detection, lane detection, and traffic sign\\rrecognition are discussed. Additionally, we review public opinions and concerns\\ron autonomous vehicles. Based on the discussion, we analyze the current\\rtechnological challenges that autonomous vehicles meet with. Finally, we\\rpresent our insights and point out some promising directions for future\\rresearch. This paper will help the reader to understand autonomous vehicles\\rfrom the perspectives of academia and industry.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09093 ,  214kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09103\\rDate: Wed, 15 Nov 2023 16:50:01 GMT   (32360kb)\\r\\rTitle: Guided Scale Space Radon Transform for linear structures detection\\rAuthors: Aicha Baya Goumeidane, Djemel Ziou, and Nafaa Nacereddine\\rCategories: cs.CV\\r\\\\\\\\\\r  Using integral transforms to the end of lines detection in images with\\rcomplex background, makes the detection a hard task needing additional\\rprocessing to manage the detection. As an integral transform, the Scale Space\\rRadon Transform (SSRT) suffers from such drawbacks, even with its great\\rabilities for thick lines detection. In this work, we propose a method to\\raddress this issue for automatic detection of thick linear structures in gray\\rscale and binary images using the SSRT, whatever the image background content.\\rThis method involves the calculated Hessian orientations of the investigated\\rimage while computing its SSRT, in such a way that linear structures are\\remphasized in the SSRT space. As a consequence, the subsequent maxima detection\\rin the SSRT space is done on a modified transform space freed from unwanted\\rparts and, consequently, from irrelevant peaks that usually drown the peaks\\rrepresenting lines. Besides, highlighting the linear structure in the SSRT\\rspace permitting, thus, to efficiently detect lines of different thickness in\\rsynthetic and real images, the experiments show also the method robustness\\ragainst noise and complex background.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09103 ,  32360kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09104\\rDate: Wed, 15 Nov 2023 16:51:18 GMT   (10120kb,D)\\r\\rTitle: Cross-view and Cross-pose Completion for 3D Human Understanding\\rAuthors: Matthieu Armando, Salma Galaaoui, Fabien Baradel, Thomas Lucas,\\r  Vincent Leroy, Romain Br\\\\'egier, Philippe Weinzaepfel, Gr\\\\'egory Rogez\\rCategories: cs.CV\\r\\\\\\\\\\r  Human perception and understanding is a major domain of computer vision\\rwhich, like many other vision subdomains recently, stands to gain from the use\\rof large models pre-trained on large datasets. We hypothesize that the most\\rcommon pre-training strategy of relying on general purpose, object-centric\\rimage datasets such as ImageNet, is limited by an important domain shift. On\\rthe other hand, collecting domain specific ground truth such as 2D or 3D labels\\rdoes not scale well. Therefore, we propose a pre-training approach based on\\rself-supervised learning that works on human-centric data using only images.\\rOur method uses pairs of images of humans: the first is partially masked and\\rthe model is trained to reconstruct the masked parts given the visible ones and\\ra second image. It relies on both stereoscopic (cross-view) pairs, and temporal\\r(cross-pose) pairs taken from videos, in order to learn priors about 3D as well\\ras human motion. We pre-train a model for body-centric tasks and one for\\rhand-centric tasks. With a generic transformer architecture, these models\\routperform existing self-supervised pre-training methods on a wide set of\\rhuman-centric downstream tasks, and obtain state-of-the-art performance for\\rinstance when fine-tuning for model-based and model-free human mesh recovery.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09104 ,  10120kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09118\\rDate: Wed, 15 Nov 2023 17:08:09 GMT   (4969kb,D)\\r\\rTitle: WildlifeDatasets: An open-source toolkit for animal re-identification\\rAuthors: Vojt\\\\v{e}ch \\\\v{C}erm\\\\'ak, Lukas Picek, Luk\\\\'a\\\\v{s} Adam, Kostas\\r  Papafitsoros\\rCategories: cs.CV\\r\\\\\\\\\\r  In this paper, we present WildlifeDatasets\\r(https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source\\rtoolkit intended primarily for ecologists and computer-vision /\\rmachine-learning researchers. The WildlifeDatasets is written in Python, allows\\rstraightforward access to publicly available wildlife datasets, and provides a\\rwide variety of methods for dataset pre-processing, performance analysis, and\\rmodel fine-tuning. We showcase the toolkit in various scenarios and baseline\\rexperiments, including, to the best of our knowledge, the most comprehensive\\rexperimental comparison of datasets and methods for wildlife re-identification,\\rincluding both local descriptors and deep learning approaches. Furthermore, we\\rprovide the first-ever foundation model for individual re-identification within\\ra wide range of species - MegaDescriptor - that provides state-of-the-art\\rperformance on animal re-identification datasets and outperforms other\\rpre-trained models such as CLIP and DINOv2 by a significant margin. To make the\\rmodel available to the general public and to allow easy integration with any\\rexisting wildlife monitoring applications, we provide multiple MegaDescriptor\\rflavors (i.e., Small, Medium, and Large) through the HuggingFace hub\\r(https://huggingface.co/BVRA).\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09118 ,  4969kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09178\\rDate: Wed, 15 Nov 2023 18:15:30 GMT   (1914kb,D)\\r\\rTitle: RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution\\rAuthors: Dareen Hussein, Hesham Eraqi, Israa Fahmy, Marwah Sulaiman, Mohammed\\r  Barakat, Mohammed El-Naggar, Moustafa Youssef, Zahraa Shehabeldin\\rCategories: cs.CV\\r\\\\\\\\\\r  Recently, video super resolution (VSR) has become a very impactful task in\\rthe area of Computer Vision due to its various applications. In this paper, we\\rpropose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for\\rVSR in an attempt to generate temporally coherent solutions while preserving\\rspatial details. RBPGAN integrates two state-of-the-art models to get the best\\rin both worlds without compromising the accuracy of produced video. The\\rgenerator of the model is inspired by RBPN system, while the discriminator is\\rinspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal\\rconsistency over time. Our contribution together results in a model that\\routperforms earlier work in terms of temporally consistent details, as we will\\rdemonstrate qualitatively and quantitatively using different datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09178 ,  1914kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09191\\rDate: Wed, 15 Nov 2023 18:34:26 GMT   (1796kb,D)\\r\\rTitle: Domain Aligned CLIP for Few-shot Classification\\rAuthors: Muhammad Waleed Gondal, Jochen Gast, Inigo Alonso Ruiz, Richard\\r  Droste, Tommaso Macri, Suren Kumar, Luitpold Staudigl\\rCategories: cs.CV\\rComments: To appear at WACV 2024\\r\\\\\\\\\\r  Large vision-language representation learning models like CLIP have\\rdemonstrated impressive performance for zero-shot transfer to downstream tasks\\rwhile largely benefiting from inter-modal (image-text) alignment via\\rcontrastive objectives. This downstream performance can further be enhanced by\\rfull-scale fine-tuning which is often compute intensive, requires large\\rlabelled data, and can reduce out-of-distribution (OOD) robustness.\\rFurthermore, sole reliance on inter-modal alignment might overlook the rich\\rinformation embedded within each individual modality. In this work, we\\rintroduce a sample-efficient domain adaptation strategy for CLIP, termed Domain\\rAligned CLIP (DAC), which improves both intra-modal (image-image) and\\rinter-modal alignment on target distributions without fine-tuning the main\\rmodel. For intra-modal alignment, we introduce a lightweight adapter that is\\rspecifically trained with an intra-modal contrastive objective. To improve\\rinter-modal alignment, we introduce a simple framework to modulate the\\rprecomputed class text embeddings. The proposed few-shot fine-tuning framework\\ris computationally efficient, robust to distribution shifts, and does not alter\\rCLIP's parameters. We study the effectiveness of DAC by benchmarking on 11\\rwidely used image classification tasks with consistent improvements in 16-shot\\rclassification upon strong baselines by about 2.3% and demonstrate competitive\\rperformance on 4 OOD robustness benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09191 ,  1796kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09215\\rDate: Wed, 15 Nov 2023 18:56:51 GMT   (1409kb,D)\\r\\rTitle: ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy\\rAuthors: Kirill Vishniakov, Zhiqiang Shen, Zhuang Liu\\rCategories: cs.CV cs.LG\\rComments: Preprint\\r\\\\\\\\\\r  Modern computer vision offers a great variety of models to practitioners, and\\rselecting a model from multiple options for specific applications can be\\rchallenging. Conventionally, competing model architectures and training\\rprotocols are compared by their classification accuracy on ImageNet. However,\\rthis single metric does not fully capture performance nuances critical for\\rspecialized tasks. In this work, we conduct an in-depth comparative analysis of\\rmodel behaviors beyond ImageNet accuracy, for both ConvNet and Vision\\rTransformer architectures, each across supervised and CLIP training paradigms.\\rAlthough our selected models have similar ImageNet accuracies and compute\\rrequirements, we find that they differ in many other aspects: types of\\rmistakes, output calibration, transferability, and feature invariance, among\\rothers. This diversity in model characteristics, not captured by traditional\\rmetrics, highlights the need for more nuanced analysis when choosing among\\rdifferent models. Our code is available at\\rhttps://github.com/kirill-vish/Beyond-INet.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09215 ,  1409kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09217\\rDate: Wed, 15 Nov 2023 18:58:41 GMT   (22425kb,D)\\r\\rTitle: DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction\\r  Model\\rAuthors: Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan\\r  Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, Kai Zhang\\rCategories: cs.CV\\rComments: Project Page: https://justimyhxu.github.io/projects/dmv3d/\\r\\\\\\\\\\r  We propose \\\\textbf{DMV3D}, a novel 3D generation approach that uses a\\rtransformer-based 3D large reconstruction model to denoise multi-view\\rdiffusion. Our reconstruction model incorporates a triplane NeRF representation\\rand can denoise noisy multi-view images via NeRF reconstruction and rendering,\\rachieving single-stage 3D generation in $\\\\sim$30s on single A100 GPU. We train\\r\\\\textbf{DMV3D} on large-scale multi-view image datasets of highly diverse\\robjects using only image reconstruction losses, without accessing 3D assets. We\\rdemonstrate state-of-the-art results for the single-image reconstruction\\rproblem where probabilistic modeling of unseen object parts is required for\\rgenerating diverse reconstructions with sharp textures. We also show\\rhigh-quality text-to-3D generation results outperforming previous 3D diffusion\\rmodels. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09217 ,  22425kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09221\\rDate: Wed, 15 Nov 2023 18:59:56 GMT   (28744kb,D)\\r\\rTitle: Single-Image 3D Human Digitization with Shape-Guided Diffusion\\rAuthors: Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes\\r  Kopf, Jia-Bin Huang\\rCategories: cs.CV\\rComments: SIGGRAPH Asia 2023. Project website: https://human-sgd.github.io/\\rDOI: 10.1145/3610548.3618153\\r\\\\\\\\\\r  We present an approach to generate a 360-degree view of a person with a\\rconsistent, high-resolution appearance from a single input image. NeRF and its\\rvariants typically require videos or images from different viewpoints. Most\\rexisting approaches taking monocular input either rely on ground-truth 3D scans\\rfor supervision or lack 3D consistency. While recent 3D generative models show\\rpromise of 3D consistent human digitization, these approaches do not generalize\\rwell to diverse clothing appearances, and the results lack photorealism. Unlike\\rexisting work, we utilize high-capacity 2D diffusion models pretrained for\\rgeneral image synthesis tasks as an appearance prior of clothed humans. To\\rachieve better 3D consistency while retaining the input identity, we\\rprogressively synthesize multiple views of the human in the input image by\\rinpainting missing regions with shape-guided diffusion conditioned on\\rsilhouette and surface normal. We then fuse these synthesized multi-view images\\rvia inverse rendering to obtain a fully textured high-resolution 3D mesh of the\\rgiven person. Experiments show that our approach outperforms prior methods and\\rachieves photorealistic 360-degree synthesis of a wide range of clothed humans\\rwith complex textures from a single image.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09221 ,  28744kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09168\\rDate: Wed, 15 Nov 2023 18:06:25 GMT   (2448kb,D)\\r\\rTitle: Generalized Neighbor Search using Commodity Hardware Acceleration\\rAuthors: Durga Mandarapu, Vani Nagarajan, Milind Kulkarni\\rCategories: cs.GR cs.DC cs.PF\\r\\\\\\\\\\r  Tree-based Nearest Neighbor Search (NNS) is hard to parallelize on GPUs.\\rHowever, newer Nvidia GPUs are equipped with Ray Tracing (RT) cores that can\\rbuild a spatial tree called Bounding Volume Hierarchy (BVH) to accelerate\\rgraphics rendering. Recent work proposed using RT cores to implement NNS, but\\rthey all have a hardware-imposed constraint on the type of distance metric,\\rwhich is the Euclidean distance. We propose and implement two approaches for\\rgeneralized distance computations: filter-refine, and monotone transformation,\\reach of which allows non-euclidean nearest neighbor queries to be performed in\\rterms of Euclidean distances. We find that our reductions improve the time\\rtaken to perform distance computations during the search, thereby improving the\\roverall performance of the NNS.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09168 ,  2448kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09211\\rDate: Wed, 15 Nov 2023 18:55:14 GMT   (1459kb)\\r\\rTitle: Digitally reproducing the artistic style of XVI century artist Antonio\\r  Campelo in Alegoria Prudencia\\rAuthors: Joao Fradinho Oliveira and Joao Madeiras Pereira\\rCategories: cs.GR\\r\\\\\\\\\\r  In this work, the artistic style of the sixteenth century Portuguese artist\\rAnt\\\\'onio Campelo in Alegoria \\\\`a Prud\\\\^encia is analyzed in order to create a\\rcomputational tool that allows one to transform any 3D digital sculpture model\\rinto an image that resembles the modeled style. From this analysis the problem\\ris divided into two parts: detection and drawing of contour lines and the\\rshading of the scene. Several techniques from Non Photorealistic Rendering\\r(NPR) and from Photorealistic Rendering that can resolve the problem are\\rpresented and, based on this study, a possible solution is presented. Each\\rmodeled rendering component is then analyzed using image based methods against\\rthe proposed artistic style and parameters are adjusted for a closer match. In\\rthe final stage a group of people was asked to answer a questionnaire where the\\rsimilarity between the renderings of different objects and the original style\\rwas classified according to their personal opinion. One of our findings is that\\ralthough the source 3D objects cannot be readily found for a direct comparison,\\rnor can the paper medium with centuries old damage be the same, the comparison\\rof sub -parts of both images of the same topology was still possible validating\\rour method and discarding other styles from the comparison.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09211 ,  1459kb)\\r%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08516 (*cross-listing*)\\rDate: Tue, 14 Nov 2023 20:12:38 GMT   (7191kb,D)\\r\\rTitle: LLMs cannot find reasoning errors, but can correct them!\\rAuthors: Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, Victor C\\\\u{a}rbune\\rCategories: cs.AI cs.CL cs.LG\\r\\\\\\\\\\r  While self-correction has shown promise in improving LLM outputs in terms of\\rstyle and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent\\rattempts to self-correct logical or reasoning errors often cause correct\\ranswers to become incorrect, resulting in worse performances overall (Huang et\\ral., 2023). In this paper, we break down the self-correction process into two\\rcore components: mistake finding and output correction. For mistake finding, we\\rrelease BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought\\rreasoning traces. We provide benchmark numbers for several state-of-the-art\\rLLMs, and demonstrate that LLMs generally struggle with finding logical\\rmistakes. For output correction, we propose a backtracking method which\\rprovides large improvements when given information on mistake location. We\\rconstrue backtracking as a lightweight alternative to reinforcement learning\\rmethods, and show that it remains effective with a reward model at 60-70%\\raccuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08516 ,  7191kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08576 (*cross-listing*)\\rDate: Tue, 14 Nov 2023 22:45:44 GMT   (85kb,D)\\r\\rTitle: Towards Evaluating AI Systems for Moral Status Using Self-Reports\\rAuthors: Ethan Perez and Robert Long\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\\\r  As AI systems become more advanced and widely deployed, there will likely be\\rincreasing debate over whether AI systems could have conscious experiences,\\rdesires, or other states of potential moral significance. It is important to\\rinform these discussions with empirical evidence to the extent possible. We\\rargue that under the right circumstances, self-reports, or an AI system's\\rstatements about its own internal states, could provide an avenue for\\rinvestigating whether AI systems have states of moral significance.\\rSelf-reports are the main way such states are assessed in humans (Are you in\\rpain?), but self-reports from current systems like large language models are\\rspurious for many reasons (e.g. often just reflecting what humans would say).\\rTo make self-reports more appropriate for this purpose, we propose to train\\rmodels to answer many kinds of questions about themselves with known answers,\\rwhile avoiding or limiting training incentives that bias self-reports. The hope\\rof this approach is that models will develop introspection-like capabilities,\\rand that these capabilities will generalize to questions about states of moral\\rsignificance. We then propose methods for assessing the extent to which these\\rtechniques have succeeded: evaluating self-report consistency across contexts\\rand between similar models, measuring the confidence and resilience of models'\\rself-reports, and using interpretability to corroborate self-reports. We also\\rdiscuss challenges for our approach, from philosophical difficulties in\\rinterpreting self-reports to technical reasons why our proposal might fail. We\\rhope our discussion inspires philosophers and AI researchers to criticize and\\rimprove our proposed methodology, as well as to run experiments to test whether\\rself-reports can be made reliable enough to provide information about states of\\rmoral significance.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08576 ,  85kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08592 (*cross-listing*)\\rDate: Tue, 14 Nov 2023 23:28:23 GMT   (1277kb,D)\\r\\rTitle: AART: AI-Assisted Red-Teaming with Diverse Data Generation for New\\r  LLM-powered Applications\\rAuthors: Bhaktipriya Radharapu, Kevin Robinson, Lora Aroyo, Preethi Lahoti\\rCategories: cs.SE cs.AI cs.CL\\r\\\\\\\\\\r  Adversarial testing of large language models (LLMs) is crucial for their safe\\rand responsible deployment. We introduce a novel approach for automated\\rgeneration of adversarial evaluation datasets to test the safety of LLM\\rgenerations on new downstream applications. We call it AI-assisted Red-Teaming\\r(AART) - an automated alternative to current manual red-teaming efforts. AART\\roffers a data generation and augmentation pipeline of reusable and customizable\\rrecipes that reduce human effort significantly and enable integration of\\radversarial testing earlier in new product development. AART generates\\revaluation datasets with high diversity of content characteristics critical for\\reffective adversarial testing (e.g. sensitive and harmful concepts, specific to\\ra wide range of cultural and geographic regions and application scenarios). The\\rdata generation is steered by AI-assisted recipes to define, scope and\\rprioritize diversity within the application context. This feeds into a\\rstructured LLM-generation process that scales up evaluation priorities.\\rCompared to some state-of-the-art tools, AART shows promising results in terms\\rof concept coverage and data quality.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08592 ,  1277kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08695 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 04:50:30 GMT   (2910kb,D)\\r\\rTitle: Attribute Diversity Determines the Systematicity Gap in VQA\\rAuthors: Ian Berlot-Attwell, A. Michael Carrell, Kumar Krishna Agrawal, Yash\\r  Sharma, Naomi Saphra\\rCategories: cs.LG cs.CL cs.CV\\rComments: 18 pages, 20 figures\\r\\\\\\\\\\r  The degree to which neural networks can generalize to new combinations of\\rfamiliar concepts, and the conditions under which they are able to do so, has\\rlong been an open question. In this work, we study the systematicity gap in\\rvisual question answering: the performance difference between reasoning on\\rpreviously seen and unseen combinations of object attributes. To test, we\\rintroduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased\\rquantity of training data does not reduce the systematicity gap, increased\\rtraining data diversity of the attributes in the unseen combination does. In\\rall, our experiments suggest that the more distinct attribute type combinations\\rare seen during training, the more systematic we can expect the resulting model\\rto be.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08695 ,  2910kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08702 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 05:05:40 GMT   (2019kb,D)\\r\\rTitle: Debate Helps Supervise Unreliable Experts\\rAuthors: Julian Michael, Salsabila Mahdi, David Rein, Jackson Petty, Julien\\r  Dirani, Vishakh Padmakumar, Samuel R. Bowman\\rCategories: cs.AI cs.CL\\rComments: 84 pages, 13 footnotes, 5 figures, 4 tables, 28 debate transcripts;\\r  data and code at\\r  https://github.com/julianmichael/debate/tree/2023-nyu-experiments\\rACM-class: I.2.0\\r\\\\\\\\\\r  As AI systems are used to answer more difficult questions and potentially\\rhelp create new knowledge, judging the truthfulness of their outputs becomes\\rmore difficult and more important. How can we supervise unreliable experts,\\rwhich have access to the truth but may not accurately report it, to give\\ranswers that are systematically true and don't just superficially seem true,\\rwhen the supervisor can't tell the difference between the two on their own? In\\rthis work, we show that debate between two unreliable experts can help a\\rnon-expert judge more reliably identify the truth. We collect a dataset of\\rhuman-written debates on hard reading comprehension questions where the judge\\rhas not read the source passage, only ever seeing expert arguments and short\\rquotes selectively revealed by 'expert' debaters who have access to the\\rpassage. In our debates, one expert argues for the correct answer, and the\\rother for an incorrect answer. Comparing debate to a baseline we call\\rconsultancy, where a single expert argues for only one answer which is correct\\rhalf of the time, we find that debate performs significantly better, with 84%\\rjudge accuracy compared to consultancy's 74%. Debates are also more efficient,\\rbeing 68% of the length of consultancies. By comparing human to AI debaters, we\\rfind evidence that with more skilled (in this case, human) debaters, the\\rperformance of debate goes up but the performance of consultancy goes down. Our\\rerror analysis also supports this trend, with 46% of errors in human debate\\rattributable to mistakes by the honest debater (which should go away with\\rincreased skill); whereas 52% of errors in human consultancy are due to\\rdebaters obfuscating the relevant evidence from the judge (which should become\\rworse with increased skill). Overall, these results show that debate is a\\rpromising approach for supervising increasingly capable but potentially\\runreliable AI systems.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08702 ,  2019kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08768 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 08:24:41 GMT   (40kb)\\r\\rTitle: Three Conjectures on Unexpectedeness\\rAuthors: Giovanni Sileno, Jean-Louis Dessalles\\rCategories: cs.AI cs.CL cs.IT cs.SY eess.SY math.IT\\rComments: Working paper\\r\\\\\\\\\\r  Unexpectedness is a central concept in Simplicity Theory, a theory of\\rcognition relating various inferential processes to the computation of\\rKolmogorov complexities, rather than probabilities. Its predictive power has\\rbeen confirmed by several experiments with human subjects, yet its theoretical\\rbasis remains largely unexplored: why does it work? This paper lays the\\rgroundwork for three theoretical conjectures. First, unexpectedness can be seen\\ras a generalization of Bayes' rule. Second, the frequentist core of\\runexpectedness can be connected to the function of tracking ergodic properties\\rof the world. Third, unexpectedness can be seen as constituent of various\\rmeasures of divergence between the entropy of the world (environment) and the\\rvariety of the observer (system). The resulting framework hints to research\\rdirections that go beyond the division between probabilistic and logical\\rapproaches, potentially bringing new insights into the extraction of causal\\rrelations, and into the role of descriptive mechanisms in learning.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08768 ,  40kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08417 (*cross-listing*)\\rDate: Fri, 3 Nov 2023 14:05:57 GMT   (3053kb,D)\\r\\rTitle: Image complexity based fMRI-BOLD visual network categorization across\\r  visual datasets using topological descriptors and deep-hybrid learning\\rAuthors: Debanjali Bhattacharya, Neelam Sinha, Yashwanth R. and Amit\\r  Chattopadhyay\\rCategories: eess.IV cs.CV cs.LG eess.SP q-bio.NC\\r\\\\\\\\\\r  This study proposes a new approach that investigates differences in\\rtopological characteristics of visual networks, which are constructed using\\rfMRI BOLD time-series corresponding to visual datasets of COCO, ImageNet, and\\rSUN. A publicly available BOLD5000 dataset is utilized that contains fMRI scans\\rwhile viewing 5254 images of diverse complexities. The objective of this study\\ris to examine how network topology differs in response to distinct visual\\rstimuli from these visual datasets. To achieve this, 0- and 1-dimensional\\rpersistence diagrams are computed for each visual network representing COCO,\\rImageNet, and SUN. For extracting suitable features from topological\\rpersistence diagrams, K-means clustering is executed. The extracted K-means\\rcluster features are fed to a novel deep-hybrid model that yields accuracy in\\rthe range of 90%-95% in classifying these visual networks. To understand\\rvision, this type of visual network categorization across visual datasets is\\rimportant as it captures differences in BOLD signals while perceiving images\\rwith different contexts and complexities. Furthermore, distinctive topological\\rpatterns of visual network associated with each dataset, as revealed from this\\rstudy, could potentially lead to the development of future neuroimaging\\rbiomarkers for diagnosing visual processing disorders like visual agnosia or\\rprosopagnosia, and tracking changes in visual cognition over time.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08417 ,  3053kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08439 (*cross-listing*)\\rDate: Tue, 14 Nov 2023 15:10:05 GMT   (3176kb,D)\\r\\rTitle: A Unified Approach for Comprehensive Analysis of Various Spectral and\\r  Tissue Doppler Echocardiography\\rAuthors: Jaeik Jeon, Jiyeon Kim, Yeonggul Jang, Yeonyee E. Yoon, Dawun Jeong,\\r  Youngtaek Hong, Seung-Ah Lee, Hyuk-Jae Chang\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  Doppler echocardiography offers critical insights into cardiac function and\\rphases by quantifying blood flow velocities and evaluating myocardial motion.\\rHowever, previous methods for automating Doppler analysis, ranging from initial\\rsignal processing techniques to advanced deep learning approaches, have been\\rconstrained by their reliance on electrocardiogram (ECG) data and their\\rinability to process Doppler views collectively. We introduce a novel unified\\rframework using a convolutional neural network for comprehensive analysis of\\rspectral and tissue Doppler echocardiography images that combines automatic\\rmeasurements and end-diastole (ED) detection into a singular method. The\\rnetwork automatically recognizes key features across various Doppler views,\\rwith novel Doppler shape embedding and anti-aliasing modules enhancing\\rinterpretation and ensuring consistent analysis. Empirical results indicate a\\rconsistent outperformance in performance metrics, including dice similarity\\rcoefficients (DSC) and intersection over union (IoU). The proposed framework\\rdemonstrates strong agreement with clinicians in Doppler automatic measurements\\rand competitive performance in ED detection.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08439 ,  3176kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08479 (*cross-listing*)\\rDate: Tue, 14 Nov 2023 19:10:56 GMT   (1019kb,D)\\r\\rTitle: Leveraging Foundation Models to Improve Lightweight Clients in Federated\\r  Learning\\rAuthors: Xidong Wu, Wan-Yi Lin, Devin Willmott, Filipe Condessa, Yufei Huang,\\r  Zhenzhen Li and Madan Ravi Ganesh\\rCategories: cs.LG cs.CV cs.DC\\rComments: 6 Pages + Appendices\\r\\\\\\\\\\r  Federated Learning (FL) is a distributed training paradigm that enables\\rclients scattered across the world to cooperatively learn a global model\\rwithout divulging confidential data. However, FL faces a significant challenge\\rin the form of heterogeneous data distributions among clients, which leads to a\\rreduction in performance and robustness. A recent approach to mitigating the\\rimpact of heterogeneous data distributions is through the use of foundation\\rmodels, which offer better performance at the cost of larger computational\\roverheads and slower inference speeds. We introduce foundation model\\rdistillation to assist in the federated training of lightweight client models\\rand increase their performance under heterogeneous data settings while keeping\\rinference costs low. Our results show improvement in the global model\\rperformance on a balanced testing set, which contains rarely observed samples,\\reven under extreme non-IID client data distributions. We conduct a thorough\\revaluation of our framework with different foundation model backbones on\\rCIFAR10, with varying degrees of heterogeneous data distributions ranging from\\rclass-specific data partitions across clients to dirichlet data sampling,\\rparameterized by values between 0.01 and 1.0.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08479 ,  1019kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08493 (*cross-listing*)\\rDate: Tue, 14 Nov 2023 19:41:19 GMT   (986kb,D)\\r\\rTitle: Performance of Machine Learning Classification in Mammography Images\\r  using BI-RADS\\rAuthors: Malitha Gunawardhana, Norbert Zolek\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  This research aims to investigate the classification accuracy of various\\rstate-of-the-art image classification models across different categories of\\rbreast ultrasound images, as defined by the Breast Imaging Reporting and Data\\rSystem (BI-RADS). To achieve this, we have utilized a comprehensively assembled\\rdataset of 2,945 mammographic images sourced from 1,540 patients. In order to\\rconduct a thorough analysis, we employed six advanced classification\\rarchitectures, including VGG19 \\\\cite{simonyan2014very}, ResNet50\\r\\\\cite{he2016deep}, GoogleNet \\\\cite{szegedy2015going}, ConvNext\\r\\\\cite{liu2022convnet}, EfficientNet \\\\cite{tan2019efficientnet}, and Vision\\rTransformers (ViT) \\\\cite{dosovitskiy2020image}, instead of traditional machine\\rlearning models. We evaluate models in three different settings: full\\rfine-tuning, linear evaluation and training from scratch. Our findings\\rdemonstrate the effectiveness and capability of our Computer-Aided Diagnosis\\r(CAD) system, with a remarkable accuracy of 76.39\\\\% and an F1 score of 67.94\\\\%\\rin the full fine-tuning setting. Our findings indicate the potential for\\renhanced diagnostic accuracy in the field of breast imaging, providing a solid\\rfoundation for future endeavors aiming to improve the precision and reliability\\rof CAD systems in medical imaging.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08493 ,  986kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08524 (*cross-listing*)\\rDate: Tue, 14 Nov 2023 20:36:34 GMT   (2414kb,D)\\r\\rTitle: Cross-dataset domain adaptation for the classification COVID-19 using\\r  chest computed tomography images\\rAuthors: Ridha Ouni and Haikel Alhichri\\rCategories: eess.IV cs.CV cs.LG\\rComments: 31 pages, 15 figures\\r\\\\\\\\\\r  Detecting COVID-19 patients using Computed Tomography (CT) images of the\\rlungs is an active area of research. Datasets of CT images from COVID-19\\rpatients are becoming available. Deep learning (DL) solutions and in particular\\rConvolutional Neural Networks (CNN) have achieved impressive results for the\\rclassification of COVID-19 CT images, but only when the training and testing\\rtake place within the same dataset. Work on the cross-dataset problem is still\\rlimited and the achieved results are low. Our work tackles the cross-dataset\\rproblem through a Domain Adaptation (DA) technique with deep learning. Our\\rproposed solution, COVID19-DANet, is based on pre-trained CNN backbone for\\rfeature extraction. For this task, we select the pre-trained Efficientnet-B3\\rCNN because it has achieved impressive classification accuracy in previous\\rwork. The backbone CNN is followed by a prototypical layer which is a concept\\rborrowed from prototypical networks in few-shot learning (FSL). It computes a\\rcosine distance between given samples and the class prototypes and then\\rconverts them to class probabilities using the Softmax function. To train the\\rCOVID19-DANet model, we propose a combined loss function that is composed of\\rthe standard cross-entropy loss for class discrimination and another entropy\\rloss computed over the unlabelled target set only. This so-called unlabelled\\rtarget entropy loss is minimized and maximized in an alternative fashion, to\\rreach the two objectives of class discrimination and domain invariance.\\rCOVID19-DANet is tested under four cross-dataset scenarios using the\\rSARS-CoV-2-CT and COVID19-CT datasets and has achieved encouraging results\\rcompared to recent work in the literature.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08524 ,  2414kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08530 (*cross-listing*)\\rDate: Tue, 14 Nov 2023 20:55:40 GMT   (14123kb,D)\\r\\rTitle: SceneScore: Learning a Cost Function for Object Arrangement\\rAuthors: Ivan Kapelyukh, Edward Johns\\rCategories: cs.RO cs.CV cs.LG\\rComments: Presented at CoRL 2023 LEAP Workshop. Webpage:\\r  https://sites.google.com/view/scenescore\\r\\\\\\\\\\r  Arranging objects correctly is a key capability for robots which unlocks a\\rwide range of useful tasks. A prerequisite for creating successful arrangements\\ris the ability to evaluate the desirability of a given arrangement. Our method\\rSceneScore learns a cost function for arrangements, such that desirable,\\rhuman-like arrangements have a low cost. We learn the distribution of training\\rarrangements offline using an energy-based model, solely from example images\\rwithout requiring environment interaction or human supervision. Our model is\\rrepresented by a graph neural network which learns object-object relations,\\rusing graphs constructed from images. Experiments demonstrate that the learned\\rcost function can be used to predict poses for missing objects, generalise to\\rnovel objects using semantic features, and can be composed with other cost\\rfunctions to satisfy constraints at inference time.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08530 ,  14123kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08544 (*cross-listing*)\\rDate: Sun, 22 Oct 2023 02:16:48 GMT   (10652kb,D)\\r\\rTitle: JOSA: Joint surface-based registration and atlas construction of brain\\r  geometry and function\\rAuthors: Jian Li, Greta Tuckute, Evelina Fedorenko, Brian L. Edlow, Adrian V.\\r  Dalca, Bruce Fischl\\rCategories: q-bio.NC cs.CV eess.IV\\rComments: A. V. Dalca and B. Fischl are co-senior authors with equal\\r  contribution. arXiv admin note: text overlap with arXiv:2303.01592\\r\\\\\\\\\\r  Surface-based cortical registration is an important topic in medical image\\ranalysis and facilitates many downstream applications. Current approaches for\\rcortical registration are mainly driven by geometric features, such as sulcal\\rdepth and curvature, and often assume that registration of folding patterns\\rleads to alignment of brain function. However, functional variability of\\ranatomically corresponding areas across subjects has been widely reported,\\rparticularly in higher-order cognitive areas. In this work, we present JOSA, a\\rnovel cortical registration framework that jointly models the mismatch between\\rgeometry and function while simultaneously learning an unbiased\\rpopulation-specific atlas. Using a semi-supervised training strategy, JOSA\\rachieves superior registration performance in both geometry and function to the\\rstate-of-the-art methods but without requiring functional data at inference.\\rThis learning framework can be extended to any auxiliary data to guide\\rspherical registration that is available during training but is difficult or\\rimpossible to obtain during inference, such as parcellations, architectonic\\ridentity, transcriptomic information, and molecular profiles. By recognizing\\rthe mismatch between geometry and function, JOSA provides new insights into the\\rfuture development of registration methods using joint analysis of the brain\\rstructure and function.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08544 ,  10652kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08548 (*cross-listing*)\\rDate: Tue, 14 Nov 2023 21:20:54 GMT   (4459kb,D)\\r\\rTitle: Topology of Surface Electromyogram Signals: Hand Gesture Decoding on\\r  Riemannian Manifolds\\rAuthors: Harshavardhana T. Gowda, Lee M. Miller\\rCategories: eess.SP cs.CV cs.HC q-bio.QM\\rComments: 15 pages, 8 figures, 5 tables\\rMSC-class: 53\\rACM-class: G.3\\r\\\\\\\\\\r  Decoding gestures from the upper limb using noninvasive surface\\relectromyogram (sEMG) signals is of keen interest for the rehabilitation of\\ramputees, artificial supernumerary limb augmentation, gestural control of\\rcomputers, and virtual/augmented realities. We show that sEMG signals recorded\\racross an array of sensor electrodes in multiple spatial locations around the\\rforearm evince a rich geometric pattern of global motor unit (MU) activity that\\rcan be leveraged to distinguish different hand gestures. We demonstrate a\\rsimple technique to analyze spatial patterns of muscle MU activity within a\\rtemporal window and show that distinct gestures can be classified in both\\rsupervised and unsupervised manners. Specifically, we construct symmetric\\rpositive definite (SPD) covariance matrices to represent the spatial\\rdistribution of MU activity in a time window of interest, calculated as\\rpairwise covariance of electrical signals measured across different electrodes.\\rThis allows us to understand and manipulate multivariate sEMG timeseries on a\\rmore natural subspace -the Riemannian manifold. Furthermore, it directly\\raddresses signal variability across individuals and sessions, which remains a\\rmajor challenge in the field. sEMG signals measured at a single electrode lack\\rcontextual information such as how various anatomical and physiological factors\\rinfluence the signals and how their combined effect alters the evident\\rinteraction among neighboring muscles. As we show here, analyzing spatial\\rpatterns using covariance matrices on Riemannian manifolds allows us to\\rrobustly model complex interactions across spatially distributed MUs and\\rprovides a flexible and transparent framework to quantify differences in sEMG\\rsignals across individuals. The proposed method is novel in the study of sEMG\\rsignals and its performance exceeds the current benchmarks while maintaining\\rexceptional computational efficiency.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08548 ,  4459kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08585 (*cross-listing*)\\rDate: Tue, 14 Nov 2023 23:13:59 GMT   (17220kb,D)\\r\\rTitle: Unsupervised segmentation of irradiation$\\\\unicode{x2010}$induced\\r  order$\\\\unicode{x2010}$disorder phase transitions in electron microscopy\\rAuthors: Arman H Ter-Petrosyan, Jenna A Bilbrey, Christina M Doty, Bethany E\\r  Matthews, Le Wang, Yingge Du, Eric Lang, Khalid Hattar, Steven R Spurgeon\\rCategories: cond-mat.mtrl-sci cs.CV cs.LG eess.IV\\rComments: 7 pages, 3 figures. Accepted to Machine Learning and the Physical\\r  Sciences Workshop, NeurIPS 2023\\r\\\\\\\\\\r  We present a method for the unsupervised segmentation of electron microscopy\\rimages, which are powerful descriptors of materials and chemical systems.\\rImages are oversegmented into overlapping chips, and similarity graphs are\\rgenerated from embeddings extracted from a domain$\\\\unicode{x2010}$pretrained\\rconvolutional neural network (CNN). The Louvain method for community detection\\ris then applied to perform segmentation. The graph representation provides an\\rintuitive way of presenting the relationship between chips and communities. We\\rdemonstrate our method to track irradiation$\\\\unicode{x2010}$induced amorphous\\rfronts in thin films used for catalysis and electronics. This method has\\rpotential for on$\\\\unicode{x2010}$the$\\\\unicode{x2010}$fly segmentation to\\rguide emerging automated electron microscopes.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08585 ,  17220kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08652 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 02:26:41 GMT   (16778kb,D)\\r\\rTitle: Refining Perception Contracts: Case Studies in Vision-based Safe\\r  Auto-landing\\rAuthors: Yangge Li, Benjamin C Yang, Yixuan Jia, Daniel Zhuang, Sayan Mitra\\rCategories: cs.RO cs.CV\\r\\\\\\\\\\r  Perception contracts provide a method for evaluating safety of control\\rsystems that use machine learning for perception. A perception contract is a\\rspecification for testing the ML components, and it gives a method for proving\\rend-to-end system-level safety requirements. The feasibility of contract-based\\rtesting and assurance was established earlier in the context of straight lane\\rkeeping: a 3-dimensional system with relatively simple dynamics. This paper\\rpresents the analysis of two 6 and 12-dimensional flight control systems that\\ruse multi-stage, heterogeneous, ML-enabled perception. The paper advances\\rmethodology by introducing an algorithm for constructing data and requirement\\rguided refinement of perception contracts (DaRePC). The resulting analysis\\rprovides testable contracts which establish the state and environment\\rconditions under which an aircraft can safety touchdown on the runway and a\\rdrone can safely pass through a sequence of gates. It can also discover\\rconditions (e.g., low-horizon sun) that can possibly violate the safety of the\\rvision-based control system.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08652 ,  16778kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08661 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 02:57:59 GMT   (6247kb,D)\\r\\rTitle: Deep Neural Network Identification of Limnonectes Species and New Class\\r  Detection Using Image Data\\rAuthors: Li Xu, Yili Hong, Eric P. Smith, David S. McLeod, Xinwei Deng, Laura\\r  J. Freeman\\rCategories: stat.ML cs.CV cs.LG eess.IV\\rComments: 26 pages, 11 Figures\\r\\\\\\\\\\r  As is true of many complex tasks, the work of discovering, describing, and\\runderstanding the diversity of life on Earth (viz., biological systematics and\\rtaxonomy) requires many tools. Some of this work can be accomplished as it has\\rbeen done in the past, but some aspects present us with challenges which\\rtraditional knowledge and tools cannot adequately resolve. One such challenge\\ris presented by species complexes in which the morphological similarities among\\rthe group members make it difficult to reliably identify known species and\\rdetect new ones. We address this challenge by developing new tools using the\\rprinciples of machine learning to resolve two specific questions related to\\rspecies complexes. The first question is formulated as a classification problem\\rin statistics and machine learning and the second question is an\\rout-of-distribution (OOD) detection problem. We apply these tools to a species\\rcomplex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex)\\rand employ a morphological character (hind limb skin texture) traditionally\\rtreated qualitatively in a quantitative and objective manner. We demonstrate\\rthat deep neural networks can successfully automate the classification of an\\rimage into a known species group for which it has been trained. We further\\rdemonstrate that the algorithm can successfully classify an image into a new\\rclass if the image does not belong to the existing classes. Additionally, we\\ruse the larger MNIST dataset to test the performance of our OOD detection\\ralgorithm. We finish our paper with some concluding remarks regarding the\\rapplication of these methods to species complexes and our efforts to document\\rtrue biodiversity. This paper has online supplementary materials.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08661 ,  6247kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08716 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 05:43:14 GMT   (157kb,D)\\r\\rTitle: Scalable Federated Learning for Clients with Different Input Image Sizes\\r  and Numbers of Output Categories\\rAuthors: Shuhei Nitta, Taiji Suzuki, Albert Rodr\\\\'iguez Mulet, Atsushi Yaguchi\\r  and Ryusuke Hirai\\rCategories: cs.LG cs.CR cs.CV\\rComments: 15 pages, 1 figure, 2023 22nd International Conference on Machine\\r  Learning and Applications (ICMLA)\\r\\\\\\\\\\r  Federated learning is a privacy-preserving training method which consists of\\rtraining from a plurality of clients but without sharing their confidential\\rdata. However, previous work on federated learning do not explore suitable\\rneural network architectures for clients with different input images sizes and\\rdifferent numbers of output categories. In this paper, we propose an effective\\rfederated learning method named ScalableFL, where the depths and widths of the\\rlocal models for each client are adjusted according to the clients' input image\\rsize and the numbers of output categories. In addition, we provide a new bound\\rfor the generalization gap of federated learning. In particular, this bound\\rhelps to explain the effectiveness of our scalable neural network approach. We\\rdemonstrate the effectiveness of ScalableFL in several heterogeneous client\\rsettings for both image classification and object detection tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08716 ,  157kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08746 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 07:29:23 GMT   (171kb,D)\\r\\rTitle: A Diffusion Model Based Quality Enhancement Method for HEVC Compressed\\r  Video\\rAuthors: Zheng Liu, Honggang Qi\\rCategories: eess.IV cs.CV\\rComments: 10 pages, conference\\r\\\\\\\\\\r  Video post-processing methods can improve the quality of compressed videos at\\rthe decoder side. Most of the existing methods need to train corresponding\\rmodels for compressed videos with different quantization parameters to improve\\rthe quality of compressed videos. However, in most cases, the quantization\\rparameters of the decoded video are unknown. This makes existing methods have\\rtheir limitations in improving video quality. To tackle this problem, this work\\rproposes a diffusion model based post-processing method for compressed videos.\\rThe proposed method first estimates the feature vectors of the compressed video\\rand then uses the estimated feature vectors as the prior information for the\\rquality enhancement model to adaptively enhance the quality of compressed video\\rwith different quantization parameters. Experimental results show that the\\rquality enhancement results of our proposed method on mixed datasets are\\rsuperior to existing methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08746 ,  171kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08774 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 08:37:11 GMT   (38618kb,D)\\r\\rTitle: Two-stage Joint Transductive and Inductive learning for Nuclei\\r  Segmentation\\rAuthors: Hesham Ali, Idriss Tondji, Mennatullah Siam\\rCategories: eess.IV cs.CV cs.LG\\rComments: 5 pages\\r\\\\\\\\\\r  AI-assisted nuclei segmentation in histopathological images is a crucial task\\rin the diagnosis and treatment of cancer diseases. It decreases the time\\rrequired to manually screen microscopic tissue images and can resolve the\\rconflict between pathologists during diagnosis. Deep Learning has proven useful\\rin such a task. However, lack of labeled data is a significant barrier for deep\\rlearning-based approaches. In this study, we propose a novel approach to nuclei\\rsegmentation that leverages the available labelled and unlabelled data. The\\rproposed method combines the strengths of both transductive and inductive\\rlearning, which have been previously attempted separately, into a single\\rframework. Inductive learning aims at approximating the general function and\\rgeneralizing to unseen test data, while transductive learning has the potential\\rof leveraging the unlabelled test data to improve the classification. To the\\rbest of our knowledge, this is the first study to propose such a hybrid\\rapproach for medical image segmentation. Moreover, we propose a novel two-stage\\rtransductive inference scheme. We evaluate our approach on MoNuSeg benchmark to\\rdemonstrate the efficacy and potential of our method.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08774 ,  38618kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08799 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 09:11:37 GMT   (7120kb,D)\\r\\rTitle: EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target\\r  Approaching in Robotic Eye Surgery\\rAuthors: Junjie Yang, Zhihao Zhao, Siyuan Shen, Daniel Zapp, Mathias Maier, Kai\\r  Huang, Nassir Navab and M. Ali Nasseri\\rCategories: cs.RO cs.CV\\rComments: 10 pages\\r\\\\\\\\\\r  Robotic ophthalmic surgery is an emerging technology to facilitate\\rhigh-precision interventions such as retina penetration in subretinal injection\\rand removal of floating tissues in retinal detachment depending on the input\\rimaging modalities such as microscopy and intraoperative OCT (iOCT). Although\\riOCT is explored to locate the needle tip within its range-limited ROI, it is\\rstill difficult to coordinate iOCT's motion with the needle, especially at the\\rinitial target-approaching stage. Meanwhile, due to 2D perspective projection\\rand thus the loss of depth information, current image-based methods cannot\\reffectively estimate the needle tip's trajectory towards both retinal and\\rfloating targets. To address this limitation, we propose to use the shadow\\rpositions of the target and the instrument tip to estimate their relative depth\\rposition and accordingly optimize the instrument tip's insertion trajectory\\runtil the tip approaches targets within iOCT's scanning area. Our method\\rsucceeds target approaching on a retina model, and achieves an average depth\\rerror of 0.0127 mm and 0.3473 mm for floating and retinal targets respectively\\rin the surgical simulator without damaging the retina.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08799 ,  7120kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08815 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 09:34:08 GMT   (4531kb,D)\\r\\rTitle: Self-Supervised Disentanglement by Leveraging Structure in Data\\r  Augmentations\\rAuthors: Cian Eastwood, Julius von K\\\\ugelgen, Linus Ericsson, Diane\\r  Bouchacourt, Pascal Vincent, Bernhard Sch\\\\olkopf, Mark Ibrahim\\rCategories: cs.LG cs.AI cs.CV stat.ML\\r\\\\\\\\\\r  Self-supervised representation learning often uses data augmentations to\\rinduce some invariance to style attributes of the data. However, with\\rdownstream tasks generally unknown at training time, it is difficult to deduce\\ra priori which attributes of the data are indeed style and can be safely\\rdiscarded. To address this, we introduce a more principled approach that seeks\\rto disentangle style features rather than discard them. The key idea is to add\\rmultiple style embedding spaces where: (i) each is invariant to all-but-one\\raugmentation; and (ii) joint entropy is maximized. We formalize our structured\\rdata-augmentation procedure from a causal latent-variable-model perspective,\\rand prove identifiability of both content and (multiple blocks of) style\\rvariables. We empirically demonstrate the benefits of our approach on synthetic\\rdatasets and then present promising but limited results on ImageNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08815 ,  4531kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08816 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 09:35:07 GMT   (33514kb,D)\\r\\rTitle: Target-oriented Domain Adaptation for Infrared Image Super-Resolution\\rAuthors: Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Yafei Dong, Shinichiro\\r  Omachi\\rCategories: eess.IV cs.CV\\rComments: 11 pages, 9 figures\\r\\\\\\\\\\r  Recent efforts have explored leveraging visible light images to enrich\\rtexture details in infrared (IR) super-resolution. However, this direct\\radaptation approach often becomes a double-edged sword, as it improves texture\\rat the cost of introducing noise and blurring artifacts. To address these\\rchallenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN),\\ran innovative framework specifically engineered for robust IR super-resolution\\rmodel adaptation. DASRGAN operates on the synergy of two key components: 1)\\rTexture-Oriented Adaptation (TOA) to refine texture details meticulously, and\\r2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer.\\rSpecifically, TOA uniquely integrates a specialized discriminator,\\rincorporating a prior extraction branch, and employs a Sobel-guided adversarial\\rloss to align texture distributions effectively. Concurrently, NOA utilizes a\\rnoise adversarial loss to distinctly separate the generative and Gaussian noise\\rpattern distributions during adversarial training. Our extensive experiments\\rconfirm DASRGAN's superiority. Comparative analyses against leading methods\\racross multiple benchmarks and upsampling factors reveal that DASRGAN sets new\\rstate-of-the-art performance standards. Code are available at\\r\\\\url{https://github.com/yongsongH/DASRGAN}.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08816 ,  33514kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08819 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 09:46:30 GMT   (26076kb,D)\\r\\rTitle: Frequency Domain-based Dataset Distillation\\rAuthors: Donghyeok Shin, Seungjae Shin, Il-Chul Moon\\rCategories: cs.LG cs.AI cs.CV\\rComments: Accepted at NeurIPS 2023\\r\\\\\\\\\\r  This paper presents FreD, a novel parameterization method for dataset\\rdistillation, which utilizes the frequency domain to distill a small-sized\\rsynthetic dataset from a large-sized original dataset. Unlike conventional\\rapproaches that focus on the spatial domain, FreD employs frequency-based\\rtransforms to optimize the frequency representations of each data instance. By\\rleveraging the concentration of spatial domain information on specific\\rfrequency components, FreD intelligently selects a subset of frequency\\rdimensions for optimization, leading to a significant reduction in the required\\rbudget for synthesizing an instance. Through the selection of frequency\\rdimensions based on the explained variance, FreD demonstrates both theoretical\\rand empirical evidence of its ability to operate efficiently within a limited\\rbudget, while better preserving the information of the original dataset\\rcompared to conventional parameterization methods. Furthermore, based on the\\rorthogonal compatibility of FreD with existing methods, we confirm that FreD\\rconsistently improves the performances of existing distillation methods over\\rthe evaluation scenarios with different benchmark datasets. We release the code\\rat https://github.com/sdh0818/FreD.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08819 ,  26076kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08851 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 10:43:13 GMT   (7009kb,D)\\r\\rTitle: Data Augmentations in Deep Weight Spaces\\rAuthors: Aviv Shamsian, David W. Zhang, Aviv Navon, Yan Zhang, Miltiadis\\r  Kofinas, Idan Achituve, Riccardo Valperga, Gertjan J. Burghouts, Efstratios\\r  Gavves, Cees G. M. Snoek, Ethan Fetaya, Gal Chechik, Haggai Maron\\rCategories: cs.LG cs.CV\\rComments: Accepted to NeurIPS 2023 Workshop on Symmetry and Geometry in Neural\\r  Representations\\r\\\\\\\\\\r  Learning in weight spaces, where neural networks process the weights of other\\rdeep neural networks, has emerged as a promising research direction with\\rapplications in various fields, from analyzing and editing neural fields and\\rimplicit neural representations, to network pruning and quantization. Recent\\rworks designed architectures for effective learning in that space, which takes\\rinto account its unique, permutation-equivariant, structure. Unfortunately, so\\rfar these architectures suffer from severe overfitting and were shown to\\rbenefit from large datasets. This poses a significant challenge because\\rgenerating data for this learning setup is laborious and time-consuming since\\reach data sample is a full set of network weights that has to be trained. In\\rthis paper, we address this difficulty by investigating data augmentations for\\rweight spaces, a set of techniques that enable generating new data examples on\\rthe fly without having to train additional input weight space elements. We\\rfirst review several recently proposed data augmentation schemes %that were\\rproposed recently and divide them into categories. We then introduce a novel\\raugmentation scheme based on the Mixup method. We evaluate the performance of\\rthese techniques on existing benchmarks as well as new benchmarks we generate,\\rwhich can be valuable for future studies.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08851 ,  7009kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08908 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 12:26:24 GMT   (1216kb,D)\\r\\rTitle: Robust Brain MRI Image Classification with SIBOW-SVM\\rAuthors: Liyun Zeng and Hao Helen Zhang\\rCategories: stat.ME cs.CV\\r\\\\\\\\\\r  The majority of primary Central Nervous System (CNS) tumors in the brain are\\ramong the most aggressive diseases affecting humans. Early detection of brain\\rtumor types, whether benign or malignant, glial or non-glial, is critical for\\rcancer prevention and treatment, ultimately improving human life expectancy.\\rMagnetic Resonance Imaging (MRI) stands as the most effective technique to\\rdetect brain tumors by generating comprehensive brain images through scans.\\rHowever, human examination can be error-prone and inefficient due to the\\rcomplexity, size, and location variability of brain tumors. Recently, automated\\rclassification techniques using machine learning (ML) methods, such as\\rConvolutional Neural Network (CNN), have demonstrated significantly higher\\raccuracy than manual screening, while maintaining low computational costs.\\rNonetheless, deep learning-based image classification methods, including CNN,\\rface challenges in estimating class probabilities without proper model\\rcalibration. In this paper, we propose a novel brain tumor image classification\\rmethod, called SIBOW-SVM, which integrates the Bag-of-Features (BoF) model with\\rSIFT feature extraction and weighted Support Vector Machines (wSVMs). This new\\rapproach effectively captures hidden image features, enabling the\\rdifferentiation of various tumor types and accurate label predictions.\\rAdditionally, the SIBOW-SVM is able to estimate the probabilities of images\\rbelonging to each class, thereby providing high-confidence classification\\rdecisions. We have also developed scalable and parallelable algorithms to\\rfacilitate the practical implementation of SIBOW-SVM for massive images. As a\\rbenchmark, we apply the SIBOW-SVM to a public data set of brain tumor MRI\\rimages containing four classes: glioma, meningioma, pituitary, and normal. Our\\rresults show that the new method outperforms state-of-the-art methods,\\rincluding CNN.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08908 ,  1216kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08909 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 12:26:31 GMT   (4772kb,D)\\r\\rTitle: DLAS: An Exploration and Assessment of the Deep Learning Acceleration\\r  Stack\\rAuthors: Perry Gibson, Jos\\\\'e Cano, Elliot J. Crowley, Amos Storkey, Michael\\r  O'Boyle\\rCategories: cs.LG cs.CV cs.PF\\r\\\\\\\\\\r  Deep Neural Networks (DNNs) are extremely computationally demanding, which\\rpresents a large barrier to their deployment on resource-constrained devices.\\rSince such devices are where many emerging deep learning applications lie\\r(e.g., drones, vision-based medical technology), significant bodies of work\\rfrom both the machine learning and systems communities have attempted to\\rprovide optimizations to accelerate DNNs. To help unify these two perspectives,\\rin this paper we combine machine learning and systems techniques within the\\rDeep Learning Acceleration Stack (DLAS), and demonstrate how these layers can\\rbe tightly dependent on each other with an across-stack perturbation study. We\\revaluate the impact on accuracy and inference time when varying different\\rparameters of DLAS across two datasets, seven popular DNN architectures, four\\rDNN compression techniques, three algorithmic primitives with sparse and dense\\rvariants, untuned and auto-scheduled code generation, and four hardware\\rplatforms. Our evaluation highlights how perturbations across DLAS parameters\\rcan cause significant variation and across-stack interactions. The highest\\rlevel observation from our evaluation is that the model size, accuracy, and\\rinference time are not guaranteed to be correlated. Overall we make 13 key\\robservations, including that speedups provided by compression techniques are\\rvery hardware dependent, and that compiler auto-tuning can significantly alter\\rwhat the best algorithm to use for a given configuration is. With DLAS, we aim\\rto provide a reference framework to aid machine learning and systems\\rpractitioners in reasoning about the context in which their respective DNN\\racceleration solutions exist in. With our evaluation strongly motivating the\\rneed for co-design, we believe that DLAS can be a valuable concept for\\rexploring the next generation of co-designed accelerated deep learning\\rsolutions.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08909 ,  4772kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08936 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 13:19:02 GMT   (3979kb,D)\\r\\rTitle: Confident Naturalness Explanation (CNE): A Framework to Explain and\\r  Assess Patterns Forming Naturalness\\rAuthors: Ahmed Emam, Mohamed Farag, Ribana Roscher\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Protected natural areas are regions that have been minimally affected by\\rhuman activities such as urbanization, agriculture, and other human\\rinterventions. To better understand and map the naturalness of these areas,\\rmachine learning models can be used to analyze satellite imagery. Specifically,\\rexplainable machine learning methods show promise in uncovering patterns that\\rcontribute to the concept of naturalness within these protected environments.\\rAdditionally, addressing the uncertainty inherent in machine learning models is\\rcrucial for a comprehensive understanding of this concept. However, existing\\rapproaches have limitations. They either fail to provide explanations that are\\rboth valid and objective or struggle to offer a quantitative metric that\\raccurately measures the contribution of specific patterns to naturalness, along\\rwith the associated confidence. In this paper, we propose a novel framework\\rcalled the Confident Naturalness Explanation (CNE) framework. This framework\\rcombines explainable machine learning and uncertainty quantification to assess\\rand explain naturalness. We introduce a new quantitative metric that describes\\rthe confident contribution of patterns to the concept of naturalness.\\rFurthermore, we generate an uncertainty-aware segmentation mask for each input\\rsample, highlighting areas where the model lacks knowledge. To demonstrate the\\reffectiveness of our framework, we apply it to a study site in Fennoscandia\\rusing two open-source satellite datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08936 ,  3979kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08949 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 13:35:40 GMT   (816kb,D)\\r\\rTitle: Automated Volume Corrected Mitotic Index Calculation Through\\r  Annotation-Free Deep Learning using Immunohistochemistry as Reference\\r  Standard\\rAuthors: Jonas Ammeling, Moritz Hecker, Jonathan Ganz, Taryn A. Donovan,\\r  Christof A. Bertram, Katharina Breininger, Marc Aubreville\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  The volume-corrected mitotic index (M/V-Index) was shown to provide\\rprognostic value in invasive breast carcinomas. However, despite its prognostic\\rsignificance, it is not established as the standard method for assessing\\raggressive biological behaviour, due to the high additional workload associated\\rwith determining the epithelial proportion. In this work, we show that using a\\rdeep learning pipeline solely trained with an annotation-free,\\rimmunohistochemistry-based approach, provides accurate estimations of\\repithelial segmentation in canine breast carcinomas. We compare our automatic\\rframework with the manually annotated M/V-Index in a study with three\\rboard-certified pathologists. Our results indicate that the deep learning-based\\rpipeline shows expert-level performance, while providing time efficiency and\\rreproducibility.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08949 ,  816kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09190 (*cross-listing*)\\rDate: Wed, 15 Nov 2023 18:34:03 GMT   (1440kb)\\r\\rTitle: On the Computation of the Gaussian Rate-Distortion-Perception Function\\rAuthors: Giuseppe Serra, Photios A. Stavrou, and Marios Kountouris\\rCategories: cs.IT cs.CV cs.LG cs.NI math.IT\\rComments: This paper has been submitted for journal publication\\r\\\\\\\\\\r  In this paper, we study the computation of the rate-distortion-perception\\rfunction (RDPF) for a multivariate Gaussian source under mean squared error\\r(MSE) distortion and, respectively, Kullback-Leibler divergence, geometric\\rJensen-Shannon divergence, squared Hellinger distance, and squared\\rWasserstein-2 distance perception metrics. To this end, we first characterize\\rthe analytical bounds of the scalar Gaussian RDPF for the aforementioned\\rdivergence functions, also providing the RDPF-achieving forward test-channel\\rrealization. Focusing on the multivariate case, we establish that, for\\rtensorizable distortion and perception metrics, the optimal solution resides on\\rthe vector space spanned by the eigenvector of the source covariance matrix.\\rConsequently, the multivariate optimization problem can be expressed as a\\rfunction of the scalar Gaussian RDPFs of the source marginals, constrained by\\rglobal distortion and perception levels. Leveraging this characterization, we\\rdesign an alternating minimization scheme based on the block nonlinear\\rGauss-Seidel method, which optimally solves the problem while identifying the\\rGaussian RDPF-achieving realization. Furthermore, the associated algorithmic\\rembodiment is provided, as well as the convergence and the rate of convergence\\rcharacterization. Lastly, for the perfect realism regime, the analytical\\rsolution for the multivariate Gaussian RDPF is obtained. We corroborate our\\rresults with numerical simulations and draw connections to existing results.\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09190 ,  1440kb)\\r%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2206.06807\\rreplaced with revised version Wed, 15 Nov 2023 11:42:28 GMT   (223kb,D)\\r\\rTitle: The Causal Structure of Semantic Ambiguities\\rAuthors: Daphne Wang (University College London), Mehrnoosh Sadrzadeh\\r  (University College London)\\rCategories: cs.CL cs.AI quant-ph\\rComments: In Proceedings QPL 2022, arXiv:2311.08375\\rJournal-ref: EPTCS 394, 2023, pp. 208-220\\rDOI: 10.4204/EPTCS.394.12\\r\\\\\\\\ ( https://arxiv.org/abs/2206.06807 ,  223kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.13034\\rreplaced with revised version Wed, 15 Nov 2023 03:24:03 GMT   (4671kb,D)\\r\\rTitle: Beyond Vectors: Subspace Representations for Set Operations of\\r  Embeddings\\rAuthors: Yoichi Ishibashi, Sho Yokoi, Katsuhito Sudoh, Satoshi Nakamura\\rCategories: cs.CL cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2210.13034 ,  4671kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.11483\\rreplaced with revised version Wed, 15 Nov 2023 16:21:58 GMT   (6765kb,D)\\r\\rTitle: Deanthropomorphising NLP: Can a Language Model Be Conscious?\\rAuthors: Matthew Shardlow and Piotr Przyby{\\\\l}a\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2211.11483 ,  6765kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.03111\\rreplaced with revised version Wed, 15 Nov 2023 04:56:25 GMT   (17401kb,D)\\r\\rTitle: Can LLM Already Serve as A Database Interface? A BIg Bench for\\r  Large-Scale Database Grounded Text-to-SQLs\\rAuthors: Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li,\\r  Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou,\\r  Chenhao Ma, Guoliang Li, Kevin C.C. Chang, Fei Huang, Reynold Cheng, Yongbin\\r  Li\\rCategories: cs.CL\\rComments: NeurIPS 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2305.03111 ,  17401kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.04978\\rreplaced with revised version Wed, 15 Nov 2023 17:34:56 GMT   (5399kb,D)\\r\\rTitle: NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge\\rAuthors: Phillip Howard, Junlin Wang, Vasudev Lal, Gadi Singer, Yejin Choi,\\r  Swabha Swayamdipta\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2305.04978 ,  5399kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.08703\\rreplaced with revised version Wed, 15 Nov 2023 12:55:56 GMT   (1442kb,D)\\r\\rTitle: Schema-adaptable Knowledge Graph Construction\\rAuthors: Hongbin Ye, Honghao Gui, Xin Xu, Xi Chen, Huajun Chen, Ningyu Zhang\\rCategories: cs.CL cs.AI cs.DB cs.IR cs.LG\\rComments: EMNLP 2023 (Findings)\\r\\\\\\\\ ( https://arxiv.org/abs/2305.08703 ,  1442kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.12057\\rreplaced with revised version Tue, 14 Nov 2023 21:02:57 GMT   (7711kb,D)\\r\\rTitle: Accurate Knowledge Distillation with n-best Reranking\\rAuthors: Hendra Setiawan\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2305.12057 ,  7711kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.13062\\rreplaced with revised version Wed, 15 Nov 2023 12:18:39 GMT   (237kb,D)\\r\\rTitle: GPT4Table: Can Large Language Models Understand Structured Table Data? A\\r  Benchmark and Empirical Study\\rAuthors: Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, Dongmei Zhang\\rCategories: cs.CL cs.AI cs.IR\\rComments: This paper has been accepted as a full paper at WSDM 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2305.13062 ,  237kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14332\\rreplaced with revised version Wed, 15 Nov 2023 17:15:31 GMT   (2593kb,D)\\r\\rTitle: Evaluating and Modeling Attribution for Cross-Lingual Question Answering\\rAuthors: Benjamin Muller, John Wieting, Jonathan H. Clark, Tom Kwiatkowski,\\r  Sebastian Ruder, Livio Baldini Soares, Roee Aharoni, Jonathan Herzig, Xinyi\\r  Wang\\rCategories: cs.CL\\rComments: Published as a long paper at EMNLP 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14332 ,  2593kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14336\\rreplaced with revised version Wed, 15 Nov 2023 18:56:34 GMT   (9480kb,D)\\r\\rTitle: Schema-Driven Information Extraction from Heterogeneous Tables\\rAuthors: Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, Alan Ritter\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14336 ,  9480kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14463\\rreplaced with revised version Wed, 15 Nov 2023 15:50:31 GMT   (3055kb,D)\\r\\rTitle: ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain\\r  Readability Assessment\\rAuthors: Tarek Naous, Michael J. Ryan, Anton Lavrouk, Mohit Chandra, Wei Xu\\rCategories: cs.CL cs.AI cs.LG\\rComments: We have added French and Russian as two new languages to the corpus\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14463 ,  3055kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14489\\rreplaced with revised version Wed, 15 Nov 2023 04:51:27 GMT   (9499kb,D)\\r\\rTitle: Are Large Language Models Robust Coreference Resolvers?\\rAuthors: Nghia T. Le, Alan Ritter\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14489 ,  9499kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14493\\rreplaced with revised version Wed, 15 Nov 2023 16:44:04 GMT   (208kb,D)\\r\\rTitle: Do prompt positions really matter?\\rAuthors: Junyu Mao and Stuart E. Middleton and Mahesan Niranjan\\rCategories: cs.CL\\rComments: 8 pages, 2 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14493 ,  208kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14613\\rreplaced with revised version Wed, 15 Nov 2023 02:15:02 GMT   (208kb,D)\\r\\rTitle: Selectively Answering Ambiguous Questions\\rAuthors: Jeremy R. Cole, Michael J.Q. Zhang, Daniel Gillick, Julian Martin\\r  Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein\\rCategories: cs.CL cs.AI\\rComments: To appear in EMNLP 2023. 9 pages, 5 figures, 2 pages of appendix\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14613 ,  208kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14658\\rreplaced with revised version Tue, 14 Nov 2023 19:16:35 GMT   (1696kb,D)\\r\\rTitle: Evaluate What You Can't Evaluate: Unassessable Quality for Generated\\r  Response\\rAuthors: Yongkang Liu and Shi Feng and Daling Wang and Yifei Zhang and Hinrich\\r  Sch\\\\utze\\rCategories: cs.CL\\rComments: preprint\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14658 ,  1696kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14964\\rreplaced with revised version Tue, 14 Nov 2023 22:41:40 GMT   (1066kb,D)\\r\\rTitle: Detecting Multidimensional Political Incivility on Social Media\\rAuthors: Sagi Pendzel, Nir Lotan, Alon Zoizner, Einat Minkov\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14964 ,  1066kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.06190\\rreplaced with revised version Tue, 14 Nov 2023 21:51:21 GMT   (4321kb,D)\\r\\rTitle: $FastDoc$: Domain-Specific Fast Pre-training Technique using\\r  Document-Level Metadata and Taxonomy\\rAuthors: Abhilash Nandy, Manav Nitin Kapadnis, Sohan Patnaik, Yash Parag\\r  Butala, Pawan Goyal, Niloy Ganguly\\rCategories: cs.CL cs.LG\\rComments: 38 pages, 7 figures\\rMSC-class: 68T50\\rACM-class: I.2.7\\r\\\\\\\\ ( https://arxiv.org/abs/2306.06190 ,  4321kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02185\\rreplaced with revised version Wed, 15 Nov 2023 07:59:16 GMT   (172kb,D)\\r\\rTitle: Citation: A Key to Building Responsible and Accountable Large Language\\r  Models\\rAuthors: Jie Huang, Kevin Chen-Chuan Chang\\rCategories: cs.CL cs.AI cs.CR\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02185 ,  172kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.07705\\rreplaced with revised version Wed, 15 Nov 2023 17:02:17 GMT   (827kb,D)\\r\\rTitle: CPET: Effective Parameter-Efficient Tuning for Compressed Large Language\\r  Models\\rAuthors: Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang,\\r  Maosong Sun\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2307.07705 ,  827kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.15452\\rreplaced with revised version Wed, 15 Nov 2023 14:06:30 GMT   (5938kb,D)\\r\\rTitle: When Do Program-of-Thoughts Work for Reasoning?\\rAuthors: Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, Huajun\\r  Chen\\rCategories: cs.CL cs.AI cs.LG cs.SE\\rComments: Work in progress\\r\\\\\\\\ ( https://arxiv.org/abs/2308.15452 ,  5938kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.02457\\rreplaced with revised version Wed, 15 Nov 2023 18:02:03 GMT   (126kb,D)\\r\\rTitle: The Empty Signifier Problem: Towards Clearer Paradigms for\\r  Operationalising Alignment in Large Language Models\\rAuthors: Hannah Rose Kirk, Bertie Vidgen, Paul R\\\\ottger, Scott A. Hale\\rCategories: cs.CL cs.CY\\rComments: Socially Responsible Language Modelling Research (SoLaR) @ NeurIPs\\r  2023\\r\\\\\\\\ ( https://arxiv.org/abs/2310.02457 ,  126kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.08130\\rreplaced with revised version Wed, 15 Nov 2023 03:59:42 GMT   (8910kb,D)\\r\\rTitle: Fine-grained Conversational Decoding via Isotropic and Proximal Search\\rAuthors: Yuxuan Yao, Han Wu, Qiling Xu, Linqi Song\\rCategories: cs.CL\\rComments: Accepted to EMNLP 2023 Main Conference\\r\\\\\\\\ ( https://arxiv.org/abs/2310.08130 ,  8910kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.10191\\rreplaced with revised version Wed, 15 Nov 2023 12:41:57 GMT   (9812kb,D)\\r\\rTitle: VIBE: Topic-Driven Temporal Adaptation for Twitter Classification\\rAuthors: Yuji Zhang, Jing Li, Wenjie Li\\rCategories: cs.CL\\rComments: accepted by EMNLP 2023\\rJournal-ref: The 2023 Conference on Empirical Methods in Natural Language\\r  Processing (EMNLP 2023)\\r\\\\\\\\ ( https://arxiv.org/abs/2310.10191 ,  9812kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.12342\\rreplaced with revised version Wed, 15 Nov 2023 00:59:54 GMT   (7830kb,D)\\r\\rTitle: Eliminating Reasoning via Inferring with Planning: A New Framework to\\r  Guide LLMs' Non-linear Thinking\\rAuthors: Yongqi Tong, Yifan Wang, Dawei Li, Sizhe Wang, Zi Lin, Simeng Han,\\r  Jingbo Shang\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2310.12342 ,  7830kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.12963\\rreplaced with revised version Wed, 15 Nov 2023 18:23:40 GMT   (621kb,D)\\r\\rTitle: AutoMix: Automatically Mixing Language Models\\rAuthors: Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi\\r  Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik\\r  Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, Manaal Faruqui\\rCategories: cs.CL cs.AI\\rComments: The first two authors contributed equally. Work started and partly\\r  done during Aman's internship at Google. This version adds results on mixing\\r  3 models, and will be presented at the workshop on robustness of\\r  zero/few-shot learning in foundation models, Neurips 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2310.12963 ,  621kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.14088\\rreplaced with revised version Tue, 14 Nov 2023 21:59:56 GMT   (677kb,D)\\r\\rTitle: MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark\\r  for Language Model Evaluation\\rAuthors: Zexue He, Yu Wang, An Yan, Yao Liu, Eric Y. Chang, Amilcare Gentili,\\r  Julian McAuley, Chun-Nan Hsu\\rCategories: cs.CL\\rComments: Accepted to EMNLP 2023. Camera-ready version: updated IRB, added more\\r  evaluation results on LLMs such as GPT4, LLaMa2, and LLaMa2-chat\\r\\\\\\\\ ( https://arxiv.org/abs/2310.14088 ,  677kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.15612\\rreplaced with revised version Wed, 15 Nov 2023 08:47:28 GMT   (6080kb,D)\\r\\rTitle: Machine Translation for Nko: Tools, Corpora and Baseline Results\\rAuthors: Moussa Koulako Bala Doumbouya, Baba Mamadi Dian\\\\'e, Solo Farabado\\r  Ciss\\\\'e, Djibrila Dian\\\\'e, Abdoulaye Sow, S\\\\'er\\\\'e Moussa Doumbouya, Daouda\\r  Bangoura, Fod\\\\'e Moriba Bayo, Ibrahima Sory 2. Cond\\\\'e, Kalo Mory Dian\\\\'e,\\r  Chris Piech, Christopher Manning\\rCategories: cs.CL cs.CY cs.HC cs.LG\\rACM-class: I.2.6; I.2.7\\r\\\\\\\\ ( https://arxiv.org/abs/2310.15612 ,  6080kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.00684\\rreplaced with revised version Wed, 15 Nov 2023 15:55:02 GMT   (1010kb,D)\\r\\rTitle: Attention Alignment and Flexible Positional Embeddings Improve\\r  Transformer Length Extrapolation\\rAuthors: Ta-Chung Chi and Ting-Han Fan and Alexander I. Rudnicky\\rCategories: cs.CL cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2311.00684 ,  1010kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.05922\\rreplaced with revised version Wed, 15 Nov 2023 05:23:04 GMT   (0kb,I)\\r\\rTitle: Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation\\r  Extraction\\rAuthors: Xilai Ma, Jing Li and Min Zhang\\rCategories: cs.CL\\rComments: An error example is in Table 14 on Page 18. Need to carefully correct\\r  and evaluate the error\\r\\\\\\\\ ( https://arxiv.org/abs/2311.05922 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.06851\\rreplaced with revised version Wed, 15 Nov 2023 17:33:39 GMT   (1882kb,D)\\r\\rTitle: Automatic Textual Normalization for Hate Speech Detection\\rAuthors: Anh Thi-Hoang Nguyen, Dung Ha Nguyen, Nguyet Thi Nguyen, Khanh\\r  Thanh-Duy Ho, Kiet Van Nguyen\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2311.06851 ,  1882kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.07772\\rreplaced with revised version Wed, 15 Nov 2023 15:29:05 GMT   (246kb,D)\\r\\rTitle: In-context Learning and Gradient Descent Revisited\\rAuthors: Tomer Bar Natan, Gilad Deutch, Nadav Magar, Guy Dar\\rCategories: cs.CL cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2311.07772 ,  246kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08189\\rreplaced with revised version Wed, 15 Nov 2023 02:57:01 GMT   (3252kb,D)\\r\\rTitle: Unlocking Science: Novel Dataset and Benchmark for Cross-Modality\\r  Scientific Information Extraction\\rAuthors: Yuhan Li and Jian Wu and Zhiwei Yu and B\\\\orje F. Karlsson and Wei\\r  Shen and Manabu Okumura and Chin-Yew Lin\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08189 ,  3252kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08329\\rreplaced with revised version Wed, 15 Nov 2023 17:36:55 GMT   (1436kb,D)\\r\\rTitle: KTRL+F: Knowledge-Augmented In-Document Search\\rAuthors: Hanseok Oh, Haebin Shin, Miyoung Ko, Hyunji Lee, Minjoon Seo\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08329 ,  1436kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08385\\rreplaced with revised version Wed, 15 Nov 2023 16:40:13 GMT   (8852kb,D)\\r\\rTitle: ChOiRe: Characterizing and Predicting Human Opinions with Chain of\\r  Opinion Reasoning\\rAuthors: Xuan Long Do, Kenji Kawaguchi, Min-Yen Kan, Nancy F. Chen\\rCategories: cs.CL\\rComments: 17 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08385 ,  8852kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.07853\\rreplaced with revised version Wed, 15 Nov 2023 15:18:11 GMT   (9415kb,D)\\r\\rTitle: Estimating Appearance Models for Image Segmentation via Tensor\\r  Factorization\\rAuthors: Jeova Farias Sales Rocha Neto\\rCategories: cs.CV stat.ML\\r\\\\\\\\ ( https://arxiv.org/abs/2208.07853 ,  9415kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.10368\\rreplaced with revised version Wed, 15 Nov 2023 18:08:57 GMT   (8050kb,D)\\r\\rTitle: Masked Event Modeling: Self-Supervised Pretraining for Event Cameras\\rAuthors: Simon Klenk, David Bonello, Lukas Koestler, Nikita Araslanov, Daniel\\r  Cremers\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2212.10368 ,  8050kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.14516\\rreplaced with revised version Wed, 15 Nov 2023 16:49:34 GMT   (9521kb,D)\\r\\rTitle: OVeNet: Offset Vector Network for Semantic Segmentation\\rAuthors: Stamatis Alexandropoulos, Christos Sakaridis and Petros Maragos\\rCategories: cs.CV cs.AI cs.LG cs.RO\\rComments: Accepted at WACV 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2303.14516 ,  9521kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.10722\\rreplaced with revised version Wed, 15 Nov 2023 07:53:57 GMT   (969kb,D)\\r\\rTitle: Discriminative Diffusion Models as Few-shot Vision and Language Learners\\rAuthors: Xuehai He, Weixi Feng, Tsu-Jui Fu, Varun Jampani, Arjun Akula,\\r  Pradyumna Narayana, Sugato Basu, William Yang Wang, Xin Eric Wang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2305.10722 ,  969kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.12437\\rreplaced with revised version Wed, 15 Nov 2023 02:59:32 GMT   (1102kb,D)\\r\\rTitle: PLAR: Prompt Learning for Action Recognition\\rAuthors: Xijun Wang, Ruiqi Xian, Tianrui Guan, Dinesh Manocha\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2305.12437 ,  1102kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.12704\\rreplaced with revised version Wed, 15 Nov 2023 09:02:23 GMT   (8205kb,D)\\r\\rTitle: Rotation-Constrained Cross-View Feature Fusion for Multi-View\\r  Appearance-based Gaze Estimation\\rAuthors: Yoichiro Hisadome, Tianyi Wu, Jiawei Qin, Yusuke Sugano\\rCategories: cs.CV\\rComments: Accepted by WACV2024. The code will be available at\\r  https://github.com/ut-vision/Rot-MVGaze\\r\\\\\\\\ ( https://arxiv.org/abs/2305.12704 ,  8205kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.09012\\rreplaced with revised version Wed, 15 Nov 2023 10:16:55 GMT   (21154kb,D)\\r\\rTitle: Yes, we CANN: Constrained Approximate Nearest Neighbors for local\\r  feature-based visual localization\\rAuthors: Dror Aiger, Andr\\\\'e Araujo, Simon Lynen\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.09012 ,  21154kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.06507\\rreplaced with revised version Wed, 15 Nov 2023 02:24:21 GMT   (26135kb,D)\\r\\rTitle: Improving Nonalcoholic Fatty Liver Disease Classification Performance\\r  With Latent Diffusion Models\\rAuthors: Romain Hardy, Joe Klepich, Ryan Mitchell, Steve Hall, Jericho\\r  Villareal, Cornelia Ilin\\rCategories: cs.CV cs.AI\\rComments: 36 pages, 13 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2307.06507 ,  26135kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.10894\\rreplaced with revised version Wed, 15 Nov 2023 06:26:21 GMT   (5684kb,D)\\r\\rTitle: Human Motion Generation: A Survey\\rAuthors: Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi,\\r  Feng Gao, Qi Tian, and Yizhou Wang\\rCategories: cs.CV\\rComments: Accepted to TPAMI\\r\\\\\\\\ ( https://arxiv.org/abs/2307.10894 ,  5684kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.02487\\rreplaced with revised version Tue, 14 Nov 2023 19:10:49 GMT   (2252kb,D)\\r\\rTitle: Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen\\r  Convolutional CLIP\\rAuthors: Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, Liang-Chieh Chen\\rCategories: cs.CV\\rComments: NeurIPS 2023 camera ready. code and model available at\\r  https://github.com/bytedance/fc-clip\\r\\\\\\\\ ( https://arxiv.org/abs/2308.02487 ,  2252kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.10103\\rreplaced with revised version Wed, 15 Nov 2023 00:14:08 GMT   (41493kb,D)\\r\\rTitle: ASPIRE: Language-Guided Augmentation for Robust Image Classification\\rAuthors: Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Utkarsh Tyagi,\\r  Sakshi Singh, Sanjoy Chowdhury and Dinesh Manocha\\rCategories: cs.CV cs.AI cs.CL\\rComments: Pre-print Under Review\\r\\\\\\\\ ( https://arxiv.org/abs/2308.10103 ,  41493kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.14119\\rreplaced with revised version Wed, 15 Nov 2023 16:15:19 GMT   (6976kb,D)\\r\\rTitle: Semi-Supervised Learning in the Few-Shot Zero-Shot Scenario\\rAuthors: Noam Fluss, Guy Hacohen, Daphna Weinshall\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2308.14119 ,  6976kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.07944\\rreplaced with revised version Wed, 15 Nov 2023 13:46:36 GMT   (16252kb,D)\\r\\rTitle: Text-to-Image Models for Counterfactual Explanations: a Black-Box\\r  Approach\\rAuthors: Guillaume Jeanneret and Lo\\\\ic Simon and Fr\\\\'ed\\\\'eric Jurie\\rCategories: cs.CV\\rComments: WACV 2024 Camera ready + supplementary material\\r\\\\\\\\ ( https://arxiv.org/abs/2309.07944 ,  16252kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.14136\\rreplaced with revised version Wed, 15 Nov 2023 13:47:49 GMT   (8736kb,D)\\r\\rTitle: Masked Image Residual Learning for Scaling Deeper Vision Transformers\\rAuthors: Guoxi Huang, Hongtao Fu, Adrian G. Bors\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.14136 ,  8736kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.17842\\rreplaced with revised version Tue, 14 Nov 2023 23:03:35 GMT   (7514kb,D)\\r\\rTitle: What You See Is What You Detect: Towards better Object Densification in\\r  3D detection\\rAuthors: Tianran Liu, Zeping Zhang, Morteza Mousa Pasandi, Robert Laganiere\\rCategories: cs.CV cs.RO\\r\\\\\\\\ ( https://arxiv.org/abs/2310.17842 ,  7514kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.02762\\rreplaced with revised version Wed, 15 Nov 2023 04:38:09 GMT   (0kb,I)\\r\\rTitle: Fast Sparse 3D Convolution Network with VDB\\rAuthors: Fangjun Zhou, Anyong Mao, Eftychios Sifakis\\rCategories: cs.CV cs.LG\\rComments: Unauthorized publication\\r\\\\\\\\ ( https://arxiv.org/abs/2311.02762 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.04246\\rreplaced with revised version Tue, 14 Nov 2023 08:52:57 GMT   (23254kb,D)\\r\\rTitle: ADFactory: An Effective Framework for Generalizing Optical Flow with\\r  Nerf\\rAuthors: Han Ling\\rCategories: cs.CV\\rComments: 8 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2311.04246 ,  23254kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.07042\\rreplaced with revised version Wed, 15 Nov 2023 02:17:52 GMT   (3912kb,D)\\r\\rTitle: Open-Vocabulary Video Anomaly Detection\\rAuthors: Peng Wu, Xuerong Zhou, Guansong Pang, Yujia Sun, Jing Liu, Peng Wang,\\r  Yanning Zhang\\rCategories: cs.CV\\rComments: Submitted\\r\\\\\\\\ ( https://arxiv.org/abs/2311.07042 ,  3912kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.07955\\rreplaced with revised version Wed, 15 Nov 2023 02:38:37 GMT   (15251kb,D)\\r\\rTitle: Deep Learning-Based Object Detection in Maritime Unmanned Aerial Vehicle\\r  Imagery: Review and Experimental Comparisons\\rAuthors: Chenjie Zhao, Ryan Wen Liu, Jingxiang Qu, Ruobin Gao\\rCategories: cs.CV cs.AI\\rComments: 32 pages, 18 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2311.07955 ,  15251kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.00146\\rreplaced with revised version Wed, 15 Nov 2023 17:13:04 GMT   (37200kb,D)\\r\\rTitle: Bluefish: A Relational Framework for Graphic Representations\\rAuthors: Josh Pollock, Catherine Mei, Grace Huang, Daniel Jackson, Arvind\\r  Satyanarayan\\rCategories: cs.GR cs.HC cs.PL\\rComments: 27 pages, 14 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2307.00146 ,  37200kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.00239\\rreplaced with revised version Wed, 15 Nov 2023 03:44:44 GMT   (44604kb,D)\\r\\rTitle: AdaptNet: Policy Adaptation for Physics-Based Character Control\\rAuthors: Pei Xu, Kaixiang Xie, Sheldon Andrews, Paul G. Kry, Michael Neff,\\r  Morgan McGuire, Ioannis Karamouzas, Victor Zordan\\rCategories: cs.GR cs.AI cs.LG\\rComments: SIGGRAPH Asia 2023. Video: https://youtu.be/WxmJSCNFb28. Website:\\r  https://motion-lab.github.io/AdaptNet, https://pei-xu.github.io/AdaptNet\\rJournal-ref: ACM Transactions on Graphics 42, 6, Article 112.1522 (December\\r  2023)\\rDOI: 10.1145/3618375\\r\\\\\\\\ ( https://arxiv.org/abs/2310.00239 ,  44604kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.09863\\rreplaced with revised version Wed, 15 Nov 2023 17:19:10 GMT   (1245kb,D)\\r\\rTitle: Explaining black box text modules in natural language with language\\r  models\\rAuthors: Chandan Singh, Aliyah R. Hsu, Richard Antonello, Shailee Jain,\\r  Alexander G. Huth, Bin Yu, Jianfeng Gao\\rCategories: cs.AI cs.CL cs.LG q-bio.NC\\r\\\\\\\\ ( https://arxiv.org/abs/2305.09863 ,  1245kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.19011 (*cross-listing*)\\rreplaced with revised version Tue, 14 Nov 2023 21:22:25 GMT   (1320kb,D)\\r\\rTitle: MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models\\rAuthors: Yu-Hsiang Wang, Huang-Yu Chen, Kai-Wei Chang, Winston Hsu, Hung-yi Lee\\rCategories: eess.AS cs.CL cs.LG\\rComments: Accepted to IEEE ASRU 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2305.19011 ,  1320kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.00755\\rreplaced with revised version Wed, 15 Nov 2023 08:51:11 GMT   (36376kb,D)\\r\\rTitle: The Bias Amplification Paradox in Text-to-Image Generation\\rAuthors: Preethi Seshadri, Sameer Singh, Yanai Elazar\\rCategories: cs.LG cs.CL cs.CV cs.CY\\r\\\\\\\\ ( https://arxiv.org/abs/2308.00755 ,  36376kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.02971 (*cross-listing*)\\rreplaced with revised version Tue, 14 Nov 2023 21:15:01 GMT   (1253kb,D)\\r\\rTitle: Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech\\r  Model\\rAuthors: Kai-Wei Chang, Ming-Hsin Chen, Yun-Ping Lin, Jing Neng Hsu, Paul\\r  Kuo-Ming Huang, Chien-yu Huang, Shang-Wen Li, Hung-yi Lee\\rCategories: eess.AS cs.CL eess.SP\\rComments: Accepted to IEEE ASRU 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2310.02971 ,  1253kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.04378\\rreplaced with revised version Wed, 15 Nov 2023 00:21:29 GMT   (5016kb,D)\\r\\rTitle: Watermarks in the Sand: Impossibility of Strong Watermarking for\\r  Generative Models\\rAuthors: Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi,\\r  Giuseppe Ateniese, Boaz Barak\\rCategories: cs.LG cs.CL cs.CR\\rComments: Blog post:\\r  https://www.harvard.edu/kempner-institute/2023/11/09/watermarking-in-the-sand/\\r\\\\\\\\ ( https://arxiv.org/abs/2311.04378 ,  5016kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08360\\rreplaced with revised version Wed, 15 Nov 2023 04:02:44 GMT   (5854kb,D)\\r\\rTitle: The Transient Nature of Emergent In-Context Learning in Transformers\\rAuthors: Aaditya K. Singh, Stephanie C.Y. Chan, Ted Moskovitz, Erin Grant,\\r  Andrew M. Saxe, Felix Hill\\rCategories: cs.LG cs.AI cs.CL\\rComments: 19 pages, 16 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08360 ,  5854kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2206.01251\\rreplaced with revised version Tue, 14 Nov 2023 20:25:21 GMT   (174kb,D)\\r\\rTitle: Using Representation Expressiveness and Learnability to Evaluate\\r  Self-Supervised Learning Methods\\rAuthors: Yuchen Lu, Zhen Liu, Aristide Baratin, Romain Laroche, Aaron\\r  Courville, Alessandro Sordoni\\rCategories: cs.LG cs.AI cs.CV\\rJournal-ref: TMLR 2023 -- Transactions of Machine Learning Research, 11/2023\\r\\\\\\\\ ( https://arxiv.org/abs/2206.01251 ,  174kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.02474\\rreplaced with revised version Wed, 15 Nov 2023 08:35:24 GMT   (4172kb,D)\\r\\rTitle: CFARnet: deep learning for target detection with constant false alarm\\r  rate\\rAuthors: Tzvi Diskin, Yiftach Beer, Uri Okun and Ami Wiesel\\rCategories: cs.LG cs.CV\\rComments: arXiv admin note: substantial text overlap with arXiv:2206.05747\\r\\\\\\\\ ( https://arxiv.org/abs/2208.02474 ,  4172kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.02998 (*cross-listing*)\\rreplaced with revised version Wed, 15 Nov 2023 13:30:39 GMT   (4825kb,D)\\r\\rTitle: ThoraX-PriorNet: A Novel Attention-Based Architecture Using Anatomical\\r  Prior Probability Maps for Thoracic Disease Classification\\rAuthors: Md. Iqbal Hossain, Mohammad Zunaed, Md. Kawsar Ahmed, S. M. Jawwad\\r  Hossain, Anwarul Hasan, and Taufiq Hasan\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2210.02998 ,  4825kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.06891 (*cross-listing*)\\rreplaced with revised version Wed, 15 Nov 2023 13:41:43 GMT   (24763kb,D)\\r\\rTitle: Residual Degradation Learning Unfolding Framework with Mixing Priors\\r  across Spectral and Spatial for Compressive Spectral Imaging\\rAuthors: Yubo Dong, Dahua Gao, Tian Qiu, Yuyan Li, Minxi Yang, Guangming Shi\\rCategories: eess.IV cs.CV\\rComments: CVPR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2211.06891 ,  24763kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.12322 (*cross-listing*)\\rreplaced with revised version Wed, 15 Nov 2023 08:53:29 GMT   (10986kb,D)\\r\\rTitle: Infrared Image Super-Resolution: Systematic Review, and Future Trends\\rAuthors: Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Shinichiro Omachi\\rCategories: eess.IV cs.CV cs.LG\\rComments: Submitted to IEEE TNNLS\\r\\\\\\\\ ( https://arxiv.org/abs/2212.12322 ,  10986kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.12554\\rreplaced with revised version Wed, 15 Nov 2023 11:23:32 GMT   (2977kb,D)\\r\\rTitle: Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive\\r  Smoothing\\rAuthors: Yatong Bai, Brendon G. Anderson, Aerin Kim, Somayeh Sojoudi\\rCategories: cs.LG cs.CR cs.CV\\rMSC-class: 68T07\\r\\\\\\\\ ( https://arxiv.org/abs/2301.12554 ,  2977kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.01258 (*cross-listing*)\\rreplaced with revised version Wed, 15 Nov 2023 11:22:05 GMT   (1582kb,D)\\r\\rTitle: MobileNVC: Real-time 1080p Neural Video Compression on a Mobile Device\\rAuthors: Ties van Rozendaal, Tushar Singhal, Hoang Le, Guillaume Sautiere, Amir\\r  Said, Krishna Buska, Anjuman Raha, Dimitris Kalatzis, Hitarth Mehta, Frank\\r  Mayer, Liang Zhang, Markus Nagel, Auke Wiggers\\rCategories: eess.IV cs.CV cs.LG\\rComments: Matches version published at WACV 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.01258 ,  1582kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.07250 (*cross-listing*)\\rreplaced with revised version Wed, 15 Nov 2023 10:35:07 GMT   (0kb,I)\\r\\rTitle: Synthesizing Missing MRI Sequences from Available Modalities using\\r  Generative Adversarial Networks in BraTS Dataset\\rAuthors: Ibrahim Ethem Hamamci\\rCategories: q-bio.QM cs.CV cs.LG eess.IV\\rComments: Wrong paper submission\\r\\\\\\\\ ( https://arxiv.org/abs/2310.07250 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.00735\\rreplaced with revised version Wed, 15 Nov 2023 07:28:10 GMT   (1297kb)\\r\\rTitle: PET Tracer Conversion among Brain PET via Variable Augmented Invertible\\r  Network\\rAuthors: Bohui Shen, Wei Zhang, Xubiao Liu, Pengfei Yu, Shirui Jiang, Xinchong\\r  Shi, Xiangsong Zhang, Xiaoyu Zhou, Weirui Zhang, Bingxuan Li, Qiegen Liu\\rCategories: cs.LG cs.CV\\rMSC-class: 68T01\\r\\\\\\\\ ( https://arxiv.org/abs/2311.00735 ,  1297kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.03217 (*cross-listing*)\\rreplaced with revised version Wed, 15 Nov 2023 14:37:24 GMT   (1369kb,D)\\r\\rTitle: Leveraging Transformers to Improve Breast Cancer Classification and Risk\\r  Assessment with Multi-modal and Longitudinal Data\\rAuthors: Yiqiu Shen, Jungkyu Park, Frank Yeung, Eliana Goldberg, Laura Heacock,\\r  Farah Shamout, Krzysztof J. Geras\\rCategories: eess.IV cs.CV cs.LG\\rComments: ML4H 2023 Findings Track\\r\\\\\\\\ ( https://arxiv.org/abs/2311.03217 ,  1369kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.04818\\rreplaced with revised version Tue, 14 Nov 2023 21:59:36 GMT   (3500kb,D)\\r\\rTitle: Cross-Silo Federated Learning Across Divergent Domains with Iterative\\r  Parameter Alignment\\rAuthors: Matt Gorbett, Hossein Shirazi, Indrakshi Ray\\rCategories: cs.LG cs.CV cs.DC\\rComments: Published at IEEE Big Data 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2311.04818 ,  3500kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.05836 (*cross-listing*)\\rreplaced with revised version Wed, 15 Nov 2023 14:37:16 GMT   (4031kb,D)\\r\\rTitle: Uncertainty-aware Single View Volumetric Rendering for Medical Neural\\r  Radiance Fields\\rAuthors: Jing Hu, Qinrui Fan, Shu Hu, Siwei Lyu, Xi Wu, Xin Wang\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2311.05836 ,  4031kb)\\r%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%--- ------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\rSend any comments regarding submissions directly to submitter.\\r------------------------------------------------------------------------------\\rArchives at http://arxiv.org/\\rTo unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel\\r------------------------------------------------------------------------------\\r Submissions to:\\rComputation and Language\\rComputer Vision and Pattern Recognition\\rGraphics\\r received from  Fri  1 Mar 24 19:00:00 GMT  to  Mon  4 Mar 24 19:00:00 GMT\\r------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00788\\rDate: Tue, 20 Feb 2024 04:26:31 GMT   (1944kb)\\r\\rTitle: PRECISE Framework: GPT-based Text For Improved Readability, Reliability,\\r  and Understandability of Radiology Reports For Patient-Centered Care\\rAuthors: Satvik Tripathi, Liam Mutter, Meghana Muppuri, Suhani Dheer, Emiliano\\r  Garza-Frias, Komal Awan, Aakash Jha, Michael Dezube, Azadeh Tabari,\\r  Christopher P. Bridge, Dania Daye\\rCategories: cs.CL cs.AI cs.HC cs.LG\\r\\\\\\\\\\r  This study introduces and evaluates the PRECISE framework, utilizing OpenAI's\\rGPT-4 to enhance patient engagement by providing clearer and more accessible\\rchest X-ray reports at a sixth-grade reading level. The framework was tested on\\r500 reports, demonstrating significant improvements in readability,\\rreliability, and understandability. Statistical analyses confirmed the\\reffectiveness of the PRECISE approach, highlighting its potential to foster\\rpatient-centric care delivery in healthcare decision-making.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00788 ,  1944kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00791\\rDate: Thu, 22 Feb 2024 20:11:24 GMT   (833kb,D)\\r\\rTitle: $\\\\textit{L+M-24}$: Building a Dataset for Language + Molecules @ ACL\\r  2024\\rAuthors: Carl Edwards and Qingyun Wang and Lawrence Zhao and Heng Ji\\rCategories: cs.CL cs.AI q-bio.BM q-bio.QM\\rComments: The dataset, finetuned baselines, and evaluation code are released\\r  publicly at https://github.com/language-plus-molecules/LPM-24-Dataset through\\r  https://huggingface.co/language-plus-molecules\\r\\\\\\\\\\r  Language-molecule models have emerged as an exciting direction for molecular\\rdiscovery and understanding. However, training these models is challenging due\\rto the scarcity of molecule-language pair datasets. At this point, datasets\\rhave been released which are 1) small and scraped from existing databases, 2)\\rlarge but noisy and constructed by performing entity linking on the scientific\\rliterature, and 3) built by converting property prediction datasets to natural\\rlanguage using templates. In this document, we detail the $\\\\textit{L+M-24}$\\rdataset, which has been created for the Language + Molecules Workshop shared\\rtask at ACL 2024. In particular, $\\\\textit{L+M-24}$ is designed to focus on\\rthree key benefits of natural language in molecule design: compositionality,\\rfunctionality, and abstraction.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00791 ,  833kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00794\\rDate: Fri, 23 Feb 2024 02:58:12 GMT   (280kb,D)\\r\\rTitle: Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large\\r  Language Models\\rAuthors: Zachary Horvitz, Jingru Chen, Rahul Aditya, Harshvardhan Srivastava,\\r  Robert West, Zhou Yu, Kathleen McKeown\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  Humor is a fundamental facet of human cognition and interaction. Yet, despite\\rrecent advances in natural language processing, humor detection remains a\\rchallenging task that is complicated by the scarcity of datasets that pair\\rhumorous texts with similar non-humorous counterparts. In our work, we\\rinvestigate whether large language models (LLMs), can generate synthetic data\\rfor humor detection via editing texts. We benchmark LLMs on an existing human\\rdataset and show that current LLMs display an impressive ability to `unfun'\\rjokes, as judged by humans and as measured on the downstream task of humor\\rdetection. We extend our approach to a code-mixed English-Hindi humor dataset,\\rwhere we find that GPT-4's synthetic data is highly rated by bilingual\\rannotators and provides challenging adversarial examples for humor classifiers.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00794 ,  280kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00795\\rDate: Fri, 23 Feb 2024 05:31:36 GMT   (898kb)\\r\\rTitle: Executing Natural Language-Described Algorithms with Large Language\\r  Models: An Investigation\\rAuthors: Xin Zheng, Qiming Zhu, Hongyu Lin, Yaojie Lu, Xianpei Han and Le Sun\\rCategories: cs.CL cs.AI\\rComments: Accepted at LREC-COLING 2024\\r\\\\\\\\\\r  Executing computer programs described in natural language has long been a\\rpursuit of computer science. With the advent of enhanced natural language\\runderstanding capabilities exhibited by large language models (LLMs), the path\\rtoward this goal has been illuminated. In this paper, we seek to examine the\\rcapacity of present-day LLMs to comprehend and execute algorithms outlined in\\rnatural language. We established an algorithm test set sourced from\\rIntroduction to Algorithm, a well-known textbook that contains many\\rrepresentative widely-used algorithms. To systematically assess LLMs' code\\rexecution abilities, we selected 30 algorithms, generated 300 random-sampled\\rinstances in total, and evaluated whether popular LLMs can understand and\\rexecute these algorithms. Our findings reveal that LLMs, notably GPT-4, can\\reffectively execute programs described in natural language, as long as no heavy\\rnumeric computation is involved. We believe our findings contribute to\\revaluating LLMs' code execution abilities and would encourage further\\rinvestigation and application for the computation power of LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00795 ,  898kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00799\\rDate: Fri, 23 Feb 2024 17:38:43 GMT   (621kb,D)\\r\\rTitle: An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning\\rAuthors: Zui Chen, Yezeng Chen, Jiaqi Han, Zhijie Huang, Ji Qi, Yi Zhou\\rCategories: cs.CL cs.AI cs.LG\\rComments: 33 pages, 5 figures\\r\\\\\\\\\\r  Large language models (LLMs) are displaying emergent abilities for math\\rreasoning tasks,and there is a growing attention on enhancing the ability of\\ropen-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to\\rexplore a general data strategy for supervised data to help optimize and expand\\rmath reasoning ability.Firstly, we determine the ability boundary of reasoning\\rpaths augmentation by identifying these paths' minimal optimal set.Secondly, we\\rvalidate that different abilities of the model can be cumulatively enhanced by\\rMix of Minimal Optimal Sets of corresponding types of data, while our models\\rMMOS achieve SOTA performance on series base models under much lower\\rconstruction costs.Besides, we point out GSM-HARD is not really hard and\\rtoday's LLMs no longer lack numerical robustness.Also, we provide an Auto\\rProblem Generator for robustness testing and educational applications.Our code\\rand data are publicly available at https://github.com/cyzhh/MMOS.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00799 ,  621kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00800\\rDate: Fri, 23 Feb 2024 17:40:31 GMT   (201kb,D)\\r\\rTitle: Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by\\r  Imitating Human Thought Processes\\rAuthors: Yezeng Chen, Zui Chen, Yi Zhou\\rCategories: cs.CL cs.AI cs.LG\\rComments: 12 pages, 5 figures\\r\\\\\\\\\\r  Although large language models demonstrate emergent abilities in solving math\\rword problems, there is a challenging task in complex multi-step mathematical\\rreasoning tasks. To improve model performance on mathematical reasoning tasks,\\rprevious work has conducted supervised fine-tuning on open-source models by\\rimproving the quality and quantity of data. In this paper, we propose a novel\\rapproach, named Brain, to imitate human thought processes to enhance\\rmathematical reasoning abilities, using the Frontal Lobe Model to generate\\rplans, and then employing the Parietal Lobe Model to generate code and execute\\rto obtain answers. First, we achieve SOTA performance in comparison with Code\\rLLaMA 7B based models through this method. Secondly, we find that plans can be\\rexplicitly extracted from natural language, code, or formal language. Our code\\rand data are publicly available at https://github.com/cyzhh/Brain.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00800 ,  201kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00804\\rDate: Sat, 24 Feb 2024 00:15:09 GMT   (1684kb,D)\\r\\rTitle: Uncovering Customer Issues through Topological Natural Language Analysis\\rAuthors: Shu-Ting Pi, Sidarth Srinivasan, Yuying Zhu, Michael Yang, Qun Liu\\rCategories: cs.CL cs.AI cs.LG\\rComments: Accepted in KDD 2023 Workshop on Decision Intelligence and Analytics\\r  for Online Marketplaces\\r\\\\\\\\\\r  E-commerce companies deal with a high volume of customer service requests\\rdaily. While a simple annotation system is often used to summarize the topics\\rof customer contacts, thoroughly exploring each specific issue can be\\rchallenging. This presents a critical concern, especially during an emerging\\routbreak where companies must quickly identify and address specific issues. To\\rtackle this challenge, we propose a novel machine learning algorithm that\\rleverages natural language techniques and topological data analysis to monitor\\remerging and trending customer issues. Our approach involves an end-to-end deep\\rlearning framework that simultaneously tags the primary question sentence of\\reach customer's transcript and generates sentence embedding vectors. We then\\rwhiten the embedding vectors and use them to construct an undirected graph.\\r>From there, we define trending and emerging issues based on the topological\\rproperties of each transcript. We have validated our results through various\\rmethods and found that they are highly consistent with news sources.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00804 ,  1684kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00808\\rDate: Sat, 24 Feb 2024 14:18:11 GMT   (657kb,D)\\r\\rTitle: IPED: An Implicit Perspective for Relational Triple Extraction based on\\r  Diffusion Model\\rAuthors: Jianli Zhao, Changhao Xu, Bin Jiang\\rCategories: cs.CL cs.AI\\rComments: 12 pages, 4 figures, committed to NAACL 2024\\r\\\\\\\\\\r  Relational triple extraction is a fundamental task in the field of\\rinformation extraction, and a promising framework based on table filling has\\rrecently gained attention as a potential baseline for entity relation\\rextraction. However, inherent shortcomings such as redundant information and\\rincomplete triple recognition remain problematic. To address these challenges,\\rwe propose an Implicit Perspective for relational triple Extraction based on\\rDiffusion model (IPED), an innovative approach for extracting relational\\rtriples. Our classifier-free solution adopts an implicit strategy using block\\rcoverage to complete the tables, avoiding the limitations of explicit tagging\\rmethods. Additionally, we introduce a generative model structure, the\\rblock-denoising diffusion model, to collaborate with our implicit perspective\\rand effectively circumvent redundant information disruptions. Experimental\\rresults on two popular datasets demonstrate that IPED achieves state-of-the-art\\rperformance while gaining superior inference speed and low computational\\rcomplexity. To support future research, we have made our source code publicly\\ravailable online.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00808 ,  657kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00809\\rDate: Sat, 24 Feb 2024 20:00:03 GMT   (329kb,D)\\r\\rTitle: Abdelhak at SemEval-2024 Task 9 : Decoding Brainteasers, The Efficacy of\\r  Dedicated Models Versus ChatGPT\\rAuthors: Abdelhak Kelious, Mounir Okirim\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  This study introduces a dedicated model aimed at solving the BRAINTEASER task\\r9 , a novel challenge designed to assess models lateral thinking capabilities\\rthrough sentence and word puzzles. Our model demonstrates remarkable efficacy,\\rsecuring Rank 1 in sentence puzzle solving during the test phase with an\\roverall score of 0.98. Additionally, we explore the comparative performance of\\rChatGPT, specifically analyzing how variations in temperature settings affect\\rits ability to engage in lateral thinking and problem-solving. Our findings\\rindicate a notable performance disparity between the dedicated model and\\rChatGPT, underscoring the potential of specialized approaches in enhancing\\rcreative reasoning in AI.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00809 ,  329kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00812\\rDate: Sun, 25 Feb 2024 07:09:10 GMT   (10044kb,D)\\r\\rTitle: LoRA Meets Dropout under a Unified Framework\\rAuthors: Sheng Wang, Liheng Chen, Jiyue Jiang, Boyang Xue, Lingpeng Kong, Chuan\\r  Wu\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  With the remarkable capabilities, large language models (LLMs) have emerged\\ras essential elements in numerous NLP applications, while parameter-efficient\\rfinetuning, especially LoRA, has gained popularity as a lightweight approach\\rfor model customization. Meanwhile, various dropout methods, initially designed\\rfor full finetuning with all the parameters updated, alleviates overfitting\\rassociated with excessive parameter redundancy. Hence, a possible contradiction\\rarises from negligible trainable parameters of LoRA and the effectiveness of\\rprevious dropout methods, which has been largely overlooked. To fill this gap,\\rwe first confirm that parameter-efficient LoRA is also overfitting-prone. We\\rthen revisit transformer-specific dropout methods, and establish their\\requivalence and distinctions mathematically and empirically. Building upon this\\rcomparative analysis, we introduce a unified framework for a comprehensive\\rinvestigation, which instantiates these methods based on dropping position,\\rstructural pattern and compensation measure. Through this framework, we reveal\\rthe new preferences and performance comparisons of them when involved with\\rlimited trainable parameters. This framework also allows us to amalgamate the\\rmost favorable aspects into a novel dropout method named HiddenKey. Extensive\\rexperiments verify the remarkable superiority and sufficiency of HiddenKey\\racross multiple models and tasks, which highlights it as the preferred approach\\rfor high-performance and parameter-efficient finetuning of LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00812 ,  10044kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00813\\rDate: Sun, 25 Feb 2024 12:37:29 GMT   (606kb,D)\\r\\rTitle: UrbanGPT: Spatio-Temporal Large Language Models\\rAuthors: Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia,\\r  Dawei Yin, Chao Huang\\rCategories: cs.CL cs.AI cs.CY\\rComments: 11 pages\\r\\\\\\\\\\r  Spatio-temporal prediction aims to forecast and gain insights into the\\rever-changing dynamics of urban environments across both time and space. Its\\rpurpose is to anticipate future patterns, trends, and events in diverse facets\\rof urban life, including transportation, population movement, and crime rates.\\rAlthough numerous efforts have been dedicated to developing neural network\\rtechniques for accurate predictions on spatio-temporal data, it is important to\\rnote that many of these methods heavily depend on having sufficient labeled\\rdata to generate precise spatio-temporal representations. Unfortunately, the\\rissue of data scarcity is pervasive in practical urban sensing scenarios.\\rConsequently, it becomes necessary to build a spatio-temporal model with strong\\rgeneralization capabilities across diverse spatio-temporal learning scenarios.\\rTaking inspiration from the remarkable achievements of large language models\\r(LLMs), our objective is to create a spatio-temporal LLM that can exhibit\\rexceptional generalization capabilities across a wide range of downstream urban\\rtasks. To achieve this objective, we present the UrbanGPT, which seamlessly\\rintegrates a spatio-temporal dependency encoder with the instruction-tuning\\rparadigm. This integration enables LLMs to comprehend the complex\\rinter-dependencies across time and space, facilitating more comprehensive and\\raccurate predictions under data scarcity. To validate the effectiveness of our\\rapproach, we conduct extensive experiments on various public datasets, covering\\rdifferent spatio-temporal prediction tasks. The results consistently\\rdemonstrate that our UrbanGPT, with its carefully designed architecture,\\rconsistently outperforms state-of-the-art baselines. These findings highlight\\rthe potential of building large language models for spatio-temporal learning,\\rparticularly in zero-shot scenarios where labeled data is scarce.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00813 ,  606kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00815\\rDate: Sun, 25 Feb 2024 23:10:20 GMT   (8166kb,D)\\r\\rTitle: RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic\\r  Health Records\\rAuthors: Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Bowen Jin, May D. Wang,\\r  Joyce C. Ho, Carl Yang\\rCategories: cs.CL cs.AI cs.IR q-bio.OT\\r\\\\\\\\\\r  We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical\\rpredictions on Electronic Health Records (EHRs). RAM-EHR first collects\\rmultiple knowledge sources, converts them into text format, and uses dense\\rretrieval to obtain information related to medical concepts. This strategy\\raddresses the difficulties associated with complex names for the concepts.\\rRAM-EHR then augments the local EHR predictive model co-trained with\\rconsistency regularization to capture complementary information from patient\\rvisits and summarized knowledge. Experiments on two EHR datasets show the\\refficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in\\rAUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized\\rknowledge from RAM-EHR for clinical prediction tasks. The code will be\\rpublished at \\\\url{https://github.com/ritaranx/RAM-EHR}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00815 ,  8166kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00818\\rDate: Mon, 26 Feb 2024 09:21:59 GMT   (304kb,D)\\r\\rTitle: DenseMamba: State Space Models with Dense Hidden Connection for\\r  Efficient Large Language Models\\rAuthors: Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo,\\r  Yunhe Wang\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  Large language models (LLMs) face a daunting challenge due to the excessive\\rcomputational and memory requirements of the commonly used Transformer\\rarchitecture. While state space model (SSM) is a new type of foundational\\rnetwork architecture offering lower computational complexity, their performance\\rhas yet to fully rival that of Transformers. This paper introduces DenseSSM, a\\rnovel approach to enhance the flow of hidden information between layers in\\rSSMs. By selectively integrating shallowlayer hidden states into deeper layers,\\rDenseSSM retains fine-grained information crucial for the final output. Dense\\rconnections enhanced DenseSSM still maintains the training parallelizability\\rand inference efficiency. The proposed method can be widely applicable to\\rvarious SSM types like RetNet and Mamba. With similar model size, DenseSSM\\rachieves significant improvements, exemplified by DenseRetNet outperforming the\\roriginal RetNet with up to 5% accuracy improvement on public benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00818 ,  304kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00821\\rDate: Mon, 26 Feb 2024 16:17:19 GMT   (415kb,D)\\r\\rTitle: Social Media as a Sensor: Analyzing Twitter Data for Breast Cancer\\r  Medication Effects Using Natural Language Processing\\rAuthors: Seibi Kobara, Alireza Rafiei, Masoud Nateghi, Selen Bozkurt,\\r  Rishikesan Kamaleswaran, Abeed Sarker\\rCategories: cs.CL cs.LG cs.SI\\r\\\\\\\\\\r  Breast cancer is a significant public health concern and is the leading cause\\rof cancer-related deaths among women. Despite advances in breast cancer\\rtreatments, medication non-adherence remains a major problem. As electronic\\rhealth records do not typically capture patient-reported outcomes that may\\rreveal information about medication-related experiences, social media presents\\ran attractive resource for enhancing our understanding of the patients'\\rtreatment experiences. In this paper, we developed natural language processing\\r(NLP) based methodologies to study information posted by an automatically\\rcurated breast cancer cohort from social media. We employed a transformer-based\\rclassifier to identify breast cancer patients/survivors on X (Twitter) based on\\rtheir self-reported information, and we collected longitudinal data from their\\rprofiles. We then designed a multi-layer rule-based model to develop a breast\\rcancer therapy-associated side effect lexicon and detect patterns of medication\\rusage and associated side effects among breast cancer patients. 1,454,637 posts\\rwere available from 583,962 unique users, of which 62,042 were detected as\\rbreast cancer members using our transformer-based model. 198 cohort members\\rmentioned breast cancer medications with tamoxifen as the most common. Our side\\reffect lexicon identified well-known side effects of hormone and chemotherapy.\\rFurthermore, it discovered a subject feeling towards cancer and medications,\\rwhich may suggest a pre-clinical phase of side effects or emotional distress.\\rThis analysis highlighted not only the utility of NLP techniques in\\runstructured social media data to identify self-reported breast cancer posts,\\rmedication usage patterns, and treatment side effects but also the richness of\\rsocial data on such clinical questions.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00821 ,  415kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00824\\rDate: Tue, 27 Feb 2024 00:24:42 GMT   (30641kb,D)\\r\\rTitle: Information Flow Routes: Automatically Interpreting Language Models at\\r  Scale\\rAuthors: Javier Ferrando and Elena Voita\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Information flows by routes inside the network via mechanisms implemented in\\rthe model. These routes can be represented as graphs where nodes correspond to\\rtoken representations and edges to operations inside the network. We\\rautomatically build these graphs in a top-down manner, for each prediction\\rleaving only the most important nodes and edges. In contrast to the existing\\rworkflows relying on activation patching, we do this through attribution: this\\rallows us to efficiently uncover existing circuits with just a single forward\\rpass. Additionally, the applicability of our method is far beyond patching: we\\rdo not need a human to carefully design prediction templates, and we can\\rextract information flow routes for any prediction (not just the ones among the\\rallowed templates). As a result, we can talk about model behavior in general,\\rfor specific types of predictions, or different domains. We experiment with\\rLlama 2 and show that the role of some attention heads is overall important,\\re.g. previous token heads and subword merging heads. Next, we find similarities\\rin Llama 2 behavior when handling tokens of the same part of speech. Finally,\\rwe show that some model components can be specialized on domains such as coding\\ror multilingual texts.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00824 ,  30641kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00825\\rDate: Tue, 27 Feb 2024 07:26:16 GMT   (58kb)\\r\\rTitle: Comparing effectiveness of regularization methods on text\\r  classification: Simple and complex model in data shortage situation\\rAuthors: Jongga Lee, Jaeseung Yim, Seohee Park, Changwon Lim\\rCategories: cs.CL\\rComments: 13 pages, 2 figures\\r\\\\\\\\\\r  Text classification is the task of assigning a document to a predefined\\rclass. However, it is expensive to acquire enough labeled documents or to label\\rthem. In this paper, we study the regularization methods' effects on various\\rclassification models when only a few labeled data are available. We compare a\\rsimple word embedding-based model, which is simple but effective, with complex\\rmodels (CNN and BiLSTM). In supervised learning, adversarial training can\\rfurther regularize the model. When an unlabeled dataset is available, we can\\rregularize the model using semi-supervised learning methods such as the Pi\\rmodel and virtual adversarial training. We evaluate the regularization effects\\ron four text classification datasets (AG news, DBpedia, Yahoo! Answers, Yelp\\rPolarity), using only 0.1% to 0.5% of the original labeled training documents.\\rThe simple model performs relatively well in fully supervised learning, but\\rwith the help of adversarial training and semi-supervised learning, both simple\\rand complex models can be regularized, showing better results for complex\\rmodels. Although the simple model is robust to overfitting, a complex model\\rwith well-designed prior beliefs can be also robust to overfitting.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00825 ,  58kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00826\\rDate: Tue, 27 Feb 2024 10:22:45 GMT   (2114kb,D)\\r\\rTitle: LLMGuard: Guarding Against Unsafe LLM Behavior\\rAuthors: Shubh Goyal, Medha Hira, Shubham Mishra, Sukriti Goyal, Arnav Goel,\\r  Niharika Dadu, Kirushikesh DB, Sameep Mehta, Nishtha Madaan\\rCategories: cs.CL cs.CR cs.LG\\rComments: accepted in demonstration track of AAAI-24\\r\\\\\\\\\\r  Although the rise of Large Language Models (LLMs) in enterprise settings\\rbrings new opportunities and capabilities, it also brings challenges, such as\\rthe risk of generating inappropriate, biased, or misleading content that\\rviolates regulations and can have legal concerns. To alleviate this, we present\\rLLMGuard, a tool that monitors user interactions with an LLM application and\\rflags content against specific behaviours or conversation topics. To do this\\rrobustly, LLMGuard employs an ensemble of detectors.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00826 ,  2114kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00827\\rDate: Tue, 27 Feb 2024 19:13:01 GMT   (471kb,D)\\r\\rTitle: Self-Refinement of Language Models from External Proxy Metrics Feedback\\rAuthors: Keshav Ramji, Young-Suk Lee, Ram\\\\'on Fernandez Astudillo, Md Arafat\\r  Sultan, Tahira Naseem, Asim Munawar, Radu Florian, Salim Roukos\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  It is often desirable for Large Language Models (LLMs) to capture multiple\\robjectives when providing a response. In document-grounded response generation,\\rfor example, agent responses are expected to be relevant to a user's query\\rwhile also being grounded in a given document. In this paper, we introduce\\rProxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine\\rits own initial response along key dimensions of quality guided by external\\rmetrics feedback, yielding an overall better final response. ProMiSe leverages\\rfeedback on response quality through principle-specific proxy metrics, and\\riteratively refines its response one principle at a time. We apply ProMiSe to\\ropen source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its\\rperformance on document-grounded question answering datasets, MultiDoc2Dial and\\rQuAC, demonstrating that self-refinement improves response quality. We further\\rshow that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated\\rby ProMiSe yields significant performance improvements over the zero-shot\\rbaseline as well as a supervised fine-tuned model on human annotated data.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00827 ,  471kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00828\\rDate: Tue, 27 Feb 2024 19:16:39 GMT   (373kb,D)\\r\\rTitle: Deep Learning Detection Method for Large Language Models-Generated\\r  Scientific Content\\rAuthors: Bushra Alhijawi, Rawan Jarrar, Aseel AbuAlRub, and Arwa Bader\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual\\rcontent is written and communicated. These models have the potential to\\rgenerate scientific content that is indistinguishable from that written by\\rhumans. Hence, LLMs carry severe consequences for the scientific community,\\rwhich relies on the integrity and reliability of publications. This research\\rpaper presents a novel ChatGPT-generated scientific text detection method,\\rAI-Catcher. AI-Catcher integrates two deep learning models, multilayer\\rperceptron (MLP) and convolutional neural networks (CNN). The MLP learns the\\rfeature representations of the linguistic and statistical features. The CNN\\rextracts high-level representations of the sequential patterns from the textual\\rcontent. AI-Catcher is a multimodal model that fuses hidden patterns derived\\rfrom MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset\\ris collected to enhance AI-generated text detection tools, AIGTxt. AIGTxt\\rcontains 3000 records collected from published academic articles across ten\\rdomains and divided into three classes: Human-written, ChatGPT-generated, and\\rMixed text. Several experiments are conducted to evaluate the performance of\\rAI-Catcher. The comparative results demonstrate the capability of AI-Catcher to\\rdistinguish between human-written and ChatGPT-generated scientific text more\\raccurately than alternative methods. On average, AI-Catcher improved accuracy\\rby 37.4%.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00828 ,  373kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00835\\rDate: Wed, 28 Feb 2024 20:17:04 GMT   (1239kb,D)\\r\\rTitle: CLLMs: Consistency Large Language Models\\rAuthors: Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, Hao Zhang\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Parallel decoding methods such as Jacobi decoding show promise for more\\refficient LLM inference as it breaks the sequential nature of the LLM decoding\\rprocess and transforms it into parallelizable computation. However, in\\rpractice, it achieves little speedup compared to traditional autoregressive\\r(AR) decoding, primarily because Jacobi decoding seldom accurately predicts\\rmore than one token in a single fixed-point iteration step. To address this, we\\rdevelop a new approach aimed at realizing fast convergence from any state to\\rthe fixed point on a Jacobi trajectory. This is accomplished by refining the\\rtarget LLM to consistently predict the fixed point given any state as input.\\rExtensive experiments demonstrate the effectiveness of our method, showing\\r2.4$\\\\times$ to 3.4$\\\\times$ improvements in generation speed while preserving\\rgeneration quality across both domain-specific and open-domain benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00835 ,  1239kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00840\\rDate: Thu, 29 Feb 2024 09:35:41 GMT   (1597kb)\\r\\rTitle: EyeGPT: Ophthalmic Assistant with Large Language Models\\rAuthors: Xiaolan Chen, Ziwei Zhao, Weiyi Zhang, Pusheng Xu, Le Gao, Mingpu Xu,\\r  Yue Wu, Yinwen Li, Danli Shi, Mingguang He\\rCategories: cs.CL cs.AI\\rComments: 47 pages, 4 figures, 1 table, 2 supplementary figures and 9\\r  supplementary tables\\r\\\\\\\\\\r  Artificial intelligence (AI) has gained significant attention in healthcare\\rconsultation due to its potential to improve clinical workflow and enhance\\rmedical communication. However, owing to the complex nature of medical\\rinformation, large language models (LLM) trained with general world knowledge\\rmight not possess the capability to tackle medical-related tasks at an expert\\rlevel. Here, we introduce EyeGPT, a specialized LLM designed specifically for\\rophthalmology, using three optimization strategies including role-playing,\\rfinetuning, and retrieval-augmented generation. In particular, we proposed a\\rcomprehensive evaluation framework that encompasses a diverse dataset, covering\\rvarious subspecialties of ophthalmology, different users, and diverse inquiry\\rintents. Moreover, we considered multiple evaluation metrics, including\\raccuracy, understandability, trustworthiness, empathy, and the proportion of\\rhallucinations. By assessing the performance of different EyeGPT variants, we\\ridentify the most effective one, which exhibits comparable levels of\\runderstandability, trustworthiness, and empathy to human ophthalmologists (all\\rPs>0.05). Overall, ur study provides valuable insights for future research,\\rfacilitating comprehensive comparisons and evaluations of different strategies\\rfor developing specialized LLMs in ophthalmology. The potential benefits\\rinclude enhancing the patient experience in eye care and optimizing\\rophthalmologists' services.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00840 ,  1597kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00862\\rDate: Thu, 29 Feb 2024 21:05:14 GMT   (165kb,D)\\r\\rTitle: NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and\\r  Safety Adherence in Chinese Journalistic Editorial Applications\\rAuthors: Miao Li and Ming-Bin Chen and Bo Tang and Shengbin Hou and Pengyu Wang\\r  and Haiying Deng and Zhiyu Li and Feiyu Xiong and Keming Mao and Peng Cheng\\r  and Yi Luo\\rCategories: cs.CL cs.AI\\rComments: 27 pages\\r\\\\\\\\\\r  This study presents NewsBench, a novel benchmark framework developed to\\revaluate the capability of Large Language Models (LLMs) in Chinese Journalistic\\rWriting Proficiency (JWP) and their Safety Adherence (SA), addressing the gap\\rbetween journalistic ethics and the risks associated with AI utilization.\\rComprising 1,267 tasks across 5 editorial applications, 7 aspects (including\\rsafety and journalistic writing with 4 detailed facets), and spanning 24 news\\rtopics domains, NewsBench employs two GPT-4 based automatic evaluation\\rprotocols validated by human assessment. Our comprehensive analysis of 11 LLMs\\rhighlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative\\rdeficiency in journalistic ethic adherence during creative writing tasks. These\\rfindings underscore the need for enhanced ethical guidance in AI-generated\\rjournalistic content, marking a step forward in aligning AI capabilities with\\rjournalistic standards and safety considerations.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00862 ,  165kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00868\\rDate: Fri, 1 Mar 2024 04:39:16 GMT   (2118kb,D)\\r\\rTitle: SoftTiger: A Clinical Foundation Model for Healthcare Workflows\\rAuthors: Ye Chen, Igor Couto, Wei Cai, Cong Fu, Bruno Dorneles\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  We release and introduce SoftTiger, a clinical large language model (CLaM)\\rdesigned as a foundation model for healthcare workflows. The narrative and\\runstructured nature of clinical notes is a major obstacle for healthcare\\rintelligentization. We address a critical problem of structuring clinical notes\\rinto clinical data, according to international interoperability standards. We\\rcollect and annotate data for three critical subtasks, namely, international\\rpatient summary, clinical impression and medical encounter. We then supervised\\rfine-tuned a state-of-the-art LLM using public and credentialed clinical data.\\rThe training is orchestrated in a way that the target model can first support\\rbasic clinical tasks such as abbreviation expansion and temporal information\\rextraction, and then learn to perform more complex downstream clinical tasks\\rsuch as impression and encounter summary. Moreover, we address, several\\rmodeling challenges in the healthcare context, e.g., extra long context window.\\rOur blind pairwise evaluation shows that SoftTiger outperforms other popular\\ropen-source models and GPT-3.5, comparable to Gemini-pro, and only has a mild\\rgap from GPT-4. We believe that LLMs may become a step-stone towards healthcare\\rdigitalization and democratization. Therefore, we publicly release SoftTiger\\rmodels at scales of 13 billion and 70 billion parameters, as well as datasets\\rand code for our innovative scalable evaluation, hopefully, making a\\rsignificant contribution to the healthcare industry.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00868 ,  2118kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00876\\rDate: Fri, 1 Mar 2024 08:13:48 GMT   (6956kb,D)\\r\\rTitle: Word Order and World Knowledge\\rAuthors: Qinghua Zhao, Vinit Ravishankar, Nicolas Garneau and Anders S{\\\\o}gaard\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Word order is an important concept in natural language, and in this work, we\\rstudy how word order affects the induction of world knowledge from raw text\\rusing language models. We use word analogies to probe for such knowledge.\\rSpecifically, in addition to the natural word order, we first respectively\\rextract texts of six fixed word orders from five languages and then pretrain\\rthe language models on these texts. Finally, we analyze the experimental\\rresults of the fixed word orders on word analogies and show that i) certain\\rfixed word orders consistently outperform or underperform others, though the\\rspecifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in\\rpre-trained language models, and the natural word order typically yields\\rmediocre results. The source code will be made publicly available at\\rhttps://github.com/lshowway/probing_by_analogy.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00876 ,  6956kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00888\\rDate: Fri, 1 Mar 2024 11:54:14 GMT   (74kb,D)\\r\\rTitle: Margin Discrepancy-based Adversarial Training for Multi-Domain Text\\r  Classification\\rAuthors: Yuan Wu\\rCategories: cs.CL cs.LG\\rComments: 16 pages\\r\\\\\\\\\\r  Multi-domain text classification (MDTC) endeavors to harness available\\rresources from correlated domains to enhance the classification accuracy of the\\rtarget domain. Presently, most MDTC approaches that embrace adversarial\\rtraining and the shared-private paradigm exhibit cutting-edge performance.\\rUnfortunately, these methods face a non-negligible challenge: the absence of\\rtheoretical guarantees in the design of MDTC algorithms. The dearth of\\rtheoretical underpinning poses a substantial impediment to the advancement of\\rMDTC algorithms. To tackle this problem, we first provide a theoretical\\ranalysis of MDTC by decomposing the MDTC task into multiple domain adaptation\\rtasks. We incorporate the margin discrepancy as the measure of domain\\rdivergence and establish a new generalization bound based on Rademacher\\rcomplexity. Subsequently, we propose a margin discrepancy-based adversarial\\rtraining (MDAT) approach for MDTC, in accordance with our theoretical analysis.\\rTo validate the efficacy of the proposed MDAT method, we conduct empirical\\rstudies on two MDTC benchmarks. The experimental results demonstrate that our\\rMDAT approach surpasses state-of-the-art baselines on both datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00888 ,  74kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00896\\rDate: Fri, 1 Mar 2024 15:38:55 GMT   (440kb,D)\\r\\rTitle: DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large\\r  Language Models\\rAuthors: Kedi Chen and Qin Chen and Jie Zhou and Yishen He and Liang He\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Since large language models (LLMs) achieve significant success in recent\\ryears, the hallucination issue remains a challenge, numerous benchmarks are\\rproposed to detect the hallucination. Nevertheless, some of these benchmarks\\rare not naturally generated by LLMs but are intentionally induced. Also, many\\rmerely focus on the factuality hallucination while ignoring the faithfulness\\rhallucination. Additionally, although dialogue pattern is more widely utilized\\rin the era of LLMs, current benchmarks only concentrate on sentence-level and\\rpassage-level hallucination. In this study, we propose DiaHalu, the first\\rdialogue-level hallucination evaluation benchmark to our knowledge. Initially,\\rwe integrate the collected topics into system prompts and facilitate a dialogue\\rbetween two ChatGPT3.5. Subsequently, we manually modify the contents that do\\rnot adhere to human language conventions and then have LLMs re-generate,\\rsimulating authentic human-machine interaction scenarios. Finally, professional\\rscholars annotate all the samples in the dataset. DiaHalu covers four common\\rmulti-turn dialogue domains and five hallucination subtypes, extended from\\rfactuality and faithfulness hallucination. Experiments through some well-known\\rLLMs and detection methods on the dataset show that DiaHalu is a challenging\\rbenchmark, holding significant value for further research.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00896 ,  440kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00952\\rDate: Fri, 1 Mar 2024 20:03:44 GMT   (443kb,D)\\r\\rTitle: MediSwift: Efficient Sparse Pre-trained Biomedical Language Models\\rAuthors: Vithursan Thangarasa, Mahmoud Salem, Shreyas Saxena, Kevin Leong, Joel\\r  Hestness, Sean Lie\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  Large language models (LLMs) are typically trained on general source data for\\rvarious domains, but a recent surge in domain-specific LLMs has shown their\\rpotential to outperform general-purpose models in domain-specific tasks (e.g.,\\rbiomedicine). Although domain-specific pre-training enhances efficiency and\\rleads to smaller models, the computational costs of training these LLMs remain\\rhigh, posing budgeting challenges. We introduce MediSwift, a suite of\\rbiomedical LMs that leverage sparse pre-training on domain-specific biomedical\\rtext data. By inducing up to 75% weight sparsity during the pre-training phase,\\rMediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse\\rpre-training was performed on the Cerebras CS-2 system, which is specifically\\rdesigned to realize the acceleration benefits from unstructured weight\\rsparsity, thereby significantly enhancing the efficiency of the MediSwift\\rmodels. Through subsequent dense fine-tuning and strategic soft prompting,\\rMediSwift models outperform existing LLMs up to 7B parameters on biomedical\\rtasks, setting new benchmarks w.r.t efficiency-accuracy on tasks such as\\rPubMedQA. Our results show that sparse pre-training, along with dense\\rfine-tuning and soft prompting, offers an effective method for creating\\rhigh-performing, computationally efficient models in specialized domains.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00952 ,  443kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00953\\rDate: Fri, 1 Mar 2024 20:06:39 GMT   (910kb)\\r\\rTitle: AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\\r  Graph Construction Based on Ontologies-enhanced Large Language Models\\rAuthors: Lang Cao, Jimeng Sun, Adam Cross\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Objectives: Our objective is to create an end-to-end system called AutoRD,\\rwhich automates extracting information from clinical text about rare diseases.\\rWe have conducted various tests to evaluate the performance of AutoRD and\\rhighlighted its strengths and limitations in this paper.\\r  Materials and Methods: Our system, AutoRD, is a software pipeline involving\\rdata preprocessing, entity extraction, relation extraction, entity calibration,\\rand knowledge graph construction. We implement this using large language models\\rand medical knowledge graphs developed from open-source medical ontologies. We\\rquantitatively evaluate our system on entity extraction, relation extraction,\\rand the performance of knowledge graph construction.\\r  Results: AutoRD achieves an overall F1 score of 47.3%, a 14.4% improvement\\rcompared to the base LLM. In detail, AutoRD achieves an overall entity\\rextraction F1 score of 56.1% (rare_disease: 83.5%, disease: 35.8%,\\rsymptom_and_sign: 46.1%, anaphor: 67.5%) and an overall relation extraction F1\\rscore of 38.6% (produces: 34.7%, increases_risk_of: 12.4%, is_a: 37.4%,\\ris_acronym: 44.1%, is_synonym: 16.3%, anaphora: 57.5%). Our qualitative\\rexperiment also demonstrates that the performance in constructing the knowledge\\rgraph is commendable.\\r  Discussion: AutoRD demonstrates the potential of LLM applications in rare\\rdisease detection. This improvement is attributed to several design, including\\rthe integration of ontologies-enhanced LLMs.\\r  Conclusion: AutoRD is an automated end-to-end system for extracting rare\\rdisease information from text to build knowledge graphs. It uses\\rontologies-enhanced LLMs for a robust medical knowledge base. The superior\\rperformance of AutoRD is validated by experimental evaluations, demonstrating\\rthe potential of LLMs in healthcare.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00953 ,  910kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00964\\rDate: Fri, 1 Mar 2024 20:31:10 GMT   (58kb,D)\\r\\rTitle: MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM\\r  Hallucination Detection\\rAuthors: Federico Borra, Claudio Savelli, Giacomo Rosso, Alkis Koudounas,\\r  Flavio Giobergia\\rCategories: cs.CL cs.LG\\rComments: Under revision at SemEval 2024\\r\\\\\\\\\\r  In Natural Language Generation (NLG), contemporary Large Language Models\\r(LLMs) face several challenges, such as generating fluent yet inaccurate\\routputs and reliance on fluency-centric metrics. This often leads to neural\\rnetworks exhibiting hallucinations. The SHROOM challenge focuses on\\rautomatically identifying these hallucinations in the generated text. To tackle\\rthese issues, we introduce two key components, a data augmentation pipeline\\rincorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a\\rvoting ensemble from three models pre-trained on Natural Language Inference\\r(NLI) tasks and fine-tuned on diverse datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00964 ,  58kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00982\\rDate: Fri, 1 Mar 2024 21:10:20 GMT   (770kb,D)\\r\\rTitle: LocalRQA: From Generating Data to Locally Training, Testing, and\\r  Deploying Retrieval-Augmented QA Systems\\rAuthors: Xiao Yu, Yunan Lu, Zhou Yu\\rCategories: cs.CL\\r\\\\\\\\\\r  Retrieval-augmented question-answering systems combine retrieval techniques\\rwith large language models to provide answers that are more accurate and\\rinformative. Many existing toolkits allow users to quickly build such systems\\rusing off-the-shelf models, but they fall short in supporting researchers and\\rdevelopers to customize the model training, testing, and deployment process. We\\rpropose LocalRQA, an open-source toolkit that features a wide selection of\\rmodel training algorithms, evaluation methods, and deployment tools curated\\rfrom the latest research. As a showcase, we build QA systems using online\\rdocumentation obtained from Databricks and Faire's websites. We find 7B-models\\rtrained and deployed using LocalRQA reach a similar performance compared to\\rusing OpenAI's text-ada-002 and GPT-4-turbo.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00982 ,  770kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00986\\rDate: Fri, 1 Mar 2024 21:16:29 GMT   (1340kb,D)\\r\\rTitle: Merging Text Transformer Models from Different Initializations\\rAuthors: Neha Verma, Maha Elbayad\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  Recent work on one-shot permutation-based model merging has shown impressive\\rlow- or zero-barrier mode connectivity between models from completely different\\rinitializations. However, this line of work has not yet extended to the\\rTransformer architecture, despite its dominant popularity in the language\\rdomain. Therefore, in this work, we investigate the extent to which separate\\rTransformer minima learn similar features, and propose a model merging\\rtechnique to investigate the relationship between these minima in the loss\\rlandscape. The specifics of the architecture, like its residual connections,\\rmulti-headed attention, and discrete, sequential input, require specific\\rinterventions in order to compute model permutations that remain within the\\rsame functional equivalence class. In merging these models with our method, we\\rconsistently find lower loss barriers between minima compared to model\\raveraging for several models trained on a masked-language modeling task or\\rfine-tuned on a language understanding benchmark. Our results show that the\\rminima of these models are less sharp and isolated than previously understood,\\rand provide a basis for future work on merging separately trained Transformer\\rmodels.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00986 ,  1340kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00990\\rDate: Fri, 1 Mar 2024 21:24:24 GMT   (8388kb,D)\\r\\rTitle: Formulation Comparison for Timeline Construction using LLMs\\rAuthors: Kimihiro Hasegawa, Nikhil Kandukuri, Susan Holm, Yukari Yamakawa,\\r  Teruko Mitamura\\rCategories: cs.CL\\r\\\\\\\\\\r  Constructing a timeline requires identifying the chronological order of\\revents in an article. In prior timeline construction datasets, temporal orders\\rare typically annotated by either event-to-time anchoring or event-to-event\\rpairwise ordering, both of which suffer from missing temporal information. To\\rmitigate the issue, we develop a new evaluation dataset, TimeSET, consisting of\\rsingle-document timelines with document-level order annotation. TimeSET\\rfeatures saliency-based event selection and partial ordering, which enable a\\rpractical annotation workload. Aiming to build better automatic timeline\\rconstruction systems, we propose a novel evaluation framework to compare\\rmultiple task formulations with TimeSET by prompting open LLMs, i.e., Llama 2\\rand Flan-T5. Considering that identifying temporal orders of events is a core\\rsubtask in timeline construction, we further benchmark open LLMs on existing\\revent temporal ordering datasets to gain a robust understanding of their\\rcapabilities. Our experiments show that (1) NLI formulation with Flan-T5\\rdemonstrates a strong performance among others, while (2) timeline construction\\rand event temporal ordering are still challenging tasks for few-shot LLMs. Our\\rcode and data are available at https://github.com/kimihiroh/timeset.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00990 ,  8388kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00998\\rDate: Fri, 1 Mar 2024 21:48:08 GMT   (833kb,D)\\r\\rTitle: Predictions from language models for multiple-choice tasks are not\\r  robust under variation of scoring methods\\rAuthors: Polina Tsvilodub, Hening Wang, Sharon Grosch and Michael Franke\\rCategories: cs.CL\\rComments: 8 pages, 3 figures\\r\\\\\\\\\\r  This paper systematically compares different methods of deriving item-level\\rpredictions of language models for multiple-choice tasks. It compares scoring\\rmethods for answer options based on free generation of responses, various\\rprobability-based scores, a Likert-scale style rating method, and embedding\\rsimilarity. In a case study on pragmatic language interpretation, we find that\\rLLM predictions are not robust under variation of method choice, both within a\\rsingle LLM and across different LLMs. As this variability entails pronounced\\rresearcher degrees of freedom in reporting results, knowledge of the\\rvariability is crucial to secure robustness of results and research integrity.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00998 ,  833kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01002\\rDate: Fri, 1 Mar 2024 21:59:03 GMT   (442kb,D)\\r\\rTitle: Attribute Structuring Improves LLM-Based Evaluation of Clinical Text\\r  Summaries\\rAuthors: Zelalem Gero, Chandan Singh, Yiqing Xie, Sheng Zhang, Tristan Naumann,\\r  Jianfeng Gao, Hoifung Poon\\rCategories: cs.CL cs.AI\\rComments: 4 pages\\r\\\\\\\\\\r  Summarizing clinical text is crucial in health decision-support and clinical\\rresearch. Large language models (LLMs) have shown the potential to generate\\raccurate clinical text summaries, but still struggle with issues regarding\\rgrounding and evaluation, especially in safety-critical domains such as health.\\rHolistically evaluating text summaries is challenging because they may contain\\runsubstantiated information. Here, we explore a general mitigation framework\\rusing Attribute Structuring (AS), which structures the summary evaluation\\rprocess. It decomposes the evaluation process into a grounded procedure that\\ruses an LLM for relatively simple structuring and scoring tasks, rather than\\rthe full task of holistic summary evaluation. Experiments show that AS\\rconsistently improves the correspondence between human annotations and\\rautomated metrics in clinical text summarization. Additionally, AS yields\\rinterpretations in the form of a short text span corresponding to each output,\\rwhich enables efficient human auditing, paving the way towards trustworthy\\revaluation of clinical information in resource-constrained scenarios. We\\rrelease our code, prompts, and an open-source benchmark at\\rhttps://github.com/microsoft/attribute-structuring.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01002 ,  442kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01031\\rDate: Fri, 1 Mar 2024 23:38:02 GMT   (46486kb,D)\\r\\rTitle: Peacock: A Family of Arabic Multimodal Large Language Models and\\r  Benchmarks\\rAuthors: Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia,\\r  Abdelrahman Mohamed, Muhammad Abdul-Mageed\\rCategories: cs.CL cs.AI\\rComments: Under Review\\r\\\\\\\\\\r  Multimodal large language models (MLLMs) have proven effective in a wide\\rrange of tasks requiring complex reasoning and linguistic comprehension.\\rHowever, due to a lack of high-quality multimodal resources in languages other\\rthan English, success of MLLMs remains relatively limited to English-based\\rsettings. This poses significant challenges in developing comparable models for\\rother languages, including even those with large speaker populations such as\\rArabic. To alleviate this challenge, we introduce a comprehensive family of\\rArabic MLLMs, dubbed \\\\textit{Peacock}, with strong vision and language\\rcapabilities. Through comprehensive qualitative and quantitative analysis, we\\rdemonstrate the solid performance of our models on various visual reasoning\\rtasks and further show their emerging dialectal potential. Additionally, we\\rintroduce ~\\\\textit{Henna}, a new benchmark specifically designed for assessing\\rMLLMs on aspects related to Arabic culture, setting the first stone for\\rculturally-aware Arabic MLLMs.The GitHub repository for the \\\\textit{Peacock}\\rproject is available at \\\\url{https://github.com/UBC-NLP/peacock}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01031 ,  46486kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01061\\rDate: Sat, 2 Mar 2024 01:52:14 GMT   (2961kb,D)\\r\\rTitle: Reading Subtext: Evaluating Large Language Models on Short Story\\r  Summarization with Writers\\rAuthors: Melanie Subbiah, Sean Zhang, Lydia B. Chilton, Kathleen McKeown\\rCategories: cs.CL\\r\\\\\\\\\\r  We evaluate recent Large language Models (LLMs) on the challenging task of\\rsummarizing short stories, which can be lengthy, and include nuanced subtext or\\rscrambled timelines. Importantly, we work directly with authors to ensure that\\rthe stories have not been shared online (and therefore are unseen by the\\rmodels), and to obtain informed evaluations of summary quality using judgments\\rfrom the authors themselves. Through quantitative and qualitative analysis\\rgrounded in narrative theory, we compare GPT-4, Claude-2.1, and LLama-2-70B. We\\rfind that all three models make faithfulness mistakes in over 50% of summaries\\rand struggle to interpret difficult subtext. However, at their best, the models\\rcan provide thoughtful thematic analysis of stories. We additionally\\rdemonstrate that LLM judgments of summary quality do not match the feedback\\rfrom the writers.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01061 ,  2961kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01063\\rDate: Sat, 2 Mar 2024 02:00:51 GMT   (2015kb,D)\\r\\rTitle: FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based\\r  Sentiment Analysis\\rAuthors: Songhua Yang, Xinke Jiang, Hanjie Zhao, Wenxuan Zeng, Hongde Liu,\\r  Yuxiang Jia\\rCategories: cs.CL\\r\\\\\\\\\\r  Multi-domain aspect-based sentiment analysis (ABSA) seeks to capture\\rfine-grained sentiment across diverse domains. While existing research narrowly\\rfocuses on single-domain applications constrained by methodological limitations\\rand data scarcity, the reality is that sentiment naturally traverses multiple\\rdomains. Although large language models (LLMs) offer a promising solution for\\rABSA, it is difficult to integrate effectively with established techniques,\\rincluding graph-based models and linguistics, because modifying their internal\\rarchitecture is not easy. To alleviate this problem, we propose a novel\\rframework, Feature-aware In-context Learning for Multi-domain ABSA (FaiMA). The\\rcore insight of FaiMA is to utilize in-context learning (ICL) as a\\rfeature-aware mechanism that facilitates adaptive learning in multi-domain ABSA\\rtasks. Specifically, we employ a multi-head graph attention network as a text\\rencoder optimized by heuristic rules for linguistic, domain, and sentiment\\rfeatures. Through contrastive learning, we optimize sentence representations by\\rfocusing on these diverse features. Additionally, we construct an efficient\\rindexing mechanism, allowing FaiMA to stably retrieve highly relevant examples\\racross multiple dimensions for any given input. To evaluate the efficacy of\\rFaiMA, we build the first multi-domain ABSA benchmark dataset. Extensive\\rexperimental results demonstrate that FaiMA achieves significant performance\\rimprovements in multiple domains compared to baselines, increasing F1 by 2.07%\\ron average. Source code and data sets are anonymously available at\\rhttps://github.com/SupritYoung/FaiMA.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01063 ,  2015kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01069\\rDate: Sat, 2 Mar 2024 02:25:55 GMT   (9801kb,D)\\r\\rTitle: LLMCRIT: Teaching Large Language Models to Use Criteria\\rAuthors: Weizhe Yuan and Pengfei Liu and Matthias Gall\\\\'e\\rCategories: cs.CL\\rComments: 8 pages, 4 tables, 3 figures in the main text\\r\\\\\\\\\\r  Humans follow criteria when they execute tasks, and these criteria are\\rdirectly used to assess the quality of task completion. Therefore, having\\rmodels learn to use criteria to provide feedback can help humans or models to\\rperform tasks better. However, existing research in this field tends to\\rconsider only a limited set of criteria or quality assessment aspects. To fill\\rthis gap, we propose a general framework that enables large language models\\r(LLMs) to use comprehensive criteria for a task in delivering natural language\\rfeedback on task execution. In particular, we present a model-in-the-loop\\rframework that semi-automatically derives criteria from collected guidelines\\rfor different writing tasks and constructs in-context demonstrations for each\\rcriterion. We choose three tasks from real-world scenarios to operationalize\\rthis idea: paper introduction writing, Python code writing, and Reddit post\\rwriting, and evaluate our feedback generation framework using different LLMs.\\rThe results reveal the fine-grained effects of incorporating criteria and\\rdemonstrations and provide valuable insights on how to teach LLMs to use\\rcriteria more effectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01069 ,  9801kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01081\\rDate: Sat, 2 Mar 2024 03:48:37 GMT   (1468kb,D)\\r\\rTitle: LAB: Large-Scale Alignment for ChatBots\\rAuthors: Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu,\\r  David D. Cox, Akash Srivastava\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  This work introduces LAB (Large-scale Alignment for chatBots), a novel\\rmethodology designed to overcome the scalability challenges in the\\rinstruction-tuning phase of large language model (LLM) training. Leveraging a\\rtaxonomy-guided synthetic data generation process and a multi-phase tuning\\rframework, LAB significantly reduces reliance on expensive human annotations\\rand proprietary models like GPT-4. We demonstrate that LAB-trained models can\\rachieve competitive performance across several benchmarks compared to models\\rtrained with traditional human-annotated or GPT-4 generated synthetic data.\\rThus offering a scalable, cost-effective solution for enhancing LLM\\rcapabilities and instruction-following behaviors without the drawbacks of\\rcatastrophic forgetting, marking a step forward in the efficient training of\\rLLMs for a wide range of applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01081 ,  1468kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01106\\rDate: Sat, 2 Mar 2024 06:38:15 GMT   (9250kb,D)\\r\\rTitle: Distilling Text Style Transfer With Self-Explanation From LLMs\\rAuthors: Chiyu Zhang, Honglong Cai, Yuezhang (Music) Li, Yuexin Wu, Le Hou,\\r  Muhammad Abdul-Mageed\\rCategories: cs.CL cs.AI\\rComments: under review\\r\\\\\\\\\\r  Text Style Transfer (TST) seeks to alter the style of text while retaining\\rits core content. Given the constraints of limited parallel datasets for TST,\\rwe propose CoTeX, a framework that leverages large language models (LLMs)\\ralongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills\\rthe complex rewriting and reasoning capabilities of LLMs into more streamlined\\rmodels capable of working with both non-parallel and parallel data. Through\\rexperimentation across four TST datasets, CoTeX is shown to surpass traditional\\rsupervised fine-tuning and knowledge distillation methods, particularly in\\rlow-resource settings. We conduct a comprehensive evaluation, comparing CoTeX\\ragainst current unsupervised, supervised, in-context learning (ICL) techniques,\\rand instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering\\rtransparent explanations for its style transfer process.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01106 ,  9250kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01116\\rDate: Sat, 2 Mar 2024 07:49:57 GMT   (1450kb,D)\\r\\rTitle: MulCogBench: A Multi-modal Cognitive Benchmark Dataset for Evaluating\\r  Chinese and English Computational Language Models\\rAuthors: Yunhao Zhang, Xiaohan Zhang, Chong Li, Shaonan Wang, Chengqing Zong\\rCategories: cs.CL\\r\\\\\\\\\\r  Pre-trained computational language models have recently made remarkable\\rprogress in harnessing the language abilities which were considered unique to\\rhumans. Their success has raised interest in whether these models represent and\\rprocess language like humans. To answer this question, this paper proposes\\rMulCogBench, a multi-modal cognitive benchmark dataset collected from native\\rChinese and English participants. It encompasses a variety of cognitive data,\\rincluding subjective semantic ratings, eye-tracking, functional magnetic\\rresonance imaging (fMRI), and magnetoencephalography (MEG). To assess the\\rrelationship between language models and cognitive data, we conducted a\\rsimilarity-encoding analysis which decodes cognitive data based on its pattern\\rsimilarity with textual embeddings. Results show that language models share\\rsignificant similarities with human cognitive data and the similarity patterns\\rare modulated by the data modality and stimuli complexity. Specifically,\\rcontext-aware models outperform context-independent models as language stimulus\\rcomplexity increases. The shallow layers of context-aware models are better\\raligned with the high-temporal-resolution MEG signals whereas the deeper layers\\rshow more similarity with the high-spatial-resolution fMRI. These results\\rindicate that language models have a delicate relationship with brain language\\rrepresentations. Moreover, the results between Chinese and English are highly\\rconsistent, suggesting the generalizability of these findings across languages.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01116 ,  1450kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01139\\rDate: Sat, 2 Mar 2024 08:53:40 GMT   (4489kb,D)\\r\\rTitle: ParallelPARC: A Scalable Pipeline for Generating Natural-Language\\r  Analogies\\rAuthors: Oren Sultan, Yonatan Bitton, Ron Yosef, Dafna Shahaf\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Analogy-making is central to human cognition, allowing us to adapt to novel\\rsituations -- an ability that current AI systems still lack. Most analogy\\rdatasets today focus on simple analogies (e.g., word analogies); datasets\\rincluding complex types of analogies are typically manually curated and very\\rsmall. We believe that this holds back progress in computational analogy. In\\rthis work, we design a data generation pipeline, ParallelPARC (Parallel\\rParagraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to\\rcreate complex, paragraph-based analogies, as well as distractors, both simple\\rand challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset\\rof analogies between scientific processes. We publish a gold-set, validated by\\rhumans, and a silver-set, generated automatically. We test LLMs' and humans'\\ranalogy recognition in binary and multiple-choice settings, and found that\\rhumans outperform the best models (~13% gap) after a light supervision. We\\rdemonstrate that our silver-set is useful for training models. Lastly, we show\\rchallenging distractors confuse LLMs, but not humans. We hope our pipeline will\\rencourage research in this emerging field.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01139 ,  4489kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01152\\rDate: Sat, 2 Mar 2024 09:39:13 GMT   (342kb,D)\\r\\rTitle: A Survey of AI-generated Text Forensic Systems: Detection, Attribution,\\r  and Characterization\\rAuthors: Tharindu Kumarage, Garima Agrawal, Paras Sheth, Raha Moraffah, Aman\\r  Chadha, Joshua Garland, Huan Liu\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  We have witnessed lately a rapid proliferation of advanced Large Language\\rModels (LLMs) capable of generating high-quality text. While these LLMs have\\rrevolutionized text generation across various domains, they also pose\\rsignificant risks to the information ecosystem, such as the potential for\\rgenerating convincing propaganda, misinformation, and disinformation at scale.\\rThis paper offers a review of AI-generated text forensic systems, an emerging\\rfield addressing the challenges of LLM misuses. We present an overview of the\\rexisting efforts in AI-generated text forensics by introducing a detailed\\rtaxonomy, focusing on three primary pillars: detection, attribution, and\\rcharacterization. These pillars enable a practical understanding of\\rAI-generated text, from identifying AI-generated content (detection),\\rdetermining the specific AI model involved (attribution), and grouping the\\runderlying intents of the text (characterization). Furthermore, we explore\\ravailable resources for AI-generated text forensics research and discuss the\\revolving challenges and future directions of forensic systems in an AI era.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01152 ,  342kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01163\\rDate: Sat, 2 Mar 2024 10:34:11 GMT   (1214kb,D)\\r\\rTitle: BootTOD: Bootstrap Task-oriented Dialogue Representations by Aligning\\r  Diverse Responses\\rAuthors: Weihao Zeng, Keqing He, Yejie Wang, Dayuan Fu, Weiran Xu\\rCategories: cs.CL\\rComments: Accepted at LREC-COLING 2024\\r\\\\\\\\\\r  Pre-trained language models have been successful in many scenarios. However,\\rtheir usefulness in task-oriented dialogues is limited due to the intrinsic\\rlinguistic differences between general text and task-oriented dialogues.\\rCurrent task-oriented dialogue pre-training methods rely on a contrastive\\rframework, which faces challenges such as selecting true positives and hard\\rnegatives, as well as lacking diversity. In this paper, we propose a novel\\rdialogue pre-training model called BootTOD. It learns task-oriented dialogue\\rrepresentations via a self-bootstrapping framework. Unlike contrastive\\rcounterparts, BootTOD aligns context and context+response representations and\\rdismisses the requirements of contrastive pairs. BootTOD also uses multiple\\rappropriate response targets to model the intrinsic one-to-many diversity of\\rhuman conversations. Experimental results show that BootTOD outperforms strong\\rTOD baselines on diverse downstream dialogue tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01163 ,  1214kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01165\\rDate: Sat, 2 Mar 2024 10:38:10 GMT   (7103kb,D)\\r\\rTitle: STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient\\r  Fine-Tuning of Large Language Models\\rAuthors: Linhai Zhang, Jialong Wu, Deyu Zhou, Guoqiang Xu\\rCategories: cs.CL cs.AI\\rComments: Our code and results will be available at\\r  https://github.com/callanwu/STAR\\r\\\\\\\\\\r  Though Large Language Models (LLMs) have demonstrated the powerful\\rcapabilities of few-shot learning through prompting methods, supervised\\rtraining is still necessary for complex reasoning tasks. Because of their\\rextensive parameters and memory consumption, both Parameter-Efficient\\rFine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been\\rproposed for LLMs. Nevertheless, the issue of large annotated data consumption,\\rthe aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is\\rto combine the PEFT method with active learning. However, the experimental\\rresults show that such a combination is not trivial and yields inferior\\rresults. Through probe experiments, such observation might be explained by two\\rmain reasons: uncertainty gap and poor model calibration. Therefore, in this\\rpaper, we propose a novel approach to effectively integrate uncertainty-based\\ractive learning and LoRA. Specifically, for the uncertainty gap, we introduce a\\rdynamic uncertainty measurement that combines the uncertainty of the base model\\rand the uncertainty of the full model during the iteration of active learning.\\rFor poor model calibration, we incorporate the regularization method during\\rLoRA training to keep the model from being over-confident, and the Monte-Carlo\\rdropout mechanism is employed to enhance the uncertainty estimation.\\rExperimental results show that the proposed approach outperforms existing\\rbaseline models on three complex reasoning tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01165 ,  7103kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01166\\rDate: Sat, 2 Mar 2024 10:38:31 GMT   (7514kb,D)\\r\\rTitle: DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable\\r  Causal Inference\\rAuthors: Jialong Wu, Linhai Zhang, Deyu Zhou, Guoqiang Xu\\rCategories: cs.CL cs.AI\\rComments: Our code and results will be available at\\r  https://github.com/callanwu/DINER\\r\\\\\\\\\\r  Though notable progress has been made, neural-based aspect-based sentiment\\ranalysis (ABSA) models are prone to learn spurious correlations from annotation\\rbiases, resulting in poor robustness on adversarial data transformations. Among\\rthe debiasing solutions, causal inference-based methods have attracted much\\rresearch attention, which can be mainly categorized into causal intervention\\rmethods and counterfactual reasoning methods. However, most of the present\\rdebiasing methods focus on single-variable causal inference, which is not\\rsuitable for ABSA with two input variables (the target aspect and the review).\\rIn this paper, we propose a novel framework based on multi-variable causal\\rinference for debiasing ABSA. In this framework, different types of biases are\\rtackled based on different causal intervention methods. For the review branch,\\rthe bias is modeled as indirect confounding from context, where backdoor\\radjustment intervention is employed for debiasing. For the aspect branch, the\\rbias is described as a direct correlation with labels, where counterfactual\\rreasoning is adopted for debiasing. Extensive experiments demonstrate the\\reffectiveness of the proposed method compared to various baselines on the two\\rwidely used real-world aspect robustness test set datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01166 ,  7514kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01185\\rDate: Sat, 2 Mar 2024 11:54:55 GMT   (841kb,D)\\r\\rTitle: Balancing Exploration and Exploitation in LLM using Soft RLLF for\\r  Enhanced Negation Understanding\\rAuthors: Ha-Thanh Nguyen, Ken Satoh\\rCategories: cs.CL cs.AI\\rComments: JURISIN 2024\\r\\\\\\\\\\r  Finetuning approaches in NLP often focus on exploitation rather than\\rexploration, which may lead to suboptimal models. Given the vast search space\\rof natural language, this limited exploration can restrict their performance in\\rcomplex, high-stakes domains, where accurate negation understanding and logical\\rreasoning abilities are crucial. To address this issue, we leverage\\rReinforcement Learning from Logical Feedback (RLLF) to create an effective\\rbalance between exploration and exploitation in LLMs. Our approach employs an\\rappropriate benchmark dataset for training and evaluation, highlighting the\\rimportance of exploration in enhancing negation understanding capabilities. We\\rcompare the performance of our RLLF-enhanced LLMs with baseline models trained\\rwithout RLLF, demonstrating the value of this balanced approach. Furthermore,\\rwe showcase the potential of our method in legal AI applications by employing\\rtransfer learning and evaluating its impact on negation understanding. Our\\rexperimental results exhibit the effectiveness of balancing exploration and\\rexploitation with RLLF in improving LLMs' negation capabilities. This has\\rimplications for the development of more accurate, reliable, and logically\\rconsistent language models in high-stakes domains.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01185 ,  841kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01187\\rDate: Sat, 2 Mar 2024 11:58:24 GMT   (18kb)\\r\\rTitle: A Compositional Typed Semantics for Universal Dependencies\\rAuthors: Laurestine Bradford, Timothy John O'Donnell, Siva Reddy\\rCategories: cs.CL\\rComments: 10 pages, 6 figures, 1 table. For related code, see\\r  https://github.com/McGill-NLP/ud-to-meaning\\r\\\\\\\\\\r  Languages may encode similar meanings using different sentence structures.\\rThis makes it a challenge to provide a single set of formal rules that can\\rderive meanings from sentences in many languages at once. To overcome the\\rchallenge, we can take advantage of language-general connections between\\rmeaning and syntax, and build on cross-linguistically parallel syntactic\\rstructures. We introduce UD Type Calculus, a compositional, principled, and\\rlanguage-independent system of semantic types and logical forms for lexical\\ritems which builds on a widely-used language-general dependency syntax\\rframework. We explain the essential features of UD Type Calculus, which all\\rinvolve giving dependency relations denotations just like those of words. These\\rallow UD-TC to derive correct meanings for sentences with a wide range of\\rsyntactic structures by making use of dependency labels. Finally, we present\\revaluation results on a large existing corpus of sentences and their logical\\rforms, showing that UD-TC can produce meanings comparable with our baseline.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01187 ,  18kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01193\\rDate: Sat, 2 Mar 2024 12:19:04 GMT   (520kb,D)\\r\\rTitle: RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots\\rAuthors: Philip Feldman. James R. Foulds, Shimei Pan\\rCategories: cs.CL cs.AI\\rComments: 7 Pages, 1 Figure, 1 Table\\rACM-class: H.3.3; I.2.7\\r\\\\\\\\\\r  Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\\rof artificial intelligence. However, their tendency to hallucinate -- generate\\rplausible but false information -- poses a significant challenge. This issue is\\rcritical, as seen in recent court cases where ChatGPT's use led to citations of\\rnon-existent legal rulings. This paper explores how Retrieval-Augmented\\rGeneration (RAG) can counter hallucinations by integrating external knowledge\\rwith prompts. We empirically evaluate RAG against standard LLMs using prompts\\rdesigned to induce hallucinations. Our results show that RAG increases accuracy\\rin some cases, but can still be misled when prompts directly contradict the\\rmodel's pre-trained understanding. These findings highlight the complex nature\\rof hallucinations and the need for more robust solutions to ensure LLM\\rreliability in real-world applications. We offer practical recommendations for\\rRAG deployment and discuss implications for the development of more trustworthy\\rLLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01193 ,  520kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01196\\rDate: Sat, 2 Mar 2024 12:29:28 GMT   (2790kb,D)\\r\\rTitle: Machine Translation in the Covid domain: an English-Irish case study for\\r  LoResMT 2021\\rAuthors: S\\\\'eamus Lankford, Haithem Afli and Andy Way\\rCategories: cs.CL cs.AI\\rJournal-ref: Proceedings of the 4th Workshop on Technologies for MT of Low\\r  Resource Languages (LoResMT2021)\\r\\\\\\\\\\r  Translation models for the specific domain of translating Covid data from\\rEnglish to Irish were developed for the LoResMT 2021 shared task. Domain\\radaptation techniques, using a Covid-adapted generic 55k corpus from the\\rDirectorate General of Translation, were applied. Fine-tuning, mixed\\rfine-tuning and combined dataset approaches were compared with models trained\\ron an extended in-domain dataset. As part of this study, an English-Irish\\rdataset of Covid related data, from the Health and Education domains, was\\rdeveloped. The highest-performing model used a Transformer architecture trained\\rwith an extended in-domain Covid dataset. In the context of this study, we have\\rdemonstrated that extending an 8k in-domain baseline dataset by just 5k lines\\rimproved the BLEU score by 27 points.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01196 ,  2790kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01197\\rDate: Sat, 2 Mar 2024 12:31:22 GMT   (7463kb,D)\\r\\rTitle: DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling\\rAuthors: Shanghaoran Quan\\rCategories: cs.CL\\rComments: 20 pages, 7 figures\\r\\\\\\\\\\r  The performance of the reward model (RM) is a critical factor in improving\\rthe effectiveness of the large language model (LLM) during alignment\\rfine-tuning. There remain two challenges in RM training: 1) training the same\\rRM using various categories of data may cause its generalization performance to\\rsuffer from multi-task disturbance, and 2) the human annotation consistency\\rrate is generally only $60\\\\%$ to $75\\\\%$, causing training data to contain a lot\\rof noise. To tackle these two challenges, we introduced the idea of\\rMixture-of-Experts (MoE) into the field of RM for the first time. We propose\\rthe Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After\\rclassifying an input into task categories, we route it to the corresponding\\rinner layer task-specific model. The inner layer MoE is a dense model. We\\rdecompose the specific task into multiple capability dimensions and\\rindividually fine-tune a LoRA expert on each one. Their outputs are then\\rsynthesized by an MLP to compute the final rewards. To minimize costs, we call\\ra public LLM API to obtain the capability preference labels. The validation on\\rmanually labeled datasets confirms that our model attains superior consistency\\rwith human preference and outstrips advanced generative approaches. Meanwhile,\\rthrough BoN sampling and RL experiments, we demonstrate that our model\\routperforms state-of-the-art ensemble methods of RM and mitigates the\\roveroptimization problem. Our code and dataset are available at:\\rhttps://github.com/quanshr/DMoERM-v1.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01197 ,  7463kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01216\\rDate: Sat, 2 Mar 2024 14:14:45 GMT   (283kb,D)\\r\\rTitle: API Is Enough: Conformal Prediction for Large Language Models Without\\r  Logit-Access\\rAuthors: Jiayuan Su, Jing Luo, Hongwei Wang, Lu Cheng\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  This study aims to address the pervasive challenge of quantifying uncertainty\\rin large language models (LLMs) without logit-access. Conformal Prediction\\r(CP), known for its model-agnostic and distribution-free features, is a desired\\rapproach for various LLMs and data distributions. However, existing CP methods\\rfor LLMs typically assume access to the logits, which are unavailable for some\\rAPI-only LLMs. In addition, logits are known to be miscalibrated, potentially\\rleading to degraded CP performance. To tackle these challenges, we introduce a\\rnovel CP method that (1) is tailored for API-only LLMs without logit-access;\\r(2) minimizes the size of prediction sets; and (3) ensures a statistical\\rguarantee of the user-defined coverage. The core idea of this approach is to\\rformulate nonconformity measures using both coarse-grained (i.e., sample\\rfrequency) and fine-grained uncertainty notions (e.g., semantic similarity).\\rExperimental results on both close-ended and open-ended Question Answering\\rtasks show our approach can mostly outperform the logit-based CP baselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01216 ,  283kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01222\\rDate: Sat, 2 Mar 2024 14:38:03 GMT   (8258kb,D)\\r\\rTitle: Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions\\rAuthors: Flor Miriam Plaza-del-Arco, Alba Curry, Amanda Cercas Curry, Dirk Hovy\\rCategories: cs.CL\\rComments: Accepted to LREC-COLING 2024\\r\\\\\\\\\\r  Emotions are a central aspect of communication. Consequently, emotion\\ranalysis (EA) is a rapidly growing field in natural language processing (NLP).\\rHowever, there is no consensus on scope, direction, or methods. In this paper,\\rwe conduct a thorough review of 154 relevant NLP publications from the last\\rdecade. Based on this review, we address four different questions: (1) How are\\rEA tasks defined in NLP? (2) What are the most prominent emotion frameworks and\\rwhich emotions are modeled? (3) Is the subjectivity of emotions considered in\\rterms of demographics and cultural factors? and (4) What are the primary NLP\\rapplications for EA? We take stock of trends in EA and tasks, emotion\\rframeworks used, existing datasets, methods, and applications. We then discuss\\rfour lacunae: (1) the absence of demographic and cultural aspects does not\\raccount for the variation in how emotions are perceived, but instead assumes\\rthey are universally experienced in the same manner; (2) the poor fit of\\remotion categories from the two main emotion theories to the task; (3) the lack\\rof standardized EA terminology hinders gap identification, comparison, and\\rfuture goals; and (4) the absence of interdisciplinary research isolates EA\\rfrom insights in other fields. Our work will enable more focused research into\\rEA and a more holistic approach to modeling emotions in NLP.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01222 ,  8258kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01241\\rDate: Sat, 2 Mar 2024 16:05:26 GMT   (33839kb,D)\\r\\rTitle: IntactKV: Improving Large Language Model Quantization by Keeping Pivot\\r  Tokens Intact\\rAuthors: Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu,\\r  Lu Hou, Jun Yao, Chun Yuan\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Large language models (LLMs) excel in natural language processing but demand\\rintensive computation. To mitigate this, various quantization methods have been\\rexplored, yet they compromise LLM performance. This paper unveils a previously\\roverlooked type of outlier in LLMs. Such outliers are found to allocate most of\\rthe attention scores on initial tokens of input, termed as pivot tokens, which\\ris crucial to the performance of quantized LLMs. Given that, we propose\\rIntactKV to generate the KV cache of pivot tokens losslessly from the\\rfull-precision model. The approach is simple and easy to combine with existing\\rquantization solutions. Besides, IntactKV can be calibrated as additional LLM\\rparameters to boost the quantized LLMs further. Mathematical analysis also\\rproves that IntactKV effectively reduces the upper bound of quantization error.\\rEmpirical results show that IntactKV brings consistent improvement and achieves\\rlossless weight-only INT4 quantization on various downstream tasks, leading to\\rthe new state-of-the-art for LLM quantization.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01241 ,  33839kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01244\\rDate: Sat, 2 Mar 2024 16:11:23 GMT   (397kb,D)\\r\\rTitle: Mitigating Catastrophic Forgetting in Large Language Models with\\r  Self-Synthesized Rehearsal\\rAuthors: Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao,\\r  Linfeng Song, Junfeng Yao, Jinsong Su\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Large language models (LLMs) suffer from catastrophic forgetting during\\rcontinual learning. Conventional rehearsal-based methods rely on previous\\rtraining data to retain the model's ability, which may not be feasible in\\rreal-world applications. When conducting continual learning based on a\\rpublicly-released LLM checkpoint, the availability of the original training\\rdata may be non-existent. To address this challenge, we propose a framework\\rcalled Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic\\rinstances for rehearsal. Concretely, we first employ the base LLM for\\rin-context learning to generate synthetic instances. Subsequently, we utilize\\rthe latest LLM to refine the instance outputs based on the synthetic inputs,\\rpreserving its acquired ability. Finally, we select diverse high-quality\\rsynthetic instances for rehearsal in future stages. Experimental results\\rdemonstrate that SSR achieves superior or comparable performance compared to\\rconventional rehearsal-based approaches while being more data-efficient.\\rBesides, SSR effectively preserves the generalization capabilities of LLMs in\\rgeneral domains.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01244 ,  397kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01251\\rDate: Sat, 2 Mar 2024 16:23:44 GMT   (3827kb,D)\\r\\rTitle: Accelerating Greedy Coordinate Gradient via Probe Sampling\\rAuthors: Yiran Zhao, Wenyue Zheng, Tianle Cai, Xuan Long Do, Kenji Kawaguchi,\\r  Anirudh Goyal, Michael Shieh\\rCategories: cs.CL\\r\\\\\\\\\\r  Safety of Large Language Models (LLMs) has become a central issue given their\\rrapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown\\rto be effective in constructing prompts containing adversarial suffixes to\\rbreak the presumingly safe LLMs, but the optimization of GCG is time-consuming\\rand limits its practicality. To reduce the time cost of GCG and enable more\\rcomprehensive studies of LLM safety, in this work, we study a new algorithm\\rcalled $\\\\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core\\rof the algorithm is a mechanism that dynamically determines how similar a\\rsmaller draft model's predictions are to the target model's predictions for\\rprompt candidates. When the target model is similar to the draft model, we rely\\rheavily on the draft model to filter out a large number of potential prompt\\rcandidates to reduce the computation time. Probe sampling achieves up to $5.6$\\rtimes speedup using Llama2-7b and leads to equal or improved attack success\\rrate (ASR) on the AdvBench.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01251 ,  3827kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01270\\rDate: Sat, 2 Mar 2024 17:13:47 GMT   (4942kb)\\r\\rTitle: A comprehensive cross-language framework for harmful content detection\\r  with the aid of sentiment analysis\\rAuthors: Mohammad Dehghani\\rCategories: cs.CL\\r\\\\\\\\\\r  In today's digital world, social media plays a significant role in\\rfacilitating communication and content sharing. However, the exponential rise\\rin user-generated content has led to challenges in maintaining a respectful\\ronline environment. In some cases, users have taken advantage of anonymity in\\rorder to use harmful language, which can negatively affect the user experience\\rand pose serious social problems. Recognizing the limitations of manual\\rmoderation, automatic detection systems have been developed to tackle this\\rproblem. Nevertheless, several obstacles persist, including the absence of a\\runiversal definition for harmful language, inadequate datasets across\\rlanguages, the need for detailed annotation guideline, and most importantly, a\\rcomprehensive framework. This study aims to address these challenges by\\rintroducing, for the first time, a detailed framework adaptable to any\\rlanguage. This framework encompasses various aspects of harmful language\\rdetection. A key component of the framework is the development of a general and\\rdetailed annotation guideline. Additionally, the integration of sentiment\\ranalysis represents a novel approach to enhancing harmful language detection.\\rAlso, a definition of harmful language based on the review of different related\\rconcepts is presented. To demonstrate the effectiveness of the proposed\\rframework, its implementation in a challenging low-resource language is\\rconducted. We collected a Persian dataset and applied the annotation guideline\\rfor harmful detection and sentiment analysis. Next, we present baseline\\rexperiments utilizing machine and deep learning methods to set benchmarks.\\rResults prove the framework's high performance, achieving an accuracy of 99.4%\\rin offensive language detection and 66.2% in sentiment analysis.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01270 ,  4942kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01289\\rDate: Sat, 2 Mar 2024 19:01:40 GMT   (33kb,D)\\r\\rTitle: Greed is All You Need: An Evaluation of Tokenizer Inference Methods\\rAuthors: Omri Uzan, Craig W. Schmidt, Chris Tanner, Yuval Pinter\\rCategories: cs.CL\\r\\\\\\\\\\r  While subword tokenizers such as BPE and WordPiece are typically used to\\rbuild vocabularies for NLP models, the method of decoding text into a sequence\\rof tokens from these vocabularies is often left unspecified, or ill-suited to\\rthe method in which they were constructed. We provide a controlled analysis of\\rseven tokenizer inference methods across four different algorithms and three\\rvocabulary sizes, performed on a novel intrinsic evaluation suite we curated\\rfor English, combining measures rooted in morphology, cognition, and\\rinformation theory. We show that for the most commonly used tokenizers, greedy\\rinference performs surprisingly well; and that SaGe, a recently-introduced\\rcontextually-informed tokenizer, outperforms all others on morphological\\ralignment.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01289 ,  33kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01304\\rDate: Sat, 2 Mar 2024 20:25:50 GMT   (39kb)\\r\\rTitle: Improving the Validity of Automatically Generated Feedback via\\r  Reinforcement Learning\\rAuthors: Alexander Scarlatos, Digory Smith, Simon Woodhead, Andrew Lan\\rCategories: cs.CL\\r\\\\\\\\\\r  Automatically generating feedback via large language models (LLMs) in\\rintelligent tutoring systems and online learning platforms has the potential to\\rimprove the learning outcomes of many students. However, both feedback\\rgeneration and evaluation are challenging: feedback content has to be valid\\respecially in subjects like math, which requires models to understand the\\rproblem, the solution, and where the student's error lies. Feedback also has to\\rbe pedagogically valid to reflect effective tutoring strategies, such as\\rexplaining possible misconceptions and encouraging the student, among other\\rdesirable features. In this work, we address both problems of automatically\\rgenerating and evaluating feedback while considering both correctness and\\ralignment. First, we propose a rubric for evaluating math feedback and show\\rthat GPT-4 is able to effectively use it to annotate human-written and\\rLLM-generated feedback. Second, we propose a framework for feedback generation\\rthat optimizes both correctness and alignment using reinforcement learning\\r(RL). Specifically, we use GPT-4's annotations to create preferences over\\rfeedback pairs in an augmented dataset for training via direct preference\\roptimization (DPO). We show that our methods significantly increase the\\rcorrectness and alignment of generated feedback with Llama 2, an open-source\\rLLM, qualitatively analyze our generation and evaluation systems using case\\rstudies, and outline several areas for future work.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01304 ,  39kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01308\\rDate: Sat, 2 Mar 2024 20:40:11 GMT   (101kb,D)\\r\\rTitle: VBART: The Turkish LLM\\rAuthors: Meliksah Turker, Mehmet Erdi Ari, Aydin Han\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  We present VBART, the first Turkish sequence-to-sequence Large Language\\rModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\\rLLMs based on good ideas leveraged from BART and mBART models and come in two\\rsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\\rstate-of-the-art results in abstractive text summarization, title generation,\\rtext paraphrasing, question answering and question generation tasks. They allow\\rfine-tuning for future text generation tasks and datasets, carving a new path\\rfor Turkish Natural Language Processing (NLP) research. Our work shows that\\rhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\\rimproving existing results and providing efficient models for training and\\rinference. Moreover, we show that our monolingual tokenizer is 7x more\\refficient than OpenAI's multilingual tokenizer. Last but not least, we\\rintroduce a method to enlarge an existing pre-trained LLM and question the\\rrelevancy of Chinchilla Scaling Law to sequence-to-sequence masked language\\rmodels. Our fine-tuned models, tokenizer and cleaned web corpus of 135 GB are\\rpublicly available at huggingface.co/vngrs-ai.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01308 ,  101kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01309\\rDate: Sat, 2 Mar 2024 20:46:56 GMT   (166kb,D)\\r\\rTitle: VNLP: Turkish NLP Package\\rAuthors: Meliksah Turker, Mehmet Erdi Ari, Aydin Han\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  In this work, we present VNLP: the first dedicated, complete, open-source,\\rwell-documented, lightweight, production-ready, state-of-the-art Natural\\rLanguage Processing (NLP) package for the Turkish language. It contains a wide\\rvariety of tools, ranging from the simplest tasks, such as sentence splitting\\rand text normalization, to the more advanced ones, such as text and token\\rclassification models. Its token classification models are based on Context\\rModel, a novel architecture that is both an encoder and an auto-regressive\\rmodel. NLP tasks solved by VNLP models include but are not limited to Sentiment\\rAnalysis, Named Entity Recognition, Morphological Analysis \\\\& Disambiguation\\rand Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings\\rand corresponding SentencePiece Unigram tokenizers. VNLP has an open-source\\rGitHub repository, ReadtheDocs documentation, PyPi package for convenient\\rinstallation, Python and command-line API and a demo page to test all the\\rfunctionality. Consequently, our main contribution is a complete, compact,\\reasy-to-install and easy-to-use NLP package for Turkish.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01309 ,  166kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01342\\rDate: Sat, 2 Mar 2024 23:32:33 GMT   (1339kb)\\r\\rTitle: LM4OPT: Unveiling the Potential of Large Language Models in Formulating\\r  Mathematical Optimization Problems\\rAuthors: Tasnim Ahmed, Salimur Choudhury\\rCategories: cs.CL cs.IR\\r\\\\\\\\\\r  In the rapidly evolving field of natural language processing, the translation\\rof linguistic descriptions into mathematical formulation of optimization\\rproblems presents a formidable challenge, demanding intricate understanding and\\rprocessing capabilities from Large Language Models (LLMs). This study compares\\rprominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and\\rone-shot settings for this task. Our findings show GPT-4's superior\\rperformance, particularly in the one-shot scenario. A central part of this\\rresearch is the introduction of `LM4OPT,' a progressive fine-tuning framework\\rfor Llama-2-7b that utilizes noisy embeddings and specialized datasets.\\rHowever, this research highlights a notable gap in the contextual understanding\\rcapabilities of smaller models such as Llama-2-7b compared to larger\\rcounterparts, especially in processing lengthy and complex input contexts. Our\\rempirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4\\rsurpasses the baseline performance established by previous research, achieving\\ran F1-score of 0.63, solely based on the problem description in natural\\rlanguage, and without relying on any additional named entity information.\\rGPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b. These\\rfindings not only benchmark the current capabilities of LLMs in a novel\\rapplication area but also lay the groundwork for future improvements in\\rmathematical formulation of optimization problems from natural language input.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01342 ,  1339kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01364\\rDate: Sun, 3 Mar 2024 01:47:52 GMT   (372kb,D)\\r\\rTitle: Improving Cross-lingual Representation for Semantic Retrieval with\\r  Code-switching\\rAuthors: Mieradilijiang Maimaiti, Yuanhang Zheng, Ji Zhang, Fei Huang, Yue\\r  Zhang, Wenpei Luo, Kaiyu Huang\\rCategories: cs.CL\\r\\\\\\\\\\r  Semantic Retrieval (SR) has become an indispensable part of the FAQ system in\\rthe task-oriented question-answering (QA) dialogue scenario. The demands for a\\rcross-lingual smart-customer-service system for an e-commerce platform or some\\rparticular business conditions have been increasing recently. Most previous\\rstudies exploit cross-lingual pre-trained models (PTMs) for multi-lingual\\rknowledge retrieval directly, while some others also leverage the continual\\rpre-training before fine-tuning PTMs on the downstream tasks. However, no\\rmatter which schema is used, the previous work ignores to inform PTMs of some\\rfeatures of the downstream task, i.e. train their PTMs without providing any\\rsignals related to SR. To this end, in this work, we propose an Alternative\\rCross-lingual PTM for SR via code-switching. We are the first to utilize the\\rcode-switching approach for cross-lingual SR. Besides, we introduce the novel\\rcode-switched continual pre-training instead of directly using the PTMs on the\\rSR tasks. The experimental results show that our proposed approach consistently\\routperforms the previous SOTA methods on SR and semantic textual similarity\\r(STS) tasks with three business corpora and four open datasets in 20+\\rlanguages.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01364 ,  372kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01373\\rDate: Sun, 3 Mar 2024 02:31:11 GMT   (522kb,D)\\r\\rTitle: Evaluating and Mitigating Number Hallucinations in Large Vision-Language\\r  Models: A Consistency Perspective\\rAuthors: Huixuan Zhang, Junzhe Zhang, Xiaojun Wan\\rCategories: cs.CL\\rComments: 13 pages\\r\\\\\\\\\\r  Large vision language models have demonstrated remarkable efficacy in\\raddressing challenges related to both textual and visual content. Nevertheless,\\rthese models are susceptible to various hallucinations. In this paper, we focus\\ron a new form of hallucination, specifically termed as number hallucination,\\rwhich denotes instances where models fail to accurately identify the quantity\\rof objects in an image. We establish a dataset and employ evaluation metrics to\\rassess number hallucination, revealing a pronounced prevalence of this issue\\racross mainstream large vision language models (LVLMs). Additionally, we delve\\rinto a thorough analysis of number hallucination, examining inner and outer\\rinconsistency problem from two related perspectives. We assert that this\\rinconsistency is one cause of number hallucination and propose a consistency\\rtraining method as a means to alleviate such hallucination, which achieves an\\raverage improvement of 8\\\\% compared with direct finetuning method.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01373 ,  522kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01382\\rDate: Sun, 3 Mar 2024 03:06:31 GMT   (468kb,D)\\r\\rTitle: Automatic Question-Answer Generation for Long-Tail Knowledge\\rAuthors: Rohan Kumar, Youngmin Kim, Sunitha Ravi, Haitian Sun, Christos\\r  Faloutsos, Ruslan Salakhutdinov, Minji Yoon\\rCategories: cs.CL\\rComments: Accepted at KDD 2023 KnowledgeNLP\\r\\\\\\\\\\r  Pretrained Large Language Models (LLMs) have gained significant attention for\\raddressing open-domain Question Answering (QA). While they exhibit high\\raccuracy in answering questions related to common knowledge, LLMs encounter\\rdifficulties in learning about uncommon long-tail knowledge (tail entities).\\rSince manually constructing QA datasets demands substantial human resources,\\rthe types of existing QA datasets are limited, leaving us with a scarcity of\\rdatasets to study the performance of LLMs on tail entities. In this paper, we\\rpropose an automatic approach to generate specialized QA datasets for tail\\rentities and present the associated research challenges. We conduct extensive\\rexperiments by employing pretrained LLMs on our newly generated long-tail QA\\rdatasets, comparing their performance with and without external resources\\rincluding Wikipedia and Wikidata knowledge graphs.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01382 ,  468kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01390\\rDate: Sun, 3 Mar 2024 04:22:13 GMT   (968kb,D)\\r\\rTitle: Right for Right Reasons: Large Language Models for Verifiable\\r  Commonsense Knowledge Graph Question Answering\\rAuthors: Armin Toroghi, Willis Guo, Mohammad Mahdi Abdollah Pour, Scott Sanner\\rCategories: cs.CL\\rComments: 8 pages\\rACM-class: I.2.7\\r\\\\\\\\\\r  Knowledge Graph Question Answering (KGQA) methods seek to answer Natural\\rLanguage questions using the relational information stored in Knowledge Graphs\\r(KGs). With the recent advancements of Large Language Models (LLMs) and their\\rremarkable reasoning abilities, there is a growing trend to leverage them for\\rKGQA. However, existing methodologies have only focused on answering factual\\rquestions, e.g., In which city was Silvio Berlusconi's first wife born?,\\rleaving questions involving commonsense reasoning that real-world users may\\rpose more often, e.g., Do I need separate visas to see the Venus of Willendorf\\rand attend the Olympics this summer? unaddressed. In this work, we first\\robserve that existing LLM-based methods for KGQA struggle with hallucination on\\rsuch questions, especially on queries targeting long-tail entities (e.g.,\\rnon-mainstream and recent entities), thus hindering their applicability in\\rreal-world applications especially since their reasoning processes are not\\reasily verifiable. In response, we propose Right for Right Reasons (R3), a\\rcommonsense KGQA methodology that allows for a verifiable reasoning procedure\\rby axiomatically surfacing intrinsic commonsense knowledge of LLMs and\\rgrounding every factual reasoning step on KG triples. Through experimental\\revaluations across three different tasks--question answering, claim\\rverification, and preference matching--our findings showcase R3 as a superior\\rapproach, outperforming existing methodologies and notably reducing instances\\rof hallucination and reasoning errors.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01390 ,  968kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01395\\rDate: Sun, 3 Mar 2024 04:47:01 GMT   (874kb,D)\\r\\rTitle: CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring\\r  Commonsense Reasoning and Long-Tail Knowledge\\rAuthors: Willis Guo, Armin Toroghi, Scott Sanner\\rCategories: cs.CL\\rComments: 7 pages\\rACM-class: I.2.4, I.2.7\\r\\\\\\\\\\r  Knowledge graph question answering (KGQA) is a well-established field that\\rseeks to provide factual answers to natural language (NL) questions by\\rleveraging knowledge graphs (KGs). However, existing KGQA datasets suffer from\\rtwo significant limitations: (1) no existing KGQA dataset requires commonsense\\rreasoning to arrive at an answer and (2) existing KGQA datasets focus on\\rpopular entities for which large language models (LLMs) can directly answer\\rwithout hallucinating and without leveraging the KG. In this work, we seek a\\rnovel KGQA dataset that supports commonsense reasoning and focuses on long-tail\\rentities (e.g., non-mainstream and recent entities) where LLMs frequently\\rhallucinate, and thus create the need for novel methodologies that leverage the\\rKG for factual and attributable commonsense inference. We create a novel\\rCommonsense Reasoning (CR) and Long-Tail (LT) KGQA dataset with two subtasks --\\rquestion answering and claim verification -- that address both limitations (1)\\rand (2). We construct CR-LT-KGQA by building extensions to existing reasoning\\rdatasets StrategyQA and CREAK over Wikidata. While existing KGQA methods are\\rnot applicable due to their lack of commonsense inference support, baseline\\revaluation of LLMs on CR-LT KGQA demonstrate a high rate of hallucination.\\rThus, CR-LT KGQA poses significant challenges for hallucination-prone LLMs,\\rhence paving the way for future commonsense KGQA research to provide accurate\\rand factual answers for long-tail entities in the era of LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01395 ,  874kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01404\\rDate: Sun, 3 Mar 2024 05:45:27 GMT   (13535kb,D)\\r\\rTitle: What Is Missing in Multilingual Visual Reasoning and How to Fix It\\rAuthors: Yueqi Song, Simran Khanuja, Graham Neubig\\rCategories: cs.CL\\r\\\\\\\\\\r  NLP models today strive for supporting multiple languages and modalities,\\rimproving accessibility for diverse users. In this paper, we evaluate their\\rmultilingual, multimodal capabilities by testing on a visual reasoning task. We\\robserve that proprietary systems like GPT-4V obtain the best performance on\\rthis task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits\\rsimilar performance between English and other languages, indicating the\\rpotential for equitable system development across languages. Our analysis on\\rmodel failures reveals three key aspects that make this task challenging:\\rmultilinguality, complex reasoning, and multimodality. To address these\\rchallenges, we propose three targeted interventions including a translate-test\\rapproach to tackle multilinguality, a visual programming approach to break down\\rcomplex reasoning, and a novel method that leverages image captioning to\\raddress multimodality. Our interventions achieve the best open performance on\\rthis task in a zero-shot setting, boosting open model LLaVA by 13.4%, while\\ralso minorly improving GPT-4V's performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01404 ,  13535kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01411\\rDate: Sun, 3 Mar 2024 06:47:51 GMT   (2517kb,D)\\r\\rTitle: OVEL: Large Language Model as Memory Manager for Online Video Entity\\r  Linking\\rAuthors: Haiquan Zhao and Xuwu Wang and Shisong Chen and Zhixu Li and Xin Zheng\\r  and Yanghua Xiao\\rCategories: cs.CL\\rComments: 13 pages, 6 figures\\r\\\\\\\\\\r  In recent years, multi-modal entity linking (MEL) has garnered increasing\\rattention in the research community due to its significance in numerous\\rmulti-modal applications. Video, as a popular means of information\\rtransmission, has become prevalent in people's daily lives. However, most\\rexisting MEL methods primarily focus on linking textual and visual mentions or\\roffline videos's mentions to entities in multi-modal knowledge bases, with\\rlimited efforts devoted to linking mentions within online video content. In\\rthis paper, we propose a task called Online Video Entity Linking OVEL, aiming\\rto establish connections between mentions in online videos and a knowledge base\\rwith high accuracy and timeliness. To facilitate the research works of OVEL, we\\rspecifically concentrate on live delivery scenarios and construct a live\\rdelivery entity linking dataset called LIVE. Besides, we propose an evaluation\\rmetric that considers timelessness, robustness, and accuracy. Furthermore, to\\reffectively handle OVEL task, we leverage a memory block managed by a Large\\rLanguage Model and retrieve entity candidates from the knowledge base to\\raugment LLM performance on memory management. The experimental results prove\\rthe effectiveness and efficiency of our method.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01411 ,  2517kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01432\\rDate: Sun, 3 Mar 2024 08:07:55 GMT   (9659kb,D)\\r\\rTitle: Fine Tuning vs. Retrieval Augmented Generation for Less Popular\\r  Knowledge\\rAuthors: Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi\\rCategories: cs.CL\\r\\\\\\\\\\r  Large language models (LLMs) memorize a vast amount of factual knowledge,\\rexhibiting strong performance across diverse tasks and domains. However, it has\\rbeen observed that the performance diminishes when dealing with less-popular or\\rlow-frequency concepts and entities, for example in domain specific\\rapplications. The two prominent approaches to enhance the performance of LLMs\\ron low-frequent topics are: Retrieval Augmented Generation (RAG) and\\rfine-tuning (FT) over synthetic data. This paper explores and evaluates the\\rimpact of RAG and FT on customizing LLMs in handling low-frequency entities on\\rquestion answering task. Our findings indicate that FT significantly boosts the\\rperformance across entities of varying popularity, especially in the most and\\rleast popular groups, while RAG surpasses other methods. Additionally, the\\rsuccess of both RAG and FT approaches is amplified by advancements in retrieval\\rand data augmentation techniques. We release our data and code at\\rhttps://github.com/HeydarSoudani/RAGvsFT.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01432 ,  9659kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01456\\rDate: Sun, 3 Mar 2024 09:18:05 GMT   (6489kb)\\r\\rTitle: Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate\\r  Models for IRT Assessment\\rAuthors: Jingshen Zhang and Jiajun Xie and Xinying Qiu\\rCategories: cs.CL cs.AI cs.CY\\r\\\\\\\\\\r  Item difficulty plays a crucial role in adaptive testing. However, few works\\rhave focused on generating questions of varying difficulty levels, especially\\rfor multiple-choice (MC) cloze tests. We propose training pre-trained language\\rmodels (PLMs) as surrogate models to enable item response theory (IRT)\\rassessment, avoiding the need for human test subjects. We also propose two\\rstrategies to control the difficulty levels of both the gaps and the\\rdistractors using ranking rules to reduce invalid distractors. Experimentation\\ron a benchmark dataset demonstrates that our proposed framework and methods can\\reffectively control and evaluate the difficulty levels of MC cloze tests.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01456 ,  6489kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01461\\rDate: Sun, 3 Mar 2024 09:55:35 GMT   (7247kb,D)\\r\\rTitle: Answerability in Retrieval-Augmented Open-Domain Question Answering\\rAuthors: Rustam Abdumalikov, Pasquale Minervini and Yova Kementchedjhieva\\rCategories: cs.CL\\rComments: 5 pages, 3 tables\\r\\\\\\\\\\r  The performance of Open-Domain Question Answering (ODQA) retrieval systems\\rcan exhibit sub-optimal behavior, providing text excerpts with varying degrees\\rof irrelevance. Unfortunately, many existing ODQA datasets lack examples\\rspecifically targeting the identification of irrelevant text excerpts. Previous\\rattempts to address this gap have relied on a simplistic approach of pairing\\rquestions with random text excerpts. This paper aims to investigate the\\reffectiveness of models trained using this randomized strategy, uncovering an\\rimportant limitation in their ability to generalize to irrelevant text excerpts\\rwith high semantic overlap. As a result, we observed a substantial decrease in\\rpredictive accuracy, from 98% to 1%. To address this limitation, we discovered\\ran efficient approach for training models to recognize such excerpts. By\\rleveraging unanswerable pairs from the SQuAD 2.0 dataset, our models achieve a\\rnearly perfect (~100%) accuracy when confronted with these challenging text\\rexcerpts.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01461 ,  7247kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01469\\rDate: Sun, 3 Mar 2024 10:31:49 GMT   (934kb,D)\\r\\rTitle: KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean\\r  Healthcare Professional Licensing Examinations\\rAuthors: Sunjun Kweon, Byungjin Choi, Minkyu Kim, Rae Woong Park, Edward Choi\\rCategories: cs.CL\\r\\\\\\\\\\r  We introduce KorMedMCQA, the first Korean multiple-choice question answering\\r(MCQA) benchmark derived from Korean healthcare professional licensing\\rexaminations, covering from the year 2012 to year 2023. This dataset consists\\rof a selection of questions from the license examinations for doctors, nurses,\\rand pharmacists, featuring a diverse array of subjects. We conduct baseline\\rexperiments on various large language models, including\\rproprietary/open-source, multilingual/Korean-additional pretrained, and\\rclinical context pretrained models, highlighting the potential for further\\renhancements. We make our data publicly available on HuggingFace and provide a\\revaluation script via LM-Harness, inviting further exploration and advancement\\rin Korean healthcare environments.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01469 ,  934kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01479\\rDate: Sun, 3 Mar 2024 11:13:44 GMT   (364kb,D)\\r\\rTitle: Align-to-Distill: Trainable Attention Alignment for Knowledge\\r  Distillation in Neural Machine Translation\\rAuthors: Heegon Jin, Seonil Son, Jemin Park, Youngseok Kim, Hyungjong Noh,\\r  Yeonsoo Lee\\rCategories: cs.CL cs.AI\\rComments: Accepted to LREC-COLING 2024\\rMSC-class: 68T50\\rACM-class: I.2.7\\r\\\\\\\\\\r  The advent of scalable deep models and large datasets has improved the\\rperformance of Neural Machine Translation. Knowledge Distillation (KD) enhances\\refficiency by transferring knowledge from a teacher model to a more compact\\rstudent model. However, KD approaches to Transformer architecture often rely on\\rheuristics, particularly when deciding which teacher layers to distill from. In\\rthis paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to\\raddress the feature mapping problem by adaptively aligning student attention\\rheads with their teacher counterparts during training. The Attention Alignment\\rModule in A2D performs a dense head-by-head comparison between student and\\rteacher attention heads across layers, turning the combinatorial mapping\\rheuristics into a learning problem. Our experiments show the efficacy of A2D,\\rdemonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb\\rand WMT-2014 En->De, respectively, compared to Transformer baselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01479 ,  364kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01481\\rDate: Sun, 3 Mar 2024 11:19:26 GMT   (325kb,D)\\r\\rTitle: Infusing Knowledge into Large Language Models with Contextual Prompts\\rAuthors: Kinshuk Vasisht, Balaji Ganesan, Vikas Kumar, Vasudha Bhatnagar\\rCategories: cs.CL\\rComments: 5 pages, 1 figure, In Proceedings of ICON 2023\\r\\\\\\\\\\r  Knowledge infusion is a promising method for enhancing Large Language Models\\rfor domain-specific NLP tasks rather than pre-training models over large data\\rfrom scratch. These augmented LLMs typically depend on additional pre-training\\ror knowledge prompts from an existing knowledge graph, which is impractical in\\rmany applications. In contrast, knowledge infusion directly from relevant\\rdocuments is more generalisable and alleviates the need for structured\\rknowledge graphs while also being useful for entities that are usually not\\rfound in any knowledge graph. With this motivation, we propose a simple yet\\rgeneralisable approach for knowledge infusion by generating prompts from the\\rcontext in the input text. Our experiments show the effectiveness of our\\rapproach which we evaluate by probing the fine-tuned LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01481 ,  325kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01509\\rDate: Sun, 3 Mar 2024 13:14:47 GMT   (200kb,D)\\r\\rTitle: Fantastic Semantics and Where to Find Them: Investigating Which Layers\\r  of Generative LLMs Reflect Lexical Semantics\\rAuthors: Zhu Liu, Cunliang Kong, Ying Liu and Maosong Sun\\rCategories: cs.CL\\rComments: This work was completed on February 15th, 2024, and submitted to ACL\\r  2024\\r\\\\\\\\\\r  Large language models have achieved remarkable success in general language\\runderstanding tasks. However, as a family of generative methods with the\\robjective of next token prediction, the semantic evolution with the depth of\\rthese models are not fully explored, unlike their predecessors, such as\\rBERT-like architectures. In this paper, we specifically investigate the\\rbottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by\\rprobing its hidden states at the end of each layer using a contextualized word\\ridentification task. Our experiments show that the representations in lower\\rlayers encode lexical semantics, while the higher layers, with weaker semantic\\rinduction, are responsible for prediction. This is in contrast to models with\\rdiscriminative objectives, such as mask language modeling, where the higher\\rlayers obtain better lexical semantics. The conclusion is further supported by\\rthe monotonic increase in performance via the hidden states for the last\\rmeaningless symbols, such as punctuation, in the prompting strategy.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01509 ,  200kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01518\\rDate: Sun, 3 Mar 2024 14:03:48 GMT   (1486kb,D)\\r\\rTitle: Revisiting Dynamic Evaluation: Online Adaptation for Large Language\\r  Models\\rAuthors: Amal Rannen-Triki, Jorg Bornschein, Razvan Pascanu, Marcus Hutter,\\r  Andras Gy\\\\orgy, Alexandre Galashov, Yee Whye Teh, Michalis K. Titsias\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  We consider the problem of online fine tuning the parameters of a language\\rmodel at test time, also known as dynamic evaluation. While it is generally\\rknown that this approach improves the overall predictive performance,\\respecially when considering distributional shift between training and\\revaluation data, we here emphasize the perspective that online adaptation turns\\rparameters into temporally changing states and provides a form of\\rcontext-length extension with memory in weights, more in line with the concept\\rof memory in neuroscience. We pay particular attention to the speed of\\radaptation (in terms of sample efficiency),sensitivity to the overall\\rdistributional drift, and the computational overhead for performing gradient\\rcomputations and parameter updates. Our empirical study provides insights on\\rwhen online adaptation is particularly interesting. We highlight that with\\ronline adaptation the conceptual distinction between in-context learning and\\rfine tuning blurs: both are methods to condition the model on previously\\robserved tokens.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01518 ,  1486kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01528\\rDate: Sun, 3 Mar 2024 14:59:47 GMT   (1299kb,D)\\r\\rTitle: Leveraging Biomolecule and Natural Language through Multi-Modal\\r  Learning: A Survey\\rAuthors: Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao\\r  Qin, and Rui Yan\\rCategories: cs.CL cs.AI q-bio.BM\\rComments: Survey Paper. 27 pages, 9 figures, and 3 tables\\r\\\\\\\\\\r  The integration of biomolecular modeling with natural language (BL) has\\remerged as a promising interdisciplinary area at the intersection of artificial\\rintelligence, chemistry and biology. This approach leverages the rich,\\rmultifaceted descriptions of biomolecules contained within textual data sources\\rto enhance our fundamental understanding and enable downstream computational\\rtasks such as biomolecule property prediction. The fusion of the nuanced\\rnarratives expressed through natural language with the structural and\\rfunctional specifics of biomolecules described via various molecular modeling\\rtechniques opens new avenues for comprehensively representing and analyzing\\rbiomolecules. By incorporating the contextual language data that surrounds\\rbiomolecules into their modeling, BL aims to capture a holistic view\\rencompassing both the symbolic qualities conveyed through language as well as\\rquantitative structural characteristics. In this review, we provide an\\rextensive analysis of recent advancements achieved through cross modeling of\\rbiomolecules and natural language. (1) We begin by outlining the technical\\rrepresentations of biomolecules employed, including sequences, 2D graphs, and\\r3D structures. (2) We then examine in depth the rationale and key objectives\\runderlying effective multi-modal integration of language and molecular data\\rsources. (3) We subsequently survey the practical applications enabled to date\\rin this developing research area. (4) We also compile and summarize the\\ravailable resources and datasets to facilitate future work. (5) Looking ahead,\\rwe identify several promising research directions worthy of further exploration\\rand investment to continue advancing the field. The related resources and\\rcontents are updating in\\r\\\\url{https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01528 ,  1299kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01548\\rDate: Sun, 3 Mar 2024 15:53:41 GMT   (1339kb,D)\\r\\rTitle: In-Context Sharpness as Alerts: An Inner Representation Perspective for\\r  Hallucination Mitigation\\rAuthors: Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang\\r  Gao, Junxian He\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  Large language models (LLMs) frequently hallucinate and produce factual\\rerrors, yet our understanding of why they make these errors remains limited. In\\rthis study, we delve into the underlying mechanisms of LLM hallucinations from\\rthe perspective of inner representations, and discover a salient pattern\\rassociated with hallucinations: correct generations tend to have sharper\\rcontext activations in the hidden states of the in-context tokens, compared to\\rthe incorrect ones. Leveraging this insight, we propose an entropy-based metric\\rto quantify the ``sharpness'' among the in-context hidden states and\\rincorporate it into the decoding process to formulate a constrained decoding\\rapproach. Experiments on various knowledge-seeking and hallucination benchmarks\\rdemonstrate our approach's consistent effectiveness, for example, achieving up\\rto an 8.6 point improvement on TruthfulQA. We believe this study can improve\\rour understanding of hallucinations and serve as a practical solution for\\rhallucination mitigation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01548 ,  1339kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01570\\rDate: Sun, 3 Mar 2024 17:35:52 GMT   (1381kb,D)\\r\\rTitle: SERVAL: Synergy Learning between Vertical Models and LLMs towards\\r  Oracle-Level Zero-shot Medical Prediction\\rAuthors: Jiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun,\\r  Jian Wu\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  Recent development of large language models (LLMs) has exhibited impressive\\rzero-shot proficiency on generic and common sense questions. However, LLMs'\\rapplication on domain-specific vertical questions still lags behind, primarily\\rdue to the humiliation problems and deficiencies in vertical knowledge.\\rFurthermore, the vertical data annotation process often requires\\rlabor-intensive expert involvement, thereby presenting an additional challenge\\rin enhancing the model's vertical capabilities. In this paper, we propose\\rSERVAL, a synergy learning pipeline designed for unsupervised development of\\rvertical capabilities in both LLMs and small models by mutual enhancement.\\rSpecifically, SERVAL utilizes the LLM's zero-shot outputs as annotations,\\rleveraging its confidence to teach a robust vertical model from scratch.\\rReversely, the trained vertical model guides the LLM fine-tuning to enhance its\\rzero-shot capability, progressively improving both models through an iterative\\rprocess. In medical domain, known for complex vertical knowledge and costly\\rannotations, comprehensive experiments show that, without access to any gold\\rlabels, SERVAL with the synergy learning of OpenAI GPT-3.5 and a simple model\\rattains fully-supervised competitive performance across ten widely used medical\\rdatasets. These datasets represent vertically specialized medical diagnostic\\rscenarios (e.g., diabetes, heart diseases, COVID-19), highlighting the\\rpotential of SERVAL in refining the vertical capabilities of LLMs and training\\rvertical models from scratch, all achieved without the need for annotations.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01570 ,  1381kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01580\\rDate: Sun, 3 Mar 2024 18:08:30 GMT   (8038kb,D)\\r\\rTitle: Enhancing Neural Machine Translation of Low-Resource Languages: Corpus\\r  Development, Human Evaluation and Explainable AI Architectures\\rAuthors: S\\\\'eamus Lankford\\rCategories: cs.CL cs.AI\\rComments: PhD thesis\\r\\\\\\\\\\r  In the current machine translation (MT) landscape, the Transformer\\rarchitecture stands out as the gold standard, especially for high-resource\\rlanguage pairs. This research delves into its efficacy for low-resource\\rlanguage pairs including both the English$\\\\leftrightarrow$Irish and\\rEnglish$\\\\leftrightarrow$Marathi language pairs. Notably, the study identifies\\rthe optimal hyperparameters and subword model type to significantly improve the\\rtranslation quality of Transformer models for low-resource language pairs.\\r  The scarcity of parallel datasets for low-resource languages can hinder MT\\rdevelopment. To address this, gaHealth was developed, the first bilingual\\rcorpus of health data for the Irish language. Focusing on the health domain,\\rmodels developed using this in-domain dataset exhibited very significant\\rimprovements in BLEU score when compared with models from the LoResMT2021\\rShared Task. A subsequent human evaluation using the multidimensional quality\\rmetrics error taxonomy showcased the superior performance of the Transformer\\rsystem in reducing both accuracy and fluency errors compared to an RNN-based\\rcounterpart.\\r  Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source\\rapplications streamlined for the development, fine-tuning, and deployment of\\rneural machine translation models. These tools considerably simplify the setup\\rand evaluation process, making MT more accessible to both developers and\\rtranslators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes\\reco-friendly natural language processing research by highlighting the\\renvironmental footprint of model development. Fine-tuning of MLLMs by adaptMLLM\\rdemonstrated advancements in translation performance for two low-resource\\rlanguage pairs: English$\\\\leftrightarrow$Irish and\\rEnglish$\\\\leftrightarrow$Marathi, compared to baselines from the LoResMT2021\\rShared Task.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01580 ,  8038kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01616\\rDate: Sun, 3 Mar 2024 21:24:35 GMT   (147kb,D)\\r\\rTitle: Towards Comprehensive Vietnamese Retrieval-Augmented Generation and\\r  Large Language Models\\rAuthors: Nguyen Quang Duc, Le Hai Son, Nguyen Duc Nhan, Nguyen Dich Nhat Minh,\\r  Le Thanh Huong, Dinh Viet Sang\\rCategories: cs.CL\\r\\\\\\\\\\r  This paper presents our contributions towards advancing the state of\\rVietnamese language understanding and generation through the development and\\rdissemination of open datasets and pre-trained models for Vietnamese\\rRetrieval-Augmented Generation (RAG) and Large Language Models (LLMs).\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01616 ,  147kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01638\\rDate: Sun, 3 Mar 2024 23:10:36 GMT   (931kb,D)\\r\\rTitle: Multi-level Product Category Prediction through Text Classification\\rAuthors: Wesley Ferreira Maia, Angelo Carmignani, Gabriel Bortoli, Lucas\\r  Maretti, David Luz, Daniel Camilo Fuentes Guzman, Marcos Jardel Henriques,\\r  Francisco Louzada Neto\\rCategories: cs.CL\\r\\\\\\\\\\r  This article investigates applying advanced machine learning models,\\rspecifically LSTM and BERT, for text classification to predict multiple\\rcategories in the retail sector. The study demonstrates how applying data\\raugmentation techniques and the focal loss function can significantly enhance\\raccuracy in classifying products into multiple categories using a robust\\rBrazilian retail dataset. The LSTM model, enriched with Brazilian word\\rembedding, and BERT, known for its effectiveness in understanding complex\\rcontexts, were adapted and optimized for this specific task. The results showed\\rthat the BERT model, with an F1 Macro Score of up to $99\\\\%$ for segments,\\r$96\\\\%$ for categories and subcategories and $93\\\\%$ for name products,\\routperformed LSTM in more detailed categories. However, LSTM also achieved high\\rperformance, especially after applying data augmentation and focal loss\\rtechniques. These results underscore the effectiveness of NLP techniques in\\rretail and highlight the importance of the careful selection of modelling and\\rpreprocessing strategies. This work contributes significantly to the field of\\rNLP in retail, providing valuable insights for future research and practical\\rapplications.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01638 ,  931kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01698\\rDate: Mon, 4 Mar 2024 03:21:40 GMT   (2556kb,D)\\r\\rTitle: Hypertext Entity Extraction in Webpage\\rAuthors: Yifei Yang, Tianqiao Liu, Bo Shao, Hai Zhao, Linjun Shou, Ming Gong,\\r  Daxin Jiang\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Webpage entity extraction is a fundamental natural language processing task\\rin both research and applications. Nowadays, the majority of webpage entity\\rextraction models are trained on structured datasets which strive to retain\\rtextual content and its structure information. However, existing datasets all\\roverlook the rich hypertext features (e.g., font color, font size) which show\\rtheir effectiveness in previous works. To this end, we first collect a\\r\\\\textbf{H}ypertext \\\\textbf{E}ntity \\\\textbf{E}xtraction \\\\textbf{D}ataset\\r(\\\\textit{HEED}) from the e-commerce domains, scraping both the text and the\\rcorresponding explicit hypertext features with high-quality manual entity\\rannotations. Furthermore, we present the \\\\textbf{Mo}E-based \\\\textbf{E}ntity\\r\\\\textbf{E}xtraction \\\\textbf{F}ramework (\\\\textit{MoEEF}), which efficiently\\rintegrates multiple features to enhance model performance by Mixture of Experts\\rand outperforms strong baselines, including the state-of-the-art small-scale\\rmodels and GPT-3.5-turbo. Moreover, the effectiveness of hypertext features in\\r\\\\textit{HEED} and several model components in \\\\textit{MoEEF} are analyzed.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01698 ,  2556kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01699\\rDate: Mon, 4 Mar 2024 03:24:18 GMT   (1259kb,D)\\r\\rTitle: Brilla AI: AI Contestant for the National Science and Maths Quiz\\rAuthors: George Boateng, Jonathan Abrefah Mensah, Kevin Takyi Yeboah, William\\r  Edor, Andrew Kojo Mensah-Onumah, Naafi Dasana Ibrahim, and Nana Sam Yeboah\\rCategories: cs.CL cs.AI cs.CY cs.SD eess.AS\\rComments: 13 pages. Under review at the 25th International Conference on AI in\\r  Education (AIED 2024)\\r\\\\\\\\\\r  The African continent lacks enough qualified teachers which hampers the\\rprovision of adequate learning support. An AI could potentially augment the\\refforts of the limited number of teachers, leading to better learning outcomes.\\rTowards that end, this work describes and evaluates the first key output for\\rthe NSMQ AI Grand Challenge, which proposes a robust, real-world benchmark for\\rsuch an AI: Build an AI to compete live in Ghana's National Science and Maths\\rQuiz (NSMQ) competition and win - performing better than the best contestants\\rin all rounds and stages of the competition. The NSMQ is an annual live\\rscience and mathematics competition for senior secondary school students in\\rGhana in which 3 teams of 2 students compete by answering questions across\\rbiology, chemistry, physics, and math in 5 rounds over 5 progressive stages\\runtil a winning team is crowned for that year. In this work, we built Brilla\\rAI, an AI contestant that we deployed to unofficially compete remotely and live\\rin the Riddles round of the 2023 NSMQ Grand Finale, the first of its kind in\\rthe 30-year history of the competition. Brilla AI is currently available as a\\rweb app that livestreams the Riddles round of the contest, and runs 4 machine\\rlearning systems: (1) speech to text (2) question extraction (3) question\\ranswering and (4) text to speech that work together in real-time to quickly and\\raccurately provide an answer, and then say it with a Ghanaian accent. In its\\rdebut, our AI answered one of the 4 riddles ahead of the 3 human contesting\\rteams, unofficially placing second (tied). Improvements and extensions of this\\rAI could potentially be deployed to offer science tutoring to students and\\reventually enable millions across Africa to have one-on-one learning\\rinteractions, democratizing science education.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01699 ,  1259kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01748\\rDate: Mon, 4 Mar 2024 05:55:01 GMT   (330kb,D)\\r\\rTitle: Decode Neural signal as Speech\\rAuthors: Yiqian Yang, Yiqun Duan, Qiang Zhang, Renjing Xu, Hui Xiong\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Decoding language from brain dynamics is an important open direction in the\\rrealm of brain-computer interface (BCI), especially considering the rapid\\rgrowth of large language models. Compared to invasive-based signals which\\rrequire electrode implantation surgery, non-invasive neural signals (e.g. EEG,\\rMEG) have attracted increasing attention considering their safety and\\rgenerality. However, the exploration is not adequate in three aspects: 1)\\rprevious methods mainly focus on EEG but none of the previous works address\\rthis problem on MEG with better signal quality; 2) prior works have\\rpredominantly used ``teacher-forcing during generative decoding, which is\\rimpractical; 3) prior works are mostly ``BART-based not fully auto-regressive,\\rwhich performs better in other sequence tasks. In this paper, we explore the\\rbrain-to-text translation of MEG signals in a speech-decoding formation. Here\\rwe are the first to investigate a cross-attention-based ``whisper model for\\rgenerating text directly from MEG signals without teacher forcing. Our model\\rachieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \\\\&\\rteacher-forcing on two major datasets (\\\\textit{GWilliams} and\\r\\\\textit{Schoffelen}). This paper conducts a comprehensive review to understand\\rhow speech decoding formation performs on the neural decoding tasks, including\\rpretraining initialization, training \\\\& evaluation set splitting, augmentation,\\rand scaling law.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01748 ,  330kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01749\\rDate: Mon, 4 Mar 2024 05:57:50 GMT   (884kb,D)\\r\\rTitle: Differentially Private Synthetic Data via Foundation Model APIs 2: Text\\rAuthors: Chulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin\\r  A Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li,\\r  Sergey Yekhanin\\rCategories: cs.CL\\r\\\\\\\\\\r  Text data has become extremely valuable due to the emergence of machine\\rlearning algorithms that learn from it. A lot of high-quality text data\\rgenerated in the real world is private and therefore cannot be shared or used\\rfreely due to privacy concerns. Generating synthetic replicas of private text\\rdata with a formal privacy guarantee, i.e., differential privacy (DP), offers a\\rpromising and scalable solution. However, existing methods necessitate DP\\rfinetuning of large language models (LLMs) on private data to generate DP\\rsynthetic data. This approach is not viable for proprietary LLMs (e.g.,\\rGPT-3.5) and also demands considerable computational resources for open-source\\rLLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)\\ralgorithm to generate DP synthetic images with only API access to diffusion\\rmodels. In this work, we propose an augmented PE algorithm, named Aug-PE, that\\rapplies to the complex setting of text. We use API access to an LLM and\\rgenerate DP synthetic text without any model training. We conduct comprehensive\\rexperiments on three benchmark datasets. Our results demonstrate that Aug-PE\\rproduces DP synthetic text that yields competitive utility with the SOTA DP\\rfinetuning baselines. This underscores the feasibility of relying solely on API\\raccess of LLMs to produce high-quality DP synthetic texts, thereby facilitating\\rmore accessible routes to privacy-preserving LLM applications. Our code and\\rdata are available at https://github.com/AI-secure/aug-pe.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01749 ,  884kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01754\\rDate: Mon, 4 Mar 2024 06:20:31 GMT   (170kb,D)\\r\\rTitle: Derivative-Free Optimization for Low-Rank Adaptation in Large Language\\r  Models\\rAuthors: Feihu Jin, Yin Liu, Ying Tan\\rCategories: cs.CL\\rComments: 14 pages, 4 figures, 5 tables\\r\\\\\\\\\\r  Parameter-efficient tuning methods such as LoRA could achieve comparable\\rperformance to model tuning by tuning a small portion of the parameters.\\rHowever, substantial computational resources are still required, as this\\rprocess involves calculating gradients and performing back-propagation\\rthroughout the model. Much effort has recently been devoted to utilizing the\\rderivative-free optimization method to eschew the computation of gradients and\\rshowcase an augmented level of robustness in few-shot settings. In this paper,\\rwe prepend the low-rank modules into each self-attention layer of the model and\\remploy two derivative-free optimization methods to optimize these low-rank\\rmodules at each layer alternately. Extensive results on various tasks and\\rlanguage models demonstrate that our proposed method achieves substantial\\rimprovement and exhibits clear advantages in memory usage and convergence speed\\rcompared to existing gradient-based parameter-efficient tuning and\\rderivative-free optimization methods in few-shot settings.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01754 ,  170kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01767\\rDate: Mon, 4 Mar 2024 06:52:19 GMT   (14620kb,D)\\r\\rTitle: KeNet:Knowledge-enhanced Doc-Label Attention Network for Multi-label\\r  text classification\\rAuthors: Bo Li and Yuyan Chen and Liang Zeng\\rCategories: cs.CL\\rComments: Accepted in ICASSP 2024\\r\\\\\\\\\\r  Multi-Label Text Classification (MLTC) is a fundamental task in the field of\\rNatural Language Processing (NLP) that involves the assignment of multiple\\rlabels to a given text. MLTC has gained significant importance and has been\\rwidely applied in various domains such as topic recognition, recommendation\\rsystems, sentiment analysis, and information retrieval. However, traditional\\rmachine learning and Deep neural network have not yet addressed certain issues,\\rsuch as the fact that some documents are brief but have a large number of\\rlabels and how to establish relationships between the labels. It is imperative\\rto additionally acknowledge that the significance of knowledge is substantiated\\rin the realm of MLTC. To address this issue, we provide a novel approach known\\ras Knowledge-enhanced Doc-Label Attention Network (KeNet). Specifically, we\\rdesign an Attention Network that incorporates external knowledge, label\\rembedding, and a comprehensive attention mechanism. In contrast to conventional\\rmethods, we use comprehensive representation of documents, knowledge and labels\\rto predict all labels for each single text. Our approach has been validated by\\rcomprehensive research conducted on three multi-label datasets. Experimental\\rresults demonstrate that our method outperforms state-of-the-art MLTC method.\\rAdditionally, a case study is undertaken to illustrate the practical\\rimplementation of KeNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01767 ,  14620kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01774\\rDate: Mon, 4 Mar 2024 07:06:41 GMT   (833kb,D)\\r\\rTitle: WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search\\r  Results with Citations\\rAuthors: Haolin Deng, Chang Wang, Xin Li, Dezhang Yuan, Junlang Zhan, Tianhua\\r  Zhou, Jin Ma, Jun Gao, Ruifeng Xu\\rCategories: cs.CL\\rComments: 19 pages, 7 figures\\r\\\\\\\\\\r  Enhancing the attribution in large language models (LLMs) is a crucial task.\\rOne feasible approach is to enable LLMs to cite external sources that support\\rtheir generations. However, existing datasets and evaluation methods in this\\rdomain still exhibit notable limitations. In this work, we formulate the task\\rof attributed query-focused summarization (AQFS) and present WebCiteS, a\\rChinese dataset featuring 7k human-annotated summaries with citations. WebCiteS\\rderives from real-world user queries and web search results, offering a\\rvaluable resource for model training and evaluation. Prior works in attribution\\revaluation do not differentiate between groundedness errors and citation\\rerrors. They also fall short in automatically verifying sentences that draw\\rpartial support from multiple sources. We tackle these issues by developing\\rdetailed metrics and enabling the automatic evaluator to decompose the\\rsentences into sub-claims for fine-grained verification. Our comprehensive\\revaluation of both open-source and proprietary models on WebCiteS highlights\\rthe challenge LLMs face in correctly citing sources, underscoring the necessity\\rfor further improvement. The dataset and code will be open-sourced to\\rfacilitate further research in this crucial field.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01774 ,  833kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01777\\rDate: Mon, 4 Mar 2024 07:10:31 GMT   (7053kb,D)\\r\\rTitle: NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language\\r  Models\\rAuthors: Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li,\\r  Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, Yongfeng Zhang\\rCategories: cs.CL cs.CV\\rComments: 16 pages, 10 figures, 2 tables\\r\\\\\\\\\\r  Understanding the reasoning capabilities of Multimodal Large Language Models\\r(MLLMs) is an important area of research. In this study, we introduce a dynamic\\rbenchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluating\\rthe pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to\\rdisentangle the effect of various factors such as image recognition and\\rinstruction following, from the overall performance of the models, allowing us\\rto focus solely on evaluating their reasoning abilities. Our findings reveal\\rsignificant discrepancies in reasoning abilities across different models and\\rhighlight the relatively weak performance of MLLMs compared to LLMs in terms of\\rreasoning. We also investigate the impact of different prompting styles,\\rincluding visual, text, and combined vision and text prompts, on the reasoning\\rabilities of MLLMs, demonstrating the different impacts of multimodal inputs in\\rmodel performance. Unlike traditional benchmarks, which primarily focus on\\rstatic evaluations, our benchmark will update on a monthly basis to prevent\\roverfitting and ensure a more accurate evaluation of the models. We believe\\rthat this benchmark can aid understand and guide the further development of\\rreasoning abilities in MLLMs. The benchmark dataset and code are available at\\rhttps://github.com/lizhouf/NPHardEval4V\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01777 ,  7053kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01811\\rDate: Mon, 4 Mar 2024 07:58:26 GMT   (246kb,D)\\r\\rTitle: Enhancing Multi-Domain Automatic Short Answer Grading through an\\r  Explainable Neuro-Symbolic Pipeline\\rAuthors: Felix K\\\\unnecke, Anna Filighera, Colin Leong, Tim Steuer\\rCategories: cs.CL\\r\\\\\\\\\\r  Grading short answer questions automatically with interpretable reasoning\\rbehind the grading decision is a challenging goal for current transformer\\rapproaches. Justification cue detection, in combination with logical reasoners,\\rhas shown a promising direction for neuro-symbolic architectures in ASAG. But,\\rone of the main challenges is the requirement of annotated justification cues\\rin the students' responses, which only exist for a few ASAG datasets. To\\rovercome this challenge, we contribute (1) a weakly supervised annotation\\rprocedure for justification cues in ASAG datasets, and (2) a neuro-symbolic\\rmodel for explainable ASAG based on justification cues. Our approach improves\\rupon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short\\rAnswer Feedback dataset in a bilingual, multi-domain, and multi-question\\rtraining setup. This result shows that our approach provides a promising\\rdirection for generating high-quality grades and accompanying explanations for\\rfuture research in ASAG and educational NLP.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01811 ,  246kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01817\\rDate: Mon, 4 Mar 2024 08:05:34 GMT   (6904kb,D)\\r\\rTitle: NusaBERT: Teaching IndoBERT to be Multilingual and Multicultural\\rAuthors: Wilson Wongso, David Samuel Setiawan, Steven Limcorn, Ananto\\r  Joyoadikusumo\\rCategories: cs.CL\\r\\\\\\\\\\r  Indonesia's linguistic landscape is remarkably diverse, encompassing over 700\\rlanguages and dialects, making it one of the world's most linguistically rich\\rnations. This diversity, coupled with the widespread practice of code-switching\\rand the presence of low-resource regional languages, presents unique challenges\\rfor modern pre-trained language models. In response to these challenges, we\\rdeveloped NusaBERT, building upon IndoBERT by incorporating vocabulary\\rexpansion and leveraging a diverse multilingual corpus that includes regional\\rlanguages and dialects. Through rigorous evaluation across a range of\\rbenchmarks, NusaBERT demonstrates state-of-the-art performance in tasks\\rinvolving multiple languages of Indonesia, paving the way for future natural\\rlanguage understanding research for under-represented languages.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01817 ,  6904kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01841\\rDate: Mon, 4 Mar 2024 08:38:56 GMT   (1471kb,D)\\r\\rTitle: Making Pre-trained Language Models Great on Tabular Prediction\\rAuthors: Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun,\\r  Jian Wu, Jintai Chen\\rCategories: cs.CL cs.LG\\rComments: Accepted to ICLR 2024 as spotlight presentation (Notable Top 5%).\\r  Codes will be available soon\\r\\\\\\\\\\r  The transferability of deep neural networks (DNNs) has made significant\\rprogress in image and language processing. However, due to the heterogeneity\\ramong tables, such DNN bonus is still far from being well exploited on tabular\\rdata prediction (e.g., regression or classification tasks). Condensing\\rknowledge from diverse domains, language models (LMs) possess the capability to\\rcomprehend feature names from various tables, potentially serving as versatile\\rlearners in transferring knowledge across distinct tables and diverse\\rprediction tasks, but their discrete text representation space is inherently\\rincompatible with numerical feature values in tables. In this paper, we present\\rTP-BERTa, a specifically pre-trained LM model for tabular data prediction.\\rConcretely, a novel relative magnitude tokenization converts scalar numerical\\rfeature values to finely discrete, high-dimensional tokens, and an\\rintra-feature attention approach integrates feature values with the\\rcorresponding feature names. Comprehensive experiments demonstrate that our\\rpre-trained TP-BERTa leads the performance among tabular DNNs and is\\rcompetitive with Gradient Boosted Decision Tree models in typical tabular data\\rregime.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01841 ,  1471kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01848\\rDate: Mon, 4 Mar 2024 08:55:34 GMT   (9469kb,D)\\r\\rTitle: CET2: Modelling Topic Transitions for Coherent and Engaging\\r  Knowledge-Grounded Conversations\\rAuthors: Lin Xu, Qixian Zhou, Jinlan Fu, See-Kiong Ng\\rCategories: cs.CL\\rComments: Accepted by TASLP\\r\\\\\\\\\\r  Knowledge-grounded dialogue systems aim to generate coherent and engaging\\rresponses based on the dialogue contexts and selected external knowledge.\\rPrevious knowledge selection methods tend to rely too heavily on the dialogue\\rcontexts or over-emphasize the new information in the selected knowledge,\\rresulting in the selection of repetitious or incongruous knowledge and further\\rgenerating repetitive or incoherent responses, as the generation of the\\rresponse depends on the chosen knowledge. To address these shortcomings, we\\rintroduce a Coherent and Engaging Topic Transition (CET2) framework to model\\rtopic transitions for selecting knowledge that is coherent to the context of\\rthe conversations while providing adequate knowledge diversity for topic\\rdevelopment. Our CET2 framework considers multiple factors for knowledge\\rselection, including valid transition logic from dialogue contexts to the\\rfollowing topics and systematic comparisons between available knowledge\\rcandidates. Extensive experiments on two public benchmarks demonstrate the\\rsuperiority and the better generalization ability of CET2 on knowledge\\rselection. This is due to our well-designed transition features and comparative\\rknowledge selection strategy, which are more transferable to conversations\\rabout unseen topics. Analysis of fine-grained knowledge selection accuracy also\\rshows that CET2 can better balance topic entailment (contextual coherence) and\\rdevelopment (knowledge diversity) in dialogue than existing approaches.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01848 ,  9469kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01851\\rDate: Mon, 4 Mar 2024 09:01:10 GMT   (1421kb,D)\\r\\rTitle: Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral\\rAuthors: Yiming Cui, Xin Yao\\rCategories: cs.CL cs.AI\\rComments: 13 pages\\r\\\\\\\\\\r  Mixtral, a representative sparse mixture of experts (SMoE) language model,\\rhas received significant attention due to its unique model design and superior\\rperformance. Based on Mixtral-8x7B-v0.1, in this paper, we propose\\rChinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language\\rabilities by adopting further pre-training and instruction fine-tuning.\\rExperimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct\\rsuccessfully improve Chinese understanding and generation performance while\\rretaining the original English abilities. Then, we discuss several key\\rquestions when performing language adaptation on large language models,\\rincluding the necessity of extending the language-specific vocabulary and the\\rchoice of the initialization model (foundation model v.s. instruction model),\\rby providing empirical results and analysis. We also present the visualizations\\rof each expert to examine their importance on downstream tasks. Our resources\\rare publicly available through \\\\url{https://github.com/ymcui/Chinese-Mixtral}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01851 ,  1421kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01858\\rDate: Mon, 4 Mar 2024 09:13:33 GMT   (173kb,D)\\r\\rTitle: An Improved Traditional Chinese Evaluation Suite for Foundation Model\\rAuthors: Zhi-Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Sega Cheng, Hong-Han Shuai\\rCategories: cs.CL\\r\\\\\\\\\\r  We present TMMLU+, a comprehensive dataset designed for the Traditional\\rChinese massive multitask language understanding dataset. TMMLU+ is a\\rmultiple-choice question-answering dataset with 66 subjects from elementary to\\rprofessional level. Compared to its predecessor, TMMLU, TMMLU+ is six times\\rlarger and boasts a more balanced subject distribution. We included benchmark\\rresults in TMMLU+ from closed-source models and 24 open-weight Chinese large\\rlanguage models of parameters ranging from 1.8B to 72B. Our findings reveal\\rthat Traditional Chinese models still trail behind their Simplified Chinese\\rcounterparts. Additionally, current large language models have yet to\\routperform human performance in average scores. We publicly release our dataset\\rand the corresponding benchmark source code.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01858 ,  173kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01886\\rDate: Mon, 4 Mar 2024 09:48:55 GMT   (2451kb,D)\\r\\rTitle: FCDS: Fusing Constituency and Dependency Syntax into Document-Level\\r  Relation Extraction\\rAuthors: Xudong Zhu, Zhao Kang, Bei Hui\\rCategories: cs.CL cs.AI\\rComments: Appear in COLING 2024\\r\\\\\\\\\\r  Document-level Relation Extraction (DocRE) aims to identify relation labels\\rbetween entities within a single document. It requires handling several\\rsentences and reasoning over them. State-of-the-art DocRE methods use a graph\\rstructure to connect entities across the document to capture dependency syntax\\rinformation. However, this is insufficient to fully exploit the rich syntax\\rinformation in the document. In this work, we propose to fuse constituency and\\rdependency syntax into DocRE. It uses constituency syntax to aggregate the\\rwhole sentence information and select the instructive sentences for the pairs\\rof targets. It exploits the dependency syntax in a graph structure with\\rconstituency syntax enhancement and chooses the path between entity pairs based\\ron the dependency graph. The experimental results on datasets from various\\rdomains demonstrate the effectiveness of the proposed method. The code is\\rpublicly available at this url.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01886 ,  2451kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01897\\rDate: Mon, 4 Mar 2024 09:56:47 GMT   (64kb)\\r\\rTitle: Fostering the Ecosystem of Open Neural Encoders for Portuguese with\\r  Albertina PT* Family\\rAuthors: Rodrigo Santos, Jo\\\\~ao Rodrigues, Lu\\\\'is Gomes, Jo\\\\~ao Silva,\\r  Ant\\\\'onio Branco, Henrique Lopes Cardoso, Tom\\\\'as Freitas Os\\\\'orio, Bernardo\\r  Leite\\rCategories: cs.CL\\r\\\\\\\\\\r  To foster the neural encoding of Portuguese, this paper contributes\\rfoundation encoder models that represent an expansion of the still very scarce\\recosystem of large language models specifically developed for this language\\rthat are fully open, in the sense that they are open source and openly\\rdistributed for free under an open license for any purpose, thus including\\rresearch and commercial usages. Like most languages other than English,\\rPortuguese is low-resourced in terms of these foundational language resources,\\rthere being the inaugural 900 million parameter Albertina and 335 million\\rBertimbau. Taking this couple of models as an inaugural set, we present the\\rextension of the ecosystem of state-of-the-art open encoders for Portuguese\\rwith a larger, top performance-driven model with 1.5 billion parameters, and a\\rsmaller, efficiency-driven model with 100 million parameters. While achieving\\rthis primary goal, further results that are relevant for this ecosystem were\\robtained as well, namely new datasets for Portuguese based on the SuperGLUE\\rbenchmark, which we also distribute openly.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01897 ,  64kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01921\\rDate: Mon, 4 Mar 2024 10:37:48 GMT   (864kb)\\r\\rTitle: Arabic Text Sentiment Analysis: Reinforcing Human-Performed Surveys with\\r  Wider Topic Analysis\\rAuthors: Latifah Almurqren, Ryan Hodgson, Alexandra Cristea\\rCategories: cs.CL\\r\\\\\\\\\\r  Sentiment analysis (SA) has been, and is still, a thriving research area.\\rHowever, the task of Arabic sentiment analysis (ASA) is still underrepresented\\rin the body of research. This study offers the first in-depth and in-breadth\\ranalysis of existing ASA studies of textual content and identifies their common\\rthemes, domains of application, methods, approaches, technologies and\\ralgorithms used. The in-depth study manually analyses 133 ASA papers published\\rin the English language between 2002 and 2020 from four academic databases\\r(SAGE, IEEE, Springer, WILEY) and from Google Scholar. The in-breadth study\\ruses modern, automatic machine learning techniques, such as topic modelling and\\rtemporal analysis, on Open Access resources, to reinforce themes and trends\\ridentified by the prior study, on 2297 ASA publications between 2010-2020. The\\rmain findings show the different approaches used for ASA: machine learning,\\rlexicon-based and hybrid approaches. Other findings include ASA 'winning'\\ralgorithms (SVM, NB, hybrid methods). Deep learning methods, such as LSTM can\\rprovide higher accuracy, but for ASA sometimes the corpora are not large enough\\rto support them. Additionally, whilst there are some ASA corpora and lexicons,\\rmore are required. Specifically, Arabic tweets corpora and datasets are\\rcurrently only moderately sized. Moreover, Arabic lexicons that have high\\rcoverage contain only Modern Standard Arabic (MSA) words, and those with Arabic\\rdialects are quite small. Thus, new corpora need to be created. On the other\\rhand, ASA tools are stringently lacking. There is a need to develop ASA tools\\rthat can be used in industry, as well as in academia, for Arabic text SA.\\rHence, our study offers insights into the challenges associated with ASA\\rresearch and provides suggestions for ways to move the field forward such as\\rlack of Dialectical Arabic resource, Arabic tweets, corpora and data sets for\\rSA.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01921 ,  864kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01924\\rDate: Mon, 4 Mar 2024 10:41:52 GMT   (4464kb,D)\\r\\rTitle: To Generate or to Retrieve? On the Effectiveness of Artificial Contexts\\r  for Medical Open-Domain Question Answering\\rAuthors: Giacomo Frisoni, Alessio Cocchieri, Alex Presepi, Gianluca Moro,\\r  Zaiqiao Meng\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Medical open-domain question answering demands substantial access to\\rspecialized knowledge. Recent efforts have sought to decouple knowledge from\\rmodel parameters, counteracting architectural scaling and allowing for training\\ron common low-resource hardware. The retrieve-then-read paradigm has become\\rubiquitous, with model predictions grounded on relevant knowledge pieces from\\rexternal repositories such as PubMed, textbooks, and UMLS. An alternative path,\\rstill under-explored but made possible by the advent of domain-specific large\\rlanguage models, entails constructing artificial contexts through prompting. As\\ra result, to generate or to retrieve is the modern equivalent of Hamlet's\\rdilemma. This paper presents MedGENIE, the first generate-then-read framework\\rfor multiple-choice question answering in medicine. We conduct extensive\\rexperiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical\\rperspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new\\rstate-of-the-art (SOTA) in the open-book setting of each testbed, even allowing\\ra small-scale reader to outcompete zero-shot closed-book 175B baselines while\\rusing up to 706$\\\\times$ fewer parameters. Overall, our findings reveal that\\rgenerated passages are more effective than retrieved counterparts in attaining\\rhigher accuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01924 ,  4464kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01926\\rDate: Mon, 4 Mar 2024 10:42:08 GMT   (39165kb,D)\\r\\rTitle: IndicVoices: Towards building an Inclusive Multilingual Speech Dataset\\r  for Indian Languages\\rAuthors: Tahir Javed, Janki Atul Nawale, Eldho Ittan George, Sakshi Joshi,\\r  Kaushal Santosh Bhogale, Deovrat Mehendale, Ishvinder Virender Sethi, Aparna\\r  Ananthanarayanan, Hafsah Faquih, Pratiti Palit, Sneha Ravishankar, Saranya\\r  Sukumaran, Tripura Panchagnula, Sunjay Murali, Kunal Sharad Gandhi,\\r  Ambujavalli R, Manickam K M, C Venkata Vaijayanthi, Krishnan Srinivasa\\r  Raghavan Karunganni, Pratyush Kumar, Mitesh M Khapra\\rCategories: cs.CL\\r\\\\\\\\\\r  We present INDICVOICES, a dataset of natural and spontaneous speech\\rcontaining a total of 7348 hours of read (9%), extempore (74%) and\\rconversational (17%) audio from 16237 speakers covering 145 Indian districts\\rand 22 languages. Of these 7348 hours, 1639 hours have already been\\rtranscribed, with a median of 73 hours per language. Through this paper, we\\rshare our journey of capturing the cultural, linguistic and demographic\\rdiversity of India to create a one-of-its-kind inclusive and representative\\rdataset. More specifically, we share an open-source blueprint for data\\rcollection at scale comprising of standardised protocols, centralised tools, a\\rrepository of engaging questions, prompts and conversation scenarios spanning\\rmultiple domains and topics of interest, quality control mechanisms,\\rcomprehensive transcription guidelines and transcription tools. We hope that\\rthis open source blueprint will serve as a comprehensive starter kit for data\\rcollection efforts in other multilingual regions of the world. Using\\rINDICVOICES, we build IndicASR, the first ASR model to support all the 22\\rlanguages listed in the 8th schedule of the Constitution of India. All the\\rdata, tools, guidelines, models and other materials developed as a part of this\\rwork will be made publicly available\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01926 ,  39165kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01929\\rDate: Mon, 4 Mar 2024 10:48:13 GMT   (1699kb,D)\\r\\rTitle: Analyzing and Adapting Large Language Models for Few-Shot Multilingual\\r  NLU: Are We There Yet?\\rAuthors: Evgeniia Razumovskaia, Ivan Vuli\\\\'c, Anna Korhonen\\rCategories: cs.CL\\r\\\\\\\\\\r  Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and\\rin-context learning (ICL) are three alternative, de facto standard approaches\\rto few-shot learning. ICL has gained popularity recently with the advent of\\rLLMs due to its simplicity and sample efficiency. Prior research has conducted\\ronly limited investigation into how these approaches work for multilingual\\rfew-shot learning, and the focus so far has been mostly on their performance.\\rIn this work, we present an extensive and systematic comparison of the three\\rapproaches, testing them on 6 high- and low-resource languages, three different\\rNLU tasks, and a myriad of language and domain setups. Importantly, performance\\ris only one aspect of the comparison, where we also analyse the approaches\\rthrough the optics of their computational, inference and financial costs. Our\\robservations show that supervised instruction tuning has the best trade-off\\rbetween performance and resource requirements. As another contribution, we\\ranalyse the impact of target language adaptation of pretrained LLMs and find\\rthat the standard adaptation approaches can (superficially) improve target\\rlanguage generation capabilities, but language understanding elicited through\\rICL does not improve and remains limited, with low scores especially for\\rlow-resource languages.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01929 ,  1699kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01931\\rDate: Mon, 4 Mar 2024 10:57:14 GMT   (975kb,D)\\r\\rTitle: VariErr NLI: Separating Annotation Error from Human Label Variation\\rAuthors: Leon Weber-Genzel, Siyao Peng, Marie-Catherine de Marneffe, Barbara\\r  Plank\\rCategories: cs.CL\\rComments: 13 pages, under review\\r\\\\\\\\\\r  Human label variation arises when annotators assign different labels to the\\rsame item for valid reasons, while annotation errors occur when labels are\\rassigned for invalid reasons. These two issues are prevalent in NLP benchmarks,\\ryet existing research has studied them in isolation. To the best of our\\rknowledge, there exists no prior work that focuses on teasing apart error from\\rsignal, especially in cases where signal is beyond black-and-white. To fill\\rthis gap, we introduce a systematic methodology and a new dataset, VariErr\\r(variation versus error), focusing on the NLI task in English. We propose a\\r2-round annotation scheme with annotators explaining each label and\\rsubsequently judging the validity of label-explanation pairs. \\\\name{} contains\\r7,574 validity judgments on 1,933 explanations for 500 re-annotated NLI items.\\rWe assess the effectiveness of various automatic error detection (AED) methods\\rand GPTs in uncovering errors versus human label variation. We find that\\rstate-of-the-art AED methods significantly underperform compared to GPTs and\\rhumans. While GPT-4 is the best system, it still falls short of human\\rperformance. Our methodology is applicable beyond NLI, offering fertile ground\\rfor future research on error versus plausible variation, which in turn can\\ryield better and more trustworthy NLP systems.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01931 ,  975kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01954\\rDate: Mon, 4 Mar 2024 11:49:08 GMT   (987kb,D)\\r\\rTitle: DECIDER: A Rule-Controllable Decoding Strategy for Language Generation\\r  by Imitating Dual-System Cognitive Theory\\rAuthors: Chen Xu, Tian Lan, Changlong Yu, Wei Wang, Jun Gao, Yu Ji, Qunxi Dong,\\r  Kun Qian, Piji Li, Wei Bi, and Bin Hu\\rCategories: cs.CL cs.AI cs.LO\\rComments: Submitted to IEEE TKDE, 12 pages, 6 figures\\r\\\\\\\\\\r  Lexicon-based constrained decoding approaches aim to control the meaning or\\rstyle of the generated text through certain target concepts. Existing\\rapproaches over-focus the targets themselves, leading to a lack of high-level\\rreasoning about how to achieve them. However, human usually tackles tasks by\\rfollowing certain rules that not only focuses on the targets but also on\\rsemantically relevant concepts that induce the occurrence of targets. In this\\rwork, we present DECIDER, a rule-controllable decoding strategy for constrained\\rlanguage generation inspired by dual-system cognitive theory. Specifically, in\\rDECIDER, a pre-trained language model (PLM) is equiped with a logic reasoner\\rthat takes high-level rules as input. Then, the DECIDER allows rule signals to\\rflow into the PLM at each decoding step. Extensive experimental results\\rdemonstrate that DECIDER can effectively follow given rules to guide generation\\rdirection toward the targets in a more human-like manner.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01954 ,  987kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01969\\rDate: Mon, 4 Mar 2024 12:13:59 GMT   (3418kb,D)\\r\\rTitle: AS-ES Learning: Towards Efficient CoT Learning in Small Models\\rAuthors: Nuwa Xi, Yuhan Chen, Sendong Zhao, Haochun Wang, Bing Qin and Ting Liu\\rCategories: cs.CL\\r\\\\\\\\\\r  Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs,\\respecially when it comes to logical reasoning. Attempts have been made to\\rinduce such ability in small models as well by distilling from the data with\\rCoT generated by Large Language Models (LLMs). However, existing methods often\\rsimply generate and incorporate more data from LLMs and fail to note the\\rimportance of efficiently utilizing existing CoT data. We here propose a new\\rtraining paradigm AS-ES (Abstractive Segments - Extractive Segments) learning,\\rwhich exploits the inherent information in CoT for iterative generation.\\rExperiments show that our methods surpass the direct seq2seq training on\\rCoT-extensive tasks like MWP and PET summarization, without data augmentation\\ror altering the model itself. Furthermore, we explore the reason behind the\\rinefficiency of small models in learning CoT and provide an explanation of why\\rAS-ES learning works, giving insights into the underlying mechanism of CoT.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01969 ,  3418kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01972\\rDate: Mon, 4 Mar 2024 12:16:15 GMT   (359kb,D)\\r\\rTitle: Multi-perspective Improvement of Knowledge Graph Completion with Large\\r  Language Models\\rAuthors: Derong Xu, Ziheng Zhang, Zhenxi Lin, Xian Wu, Zhihong Zhu, Tong Xu,\\r  Xiangyu Zhao, Yefeng Zheng and Enhong Chen\\rCategories: cs.CL\\rComments: Accepted by LREC-COLING 2024\\r\\\\\\\\\\r  Knowledge graph completion (KGC) is a widely used method to tackle\\rincompleteness in knowledge graphs (KGs) by making predictions for missing\\rlinks. Description-based KGC leverages pre-trained language models to learn\\rentity and relation representations with their names or descriptions, which\\rshows promising results. However, the performance of description-based KGC is\\rstill limited by the quality of text and the incomplete structure, as it lacks\\rsufficient entity descriptions and relies solely on relation names, leading to\\rsub-optimal results. To address this issue, we propose MPIKGC, a general\\rframework to compensate for the deficiency of contextualized knowledge and\\rimprove KGC by querying large language models (LLMs) from various perspectives,\\rwhich involves leveraging the reasoning, explanation, and summarization\\rcapabilities of LLMs to expand entity descriptions, understand relations, and\\rextract structures, respectively. We conducted extensive evaluation of the\\reffectiveness and improvement of our framework based on four description-based\\rKGC models and four datasets, for both link prediction and triplet\\rclassification tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01972 ,  359kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01976\\rDate: Mon, 4 Mar 2024 12:19:28 GMT   (4202kb,D)\\r\\rTitle: SciAssess: Benchmarking LLM Proficiency in Scientific Literature\\r  Analysis\\rAuthors: Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin\\r  Wang, Zhifeng Gao, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Yuqi Yin,\\r  Yaqi Li, Linfeng Zhang, Guolin Ke\\rCategories: cs.CL\\r\\\\\\\\\\r  Recent breakthroughs in Large Language Models (LLMs) have revolutionized\\rnatural language understanding and generation, igniting a surge of interest in\\rleveraging these technologies for the nuanced field of scientific literature\\ranalysis. Existing benchmarks, however, inadequately evaluate the proficiency\\rof LLMs in the scientific domain, especially in scenarios involving complex\\rcomprehension and multimodal data. In response, we introduced SciAssess, a\\rbenchmark tailored for the in-depth analysis of scientific literature, crafted\\rto provide a thorough assessment of LLMs' efficacy. SciAssess focuses on\\revaluating LLMs' abilities in memorization, comprehension, and analysis within\\rscientific contexts. It includes representative tasks from diverse scientific\\rfields, such as general chemistry, organic materials, and alloy materials. And\\rrigorous quality control measures ensure its reliability in terms of\\rcorrectness, anonymization, and copyright compliance. SciAssess evaluates\\rleading LLMs, including GPT-4, GPT-3.5-turbo, and Gemini, identifying their\\rstrengths and areas for improvement and supporting the ongoing development of\\rLLM applications in scientific literature analysis. SciAssess and its resources\\rare made available at https://sci-assess.github.io, offering a valuable tool\\rfor advancing LLM capabilities in scientific literature analysis.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01976 ,  4202kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01983\\rDate: Mon, 4 Mar 2024 12:27:32 GMT   (1898kb)\\r\\rTitle: Language and Speech Technology for Central Kurdish Varieties\\rAuthors: Sina Ahmadi, Daban Q. Jaff, Md Mahfuz Ibn Alam, Antonios\\r  Anastasopoulos\\rCategories: cs.CL\\rComments: Accepted to LREC-COLING 2024\\r\\\\\\\\\\r  Kurdish, an Indo-European language spoken by over 30 million speakers, is\\rconsidered a dialect continuum and known for its diversity in language\\rvarieties. Previous studies addressing language and speech technology for\\rKurdish handle it in a monolithic way as a macro-language, resulting in\\rdisparities for dialects and varieties for which there are few resources and\\rtools available. In this paper, we take a step towards developing resources for\\rlanguage and speech technology for varieties of Central Kurdish, creating a\\rcorpus by transcribing movies and TV series as an alternative to fieldwork.\\rAdditionally, we report the performance of machine translation, automatic\\rspeech recognition, and language identification as downstream tasks evaluated\\ron Central Kurdish varieties. Data and models are publicly available under an\\ropen license at https://github.com/sinaahmadi/CORDI.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01983 ,  1898kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01985\\rDate: Mon, 4 Mar 2024 12:29:59 GMT   (2192kb,D)\\r\\rTitle: Transformers for Low-Resource Languages:Is F\\\\'eidir Linn!\\rAuthors: S\\\\'eamus Lankford, Haithem Afli and Andy Way\\rCategories: cs.CL cs.AI\\rComments: 13 pages\\rJournal-ref: Proceedings of Machine Translation Summit XVIII: Research Track\\r  2021\\r\\\\\\\\\\r  The Transformer model is the state-of-the-art in Machine Translation.\\rHowever, in general, neural translation models often under perform on language\\rpairs with insufficient training data. As a consequence, relatively few\\rexperiments have been carried out using this architecture on low-resource\\rlanguage pairs. In this study, hyperparameter optimization of Transformer\\rmodels in translating the low-resource English-Irish language pair is\\revaluated. We demonstrate that choosing appropriate parameters leads to\\rconsiderable performance improvements. Most importantly, the correct choice of\\rsubword model is shown to be the biggest driver of translation performance.\\rSentencePiece models using both unigram and BPE approaches were appraised.\\rVariations on model architectures included modifying the number of layers,\\rtesting various regularisation techniques and evaluating the optimal number of\\rheads for attention. A generic 55k DGT corpus and an in-domain 88k public admin\\rcorpus were used for evaluation. A Transformer optimized model demonstrated a\\rBLEU score improvement of 7.8 points when compared with a baseline RNN model.\\rImprovements were observed across a range of metrics, including TER, indicating\\ra substantially reduced post editing effort for Transformer optimized models\\rwith 16k BPE subword models. Bench-marked against Google Translate, our\\rtranslation engines demonstrated significant improvements. The question of\\rwhether or not Transformers can be used effectively in a low-resource setting\\rof English-Irish translation has been addressed. Is f\\\\'eidir linn - yes we can.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01985 ,  2192kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01988\\rDate: Mon, 4 Mar 2024 12:35:09 GMT   (4751kb,D)\\r\\rTitle: FakeNewsGPT4: Advancing Multimodal Fake News Detection through\\r  Knowledge-Augmented LVLMs\\rAuthors: Xuannan Liu and Peipei Li and Huaibo Huang and Zekun Li and Xing Cui\\r  and Jiahao Liang and Lixiong Qin and Weihong Deng and Zhaofeng He\\rCategories: cs.CL\\r\\\\\\\\\\r  The massive generation of multimodal fake news exhibits substantial\\rdistribution discrepancies, prompting the need for generalized detectors.\\rHowever, the insulated nature of training within specific domains restricts the\\rcapability of classical detectors to obtain open-world facts. In this paper, we\\rpropose FakeNewsGPT4, a novel framework that augments Large Vision-Language\\rModels (LVLMs) with forgery-specific knowledge for manipulation reasoning while\\rinheriting extensive world knowledge as complementary. Knowledge augmentation\\rin FakeNewsGPT4 involves acquiring two types of forgery-specific knowledge,\\ri.e., semantic correlation and artifact trace, and merging them into LVLMs.\\rSpecifically, we design a multi-level cross-modal reasoning module that\\restablishes interactions across modalities for extracting semantic\\rcorrelations. Concurrently, a dual-branch fine-grained verification module is\\rpresented to comprehend localized details to encode artifact traces. The\\rgenerated knowledge is translated into refined embeddings compatible with\\rLVLMs. We also incorporate candidate answer heuristics and soft prompts to\\renhance input informativeness. Extensive experiments on the public benchmark\\rdemonstrate that FakeNewsGPT4 achieves superior cross-domain performance\\rcompared to previous methods. Code will be available.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01988 ,  4751kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01994\\rDate: Mon, 4 Mar 2024 12:40:28 GMT   (7930kb,D)\\r\\rTitle: Vanilla Transformers are Transfer Capability Teachers\\rAuthors: Xin Lu, Yanyan Zhao, Bing Qin\\rCategories: cs.CL\\r\\\\\\\\\\r  Recently, Mixture of Experts (MoE) Transformers have garnered increasing\\rattention due to their advantages in model capacity and computational\\refficiency. However, studies have indicated that MoE Transformers underperform\\rvanilla Transformers in many downstream tasks, significantly diminishing the\\rpractical value of MoE models. To explain this issue, we propose that the\\rpre-training performance and transfer capability of a model are joint\\rdeterminants of its downstream task performance. MoE models, in comparison to\\rvanilla models, have poorer transfer capability, leading to their subpar\\rperformance in downstream tasks. To address this issue, we introduce the\\rconcept of transfer capability distillation, positing that although vanilla\\rmodels have weaker performance, they are effective teachers of transfer\\rcapability. The MoE models guided by vanilla models can achieve both strong\\rpre-training performance and transfer capability, ultimately enhancing their\\rperformance in downstream tasks. We design a specific distillation method and\\rconduct experiments on the BERT architecture. Experimental results show a\\rsignificant improvement in downstream performance of MoE models, and many\\rfurther evidences also strongly support the concept of transfer capability\\rdistillation. Finally, we attempt to interpret transfer capability distillation\\rand provide some insights from the perspective of model feature.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01994 ,  7930kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01999\\rDate: Mon, 4 Mar 2024 12:50:25 GMT   (2895kb,D)\\r\\rTitle: LLM-Oriented Retrieval Tuner\\rAuthors: Si Sun, Hanqing Zhang, Zhiyuan Liu, Jie Bao, Dawei Song\\rCategories: cs.CL\\rComments: 16 pages, 8 figures, 5 tables\\r\\\\\\\\\\r  Dense Retrieval (DR) is now considered as a promising tool to enhance the\\rmemorization capacity of Large Language Models (LLM) such as GPT3 and GPT-4 by\\rincorporating external memories. However, due to the paradigm discrepancy\\rbetween text generation of LLM and DR, it is still an open challenge to\\rintegrate the retrieval and generation tasks in a shared LLM. In this paper, we\\rpropose an efficient LLM-Oriented Retrieval Tuner, namely LMORT, which\\rdecouples DR capacity from base LLM and non-invasively coordinates the\\roptimally aligned and uniform layers of the LLM towards a unified DR space,\\rachieving an efficient and effective DR without tuning the LLM itself. The\\rextensive experiments on six BEIR datasets show that our approach could achieve\\rcompetitive zero-shot retrieval performance compared to a range of strong DR\\rmodels while maintaining the generation ability of LLM.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01999 ,  2895kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02009\\rDate: Mon, 4 Mar 2024 13:10:08 GMT   (612kb,D)\\r\\rTitle: Topic Aware Probing: From Sentence Length Prediction to Idiom\\r  Identification how reliant are Neural Language Models on Topic?\\rAuthors: Vasudevan Nedumpozhimana, John D. Kelleher\\rCategories: cs.CL\\r\\\\\\\\\\r  Transformer-based Neural Language Models achieve state-of-the-art performance\\ron various natural language processing tasks. However, an open question is the\\rextent to which these models rely on word-order/syntactic or word\\rco-occurrence/topic-based information when processing natural language. This\\rwork contributes to this debate by addressing the question of whether these\\rmodels primarily use topic as a signal, by exploring the relationship between\\rTransformer-based models' (BERT and RoBERTa's) performance on a range of\\rprobing tasks in English, from simple lexical tasks such as sentence length\\rprediction to complex semantic tasks such as idiom token identification, and\\rthe sensitivity of these tasks to the topic information. To this end, we\\rpropose a novel probing method which we call topic-aware probing. Our initial\\rresults indicate that Transformer-based models encode both topic and non-topic\\rinformation in their intermediate layers, but also that the facility of these\\rmodels to distinguish idiomatic usage is primarily based on their ability to\\ridentify and encode topic. Furthermore, our analysis of these models'\\rperformance on other standard probing tasks suggests that tasks that are\\rrelatively insensitive to the topic information are also tasks that are\\rrelatively difficult for these models.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02009 ,  612kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02078\\rDate: Mon, 4 Mar 2024 14:24:47 GMT   (7136kb,D)\\r\\rTitle: Automated Generation of Multiple-Choice Cloze Questions for Assessing\\r  English Vocabulary Using GPT-turbo 3.5\\rAuthors: Qiao Wang, Ralph Rose, Naho Orita, Ayaka Sugawara\\rCategories: cs.CL\\rJournal-ref: Mika H\\\\am\\\\al\\\\ainen, Emily \\\\Ohman, Flammie Pirinen, Khalid\\r  Alnajjar, So Miyagawa, Yuri Bizzoni, Niko Partanen, and Jack Rueter. 2023.\\r  Proc. of the Joint 3rd International Conference on NLP4DH and 8th IWCLUL.\\r  ACL, Tokyo, Japan, edition\\r\\\\\\\\\\r  A common way of assessing language learners' mastery of vocabulary is via\\rmultiple-choice cloze (i.e., fill-in-the-blank) questions. But the creation of\\rtest items can be laborious for individual teachers or in large-scale language\\rprograms. In this paper, we evaluate a new method for automatically generating\\rthese types of questions using large language models (LLM). The VocaTT\\r(vocabulary teaching and training) engine is written in Python and comprises\\rthree basic steps: pre-processing target word lists, generating sentences and\\rcandidate word options using GPT, and finally selecting suitable word options.\\rTo test the efficiency of this system, 60 questions were generated targeting\\racademic words. The generated items were reviewed by expert reviewers who\\rjudged the well-formedness of the sentences and word options, adding comments\\rto items judged not well-formed. Results showed a 75% rate of well-formedness\\rfor sentences and 66.85% rate for suitable word options. This is a marked\\rimprovement over the generator used earlier in our research which did not take\\radvantage of GPT's capabilities. Post-hoc qualitative analysis reveals several\\rpoints for improvement in future work including cross-referencing\\rpart-of-speech tagging, better sentence validation, and improving GPT prompts.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02078 ,  7136kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02121\\rDate: Mon, 4 Mar 2024 15:27:49 GMT   (699kb,D)\\r\\rTitle: Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed\\r  Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language\\r  Models\\rAuthors: Sargam Yadav (1), Abhishek Kaushik (1) and Kevin McDaid (1) ((1)\\r  Dundalk Institute of Technology, Dundalk)\\rCategories: cs.CL cs.AI\\rComments: This paper is accepted in the 16th ISDSI-Global Conference 2023\\r  https://isdsi2023.iimranchi.ac.in\\r\\\\\\\\\\r  The advent of Large Language Models (LLMs) has advanced the benchmark in\\rvarious Natural Language Processing (NLP) tasks. However, large amounts of\\rlabelled training data are required to train LLMs. Furthermore, data annotation\\rand training are computationally expensive and time-consuming. Zero and\\rfew-shot learning have recently emerged as viable options for labelling data\\rusing large pre-trained models. Hate speech detection in mix-code low-resource\\rlanguages is an active problem area where the use of LLMs has proven\\rbeneficial. In this study, we have compiled a dataset of 100 YouTube comments,\\rand weakly labelled them for coarse and fine-grained misogyny classification in\\rmix-code Hinglish. Weak annotation was applied due to the labor-intensive\\rannotation process. Zero-shot learning, one-shot learning, and few-shot\\rlearning and prompting approaches have then been applied to assign labels to\\rthe comments and compare them to human-assigned labels. Out of all the\\rapproaches, zero-shot classification using the Bidirectional Auto-Regressive\\rTransformers (BART) large model and few-shot prompting using Generative\\rPre-trained Transformer- 3 (ChatGPT-3) achieve the best results\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02121 ,  699kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02130\\rDate: Mon, 4 Mar 2024 15:39:59 GMT   (634kb,D)\\r\\rTitle: Using LLMs for the Extraction and Normalization of Product Attribute\\r  Values\\rAuthors: Nick Baumann, Alexander Brinkmann, Christian Bizer\\rCategories: cs.CL\\r\\\\\\\\\\r  Product offers on e-commerce websites often consist of a textual product\\rtitle and a textual product description. In order to provide features such as\\rfaceted product filtering or content-based product recommendation, the websites\\rneed to extract attribute-value pairs from the unstructured product\\rdescriptions. This paper explores the potential of using large language models\\r(LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute\\rvalues from product titles and product descriptions. For our experiments, we\\rintroduce the WDC Product Attribute-Value Extraction (WDC PAVE) dataset. WDC\\rPAVE consists of product offers from 87 websites that provide schema.org\\rannotations. The offers belong to five different categories, each featuring a\\rspecific set of attributes. The dataset provides manually verified\\rattribute-value pairs in two forms: (i) directly extracted values and (ii)\\rnormalized attribute values. The normalization of the attribute values requires\\rsystems to perform the following types of operations: name expansion,\\rgeneralization, unit of measurement normalization, and string wrangling. Our\\rexperiments demonstrate that GPT-4 outperforms PLM-based extraction methods by\\r10%, achieving an F1-Score of 91%. For the extraction and normalization of\\rproduct attribute values, GPT-4 achieves a similar performance to the\\rextraction scenario, while being particularly strong at string wrangling and\\rname expansion.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02130 ,  634kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02173\\rDate: Mon, 4 Mar 2024 16:20:14 GMT   (452kb,D)\\r\\rTitle: What has LeBenchmark Learnt about French Syntax?\\rAuthors: Zdravko Dugonji\\\\'c, Adrien Pupier, Benjamin Lecouteux, Maximin Coavoux\\rCategories: cs.CL\\rComments: Accepted to LREC-COLING 2024\\r\\\\\\\\\\r  The paper reports on a series of experiments aiming at probing LeBenchmark, a\\rpretrained acoustic model trained on 7k hours of spoken French, for syntactic\\rinformation. Pretrained acoustic models are increasingly used for downstream\\rspeech tasks such as automatic speech recognition, speech translation, spoken\\rlanguage understanding or speech parsing. They are trained on very low level\\rinformation (the raw speech signal), and do not have explicit lexical\\rknowledge. Despite that, they obtained reasonable results on tasks that\\rrequires higher level linguistic knowledge. As a result, an emerging question\\ris whether these models encode syntactic information. We probe each\\rrepresentation layer of LeBenchmark for syntax, using the Orf\\\\'eo treebank, and\\robserve that it has learnt some syntactic information. Our results show that\\rsyntactic information is more easily extractable from the middle layers of the\\rnetwork, after which a very sharp decrease is observed.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02173 ,  452kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02176\\rDate: Mon, 4 Mar 2024 16:21:13 GMT   (41kb,D)\\r\\rTitle: EEE-QA: Exploring Effective and Efficient Question-Answer\\r  Representations\\rAuthors: Zhanghao Hu, Yijun Yang, Junjie Xu, Yifu Qiu, Pinzhen Chen\\rCategories: cs.CL\\rComments: LREC-COLING 2024\\r\\\\\\\\\\r  Current approaches to question answering rely on pre-trained language models\\r(PLMs) like RoBERTa. This work challenges the existing question-answer encoding\\rconvention and explores finer representations. We begin with testing various\\rpooling methods compared to using the begin-of-sentence token as a question\\rrepresentation for better quality. Next, we explore opportunities to\\rsimultaneously embed all answer candidates with the question. This enables\\rcross-reference between answer choices and improves inference throughput via\\rreduced memory usage. Despite their simplicity and effectiveness, these methods\\rhave yet to be widely studied in current frameworks. We experiment with\\rdifferent PLMs, and with and without the integration of knowledge graphs.\\rResults prove that the memory efficacy of the proposed techniques with little\\rsacrifice in performance. Practically, our work enhances 38-100% throughput\\rwith 26-65% speedups on consumer-grade GPUs by allowing for considerably larger\\rbatch sizes. Our work sends a message to the community with promising\\rdirections in both representation quality and efficiency for the\\rquestion-answering task in natural language processing.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02176 ,  41kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02177\\rDate: Mon, 4 Mar 2024 16:21:19 GMT   (1140kb,D)\\r\\rTitle: ProTrix: Building Models for Planning and Reasoning over Tables with\\r  Sentence Context\\rAuthors: Zirui Wu and Yansong Feng\\rCategories: cs.CL\\rComments: under review\\r\\\\\\\\\\r  Tables play a crucial role in conveying information in various domains,\\rserving as indispensable tools for organizing and presenting data in a\\rstructured manner. We propose a Plan-then-Reason framework to answer different\\rtypes of user queries over tables with sentence context. The framework first\\rplans the reasoning paths over the context, then assigns each step to\\rprogram-based or textual reasoning to reach the final answer. We construct an\\rinstruction tuning set TrixInstruct following the framework. Our dataset cover\\rqueries that are program-unsolvable or need combining information from tables\\rand sentences to obtain planning and reasoning abilities. We present ProTrix by\\rfinetuning Llama-2-7B on TrixInstruct. Our experiments show that ProTrix\\rgeneralizes to diverse tabular tasks and achieves comparable performance to\\rGPT-3.5-turbo. We further demonstrate that ProTrix can generate accurate and\\rfaithful explanations to answer complex free-form questions. Our work\\runderscores the importance of the planning and reasoning abilities towards a\\rmodel over tabular tasks with generalizability and interpretability. We will\\rrelease our dataset and model at https://github.com/WilliamZR/ProTrix.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02177 ,  1140kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02178\\rDate: Mon, 4 Mar 2024 16:21:54 GMT   (8963kb,D)\\r\\rTitle: Masked Thought: Simply Masking Partial Reasoning Steps Can Improve\\r  Mathematical Reasoning Learning of Language Models\\rAuthors: Changyu Chen, Xiting Wang, Ting-En Lin, Ang Lv, Yuchuan Wu, Xin Gao,\\r  Ji-Rong Wen, Rui Yan and Yongbin Li\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  In reasoning tasks, even a minor error can cascade into inaccurate results,\\rleading to suboptimal performance of large language models in such domains.\\rEarlier fine-tuning approaches sought to mitigate this by leveraging more\\rprecise supervisory signals from human labeling, larger models, or\\rself-sampling, although at a high cost. Conversely, we develop a method that\\ravoids external resources, relying instead on introducing perturbations to the\\rinput. Our training approach randomly masks certain tokens within the chain of\\rthought, a technique we found to be particularly effective for reasoning tasks.\\rWhen applied to fine-tuning with GSM8K, this method achieved a 5% improvement\\rin accuracy over standard supervised fine-tuning with a few codes modified and\\rno additional labeling effort. Furthermore, it is complementary to existing\\rmethods. When integrated with related data augmentation methods, it leads to an\\raverage improvement of 3% improvement in GSM8K accuracy and 1% improvement in\\rMATH accuracy across five datasets of various quality and size, as well as two\\rbase models. We further investigate the mechanisms behind this improvement\\rthrough case studies and quantitative analysis, suggesting that our approach\\rmay provide superior support for the model in capturing long-distance\\rdependencies, especially those related to questions. This enhancement could\\rdeepen understanding of premises in questions and prior steps. Our code is\\ravailable at Github.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02178 ,  8963kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02181\\rDate: Mon, 4 Mar 2024 16:23:58 GMT   (878kb,D)\\r\\rTitle: Not all Layers of LLMs are Necessary during Inference\\rAuthors: Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang,\\r  Aixin Sun, Yequan Wang, Zhongyuan Wang\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  The inference phase of Large Language Models (LLMs) is very expensive. An\\rideal inference stage of LLMs could utilize fewer computational resources while\\rstill maintaining its capabilities (e.g., generalization and in-context\\rlearning ability). In this paper, we try to answer the question, During LLM\\rinference, can we use shallow layers for easy instances; and deep layers for\\rhard ones? To answer this question, we first indicate that Not all Layers are\\rNecessary during Inference by statistically analyzing the activated layers\\racross tasks. Then, we propose a simple algorithm named AdaInfer to determine\\rthe inference termination moment based on the input instance adaptively. More\\rimportantly, AdaInfer does not alter LLM parameters and maintains\\rgeneralizability across tasks. Experiments on well-known LLMs (i.e., Llama2\\rseries and OPT) show that AdaInfer saves an average of 14.8% of computational\\rresources, even up to 50% on sentiment tasks, while maintaining comparable\\rperformance. Additionally, this method is orthogonal to other model\\racceleration techniques, potentially boosting inference efficiency further.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02181 ,  878kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02246\\rDate: Mon, 4 Mar 2024 17:34:34 GMT   (1272kb)\\r\\rTitle: PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large\\r  Language Models\\rAuthors: Fiona Anting Tan, Gerard Christopher Yeo, Fanyou Wu, Weijie Xu, Vinija\\r  Jain, Aman Chadha, Kokil Jaidka, Yang Liu, See-Kiong Ng\\rCategories: cs.CL\\r\\\\\\\\\\r  Recent advances in large language models (LLMs) demonstrate that their\\rcapabilities are comparable, or even superior, to humans in many tasks in\\rnatural language processing. Despite this progress, LLMs are still inadequate\\rat social-cognitive reasoning, which humans are naturally good at. Drawing\\rinspiration from psychological research on the links between certain\\rpersonality traits and Theory-of-Mind (ToM) reasoning, and from prompt\\rengineering research on the hyper-sensitivity of prompts in affecting LLMs\\rcapabilities, this study investigates how inducing personalities in LLMs using\\rprompts affects their ToM reasoning capabilities. Our findings show that\\rcertain induced personalities can significantly affect the LLMs' reasoning\\rcapabilities in three different ToM tasks. In particular, traits from the Dark\\rTriad have a larger variable effect on LLMs like GPT-3.5, Llama 2, and Mistral\\racross the different ToM tasks. We find that LLMs that exhibit a higher\\rvariance across personality prompts in ToM also tends to be more controllable\\rin personality tests: personality traits in LLMs like GPT-3.5, Llama 2 and\\rMistral can be controllably adjusted through our personality prompts. In\\rtoday's landscape where role-play is a common strategy when using LLMs, our\\rresearch highlights the need for caution, as models that adopt specific\\rpersonas with personalities potentially also alter their reasoning abilities in\\ran unexpected manner.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02246 ,  1272kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02247\\rDate: Mon, 4 Mar 2024 17:34:46 GMT   (452kb)\\r\\rTitle: Birbal: An efficient 7B instruct-model fine-tuned with curated datasets\\rAuthors: Ashvini Kumar Jindal, Pawan Kumar Rajpoot, Ankur Parikh\\rCategories: cs.CL\\r\\\\\\\\\\r  LLMOps incur significant costs due to hardware requirements, hindering their\\rwidespread accessibility. Additionally, a lack of transparency in model\\rtraining methods and data contributes to the majority of models being\\rnon-reproducible. To tackle these challenges, the LLM Efficiency Challenge was\\rintroduced at NeurIPS Workshop, aiming to adapt foundation models on a diverse\\rset of tasks via fine-tuning on a single GPU (RTX 4090 or A100 with 40GB)\\rwithin a 24-hour timeframe. In this system description paper, we introduce\\rBirbal, our Mistral-7B based winning model, fine-tuned on a single RTX 4090 for\\r16 hours. Birbal's success lies in curating high-quality instructions covering\\rdiverse tasks, resulting in a 35% performance improvement over second-best\\rQwen-14B based submission.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02247 ,  452kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02268\\rDate: Mon, 4 Mar 2024 17:56:28 GMT   (7712kb,D)\\r\\rTitle: Subjective $\\\\textit{Isms}$? On the Danger of Conflating Hate and Offence\\r  in Abusive Language Detection\\rAuthors: Amanda Cercas Curry, Gavin Abercrombie, Zeerak Talat\\rCategories: cs.CL cs.AI cs.CY\\r\\\\\\\\\\r  Natural language processing research has begun to embrace the notion of\\rannotator subjectivity, motivated by variations in labelling. This approach\\runderstands each annotator's view as valid, which can be highly suitable for\\rtasks that embed subjectivity, e.g., sentiment analysis. However, this\\rconstruction may be inappropriate for tasks such as hate speech detection, as\\rit affords equal validity to all positions on e.g., sexism or racism. We argue\\rthat the conflation of hate and offence can invalidate findings on hate speech,\\rand call for future work to be situated in theory, disentangling hate from its\\rorthogonal concept, offence.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02268 ,  7712kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02270\\rDate: Mon, 4 Mar 2024 17:57:18 GMT   (8257kb,D)\\r\\rTitle: FENICE: Factuality Evaluation of summarization based on Natural language\\r  Inference and Claim Extraction\\rAuthors: Alessandro Scir\\\\`e and Karim Ghonim and Roberto Navigli\\rCategories: cs.CL\\rComments: 9 pages, long paper\\r\\\\\\\\\\r  Recent advancements in text summarization, particularly with the advent of\\rLarge Language Models (LLMs), have shown remarkable performance. However, a\\rnotable challenge persists as a substantial number of automatically-generated\\rsummaries exhibit factual inconsistencies, such as hallucinations. In response\\rto this issue, various approaches for the evaluation of consistency for\\rsummarization have emerged. Yet, these newly-introduced metrics face several\\rlimitations, including lack of interpretability, focus on short document\\rsummaries (e.g., news articles), and computational impracticality, especially\\rfor LLM-based metrics. To address these shortcomings, we propose Factuality\\rEvaluation of summarization based on Natural language Inference and Claim\\rExtraction (FENICE), a more interpretable and efficient factuality-oriented\\rmetric. FENICE leverages an NLI-based alignment between information in the\\rsource document and a set of atomic facts, referred to as claims, extracted\\rfrom the summary. Our metric sets a new state of the art on AGGREFACT, the\\rde-facto benchmark for factuality evaluation. Moreover, we extend our\\revaluation to a more challenging setting by conducting a human annotation\\rprocess of long-form summarization.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02270 ,  8257kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02271\\rDate: Mon, 4 Mar 2024 17:58:09 GMT   (7229kb,D)\\r\\rTitle: RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language\\r  Models\\rAuthors: Saeed Najafi and Alona Fyshe\\rCategories: cs.CL cs.LG\\rComments: Version Submitted to ACL2024. Review Discussion here:\\r  https://openreview.net/forum?id=_gFGBVMRN1\\r\\\\\\\\\\r  Pre-trained Language Models (PLMs) can be accurately fine-tuned for\\rdownstream text processing tasks. Recently, researchers have introduced several\\rparameter-efficient fine-tuning methods that optimize input prompts or adjust a\\rsmall number of model parameters (e.g LoRA). In this study, we explore the\\rimpact of altering the input text of the original task in conjunction with\\rparameter-efficient fine-tuning methods. To most effectively rewrite the input\\rtext, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood\\robjective. Using six few-shot text classification datasets, we show that\\renriching data with paraphrases at train and test time enhances the performance\\rbeyond what can be achieved with parameter-efficient fine-tuning alone.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02271 ,  7229kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02281\\rDate: Mon, 4 Mar 2024 18:12:10 GMT   (7752kb,D)\\r\\rTitle: Emotion Granularity from Text: An Aggregate-Level Indicator of Mental\\r  Health\\rAuthors: Krishnapriya Vishnubhotla, Daniela Teodorescu, Mallory J. Feldman,\\r  Kristen A. Lindquist, Saif M. Mohammad\\rCategories: cs.CL\\rComments: 9 pages plus appendices\\r\\\\\\\\\\r  We are united in how emotions are central to shaping our experiences; and\\ryet, individuals differ greatly in how we each identify, categorize, and\\rexpress emotions. In psychology, variation in the ability of individuals to\\rdifferentiate between emotion concepts is called emotion granularity\\r(determined through self-reports of one's emotions). High emotion granularity\\rhas been linked with better mental and physical health; whereas low emotion\\rgranularity has been linked with maladaptive emotion regulation strategies and\\rpoor health outcomes. In this work, we propose computational measures of\\remotion granularity derived from temporally-ordered speaker utterances in\\rsocial media (in lieu of self-reports that suffer from various biases). We then\\rinvestigate the effectiveness of such text-derived measures of emotion\\rgranularity in functioning as markers of various mental health conditions\\r(MHCs). We establish baseline measures of emotion granularity derived from\\rtextual utterances, and show that, at an aggregate level, emotion granularities\\rare significantly lower for people self-reporting as having an MHC than for the\\rcontrol population. This paves the way towards a better understanding of the\\rMHCs, and specifically the role emotions play in our well-being.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02281 ,  7752kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02285\\rDate: Mon, 4 Mar 2024 18:15:14 GMT   (282kb,D)\\r\\rTitle: Detection of Non-recorded Word Senses in English and Swedish\\rAuthors: Jonathan Lautenschlager, Emma Sk\\\\oldberg, Simon Hengchen, Dominik\\r  Schlechtweg\\rCategories: cs.CL\\rComments: 9 pages\\r\\\\\\\\\\r  This study addresses the task of Unknown Sense Detection in English and\\rSwedish. The primary objective of this task is to determine whether the meaning\\rof a particular word usage is documented in a dictionary or not. For this\\rpurpose, sense entries are compared with word usages from modern and historical\\rcorpora using a pre-trained Word-in-Context embedder that allows us to model\\rthis task in a few-shot scenario. Additionally, we use human annotations to\\radapt and evaluate our models. Compared to a random sample from a corpus, our\\rmodel is able to considerably increase the detected number of word usages with\\rnon-recorded senses.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02285 ,  282kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02333\\rDate: Mon, 4 Mar 2024 18:58:30 GMT   (1224kb,D)\\r\\rTitle: Key-Point-Driven Data Synthesis with its Enhancement on Mathematical\\r  Reasoning\\rAuthors: Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan,\\r  Weizhu Chen\\rCategories: cs.CL cs.AI\\rComments: In progress\\r\\\\\\\\\\r  Large language models (LLMs) have shown great potential in complex reasoning\\rtasks, yet their performance is often hampered by the scarcity of high-quality,\\rreasoning-focused training datasets. Addressing this challenge, we propose\\rKey-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that\\rsynthesizes question-answer pairs by leveraging key points and exemplar pairs\\rfrom authentic data sources. KPDDS ensures the generation of novel questions\\rwith rigorous quality control and substantial scalability. As a result, we\\rpresent KPMath, the most extensive synthetic dataset tailored for mathematical\\rreasoning to date, comprising over one million question-answer pairs. Utilizing\\rKPMath and augmenting it with additional reasoning-intensive corpora, we create\\rthe comprehensive KPMath-Plus dataset. Fine-tuning the Mistral-7B model on\\rKPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, a\\rperformance that not only outpaces other finetuned 7B models but also exceeds\\rthat of certain 34B models. Our ablation studies further confirm the\\rsubstantial enhancement in mathematical reasoning across various subtopics,\\rmarking a significant stride in LLMs' reasoning capabilities.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02333 ,  1224kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00939\\rDate: Fri, 1 Mar 2024 19:36:11 GMT   (33620kb,D)\\r\\rTitle: G3DR: Generative 3D Reconstruction in ImageNet\\rAuthors: Pradyumna Reddy, Ismail Elezi, Jiankang Deng\\rCategories: cs.CV cs.GR\\r\\\\\\\\\\r  We introduce a novel 3D generative method, Generative 3D Reconstruction\\r(G3DR) in ImageNet, capable of generating diverse and high-quality 3D objects\\rfrom single images, addressing the limitations of existing methods. At the\\rheart of our framework is a novel depth regularization technique that enables\\rthe generation of scenes with high-geometric fidelity. G3DR also leverages a\\rpretrained language-vision model, such as CLIP, to enable reconstruction in\\rnovel views and improve the visual realism of generations. Additionally, G3DR\\rdesigns a simple but effective sampling procedure to further improve the\\rquality of generations. G3DR offers diverse and efficient 3D asset generation\\rbased on class or text conditioning. Despite its simplicity, G3DR is able to\\rbeat state-of-theart methods, improving over them by up to 22% in perceptual\\rmetrics and 90% in geometry scores, while needing only half of the training\\rtime. Code is available at https://github.com/preddy5/G3DR\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00939 ,  33620kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01058\\rDate: Sat, 2 Mar 2024 01:20:59 GMT   (7419kb,D)\\r\\rTitle: Neural Field Classifiers via Target Encoding and Classification Loss\\rAuthors: Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran\\r  Wang, Yunfeng Cai, Mingming Sun\\rCategories: cs.CV cs.LG\\rComments: ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables\\r\\\\\\\\\\r  Neural field methods have seen great progress in various long-standing tasks\\rin computer vision and computer graphics, including novel view synthesis and\\rgeometry reconstruction. As existing neural field methods try to predict some\\rcoordinate-based continuous target values, such as RGB for Neural Radiance\\rField (NeRF), all of these methods are regression models and are optimized by\\rsome regression loss. However, are regression models really better than\\rclassification models for neural field methods? In this work, we try to visit\\rthis very fundamental but overlooked question for neural fields from a machine\\rlearning perspective. We successfully propose a novel Neural Field Classifier\\r(NFC) framework which formulates existing neural field methods as\\rclassification tasks rather than regression tasks. The proposed NFC can easily\\rtransform arbitrary Neural Field Regressor (NFR) into its classification\\rvariant via employing a novel Target Encoding module and optimizing a\\rclassification loss. By encoding a continuous regression target into a\\rhigh-dimensional discrete encoding, we naturally formulate a multi-label\\rclassification task. Extensive experiments demonstrate the impressive\\reffectiveness of NFC at the nearly free extra computational costs. Moreover,\\rNFC also shows robustness to sparse inputs, corrupted images, and dynamic\\rscenes.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01058 ,  7419kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01076\\rDate: Sat, 2 Mar 2024 03:03:29 GMT   (889kb,D)\\r\\rTitle: Extracting Usable Predictions from Quantized Networks through\\r  Uncertainty Quantification for OOD Detection\\rAuthors: Rishi Singhal and Srinath Srinivasan\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  OOD detection has become more pertinent with advances in network design and\\rincreased task complexity. Identifying which parts of the data a given network\\ris misclassifying has become as valuable as the network's overall performance.\\rWe can compress the model with quantization, but it suffers minor performance\\rloss. The loss of performance further necessitates the need to derive the\\rconfidence estimate of the network's predictions. In line with this thinking,\\rwe introduce an Uncertainty Quantification(UQ) technique to quantify the\\runcertainty in the predictions from a pre-trained vision model. We subsequently\\rleverage this information to extract valuable predictions while ignoring the\\rnon-confident predictions. We observe that our technique saves up to 80% of\\rignored samples from being misclassified. The code for the same is available\\rhere.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01076 ,  889kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01083\\rDate: Sat, 2 Mar 2024 03:52:07 GMT   (12516kb,D)\\r\\rTitle: Beyond Night Visibility: Adaptive Multi-Scale Fusion of Infrared and\\r  Visible Images\\rAuthors: Shufan Pei, Junhong Lin, Wenxi Liu, Tiesong Zhao and Chia-Wen Lin\\rCategories: cs.CV\\r\\\\\\\\\\r  In addition to low light, night images suffer degradation from light effects\\r(e.g., glare, floodlight, etc). However, existing nighttime visibility\\renhancement methods generally focus on low-light regions, which neglects, or\\reven amplifies the light effects. To address this issue, we propose an Adaptive\\rMulti-scale Fusion network (AMFusion) with infrared and visible images, which\\rdesigns fusion rules according to different illumination regions. First, we\\rseparately fuse spatial and semantic features from infrared and visible images,\\rwhere the former are used for the adjustment of light distribution and the\\rlatter are used for the improvement of detection accuracy. Thereby, we obtain\\ran image free of low light and light effects, which improves the performance of\\rnighttime object detection. Second, we utilize detection features extracted by\\ra pre-trained backbone that guide the fusion of semantic features. Hereby, we\\rdesign a Detection-guided Semantic Fusion Module (DSFM) to bridge the domain\\rgap between detection and semantic features. Third, we propose a new\\rillumination loss to constrain fusion image with normal light intensity.\\rExperimental results demonstrate the superiority of AMFusion with better visual\\rquality and detection accuracy. The source code will be released after the peer\\rreview process.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01083 ,  12516kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01105\\rDate: Sat, 2 Mar 2024 06:29:44 GMT   (36597kb,D)\\r\\rTitle: Depth Information Assisted Collaborative Mutual Promotion Network for\\r  Single Image Dehazing\\rAuthors: Yafei Zhang, Shen Zhou, Huafeng Li\\rCategories: cs.CV\\r\\\\\\\\\\r  Recovering a clear image from a single hazy image is an open inverse problem.\\rAlthough significant research progress has been made, most existing methods\\rignore the effect that downstream tasks play in promoting upstream dehazing.\\r>From the perspective of the haze generation mechanism, there is a potential\\rrelationship between the depth information of the scene and the hazy image.\\rBased on this, we propose a dual-task collaborative mutual promotion framework\\rto achieve the dehazing of a single image. This framework integrates depth\\restimation and dehazing by a dual-task interaction mechanism and achieves\\rmutual enhancement of their performance. To realize the joint optimization of\\rthe two tasks, an alternative implementation mechanism with the difference\\rperception is developed. On the one hand, the difference perception between the\\rdepth maps of the dehazing result and the ideal image is proposed to promote\\rthe dehazing network to pay attention to the non-ideal areas of the dehazing.\\rOn the other hand, by improving the depth estimation performance in the\\rdifficult-to-recover areas of the hazy image, the dehazing network can\\rexplicitly use the depth information of the hazy image to assist the clear\\rimage recovery. To promote the depth estimation, we propose to use the\\rdifference between the dehazed image and the ground truth to guide the depth\\restimation network to focus on the dehazed unideal areas. It allows dehazing\\rand depth estimation to leverage their strengths in a mutually reinforcing\\rmanner. Experimental results show that the proposed method can achieve better\\rperformance than that of the state-of-the-art approaches.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01105 ,  36597kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01108\\rDate: Sat, 2 Mar 2024 07:02:17 GMT   (17473kb,D)\\r\\rTitle: Face Swap via Diffusion Model\\rAuthors: Feifei Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  This technical report presents a diffusion model based framework for face\\rswapping between two portrait images. The basic framework consists of three\\rcomponents, i.e., IP-Adapter, ControlNet, and Stable Diffusion's inpainting\\rpipeline, for face feature encoding, multi-conditional generation, and face\\rinpainting respectively. Besides, I introduce facial guidance optimization and\\rCodeFormer based blending to further improve the generation quality.\\r  Specifically, we engage a recent light-weighted customization method (i.e.,\\rDreamBooth-LoRA), to guarantee the identity consistency by 1) using a rare\\ridentifier sks to represent the source identity, and 2) injecting the image\\rfeatures of source portrait into each cross-attention layer like the text\\rfeatures. Then I resort to the strong inpainting ability of Stable Diffusion,\\rand utilize canny image and face detection annotation of the target portrait as\\rthe conditions, to guide ContorlNet's generation and align source portrait with\\rthe target portrait. To further correct face alignment, we add the facial\\rguidance loss to optimize the text embedding during the sample generation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01108 ,  17473kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01118\\rDate: Sat, 2 Mar 2024 08:03:42 GMT   (11153kb,D)\\r\\rTitle: Adversarial Testing for Visual Grounding via Image-Aware Property\\r  Reduction\\rAuthors: Zhiyuan Chang, Mingyang Li, Junjie Wang, Cheng Li, Boyu Wu, Fanjiang\\r  Xu, Qing Wang\\rCategories: cs.CV cs.AI\\rComments: 14pages, 6 figures\\r\\\\\\\\\\r  Due to the advantages of fusing information from various modalities,\\rmultimodal learning is gaining increasing attention. Being a fundamental task\\rof multimodal learning, Visual Grounding (VG), aims to locate objects in images\\rthrough natural language expressions. Ensuring the quality of VG models\\rpresents significant challenges due to the complex nature of the task. In the\\rblack box scenario, existing adversarial testing techniques often fail to fully\\rexploit the potential of both modalities of information. They typically apply\\rperturbations based solely on either the image or text information,\\rdisregarding the crucial correlation between the two modalities, which would\\rlead to failures in test oracles or an inability to effectively challenge VG\\rmodels. To this end, we propose PEELING, a text perturbation approach via\\rimage-aware property reduction for adversarial testing of the VG model. The\\rcore idea is to reduce the property-related information in the original\\rexpression meanwhile ensuring the reduced expression can still uniquely\\rdescribe the original object in the image. To achieve this, PEELING first\\rconducts the object and properties extraction and recombination to generate\\rcandidate property reduction expressions. It then selects the satisfied\\rexpressions that accurately describe the original object while ensuring no\\rother objects in the image fulfill the expression, through querying the image\\rwith a visual understanding technique. We evaluate PEELING on the\\rstate-of-the-art VG model, i.e. OFA-VG, involving three commonly used datasets.\\rResults show that the adversarial tests generated by PEELING achieves 21.4% in\\rMultiModal Impact score (MMI), and outperforms state-of-the-art baselines for\\rimages and texts by 8.2%--15.1%.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01118 ,  11153kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01123\\rDate: Sat, 2 Mar 2024 08:06:18 GMT   (1211kb,D)\\r\\rTitle: ELA: Efficient Local Attention for Deep Convolutional Neural Networks\\rAuthors: Wei Xu and Yi Wan\\rCategories: cs.CV\\r\\\\\\\\\\r  The attention mechanism has gained significant recognition in the field of\\rcomputer vision due to its ability to effectively enhance the performance of\\rdeep neural networks. However, existing methods often struggle to effectively\\rutilize spatial information or, if they do, they come at the cost of reducing\\rchannel dimensions or increasing the complexity of neural networks. In order to\\raddress these limitations, this paper introduces an Efficient Local Attention\\r(ELA) method that achieves substantial performance improvements with a simple\\rstructure. By analyzing the limitations of the Coordinate Attention method, we\\ridentify the lack of generalization ability in Batch Normalization, the adverse\\reffects of dimension reduction on channel attention, and the complexity of\\rattention generation process. To overcome these challenges, we propose the\\rincorporation of 1D convolution and Group Normalization feature enhancement\\rtechniques. This approach enables accurate localization of regions of interest\\rby efficiently encoding two 1D positional feature maps without the need for\\rdimension reduction, while allowing for a lightweight implementation. We\\rcarefully design three hyperparameters in ELA, resulting in four different\\rversions: ELA-T, ELA-B, ELA-S, and ELA-L, to cater to the specific requirements\\rof different visual tasks such as image classification, object detection and\\rsementic segmentation. ELA can be seamlessly integrated into deep CNN networks\\rsuch as ResNet, MobileNet, and DeepLab. Extensive evaluations on the ImageNet,\\rMSCOCO, and Pascal VOC datasets demonstrate the superiority of the proposed ELA\\rmodule over current state-of-the-art methods in all three aforementioned visual\\rtasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01123 ,  1211kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01124\\rDate: Sat, 2 Mar 2024 08:10:54 GMT   (7532kb,D)\\r\\rTitle: Text-guided Explorable Image Super-resolution\\rAuthors: Kanchana Vaishnavi Gandikota, Paramanand Chandramouli\\rCategories: cs.CV\\rComments: CVPR 2024\\r\\\\\\\\\\r  In this paper, we introduce the problem of zero-shot text-guided exploration\\rof the solutions to open-domain image super-resolution. Our goal is to allow\\rusers to explore diverse, semantically accurate reconstructions that preserve\\rdata consistency with the low-resolution inputs for different large\\rdownsampling factors without explicitly training for these specific\\rdegradations. We propose two approaches for zero-shot text-guided\\rsuper-resolution - i) modifying the generative process of text-to-image\\r\\\\textit{T2I} diffusion models to promote consistency with low-resolution\\rinputs, and ii) incorporating language guidance into zero-shot diffusion-based\\rrestoration methods. We show that the proposed approaches result in diverse\\rsolutions that match the semantic meaning provided by the text prompt while\\rpreserving data consistency with the degraded inputs. We evaluate the proposed\\rbaselines for the task of extreme super-resolution and demonstrate advantages\\rin terms of restoration quality, diversity, and explorability of solutions.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01124 ,  7532kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01129\\rDate: Sat, 2 Mar 2024 08:18:57 GMT   (286370kb,D)\\r\\rTitle: Dynamic 3D Point Cloud Sequences as 2D Videos\\rAuthors: Yiming Zeng, Junhui Hou, Qijian Zhang, Siyu Ren, Wenping Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  Dynamic 3D point cloud sequences serve as one of the most common and\\rpractical representation modalities of dynamic real-world environments.\\rHowever, their unstructured nature in both spatial and temporal domains poses\\rsignificant challenges to effective and efficient processing. Existing deep\\rpoint cloud sequence modeling approaches imitate the mature 2D video learning\\rmechanisms by developing complex spatio-temporal point neighbor grouping and\\rfeature aggregation schemes, often resulting in methods lacking effectiveness,\\refficiency, and expressive power. In this paper, we propose a novel generic\\rrepresentation called \\\\textit{Structured Point Cloud Videos} (SPCVs).\\rIntuitively, by leveraging the fact that 3D geometric shapes are essentially 2D\\rmanifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatial\\rsmoothness and temporal consistency, where the pixel values correspond to the\\r3D coordinates of points. The structured nature of our SPCV representation\\rallows for the seamless adaptation of well-established 2D image/video\\rtechniques, enabling efficient and effective processing and analysis of 3D\\rpoint cloud sequences. To achieve such re-organization, we design a\\rself-supervised learning pipeline that is geometrically regularized and driven\\rby self-reconstructive and deformation field learning objectives. Additionally,\\rwe construct SPCV-based frameworks for both low-level and high-level 3D point\\rcloud sequence processing and analysis tasks, including action recognition,\\rtemporal interpolation, and compression. Extensive experiments demonstrate the\\rversatility and superiority of the proposed SPCV, which has the potential to\\roffer new possibilities for deep learning on unstructured 3D point cloud\\rsequences. Code will be released at https://github.com/ZENGYIMING-EAMON/SPCV.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01129 ,  286370kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01137\\rDate: Sat, 2 Mar 2024 08:49:02 GMT   (16309kb,D)\\r\\rTitle: Neural radiance fields-based holography [Invited]\\rAuthors: Minsung Kang and Fan Wang and Kai Kumano and Tomoyoshi Ito and\\r  Tomoyoshi Shimobaba\\rCategories: cs.CV cs.GR eess.IV\\r\\\\\\\\\\r  This study presents a novel approach for generating holograms based on the\\rneural radiance fields (NeRF) technique. Generating three-dimensional (3D) data\\ris difficult in hologram computation. NeRF is a state-of-the-art technique for\\r3D light-field reconstruction from 2D images based on volume rendering. The\\rNeRF can rapidly predict new-view images that do not include a training\\rdataset. In this study, we constructed a rendering pipeline directly from a 3D\\rlight field generated from 2D images by NeRF for hologram generation using deep\\rneural networks within a reasonable time. The pipeline comprises three main\\rcomponents: the NeRF, a depth predictor, and a hologram generator, all\\rconstructed using deep neural networks. The pipeline does not include any\\rphysical calculations. The predicted holograms of a 3D scene viewed from any\\rdirection were computed using the proposed pipeline. The simulation and\\rexperimental results are presented.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01137 ,  16309kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01142\\rDate: Sat, 2 Mar 2024 09:00:57 GMT   (41634kb,D)\\r\\rTitle: Edge-guided Low-light Image Enhancement with Inertial Bregman\\r  Alternating Linearized Minimization\\rAuthors: Chaoyan Huang, Zhongming Wu, Tieyong Zeng\\rCategories: cs.CV cs.NA math.NA\\rComments: 15 pages\\rMSC-class: 65K05, 65K15, 94A08\\r\\\\\\\\\\r  Prior-based methods for low-light image enhancement often face challenges in\\rextracting available prior information from dim images. To overcome this\\rlimitation, we introduce a simple yet effective Retinex model with the proposed\\redge extraction prior. More specifically, we design an edge extraction network\\rto capture the fine edge features from the low-light image directly. Building\\rupon the Retinex theory, we decompose the low-light image into its illumination\\rand reflectance components and introduce an edge-guided Retinex model for\\renhancing low-light images. To solve the proposed model, we propose a novel\\rinertial Bregman alternating linearized minimization algorithm. This algorithm\\raddresses the optimization problem associated with the edge-guided Retinex\\rmodel, enabling effective enhancement of low-light images. Through rigorous\\rtheoretical analysis, we establish the convergence properties of the algorithm.\\rBesides, we prove that the proposed algorithm converges to a stationary point\\rof the problem through nonconvex optimization theory. Furthermore, extensive\\rexperiments are conducted on multiple real-world low-light image datasets to\\rdemonstrate the efficiency and superiority of the proposed scheme.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01142 ,  41634kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01156\\rDate: Sat, 2 Mar 2024 10:03:21 GMT   (8850kb,D)\\r\\rTitle: Auxiliary Tasks Enhanced Dual-affinity Learning for Weakly Supervised\\r  Semantic Segmentation\\rAuthors: Lian Xu, Mohammed Bennamoun, Farid Boussaid, Wanli Ouyang, Ferdous\\r  Sohel, Dan Xu\\rCategories: cs.CV\\rComments: Accepted at IEEE Transactions on Neural Networks and Learning\\r  Systems. arXiv admin note: substantial text overlap with arXiv:2107.11787\\r\\\\\\\\\\r  Most existing weakly supervised semantic segmentation (WSSS) methods rely on\\rClass Activation Mapping (CAM) to extract coarse class-specific localization\\rmaps using image-level labels. Prior works have commonly used an off-line\\rheuristic thresholding process that combines the CAM maps with off-the-shelf\\rsaliency maps produced by a general pre-trained saliency model to produce more\\raccurate pseudo-segmentation labels. We propose AuxSegNet+, a weakly supervised\\rauxiliary learning framework to explore the rich information from these\\rsaliency maps and the significant inter-task correlation between saliency\\rdetection and semantic segmentation. In the proposed AuxSegNet+, saliency\\rdetection and multi-label image classification are used as auxiliary tasks to\\rimprove the primary task of semantic segmentation with only image-level\\rground-truth labels. We also propose a cross-task affinity learning mechanism\\rto learn pixel-level affinities from the saliency and segmentation feature\\rmaps. In particular, we propose a cross-task dual-affinity learning module to\\rlearn both pairwise and unary affinities, which are used to enhance the\\rtask-specific features and predictions by aggregating both query-dependent and\\rquery-independent global context for both saliency detection and semantic\\rsegmentation. The learned cross-task pairwise affinity can also be used to\\rrefine and propagate CAM maps to provide better pseudo labels for both tasks.\\rIterative improvement of segmentation performance is enabled by cross-task\\raffinity learning and pseudo-label updating. Extensive experiments demonstrate\\rthe effectiveness of the proposed approach with new state-of-the-art WSSS\\rresults on the challenging PASCAL VOC and MS COCO benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01156 ,  8850kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01169\\rDate: Sat, 2 Mar 2024 10:42:47 GMT   (47740kb,D)\\r\\rTitle: Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection\\rAuthors: Chenchen Tao, Chong Wang, Yuexian Zou, Xiaohao Peng, Jiafei Wu and\\r  Jiangbo Qian\\rCategories: cs.CV\\r\\\\\\\\\\r  Most models for weakly supervised video anomaly detection (WS-VAD) rely on\\rmultiple instance learning, aiming to distinguish normal and abnormal snippets\\rwithout specifying the type of anomaly. The ambiguous nature of anomaly\\rdefinitions across contexts introduces bias in detecting abnormal and normal\\rsnippets within the abnormal bag. Taking the first step to show the model why\\rit is anomalous, a novel framework is proposed to guide the learning of\\rsuspected anomalies from event prompts. Given a textual prompt dictionary of\\rpotential anomaly events and the captions generated from anomaly videos, the\\rsemantic anomaly similarity between them could be calculated to identify the\\rsuspected anomalous events for each video snippet. It enables a new\\rmulti-prompt learning process to constrain the visual-semantic features across\\rall videos, as well as provides a new way to label pseudo anomalies for\\rself-training. To demonstrate effectiveness, comprehensive experiments and\\rdetailed ablation studies are conducted on four datasets, namely XD-Violence,\\rUCF-Crime, TAD, and ShanghaiTech. Our proposed model outperforms most\\rstate-of-the-art methods in terms of AP or AUC (82.6\\\\%, 87.7\\\\%, 93.1\\\\%, and\\r97.4\\\\%). Furthermore, it shows promising performance in open-set and\\rcross-dataset cases.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01169 ,  47740kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01172\\rDate: Sat, 2 Mar 2024 10:56:14 GMT   (46135kb,D)\\r\\rTitle: Run-time Introspection of 2D Object Detection in Automated Driving\\r  Systems Using Learning Representations\\rAuthors: Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger\\r  Woodman\\rCategories: cs.CV\\rComments: Submitted to IEEE Transactions on Intelligent Vehicles. 15 pages, 7\\r  figures, 11 tables\\r\\\\\\\\\\r  Reliable detection of various objects and road users in the surrounding\\renvironment is crucial for the safe operation of automated driving systems\\r(ADS). Despite recent progresses in developing highly accurate object detectors\\rbased on Deep Neural Networks (DNNs), they still remain prone to detection\\rerrors, which can lead to fatal consequences in safety-critical applications\\rsuch as ADS. An effective remedy to this problem is to equip the system with\\rrun-time monitoring, named as introspection in the context of autonomous\\rsystems. Motivated by this, we introduce a novel introspection solution, which\\roperates at the frame level for DNN-based 2D object detection and leverages\\rneural network activation patterns. The proposed approach pre-processes the\\rneural activation patterns of the object detector's backbone using several\\rdifferent modes. To provide extensive comparative analysis and fair comparison,\\rwe also adapt and implement several state-of-the-art (SOTA) introspection\\rmechanisms for error detection in 2D object detection, using one-stage and\\rtwo-stage object detectors evaluated on KITTI and BDD datasets. We compare the\\rperformance of the proposed solution in terms of error detection, adaptability\\rto dataset shift, and, computational and memory resource requirements. Our\\rperformance evaluation shows that the proposed introspection solution\\routperforms SOTA methods, achieving an absolute reduction in the missed error\\rratio of 9% to 17% in the BDD dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01172 ,  46135kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01174\\rDate: Sat, 2 Mar 2024 10:56:27 GMT   (10024kb,D)\\r\\rTitle: Consistent and Asymptotically Statistically-Efficient Solution to Camera\\r  Motion Estimation\\rAuthors: Guangyang Zeng, Qingcheng Zeng, Xinghan Li, Biqiang Mu, Jiming Chen,\\r  Ling Shi, and Junfeng Wu\\rCategories: cs.CV\\r\\\\\\\\\\r  Given 2D point correspondences between an image pair, inferring the camera\\rmotion is a fundamental issue in the computer vision community. The existing\\rworks generally set out from the epipolar constraint and estimate the essential\\rmatrix, which is not optimal in the maximum likelihood (ML) sense. In this\\rpaper, we dive into the original measurement model with respect to the rotation\\rmatrix and normalized translation vector and formulate the ML problem. We then\\rpropose a two-step algorithm to solve it: In the first step, we estimate the\\rvariance of measurement noises and devise a consistent estimator based on bias\\relimination; In the second step, we execute a one-step Gauss-Newton iteration\\ron manifold to refine the consistent estimate. We prove that the proposed\\restimate owns the same asymptotic statistical properties as the ML estimate:\\rThe first is consistency, i.e., the estimate converges to the ground truth as\\rthe point number increases; The second is asymptotic efficiency, i.e., the mean\\rsquared error of the estimate converges to the theoretical lower bound --\\rCramer-Rao bound. In addition, we show that our algorithm has linear time\\rcomplexity. These appealing characteristics endow our estimator with a great\\radvantage in the case of dense point correspondences. Experiments on both\\rsynthetic data and real images demonstrate that when the point number reaches\\rthe order of hundreds, our estimator outperforms the state-of-the-art ones in\\rterms of estimation accuracy and CPU time.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01174 ,  10024kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01183\\rDate: Sat, 2 Mar 2024 11:44:14 GMT   (37789kb,D)\\r\\rTitle: Leveraging Self-Supervised Learning for Scene Recognition in Child\\r  Sexual Abuse Imagery\\rAuthors: Pedro H. V. Valois, Jo\\\\~ao Macedo, Leo S. F. Ribeiro, Jefersson A. dos\\r  Santos, Sandra Avila\\rCategories: cs.CV cs.AI cs.CY cs.LG\\rComments: 13 pages, 5 figures, 4 tables. Under review\\r\\\\\\\\\\r  Crime in the 21st century is split into a virtual and real world. However,\\rthe former has become a global menace to people's well-being and security in\\rthe latter. The challenges it presents must be faced with unified global\\rcooperation, and we must rely more than ever on automated yet trustworthy tools\\rto combat the ever-growing nature of online offenses. Over 10 million child\\rsexual abuse reports are submitted to the US National Center for Missing &\\rExploited Children every year, and over 80% originated from online sources.\\rTherefore, investigation centers and clearinghouses cannot manually process and\\rcorrectly investigate all imagery. In light of that, reliable automated tools\\rthat can securely and efficiently deal with this data are paramount. In this\\rsense, the scene recognition task looks for contextual cues in the environment,\\rbeing able to group and classify child sexual abuse data without requiring to\\rbe trained on sensitive material. The scarcity and limitations of working with\\rchild sexual abuse images lead to self-supervised learning, a machine-learning\\rmethodology that leverages unlabeled data to produce powerful representations\\rthat can be more easily transferred to target tasks. This work shows that\\rself-supervised deep learning models pre-trained on scene-centric data can\\rreach 71.6% balanced accuracy on our indoor scene classification task and, on\\raverage, 2.2 percentage points better performance than a fully supervised\\rversion. We cooperate with Brazilian Federal Police experts to evaluate our\\rindoor classification model on actual child abuse material. The results\\rdemonstrate a notable discrepancy between the features observed in widely used\\rscene datasets and those depicted on sensitive materials.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01183 ,  37789kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01209\\rDate: Sat, 2 Mar 2024 13:43:32 GMT   (25896kb,D)\\r\\rTitle: Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning\\rAuthors: Shuo Yang, Zirui Shang, Yongqi Wang, Derong Deng, Hongwei Chen, Qiyuan\\r  Cheng, Xinxiao Wu\\rCategories: cs.CV\\r\\\\\\\\\\r  This paper proposes a novel framework for multi-label image recognition\\rwithout any training data, called data-free framework, which uses knowledge of\\rpre-trained Large Language Model (LLM) to learn prompts to adapt pretrained\\rVision-Language Model (VLM) like CLIP to multilabel classification. Through\\rasking LLM by well-designed questions, we acquire comprehensive knowledge about\\rcharacteristics and contexts of objects, which provides valuable text\\rdescriptions for learning prompts. Then we propose a hierarchical prompt\\rlearning method by taking the multi-label dependency into consideration,\\rwherein a subset of category-specific prompt tokens are shared when the\\rcorresponding objects exhibit similar attributes or are more likely to\\rco-occur. Benefiting from the remarkable alignment between visual and\\rlinguistic semantics of CLIP, the hierarchical prompts learned from text\\rdescriptions are applied to perform classification of images during inference.\\rOur framework presents a new way to explore the synergies between multiple\\rpre-trained models for novel category recognition. Extensive experiments on\\rthree public datasets (MS-COCO, VOC2007, and NUS-WIDE) demonstrate that our\\rmethod achieves better results than the state-of-the-art methods, especially\\routperforming the zero-shot multi-label recognition methods by 4.7% in mAP on\\rMS-COCO.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01209 ,  25896kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01210\\rDate: Sat, 2 Mar 2024 13:52:28 GMT   (3049kb,D)\\r\\rTitle: SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with\\r  Target Scattering Feature Parameters\\rAuthors: Jiahao Cui, Jiale Duan, Binyan Luo, Hang Cao, Wang Guo, Haifeng Li\\rCategories: cs.CV cs.AI\\rComments: 10 pages, 9 figures, 2 tables\\r\\\\\\\\\\r  Deep neural network-based Synthetic Aperture Radar (SAR) target recognition\\rmodels are susceptible to adversarial examples. Current adversarial example\\rgeneration methods for SAR imagery primarily operate in the 2D digital domain,\\rknown as image adversarial examples. Recent work, while considering SAR imaging\\rscatter mechanisms, fails to account for the actual imaging process, rendering\\rattacks in the three-dimensional physical domain infeasible, termed pseudo\\rphysics adversarial examples. To address these challenges, this paper proposes\\rSAR-AE-SFP-Attack, a method to generate real physics adversarial examples by\\raltering the scattering feature parameters of target objects. Specifically, we\\riteratively optimize the coherent energy accumulation of the target echo by\\rperturbing the reflection coefficient and scattering coefficient in the\\rscattering feature parameters of the three-dimensional target object, and\\robtain the adversarial example after echo signal processing and imaging\\rprocessing in the RaySAR simulator. Experimental results show that compared to\\rdigital adversarial attack methods, SAR-AE-SFP Attack significantly improves\\rattack efficiency on CNN-based models (over 30\\\\%) and Transformer-based models\\r(over 13\\\\%), demonstrating significant transferability of attack effects across\\rdifferent models and perspectives.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01210 ,  3049kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01212\\rDate: Sat, 2 Mar 2024 13:59:02 GMT   (19513kb,D)\\r\\rTitle: TCIG: Two-Stage Controlled Image Generation with Quality Enhancement\\r  through Diffusion\\rAuthors: Salaheldin Mohamed\\rCategories: cs.CV\\r\\\\\\\\\\r  In recent years, significant progress has been made in the development of\\rtext- to-image generation models. However, these models still face limitations\\rwhen it comes to achieving full controllability during the generation process.\\rOften, spe- cific training or the use of limited models is required, and even\\rthen, they have certain restrictions. To address these challenges, A two-stage\\rmethod that effec- tively combines controllability and high quality in the\\rgeneration of images is proposed. This approach leverages the expertise of\\rpre-trained models to achieve precise control over the generated images, while\\ralso harnessing the power of diffusion models to achieve state-of-the-art\\rquality. By separating controllability from high quality, This method achieves\\routstanding results. It is compatible with both latent and image space\\rdiffusion models, ensuring versatility and flexibil- ity. Moreover, This\\rapproach consistently produces comparable outcomes to the current\\rstate-of-the-art methods in the field. Overall, This proposed method rep-\\rresents a significant advancement in text-to-image generation, enabling\\rimproved controllability without compromising on the quality of the generated\\rimages.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01212 ,  19513kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01214\\rDate: Sat, 2 Mar 2024 14:05:15 GMT   (5854kb,D)\\r\\rTitle: Boosting Box-supervised Instance Segmentation with Pseudo Depth\\rAuthors: Xinyi Yu, Ling Yan, Pengtao Jiang, Hao Chen, Bo Li, Lin Yuanbo Wu,\\r  Linlin Ou\\rCategories: cs.CV\\r\\\\\\\\\\r  The realm of Weakly Supervised Instance Segmentation (WSIS) under box\\rsupervision has garnered substantial attention, showcasing remarkable\\radvancements in recent years. However, the limitations of box supervision\\rbecome apparent in its inability to furnish effective information for\\rdistinguishing foreground from background within the specified target box. This\\rresearch addresses this challenge by introducing pseudo-depth maps into the\\rtraining process of the instance segmentation network, thereby boosting its\\rperformance by capturing depth differences between instances. These\\rpseudo-depth maps are generated using a readily available depth predictor and\\rare not necessary during the inference stage. To enable the network to discern\\rdepth features when predicting masks, we integrate a depth prediction layer\\rinto the mask prediction head. This innovative approach empowers the network to\\rsimultaneously predict masks and depth, enhancing its ability to capture\\rnuanced depth-related information during the instance segmentation process. We\\rfurther utilize the mask generated in the training process as supervision to\\rdistinguish the foreground from the background. When selecting the best mask\\rfor each box through the Hungarian algorithm, we use depth consistency as one\\rcalculation cost item. The proposed method achieves significant improvements on\\rCityscapes and COCO dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01214 ,  5854kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01226\\rDate: Sat, 2 Mar 2024 14:52:58 GMT   (3432kb,D)\\r\\rTitle: DiffSal: Joint Audio and Video Learning for Diffusion Saliency\\r  Prediction\\rAuthors: Junwen Xiong, Peng Zhang, Tao You, Chuanyue Li, Wei Huang, Yufei Zha\\rCategories: cs.CV\\rComments: 15 pages, CVPR24\\r\\\\\\\\\\r  Audio-visual saliency prediction can draw support from diverse modality\\rcomplements, but further performance enhancement is still challenged by\\rcustomized architectures as well as task-specific loss functions. In recent\\rstudies, denoising diffusion models have shown more promising in unifying task\\rframeworks owing to their inherent ability of generalization. Following this\\rmotivation, a novel Diffusion architecture for generalized audio-visual\\rSaliency prediction (DiffSal) is proposed in this work, which formulates the\\rprediction problem as a conditional generative task of the saliency map by\\rutilizing input audio and video as the conditions. Based on the spatio-temporal\\raudio-visual features, an extra network Saliency-UNet is designed to perform\\rmulti-modal attention modulation for progressive refinement of the ground-truth\\rsaliency map from the noisy map. Extensive experiments demonstrate that the\\rproposed DiffSal can achieve excellent performance across six challenging\\raudio-visual benchmarks, with an average relative improvement of 6.3\\\\% over the\\rprevious state-of-the-art results by six metrics.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01226 ,  3432kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01229\\rDate: Sat, 2 Mar 2024 15:14:58 GMT   (13945kb,D)\\r\\rTitle: REWIND Dataset: Privacy-preserving Speaking Status Segmentation from\\r  Multimodal Body Movement Signals in the Wild\\rAuthors: Jose Vargas Quiros, Chirag Raman, Stephanie Tan, Ekin Gedik, Laura\\r  Cabrera-Quiros, Hayley Hung\\rCategories: cs.CV cs.AI cs.LG eess.SP\\r\\\\\\\\\\r  Recognizing speaking in humans is a central task towards understanding social\\rinteractions. Ideally, speaking would be detected from individual voice\\rrecordings, as done previously for meeting scenarios. However, individual voice\\rrecordings are hard to obtain in the wild, especially in crowded mingling\\rscenarios due to cost, logistics, and privacy concerns. As an alternative,\\rmachine learning models trained on video and wearable sensor data make it\\rpossible to recognize speech by detecting its related gestures in an\\runobtrusive, privacy-preserving way. These models themselves should ideally be\\rtrained using labels obtained from the speech signal. However, existing\\rmingling datasets do not contain high quality audio recordings. Instead,\\rspeaking status annotations have often been inferred by human annotators from\\rvideo, without validation of this approach against audio-based ground truth. In\\rthis paper we revisit no-audio speaking status estimation by presenting the\\rfirst publicly available multimodal dataset with high-quality individual speech\\rrecordings of 33 subjects in a professional networking event. We present three\\rbaselines for no-audio speaking status segmentation: a) from video, b) from\\rbody acceleration (chest-worn accelerometer), c) from body pose tracks. In all\\rcases we predict a 20Hz binary speaking status signal extracted from the audio,\\ra time resolution not available in previous datasets. In addition to providing\\rthe signals and ground truth necessary to evaluate a wide range of speaking\\rstatus detection methods, the availability of audio in REWIND makes it suitable\\rfor cross-modality studies not feasible with previous mingling datasets.\\rFinally, our flexible data consent setup creates new challenges for multimodal\\rsystems under missing modalities.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01229 ,  13945kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01231\\rDate: Sat, 2 Mar 2024 15:20:09 GMT   (37777kb,D)\\r\\rTitle: Benchmarking Segmentation Models with Mask-Preserved Attribute Editing\\rAuthors: Zijin Yin, Kongming Liang, Bing Li, Zhanyu Ma, Jun Guo\\rCategories: cs.CV\\rComments: CVPR 2024\\r\\\\\\\\\\r  When deploying segmentation models in practice, it is critical to evaluate\\rtheir behaviors in varied and complex scenes. Different from the previous\\revaluation paradigms only in consideration of global attribute variations (e.g.\\radverse weather), we investigate both local and global attribute variations for\\rrobustness evaluation. To achieve this, we construct a mask-preserved attribute\\rediting pipeline to edit visual attributes of real images with precise control\\rof structural information. Therefore, the original segmentation labels can be\\rreused for the edited images. Using our pipeline, we construct a benchmark\\rcovering both object and image attributes (e.g. color, material, pattern,\\rstyle). We evaluate a broad variety of semantic segmentation models, spanning\\rfrom conventional close-set models to recent open-vocabulary large models on\\rtheir robustness to different types of variations. We find that both local and\\rglobal attribute variations affect segmentation performances, and the\\rsensitivity of models diverges across different variation types. We argue that\\rlocal attributes have the same importance as global attributes, and should be\\rconsidered in the robustness evaluation of segmentation models. Code:\\rhttps://github.com/PRIS-CV/Pascal-EA.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01231 ,  37777kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01238\\rDate: Sat, 2 Mar 2024 15:47:42 GMT   (361kb,D)\\r\\rTitle: On the Road to Portability: Compressing End-to-End Motion Planner for\\r  Autonomous Driving\\rAuthors: Kaituo Feng, Changsheng Li, Dongchun Ren, Ye Yuan, Guoren Wang\\rCategories: cs.CV\\rComments: Accepted by CVPR 2024\\r\\\\\\\\\\r  End-to-end motion planning models equipped with deep neural networks have\\rshown great potential for enabling full autonomous driving. However, the\\roversized neural networks render them impractical for deployment on\\rresource-constrained systems, which unavoidably requires more computational\\rtime and resources during reference.To handle this, knowledge distillation\\roffers a promising approach that compresses models by enabling a smaller\\rstudent model to learn from a larger teacher model. Nevertheless, how to apply\\rknowledge distillation to compress motion planners has not been explored so\\rfar. In this paper, we propose PlanKD, the first knowledge distillation\\rframework tailored for compressing end-to-end motion planners. First,\\rconsidering that driving scenes are inherently complex, often containing\\rplanning-irrelevant or even noisy information, transferring such information is\\rnot beneficial for the student planner. Thus, we design an information\\rbottleneck based strategy to only distill planning-relevant information, rather\\rthan transfer all information indiscriminately. Second, different waypoints in\\ran output planned trajectory may hold varying degrees of importance for motion\\rplanning, where a slight deviation in certain crucial waypoints might lead to a\\rcollision. Therefore, we devise a safety-aware waypoint-attentive distillation\\rmodule that assigns adaptive weights to different waypoints based on the\\rimportance, to encourage the student to accurately mimic more crucial\\rwaypoints, thereby improving overall safety. Experiments demonstrate that our\\rPlanKD can boost the performance of smaller planners by a large margin, and\\rsignificantly reduce their reference time.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01238 ,  361kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01246\\rDate: Sat, 2 Mar 2024 16:13:06 GMT   (6022kb,D)\\r\\rTitle: Dual Graph Attention based Disentanglement Multiple Instance Learning\\r  for Brain Age Estimation\\rAuthors: Fanzhe Yan, Gang Yang, Yu Li, Aiping Liu, Xun Chen\\rCategories: cs.CV\\rComments: 12 pages, 9 figures\\r\\\\\\\\\\r  Deep learning techniques have demonstrated great potential for accurately\\restimating brain age by analyzing Magnetic Resonance Imaging (MRI) data from\\rhealthy individuals. However, current methods for brain age estimation often\\rdirectly utilize whole input images, overlooking two important considerations:\\r1) the heterogeneous nature of brain aging, where different brain regions may\\rdegenerate at different rates, and 2) the existence of age-independent\\rredundancies in brain structure. To overcome these limitations, we propose a\\rDual Graph Attention based Disentanglement Multi-instance Learning (DGA-DMIL)\\rframework for improving brain age estimation. Specifically, the 3D MRI data,\\rtreated as a bag of instances, is fed into a 2D convolutional neural network\\rbackbone, to capture the unique aging patterns in MRI. A dual graph attention\\raggregator is then proposed to learn the backbone features by exploiting the\\rintra- and inter-instance relationships. Furthermore, a disentanglement branch\\ris introduced to separate age-related features from age-independent structural\\rrepresentations to ameliorate the interference of redundant information on age\\rprediction. To verify the effectiveness of the proposed framework, we evaluate\\rit on two datasets, UK Biobank and ADNI, containing a total of 35,388 healthy\\rindividuals. Our proposed model demonstrates exceptional accuracy in estimating\\rbrain age, achieving a remarkable mean absolute error of 2.12 years in the UK\\rBiobank. The results establish our approach as state-of-the-art compared to\\rother competing brain age estimation models. In addition, the instance\\rcontribution scores identify the varied importance of brain areas for aging\\rprediction, which provides deeper insights into the understanding of brain\\raging.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01246 ,  6022kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01248\\rDate: Sat, 2 Mar 2024 16:16:26 GMT   (41902kb,D)\\r\\rTitle: SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code\\rAuthors: Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A.\\r  Ross, Cordelia Schmid, Alireza Fathi\\rCategories: cs.CV cs.AI cs.CL cs.LG\\r\\\\\\\\\\r  This paper introduces SceneCraft, a Large Language Model (LLM) Agent\\rconverting text descriptions into Blender-executable Python scripts which\\rrender complex scenes with up to a hundred 3D assets. This process requires\\rcomplex spatial planning and arrangement. We tackle these challenges through a\\rcombination of advanced abstraction, strategic planning, and library learning.\\rSceneCraft first models a scene graph as a blueprint, detailing the spatial\\rrelationships among assets in the scene. SceneCraft then writes Python scripts\\rbased on this graph, translating relationships into numerical constraints for\\rasset layout. Next, SceneCraft leverages the perceptual strengths of\\rvision-language foundation models like GPT-V to analyze rendered images and\\riteratively refine the scene. On top of this process, SceneCraft features a\\rlibrary learning mechanism that compiles common script functions into a\\rreusable library, facilitating continuous self-improvement without expensive\\rLLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses\\rexisting LLM-based agents in rendering complex scenes, as shown by its\\radherence to constraints and favorable human assessments. We also showcase the\\rbroader application potential of SceneCraft by reconstructing detailed 3D\\rscenes from the Sintel movie and guiding a video generative model with\\rgenerated scenes as intermediary control signal.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01248 ,  41902kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01263\\rDate: Sat, 2 Mar 2024 16:51:35 GMT   (3525kb)\\r\\rTitle: Single-image camera calibration with model-free distortion correction\\rAuthors: Katia Genovese\\rCategories: cs.CV\\r\\\\\\\\\\r  Camera calibration is a process of paramount importance in computer vision\\rapplications that require accurate quantitative measurements. The popular\\rmethod developed by Zhang relies on the use of a large number of images of a\\rplanar grid of fiducial points captured in multiple poses. Although flexible\\rand easy to implement, Zhang's method has some limitations. The simultaneous\\roptimization of the entire parameter set, including the coefficients of a\\rpredefined distortion model, may result in poor distortion correction at the\\rimage boundaries or in miscalculation of the intrinsic parameters, even with a\\rreasonably small reprojection error. Indeed, applications involving image\\rstitching (e.g. multi-camera systems) require accurate mapping of distortion up\\rto the outermost regions of the image. Moreover, intrinsic parameters affect\\rthe accuracy of camera pose estimation, which is fundamental for applications\\rsuch as vision servoing in robot navigation and automated assembly. This paper\\rproposes a method for estimating the complete set of calibration parameters\\rfrom a single image of a planar speckle pattern covering the entire sensor. The\\rcorrespondence between image points and physical points on the calibration\\rtarget is obtained using Digital Image Correlation. The effective focal length\\rand the extrinsic parameters are calculated separately after a prior evaluation\\rof the principal point. At the end of the procedure, a dense and uniform\\rmodel-free distortion map is obtained over the entire image. Synthetic data\\rwith different noise levels were used to test the feasibility of the proposed\\rmethod and to compare its metrological performance with Zhang's method.\\rReal-world tests demonstrate the potential of the developed method to reveal\\raspects of the image formation that are hidden by averaging over multiple\\rimages.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01263 ,  3525kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01281\\rDate: Sat, 2 Mar 2024 18:28:32 GMT   (130148kb,D)\\r\\rTitle: Fast Low-parameter Video Activity Localization in Collaborative Learning\\r  Environments\\rAuthors: Venkatesh Jatla, Sravani Teeparthi, Ugesh Egala, Sylvia Celedon\\r  Pattichis, Marios S. Patticis\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Research on video activity detection has primarily focused on identifying\\rwell-defined human activities in short video segments. The majority of the\\rresearch on video activity recognition is focused on the development of large\\rparameter systems that require training on large video datasets. This paper\\rdevelops a low-parameter, modular system with rapid inferencing capabilities\\rthat can be trained entirely on limited datasets without requiring transfer\\rlearning from large-parameter systems. The system can accurately detect and\\rassociate specific activities with the students who perform the activities in\\rreal-life classroom videos. Additionally, the paper develops an interactive\\rweb-based application to visualize human activity maps over long real-life\\rclassroom videos.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01281 ,  130148kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01300\\rDate: Sat, 2 Mar 2024 19:54:53 GMT   (1703kb,D)\\r\\rTitle: Causal Mode Multiplexer: A Novel Framework for Unbiased Multispectral\\r  Pedestrian Detection\\rAuthors: Taeheon Kim, Sebin Shin, Youngjoon Yu, Hak Gu Kim, and Yong Man Ro\\rCategories: cs.CV\\rComments: Accepted to CVPR2024\\r\\\\\\\\\\r  RGBT multispectral pedestrian detection has emerged as a promising solution\\rfor safety-critical applications that require day/night operations. However,\\rthe modality bias problem remains unsolved as multispectral pedestrian\\rdetectors learn the statistical bias in datasets. Specifically, datasets in\\rmultispectral pedestrian detection mainly distribute between ROTO (day) and\\rRXTO (night) data; the majority of the pedestrian labels statistically co-occur\\rwith their thermal features. As a result, multispectral pedestrian detectors\\rshow poor generalization ability on examples beyond this statistical\\rcorrelation, such as ROTX data. To address this problem, we propose a novel\\rCausal Mode Multiplexer (CMM) framework that effectively learns the causalities\\rbetween multispectral inputs and predictions. Moreover, we construct a new\\rdataset (ROTX-MP) to evaluate modality bias in multispectral pedestrian\\rdetection. ROTX-MP mainly includes ROTX examples not presented in previous\\rdatasets. Extensive experiments demonstrate that our proposed CMM framework\\rgeneralizes well on existing datasets (KAIST, CVC-14, FLIR) and the new\\rROTX-MP. We will release our new dataset to the public for future research.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01300 ,  1703kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01310\\rDate: Sat, 2 Mar 2024 21:01:01 GMT   (12921kb,D)\\r\\rTitle: Image-Based Dietary Assessment: A Healthy Eating Plate Estimation System\\rAuthors: Assylzhan Izbassar and Pakizar Shamoi\\rCategories: cs.CV\\rComments: Submitted to IEEE for consideration\\r\\\\\\\\\\r  The nutritional quality of diets has significantly deteriorated over the past\\rtwo to three decades, a decline often underestimated by the people. This\\rdeterioration, coupled with a hectic lifestyle, has contributed to escalating\\rhealth concerns. Recognizing this issue, researchers at Harvard have advocated\\rfor a balanced nutritional plate model to promote health. Inspired by this\\rresearch, our paper introduces an innovative Image-Based Dietary Assessment\\rsystem aimed at evaluating the healthiness of meals through image analysis. Our\\rsystem employs advanced image segmentation and classification techniques to\\ranalyze food items on a plate, assess their proportions, and calculate meal\\radherence to Harvard's healthy eating recommendations. This approach leverages\\rmachine learning and nutritional science to empower individuals with actionable\\rinsights for healthier eating choices. Our four-step framework involves\\rsegmenting the image, classifying the items, conducting a nutritional\\rassessment based on the Harvard Healthy Eating Plate research, and offering\\rtailored recommendations. The prototype system has shown promising results in\\rpromoting healthier eating habits by providing an accessible, evidence-based\\rtool for dietary assessment.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01310 ,  12921kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01316\\rDate: Sat, 2 Mar 2024 21:29:04 GMT   (35330kb,D)\\r\\rTitle: TUMTraf V2X Cooperative Perception Dataset\\rAuthors: Walter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou,\\r  Rui Song, Alois C. Knoll\\rCategories: cs.CV\\r\\\\\\\\\\r  Cooperative perception offers several benefits for enhancing the capabilities\\rof autonomous vehicles and improving road safety. Using roadside sensors in\\raddition to onboard sensors increases reliability and extends the sensor range.\\rExternal sensors offer higher situational awareness for automated vehicles and\\rprevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion\\rmodel, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object\\rdetection and tracking task. Our dataset contains 2,000 labeled point clouds\\rand 5,000 labeled images from five roadside and four onboard sensors. It\\rincludes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled\\reight categories and covered occlusion scenarios with challenging driving\\rmaneuvers, like traffic violations, near-miss events, overtaking, and U-turns.\\rThrough multiple experiments, we show that our CoopDet3D camera-LiDAR fusion\\rmodel achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR\\rfusion model. Finally, we make our dataset, model, labeling tool, and dev-kit\\rpublicly available on our website:\\rhttps://tum-traffic-dataset.github.io/tumtraf-v2x.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01316 ,  35330kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01325\\rDate: Sat, 2 Mar 2024 22:08:10 GMT   (3789kb,D)\\r\\rTitle: NeRF-VPT: Learning Novel View Representations with Neural Radiance\\r  Fields via View Prompt Tuning\\rAuthors: Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng,\\r  Philip H.S. Torr\\rCategories: cs.CV\\rComments: AAAI 2024\\r\\\\\\\\\\r  Neural Radiance Fields (NeRF) have garnered remarkable success in novel view\\rsynthesis. Nonetheless, the task of generating high-quality images for novel\\rviews persists as a critical challenge. While the existing efforts have\\rexhibited commendable progress, capturing intricate details, enhancing\\rtextures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics\\rwarrant further focused attention and advancement. In this work, we propose\\rNeRF-VPT, an innovative method for novel view synthesis to address these\\rchallenges. Our proposed NeRF-VPT employs a cascading view prompt tuning\\rparadigm, wherein RGB information gained from preceding rendering outcomes\\rserves as instructive visual prompts for subsequent rendering stages, with the\\raspiration that the prior knowledge embedded in the prompts can facilitate the\\rgradual enhancement of rendered image quality. NeRF-VPT only requires sampling\\rRGB data from previous stage renderings as priors at each training stage,\\rwithout relying on extra guidance or complex techniques. Thus, our NeRF-VPT is\\rplug-and-play and can be readily integrated into existing methods. By\\rconducting comparative analyses of our NeRF-VPT against several NeRF-based\\rapproaches on demanding real-scene benchmarks, such as Realistic Synthetic 360,\\rReal Forward-Facing, Replica dataset, and a user-captured dataset, we\\rsubstantiate that our NeRF-VPT significantly elevates baseline performance and\\rproficiently generates more high-quality novel view images than all the\\rcompared state-of-the-art methods. Furthermore, the cascading learning of\\rNeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in\\ra significant enhancement of accuracy for sparse-view novel view synthesis. The\\rsource code and dataset are available at\\r\\\\url{https://github.com/Freedomcls/NeRF-VPT}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01325 ,  3789kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01326\\rDate: Sat, 2 Mar 2024 22:16:47 GMT   (3659kb,D)\\r\\rTitle: DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions\\rAuthors: Guangrun Wang, Changlin Li, Liuchun Yuan, Jiefeng Peng, Xiaoyu Xian,\\r  Xiaodan Liang, Xiaojun Chang, and Liang Lin\\rCategories: cs.CV\\rComments: T-PAMI\\r\\\\\\\\\\r  Neural Architecture Search (NAS), aiming at automatically designing neural\\rarchitectures by machines, has been considered a key step toward automatic\\rmachine learning. One notable NAS branch is the weight-sharing NAS, which\\rsignificantly improves search efficiency and allows NAS algorithms to run on\\rordinary computers. Despite receiving high expectations, this category of\\rmethods suffers from low search effectiveness. By employing a generalization\\rboundedness tool, we demonstrate that the devil behind this drawback is the\\runtrustworthy architecture rating with the oversized search space of the\\rpossible architectures. Addressing this problem, we modularize a large search\\rspace into blocks with small search spaces and develop a family of models with\\rthe distilling neural architecture (DNA) techniques. These proposed models,\\rnamely a DNA family, are capable of resolving multiple dilemmas of the\\rweight-sharing NAS, such as scalability, efficiency, and multi-modal\\rcompatibility. Our proposed DNA models can rate all architecture candidates, as\\ropposed to previous works that can only access a sub- search space using\\rheuristic algorithms. Moreover, under a certain computational complexity\\rconstraint, our method can seek architectures with different depths and widths.\\rExtensive experimental evaluations show that our models achieve\\rstate-of-the-art top-1 accuracy of 78.9% and 83.6% on ImageNet for a mobile\\rconvolutional network and a small vision transformer, respectively.\\rAdditionally, we provide in-depth empirical analysis and insights into neural\\rarchitecture ratings. Codes available: \\\\url{https://github.com/changlin31/DNA}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01326 ,  3659kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01345\\rDate: Sat, 2 Mar 2024 23:40:23 GMT   (24694kb,D)\\r\\rTitle: ShapeBoost: Boosting Human Shape Estimation with Part-Based\\r  Parameterization and Clothing-Preserving Augmentation\\rAuthors: Siyuan Bian, Jiefeng Li, Jiasheng Tang, Cewu Lu\\rCategories: cs.CV\\r\\\\\\\\\\r  Accurate human shape recovery from a monocular RGB image is a challenging\\rtask because humans come in different shapes and sizes and wear different\\rclothes. In this paper, we propose ShapeBoost, a new human shape recovery\\rframework that achieves pixel-level alignment even for rare body shapes and\\rhigh accuracy for people wearing different types of clothes. Unlike previous\\rapproaches that rely on the use of PCA-based shape coefficients, we adopt a new\\rhuman shape parameterization that decomposes the human shape into bone lengths\\rand the mean width of each part slice. This part-based parameterization\\rtechnique achieves a balance between flexibility and validity using a\\rsemi-analytical shape reconstruction algorithm. Based on this new\\rparameterization, a clothing-preserving data augmentation module is proposed to\\rgenerate realistic images with diverse body shapes and accurate annotations.\\rExperimental results show that our method outperforms other state-of-the-art\\rmethods in diverse body shape situations as well as in varied clothing\\rsituations.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01345 ,  24694kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01370\\rDate: Sun, 3 Mar 2024 02:10:00 GMT   (591kb)\\r\\rTitle: Depth Estimation Algorithm Based on Transformer-Encoder and Feature\\r  Fusion\\rAuthors: Linhan Xia, Junbang Liu, Tong Wu\\rCategories: cs.CV\\rComments: ICAACE2024\\r\\\\\\\\\\r  This research presents a novel depth estimation algorithm based on a\\rTransformer-encoder architecture, tailored for the NYU and KITTI Depth Dataset.\\rThis research adopts a transformer model, initially renowned for its success in\\rnatural language processing, to capture intricate spatial relationships in\\rvisual data for depth estimation tasks. A significant innovation of the\\rresearch is the integration of a composite loss function that combines\\rStructural Similarity Index Measure (SSIM) with Mean Squared Error (MSE). This\\rcombined loss function is designed to ensure the structural integrity of the\\rpredicted depth maps relative to the original images (via SSIM) while\\rminimizing pixel-wise estimation errors (via MSE). This research approach\\raddresses the challenges of over-smoothing often seen in MSE-based losses and\\renhances the model's ability to predict depth maps that are not only accurate\\rbut also maintain structural coherence with the input images. Through rigorous\\rtraining and evaluation using the NYU Depth Dataset, the model demonstrates\\rsuperior performance, marking a significant advancement in single-image depth\\restimation, particularly in complex indoor and traffic environments.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01370 ,  591kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01381\\rDate: Sun, 3 Mar 2024 02:56:43 GMT   (2676kb,D)\\r\\rTitle: SA-MixNet: Structure-aware Mixup and Invariance Learning for\\r  Scribble-supervised Road Extraction in Remote Sensing Images\\rAuthors: Jie Feng, Hao Huang, Junpeng Zhang, Weisheng Dong, Dingwen Zhang,\\r  Licheng Jiao\\rCategories: cs.CV\\r\\\\\\\\\\r  Mainstreamed weakly supervised road extractors rely on highly confident\\rpseudo-labels propagated from scribbles, and their performance often degrades\\rgradually as the image scenes tend various. We argue that such degradation is\\rdue to the poor model's invariance to scenes with different complexities,\\rwhereas existing solutions to this problem are commonly based on crafted priors\\rthat cannot be derived from scribbles. To eliminate the reliance on such\\rpriors, we propose a novel Structure-aware Mixup and Invariance Learning\\rframework (SA-MixNet) for weakly supervised road extraction that improves the\\rmodel invariance in a data-driven manner. Specifically, we design a\\rstructure-aware Mixup scheme to paste road regions from one image onto another\\rfor creating an image scene with increased complexity while preserving the\\rroad's structural integrity. Then an invariance regularization is imposed on\\rthe predictions of constructed and origin images to minimize their conflicts,\\rwhich thus forces the model to behave consistently on various scenes. Moreover,\\ra discriminator-based regularization is designed for enhancing the connectivity\\rmeanwhile preserving the structure of roads. Combining these designs, our\\rframework demonstrates superior performance on the DeepGlobe, Wuhan, and\\rMassachusetts datasets outperforming the state-of-the-art techniques by 1.47%,\\r2.12%, 4.09% respectively in IoU metrics, and showing its potential of\\rplug-and-play. The code will be made publicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01381 ,  2676kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01407\\rDate: Sun, 3 Mar 2024 06:13:43 GMT   (5283kb,D)\\r\\rTitle: Region-Transformer: Self-Attention Region Based Class-Agnostic Point\\r  Cloud Segmentation\\rAuthors: Dipesh Gyawali, Jian Zhang, BB Karki\\rCategories: cs.CV cs.AI cs.RO\\rComments: 8 pages, 5 figures, 3 tables\\rJournal-ref: 19th International Joint Conference on Computer Vision, Imaging\\r  and Computer Graphics Theory and Applications, 2024\\r\\\\\\\\\\r  Point cloud segmentation, which helps us understand the environment of\\rspecific structures and objects, can be performed in class-specific and\\rclass-agnostic ways. We propose a novel region-based transformer model called\\rRegion-Transformer for performing class-agnostic point cloud segmentation. The\\rmodel utilizes a region-growth approach and self-attention mechanism to\\riteratively expand or contract a region by adding or removing points. It is\\rtrained on simulated point clouds with instance labels only, avoiding semantic\\rlabels. Attention-based networks have succeeded in many previous methods of\\rperforming point cloud segmentation. However, a region-growth approach with\\rattention-based networks has yet to be used to explore its performance gain. To\\rour knowledge, we are the first to use a self-attention mechanism in a\\rregion-growth approach. With the introduction of self-attention to\\rregion-growth that can utilize local contextual information of neighborhood\\rpoints, our experiments demonstrate that the Region-Transformer model\\routperforms previous class-agnostic and class-specific methods on indoor\\rdatasets regarding clustering metrics. The model generalizes well to\\rlarge-scale scenes. Key advantages include capturing long-range dependencies\\rthrough self-attention, avoiding the need for semantic labels during training,\\rand applicability to a variable number of objects. The Region-Transformer model\\rrepresents a promising approach for flexible point cloud segmentation with\\rapplications in robotics, digital twinning, and autonomous vehicles.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01407 ,  5283kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01412\\rDate: Sun, 3 Mar 2024 06:49:01 GMT   (9074kb,D)\\r\\rTitle: LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth\\r  Limited Optical Signal Acquisition\\rAuthors: Lingfeng Liu, Dong Ni, Hangjie Yuan\\rCategories: cs.CV eess.IV eess.SP\\rComments: Accepted to ICLR 2024\\r\\\\\\\\\\r  Bandwidth constraints during signal acquisition frequently impede real-time\\rdetection applications. Hyperspectral data is a notable example, whose vast\\rvolume compromises real-time hyperspectral detection. To tackle this hurdle, we\\rintroduce a novel approach leveraging pre-acquisition modulation to reduce the\\racquisition volume. This modulation process is governed by a deep learning\\rmodel, utilizing prior information. Central to our approach is LUM-ViT, a\\rVision Transformer variant. Uniquely, LUM-ViT incorporates a learnable\\runder-sampling mask tailored for pre-acquisition modulation. To further\\roptimize for optical calculations, we propose a kernel-level weight\\rbinarization technique and a three-stage fine-tuning strategy. Our evaluations\\rreveal that, by sampling a mere 10% of the original image pixels, LUM-ViT\\rmaintains the accuracy loss within 1.8% on the ImageNet classification task.\\rThe method sustains near-original accuracy when implemented on real-world\\roptical hardware, demonstrating its practicality. Code will be available at\\rhttps://github.com/MaxLLF/LUM-ViT.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01412 ,  9074kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01414\\rDate: Sun, 3 Mar 2024 06:58:35 GMT   (35983kb,D)\\r\\rTitle: Unsigned Orthogonal Distance Fields: An Accurate Neural Implicit\\r  Representation for Diverse 3D Shapes\\rAuthors: Yujie Lu, Long Wan, Nayu Ding, Yulong Wang, Shuhan Shen, Shen Cai, Lin\\r  Gao\\rCategories: cs.CV\\rComments: accepted by CVPR 2024\\r\\\\\\\\\\r  Neural implicit representation of geometric shapes has witnessed considerable\\radvancements in recent years. However, common distance field based implicit\\rrepresentations, specifically signed distance field (SDF) for watertight shapes\\ror unsigned distance field (UDF) for arbitrary shapes, routinely suffer from\\rdegradation of reconstruction accuracy when converting to explicit surface\\rpoints and meshes. In this paper, we introduce a novel neural implicit\\rrepresentation based on unsigned orthogonal distance fields (UODFs). In UODFs,\\rthe minimal unsigned distance from any spatial point to the shape surface is\\rdefined solely in one orthogonal direction, contrasting with the\\rmulti-directional determination made by SDF and UDF. Consequently, every point\\rin the 3D UODFs can directly access its closest surface points along three\\rorthogonal directions. This distinctive feature leverages the accurate\\rreconstruction of surface points without interpolation errors. We verify the\\reffectiveness of UODFs through a range of reconstruction examples, extending\\rfrom simple watertight or non-watertight shapes to complex shapes that include\\rhollows, internal or assembling structures.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01414 ,  35983kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01418\\rDate: Sun, 3 Mar 2024 07:19:50 GMT   (46204kb,D)\\r\\rTitle: A Simple-but-effective Baseline for Training-free Class-Agnostic\\r  Counting\\rAuthors: Yuhao Lin, Haiming Xu, Lingqiao Liu, Javen Qinfeng Shi\\rCategories: cs.CV\\r\\\\\\\\\\r  Class-Agnostic Counting (CAC) seeks to accurately count objects in a given\\rimage with only a few reference examples. While previous methods achieving this\\rrelied on additional training, recent efforts have shown that it's possible to\\raccomplish this without training by utilizing pre-existing foundation models,\\rparticularly the Segment Anything Model (SAM), for counting via instance-level\\rsegmentation. Although promising, current training-free methods still lag\\rbehind their training-based counterparts in terms of performance. In this\\rresearch, we present a straightforward training-free solution that effectively\\rbridges this performance gap, serving as a strong baseline. The primary\\rcontribution of our work lies in the discovery of four key technologies that\\rcan enhance performance. Specifically, we suggest employing a superpixel\\ralgorithm to generate more precise initial point prompts, utilizing an image\\rencoder with richer semantic knowledge to replace the SAM encoder for\\rrepresenting candidate objects, and adopting a multiscale mechanism and a\\rtransductive prototype scheme to update the representation of reference\\rexamples. By combining these four technologies, our approach achieves\\rsignificant improvements over existing training-free methods and delivers\\rperformance on par with training-based ones.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01418 ,  46204kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01422\\rDate: Sun, 3 Mar 2024 07:43:39 GMT   (9952kb,D)\\r\\rTitle: MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies\\rAuthors: Zhende Song, Chenchen Wang, Jiamu Sheng, Chi Zhang, Gang Yu, Jiayuan\\r  Fan, Tao Chen\\rCategories: cs.CV\\r\\\\\\\\\\r  The development of multimodal models has marked a significant step forward in\\rhow machines understand videos. These models have shown promise in analyzing\\rshort video clips. However, when it comes to longer formats like movies, they\\roften fall short. The main hurdles are the lack of high-quality, diverse video\\rdata and the intensive work required to collect or annotate such data. In the\\rface of these challenges, we propose MovieLLM, a novel framework designed to\\rcreate synthetic, high-quality data for long videos. This framework leverages\\rthe power of GPT-4 and text-to-image models to generate detailed scripts and\\rcorresponding visuals. Our approach stands out for its flexibility and\\rscalability, making it a superior alternative to traditional data collection\\rmethods. Our extensive experiments validate that the data produced by MovieLLM\\rsignificantly improves the performance of multimodal models in understanding\\rcomplex video narratives, overcoming the limitations of existing datasets\\rregarding scarcity and bias.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01422 ,  9952kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01427\\rDate: Sun, 3 Mar 2024 07:54:03 GMT   (16628kb,D)\\r\\rTitle: Logit Standardization in Knowledge Distillation\\rAuthors: Shangquan Sun, Wenqi Ren, Jingzhi Li, Rui Wang and Xiaochun Cao\\rCategories: cs.CV\\rComments: 10 pages, 5 figures, accepted by The The IEEE / CVF Computer Vision\\r  and Pattern Recognition Conference (CVPR 2024)\\r\\\\\\\\\\r  Knowledge distillation involves transferring soft labels from a teacher to a\\rstudent using a shared temperature-based softmax function. However, the\\rassumption of a shared temperature between teacher and student implies a\\rmandatory exact match between their logits in terms of logit range and\\rvariance. This side-effect limits the performance of student, considering the\\rcapacity discrepancy between them and the finding that the innate logit\\rrelations of teacher are sufficient for student to learn. To address this\\rissue, we propose setting the temperature as the weighted standard deviation of\\rlogit and performing a plug-and-play Z-score pre-process of logit\\rstandardization before applying softmax and Kullback-Leibler divergence. Our\\rpre-process enables student to focus on essential logit relations from teacher\\rrather than requiring a magnitude match, and can improve the performance of\\rexisting logit-based distillation methods. We also show a typical case where\\rthe conventional setting of sharing temperature between teacher and student\\rcannot reliably yield the authentic distillation evaluation; nonetheless, this\\rchallenge is successfully alleviated by our Z-score. We extensively evaluate\\rour method for various student and teacher models on CIFAR-100 and ImageNet,\\rshowing its significant superiority. The vanilla knowledge distillation powered\\rby our pre-process can achieve favorable performance against state-of-the-art\\rmethods, and other distillation variants can obtain considerable gain with the\\rassistance of our pre-process.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01427 ,  16628kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01431\\rDate: Sun, 3 Mar 2024 07:58:03 GMT   (3087kb,D)\\r\\rTitle: Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval\\rAuthors: Yongchao Du, Min Wang, Wengang Zhou, Shuping Hui, Houqiang Li\\rCategories: cs.CV\\rComments: ICLR 2024 spotlight\\r\\\\\\\\\\r  The task of composed image retrieval (CIR) aims to retrieve images based on\\rthe query image and the text describing the users' intent. Existing methods\\rhave made great progress with the advanced large vision-language (VL) model in\\rCIR task, however, they generally suffer from two main issues: lack of labeled\\rtriplets for model training and difficulty of deployment on resource-restricted\\renvironments when deploying the large vision-language model. To tackle the\\rabove problems, we propose Image2Sentence based Asymmetric zero-shot composed\\rimage retrieval (ISA), which takes advantage of the VL model and only relies on\\runlabeled images for composition learning. In the framework, we propose a new\\radaptive token learner that maps an image to a sentence in the word embedding\\rspace of VL model. The sentence adaptively captures discriminative visual\\rinformation and is further integrated with the text modifier. An asymmetric\\rstructure is devised for flexible deployment, in which the lightweight model is\\radopted for the query side while the large VL model is deployed on the gallery\\rside. The global contrastive distillation and the local alignment\\rregularization are adopted for the alignment between the light model and the VL\\rmodel for CIR task. Our experiments demonstrate that the proposed ISA could\\rbetter cope with the real retrieval scenarios and further improve retrieval\\raccuracy and efficiency.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01431 ,  3087kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01437\\rDate: Sun, 3 Mar 2024 08:24:28 GMT   (923kb,D)\\r\\rTitle: GPTSee: Enhancing Moment Retrieval and Highlight Detection via\\r  Description-Based Similarity Features\\rAuthors: Yunzhuo Sun, Yifang Xu, Zien Xie, Yukun Shu, and Sidan Du\\rCategories: cs.CV cs.AI\\rComments: 5 pages, 3 figures\\rDOI: 10.1109/LSP.2023.3340103\\r\\\\\\\\\\r  Moment retrieval (MR) and highlight detection (HD) aim to identify relevant\\rmoments and highlights in video from corresponding natural language query.\\rLarge language models (LLMs) have demonstrated proficiency in various computer\\rvision tasks. However, existing methods for MR\\\\&HD have not yet been integrated\\rwith LLMs. In this letter, we propose a novel two-stage model that takes the\\routput of LLMs as the input to the second-stage transformer encoder-decoder.\\rFirst, MiniGPT-4 is employed to generate the detailed description of the video\\rframe and rewrite the query statement, fed into the encoder as new features.\\rThen, semantic similarity is computed between the generated description and the\\rrewritten queries. Finally, continuous high-similarity video frames are\\rconverted into span anchors, serving as prior position information for the\\rdecoder. Experiments demonstrate that our approach achieves a state-of-the-art\\rresult, and by using only span anchors and similarity scores as outputs,\\rpositioning accuracy outperforms traditional methods, like Moment-DETR.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01437 ,  923kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01439\\rDate: Sun, 3 Mar 2024 08:25:04 GMT   (1342kb,D)\\r\\rTitle: Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer\\r  Learning for Point Cloud Analysis\\rAuthors: Xin Zhou, Dingkang Liang, Wei Xu, Xingkui Zhu, Yihan Xu, Zhikang Zou,\\r  Xiang Bai\\rCategories: cs.CV\\rComments: Accepted to CVPR 2024. Code is available at\\r  https://github.com/LMD0311/DAPT. We will post a camera-ready version later\\r\\\\\\\\\\r  Point cloud analysis has achieved outstanding performance by transferring\\rpoint cloud pre-trained models. However, existing methods for model adaptation\\rusually update all model parameters, i.e., full fine-tuning paradigm, which is\\rinefficient as it relies on high computational costs (e.g., training GPU\\rmemory) and massive storage space. In this paper, we aim to study\\rparameter-efficient transfer learning for point cloud analysis with an ideal\\rtrade-off between task performance and parameter efficiency. To achieve this\\rgoal, we freeze the parameters of the default pre-trained models and then\\rpropose the Dynamic Adapter, which generates a dynamic scale for each token,\\rconsidering the token significance to the downstream task. We further\\rseamlessly integrate Dynamic Adapter with Prompt Tuning (DAPT) by constructing\\rInternal Prompts, capturing the instance-specific features for interaction.\\rExtensive experiments conducted on five challenging datasets demonstrate that\\rthe proposed DAPT achieves superior performance compared to the full\\rfine-tuning counterparts while significantly reducing the trainable parameters\\rand training GPU memory by 95% and 35%, respectively. Code is available at\\rhttps://github.com/LMD0311/DAPT.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01439 ,  1342kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01440\\rDate: Sun, 3 Mar 2024 08:33:23 GMT   (6121kb,D)\\r\\rTitle: Pyramid Feature Attention Network for Monocular Depth Prediction\\rAuthors: Yifang Xu, Chenglei Peng, Ming Li, Yang Li, and Sidan Du\\rCategories: cs.CV\\rComments: 6 pages, 5 figures\\rDOI: 10.1109/ICME51207.2021.9428446\\r\\\\\\\\\\r  Deep convolutional neural networks (DCNNs) have achieved great success in\\rmonocular depth estimation (MDE). However, few existing works take the\\rcontributions for MDE of different levels feature maps into account, leading to\\rinaccurate spatial layout, ambiguous boundaries and discontinuous object\\rsurface in the prediction. To better tackle these problems, we propose a\\rPyramid Feature Attention Network (PFANet) to improve the high-level context\\rfeatures and low-level spatial features. In the proposed PFANet, we design a\\rDual-scale Channel Attention Module (DCAM) to employ channel attention in\\rdifferent scales, which aggregate global context and local information from the\\rhigh-level feature maps. To exploit the spatial relationship of visual\\rfeatures, we design a Spatial Pyramid Attention Module (SPAM) which can guide\\rthe network attention to multi-scale detailed information in the low-level\\rfeature maps. Finally, we introduce scale-invariant gradient loss to increase\\rthe penalty on errors in depth-wise discontinuous regions. Experimental results\\rshow that our method outperforms state-of-the-art methods on the KITTI dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01440 ,  6121kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01444\\rDate: Sun, 3 Mar 2024 08:42:40 GMT   (39165kb,D)\\r\\rTitle: 3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming\\r  of Photo-Realistic Free-Viewpoint Videos\\rAuthors: Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing\\rCategories: cs.CV\\rComments: CVPR 2024 Accepted. Project Page: https://sjojok.github.io/3dgstream\\r\\\\\\\\\\r  Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes\\rfrom multi-view videos remains a challenging endeavor. Despite the remarkable\\radvancements achieved by current neural rendering techniques, these methods\\rgenerally require complete video sequences for offline training and are not\\rcapable of real-time rendering. To address these constraints, we introduce\\r3DGStream, a method designed for efficient FVV streaming of real-world dynamic\\rscenes. Our method achieves fast on-the-fly per-frame reconstruction within 12\\rseconds and real-time rendering at 200 FPS. Specifically, we utilize 3D\\rGaussians (3DGs) to represent the scene. Instead of the na\\\\ive approach of\\rdirectly optimizing 3DGs per-frame, we employ a compact Neural Transformation\\rCache (NTC) to model the translations and rotations of 3DGs, markedly reducing\\rthe training time and storage required for each FVV frame. Furthermore, we\\rpropose an adaptive 3DG addition strategy to handle emerging objects in dynamic\\rscenes. Experiments demonstrate that 3DGStream achieves competitive performance\\rin terms of rendering speed, image quality, training time, and model storage\\rwhen compared with state-of-the-art methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01444 ,  39165kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01446\\rDate: Sun, 3 Mar 2024 09:04:34 GMT   (5972kb,D)\\r\\rTitle: GuardT2I: Defending Text-to-Image Models from Adversarial Prompts\\rAuthors: Yijun Yang, Ruiyuan Gao, Xiao Yang, Jianyuan Zhong, Qiang Xu\\rCategories: cs.CV\\r\\\\\\\\\\r  Recent advancements in Text-to-Image (T2I) models have raised significant\\rsafety concerns about their potential misuse for generating inappropriate or\\rNot-Safe-For-Work (NSFW) contents, despite existing countermeasures such as\\rNSFW classifiers or model fine-tuning for inappropriate concept removal.\\rAddressing this challenge, our study unveils GuardT2I, a novel moderation\\rframework that adopts a generative approach to enhance T2I models' robustness\\ragainst adversarial prompts. Instead of making a binary classification,\\rGuardT2I utilizes a Large Language Model (LLM) to conditionally transform text\\rguidance embeddings within the T2I models into natural language for effective\\radversarial prompt detection, without compromising the models' inherent\\rperformance. Our extensive experiments reveal that GuardT2I outperforms leading\\rcommercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a\\rsignificant margin across diverse adversarial scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01446 ,  5972kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01465\\rDate: Sun, 3 Mar 2024 10:19:18 GMT   (2379kb)\\r\\rTitle: Multiview Subspace Clustering of Hyperspectral Images based on Graph\\r  Convolutional Networks\\rAuthors: Xianju Li and Renxiang Guan and Zihao Li and Hao Liu and Jing Yang\\rCategories: cs.CV\\rComments: This paper was accepted by APWEB-WAIM 2024\\r\\\\\\\\\\r  High-dimensional and complex spectral structures make clustering of\\rhy-perspectral images (HSI) a challenging task. Subspace clustering has been\\rshown to be an effective approach for addressing this problem. However, current\\rsubspace clustering algorithms are mainly designed for a single view and do not\\rfully exploit spatial or texture feature information in HSI. This study\\rproposed a multiview subspace clustering of HSI based on graph convolutional\\rnetworks. (1) This paper uses the powerful classification ability of graph\\rconvolutional network and the learning ability of topologi-cal relationships\\rbetween nodes to analyze and express the spatial relation-ship of HSI. (2)\\rPixel texture and pixel neighbor spatial-spectral infor-mation were sent to\\rconstruct two graph convolutional subspaces. (3) An attention-based fusion\\rmodule was used to adaptively construct a more discriminative feature map. The\\rmodel was evaluated on three popular HSI datasets, including Indian Pines,\\rPavia University, and Houston. It achieved overall accuracies of 92.38%,\\r93.43%, and 83.82%, respectively and significantly outperformed the\\rstate-of-the-art clustering methods. In conclusion, the proposed model can\\reffectively improve the clustering ac-curacy of HSI.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01465 ,  2379kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01470\\rDate: Sun, 3 Mar 2024 10:35:00 GMT   (4516kb,D)\\r\\rTitle: Is in-domain data beneficial in transfer learning for landmarks\\r  detection in x-ray images?\\rAuthors: Roberto Di Via, Matteo Santacesaria, Francesca Odone, Vito Paolo\\r  Pastore\\rCategories: cs.CV\\rComments: Accepted to ISBI 2024\\r\\\\\\\\\\r  In recent years, deep learning has emerged as a promising technique for\\rmedical image analysis. However, this application domain is likely to suffer\\rfrom a limited availability of large public datasets and annotations. A common\\rsolution to these challenges in deep learning is the usage of a transfer\\rlearning framework, typically with a fine-tuning protocol, where a large-scale\\rsource dataset is used to pre-train a model, further fine-tuned on the target\\rdataset. In this paper, we present a systematic study analyzing whether the\\rusage of small-scale in-domain x-ray image datasets may provide any improvement\\rfor landmark detection over models pre-trained on large natural image datasets\\ronly. We focus on the multi-landmark localization task for three datasets,\\rincluding chest, head, and hand x-ray images. Our results show that using\\rin-domain source datasets brings marginal or no benefit with respect to an\\rImageNet out-of-domain pre-training. Our findings can provide an indication for\\rthe development of robust landmark detection systems in medical images when no\\rlarge annotated dataset is available.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01470 ,  4516kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01476\\rDate: Sun, 3 Mar 2024 11:00:15 GMT   (19863kb,D)\\r\\rTitle: CCC: Color Classified Colorization\\rAuthors: Mrityunjoy Gain, Avi Deb Raha and Rameswar Debnath\\rCategories: cs.CV\\r\\\\\\\\\\r  Automatic colorization of gray images with objects of different colors and\\rsizes is challenging due to inter- and intra-object color variation and the\\rsmall area of the main objects due to extensive backgrounds. The learning\\rprocess often favors dominant features, resulting in a biased model. In this\\rpaper, we formulate the colorization problem into a multinomial classification\\rproblem and then apply a weighted function to classes. We propose a set of\\rformulas to transform color values into color classes and vice versa. Class\\roptimization and balancing feature distribution are the keys for good\\rperformance. Observing class appearance on various extremely large-scale\\rreal-time images in practice, we propose 215 color classes for our colorization\\rtask. During training, we propose a class-weighted function based on true class\\rappearance in each batch to ensure proper color saturation of individual\\robjects. We establish a trade-off between major and minor classes to provide\\rorthodox class prediction by eliminating major classes' dominance over minor\\rclasses. As we apply regularization to enhance the stability of the minor\\rclass, occasional minor noise may appear at the object's edges. We propose a\\rnovel object-selective color harmonization method empowered by the SAM to\\rrefine and enhance these edges. We propose a new color image evaluation metric,\\rthe Chromatic Number Ratio (CNR), to quantify the richness of color components.\\rWe compare our proposed model with state-of-the-art models using five different\\rdatasets: ADE, Celeba, COCO, Oxford 102 Flower, and ImageNet, in both\\rqualitative and quantitative approaches. The experimental results show that our\\rproposed model outstrips other models in visualization and CNR measurement\\rcriteria while maintaining satisfactory performance in regression (MSE, PSNR),\\rsimilarity (SSIM, LPIPS, UIQI), and generative criteria (FID).\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01476 ,  19863kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01482\\rDate: Sun, 3 Mar 2024 11:24:16 GMT   (9938kb,D)\\r\\rTitle: EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised\\r  Semantic Segmentation\\rAuthors: Chanyoung Kim, Woojung Han, Dayun Ju, Seong Jae Hwang\\rCategories: cs.CV\\r\\\\\\\\\\r  Semantic segmentation has innately relied on extensive pixel-level labeled\\rannotated data, leading to the emergence of unsupervised methodologies. Among\\rthem, leveraging self-supervised Vision Transformers for unsupervised semantic\\rsegmentation (USS) has been making steady progress with expressive deep\\rfeatures. Yet, for semantically segmenting images with complex objects, a\\rpredominant challenge remains: the lack of explicit object-level semantic\\rencoding in patch-level features. This technical limitation often leads to\\rinadequate segmentation of complex objects with diverse structures. To address\\rthis gap, we present a novel approach, EAGLE, which emphasizes object-centric\\rrepresentation learning for unsupervised semantic segmentation. Specifically,\\rwe introduce EiCue, a spectral technique providing semantic and structural cues\\rthrough an eigenbasis derived from the semantic similarity matrix of deep image\\rfeatures and color affinity from an image. Further, by incorporating our\\robject-centric contrastive loss with EiCue, we guide our model to learn\\robject-level representations with intra- and inter-image object-feature\\rconsistency, thereby enhancing semantic accuracy. Extensive experiments on\\rCOCO-Stuff, Cityscapes, and Potsdam-3 datasets demonstrate the state-of-the-art\\rUSS results of EAGLE with accurate and consistent semantic segmentation across\\rcomplex scenes.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01482 ,  9938kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01487\\rDate: Sun, 3 Mar 2024 11:39:41 GMT   (3704kb,D)\\r\\rTitle: InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\\rAuthors: Haogeng Liu, Quanzeng You, Xiaotian Han, Yiqi Wang, Bohan Zhai,\\r  Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang\\rCategories: cs.CV\\r\\\\\\\\\\r  Multimodal Large Language Models (MLLMs) have experienced significant\\radvancements recently. Nevertheless, challenges persist in the accurate\\rrecognition and comprehension of intricate details within high-resolution\\rimages. Despite being indispensable for the development of robust MLLMs, this\\rarea remains underinvestigated. To tackle this challenge, our work introduces\\rInfiMM-HD, a novel architecture specifically designed for processing images of\\rdifferent resolutions with low computational overhead. This innovation\\rfacilitates the enlargement of MLLMs to higher-resolution capabilities.\\rInfiMM-HD incorporates a cross-attention module and visual windows to reduce\\rcomputation costs. By integrating this architectural design with a four-stage\\rtraining pipeline, our model attains improved visual perception efficiently and\\rcost-effectively. Empirical study underscores the robustness and effectiveness\\rof InfiMM-HD, opening new avenues for exploration in related areas. Codes and\\rmodels can be found at https://huggingface.co/Infi-MM/infimm-hd\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01487 ,  3704kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01489\\rDate: Sun, 3 Mar 2024 11:55:49 GMT   (10339kb,D)\\r\\rTitle: Regeneration Based Training-free Attribution of Fake Images Generated by\\r  Text-to-Image Generative Models\\rAuthors: Meiling Li, Zhenxing Qian, Xinpeng Zhang\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Text-to-image generative models have recently garnered significant attention\\rdue to their ability to generate images based on prompt descriptions. While\\rthese models have shown promising performance, concerns have been raised\\rregarding the potential misuse of the generated fake images. In response to\\rthis, we have presented a simple yet effective training-free method to\\rattribute fake images generated by text-to-image models to their source models.\\rGiven a test image to be attributed, we first inverse the textual prompt of the\\rimage, and then put the reconstructed prompt into different candidate models to\\rregenerate candidate fake images. By calculating and ranking the similarity of\\rthe test image and the candidate images, we can determine the source of the\\rimage. This attribution allows model owners to be held accountable for any\\rmisuse of their models. Note that our approach does not limit the number of\\rcandidate text-to-image generative models. Comprehensive experiments reveal\\rthat (1) Our method can effectively attribute fake images to their source\\rmodels, achieving comparable attribution performance with the state-of-the-art\\rmethod; (2) Our method has high scalability ability, which is well adapted to\\rreal-world attribution scenarios. (3) The proposed method yields satisfactory\\rrobustness to common attacks, such as Gaussian blurring, JPEG compression, and\\rResizing. We also analyze the factors that influence the attribution\\rperformance, and explore the boost brought by the proposed method as a plug-in\\rto improve the performance of existing SOTA. We hope our work can shed some\\rlight on the solutions to addressing the source of AI-generated images, as well\\ras to prevent the misuse of text-to-image generative models.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01489 ,  10339kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01497\\rDate: Sun, 3 Mar 2024 12:17:49 GMT   (2142kb,D)\\r\\rTitle: Learning A Physical-aware Diffusion Model Based on Transformer for\\r  Underwater Image Enhancement\\rAuthors: Chen Zhao, Chenyu Dong, Weiling Cai\\rCategories: cs.CV\\r\\\\\\\\\\r  Underwater visuals undergo various complex degradations, inevitably\\rinfluencing the efficiency of underwater vision tasks. Recently, diffusion\\rmodels were employed to underwater image enhancement (UIE) tasks, and gained\\rSOTA performance. However, these methods fail to consider the physical\\rproperties and underwater imaging mechanisms in the diffusion process, limiting\\rinformation completion capacity of diffusion models. In this paper, we\\rintroduce a novel UIE framework, named PA-Diff, designed to exploiting the\\rknowledge of physics to guide the diffusion process.\\r  PA-Diff consists of Physics Prior Generation (PPG) Branch and Physics-aware\\rDiffusion Transformer (PDT) Branch. Our designed PPG branch is a plug-and-play\\rnetwork to produce the physics prior, which can be integrated into any deep\\rframework. With utilizing the physics prior knowledge to guide the diffusion\\rprocess, PDT branch can obtain underwater-aware ability and model the complex\\rdistribution in real-world underwater scenes. Extensive experiments prove that\\rour method achieves best performance on UIE tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01497 ,  2142kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01505\\rDate: Sun, 3 Mar 2024 13:08:32 GMT   (42040kb,D)\\r\\rTitle: SCott: Accelerating Diffusion Models with Stochastic Consistency\\r  Distillation\\rAuthors: Hongjian Liu, Qingsong Xie, Zhijie Deng, Chen Chen, Shixiang Tang,\\r  Fueyang Fu, Zheng-jun Zha, Haonan Lu\\rCategories: cs.CV\\rComments: 22 pages, 16 figures\\r\\\\\\\\\\r  The iterative sampling procedure employed by diffusion models (DMs) often\\rleads to significant inference latency. To address this, we propose Stochastic\\rConsistency Distillation (SCott) to enable accelerated text-to-image\\rgeneration, where high-quality generations can be achieved with just 1-2\\rsampling steps, and further improvements can be obtained by adding additional\\rsteps. In contrast to vanilla consistency distillation (CD) which distills the\\rordinary differential equation solvers-based sampling process of a pretrained\\rteacher model into a student, SCott explores the possibility and validates the\\refficacy of integrating stochastic differential equation (SDE) solvers into CD\\rto fully unleash the potential of the teacher. SCott is augmented with\\relaborate strategies to control the noise strength and sampling process of the\\rSDE solver. An adversarial loss is further incorporated to strengthen the\\rsample quality with rare sampling steps. Empirically, on the MSCOCO-2017 5K\\rdataset with a Stable Diffusion-V1.5 teacher, SCott achieves an FID (Frechet\\rInceptio Distance) of 22.1, surpassing that (23.4) of the 1-step InstaFlow (Liu\\ret al., 2023) and matching that of 4-step UFOGen (Xue et al., 2023b). Moreover,\\rSCott can yield more diverse samples than other consistency models for\\rhigh-resolution image generation (Luo et al., 2023a), with up to 16%\\rimprovement in a qualified metric. The code and checkpoints are coming soon.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01505 ,  42040kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01510\\rDate: Sun, 3 Mar 2024 13:17:10 GMT   (14261kb,D)\\r\\rTitle: End-to-End Human Instance Matting\\rAuthors: Qinglin Liu, Shengping Zhang, Quanling Meng, Bineng Zhong, Peiqiang\\r  Liu, Hongxun Yao\\rCategories: cs.CV cs.AI\\rJournal-ref: IEEE T-CSVT 2023\\rDOI: 10.1109/TCSVT.2023.3306400\\r\\\\\\\\\\r  Human instance matting aims to estimate an alpha matte for each human\\rinstance in an image, which is extremely challenging and has rarely been\\rstudied so far. Despite some efforts to use instance segmentation to generate a\\rtrimap for each instance and apply trimap-based matting methods, the resulting\\ralpha mattes are often inaccurate due to inaccurate segmentation. In addition,\\rthis approach is computationally inefficient due to multiple executions of the\\rmatting method. To address these problems, this paper proposes a novel\\rEnd-to-End Human Instance Matting (E2E-HIM) framework for simultaneous multiple\\rinstance matting in a more efficient manner. Specifically, a general perception\\rnetwork first extracts image features and decodes instance contexts into latent\\rcodes. Then, a united guidance network exploits spatial attention and semantics\\rembedding to generate united semantics guidance, which encodes the locations\\rand semantic correspondences of all instances. Finally, an instance matting\\rnetwork decodes the image features and united semantics guidance to predict all\\rinstance-level alpha mattes. In addition, we construct a large-scale human\\rinstance matting dataset (HIM-100K) comprising over 100,000 human images with\\rinstance alpha matte labels. Experiments on HIM-100K demonstrate the proposed\\rE2E-HIM outperforms the existing methods on human instance matting with 50%\\rlower errors and 5X faster speed (6 instances in a 640X640 image). Experiments\\ron the PPM-100, RWP-636, and P3M datasets demonstrate that E2E-HIM also\\rachieves competitive performance on traditional human matting.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01510 ,  14261kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01517\\rDate: Sun, 3 Mar 2024 14:01:03 GMT   (17700kb,D)\\r\\rTitle: MatchU: Matching Unseen Objects for 6D Pose Estimation from RGB-D Images\\rAuthors: Junwen Huang, Hao Yu, Kuan-Ting Yu, Nassir Navab, Slobodan Ilic,\\r  Benjamin Busam\\rCategories: cs.CV\\r\\\\\\\\\\r  Recent learning methods for object pose estimation require resource-intensive\\rtraining for each individual object instance or category, hampering their\\rscalability in real applications when confronted with previously unseen\\robjects. In this paper, we propose MatchU, a Fuse-Describe-Match strategy for\\r6D pose estimation from RGB-D images. MatchU is a generic approach that fuses\\r2D texture and 3D geometric cues for 6D pose prediction of unseen objects. We\\rrely on learning geometric 3D descriptors that are rotation-invariant by\\rdesign. By encoding pose-agnostic geometry, the learned descriptors naturally\\rgeneralize to unseen objects and capture symmetries. To tackle ambiguous\\rassociations using 3D geometry only, we fuse additional RGB information into\\rour descriptor. This is achieved through a novel attention-based mechanism that\\rfuses cross-modal information, together with a matching loss that leverages the\\rlatent space learned from RGB data to guide the descriptor learning process.\\rExtensive experiments reveal the generalizability of both the RGB-D fusion\\rstrategy as well as the descriptor efficacy. Benefiting from the novel designs,\\rMatchU surpasses all existing methods by a significant margin in terms of both\\raccuracy and speed, even without the requirement of expensive re-training or\\rrendering.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01517 ,  17700kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01543\\rDate: Sun, 3 Mar 2024 15:43:11 GMT   (19890kb,D)\\r\\rTitle: Efficient Action Counting with Dynamic Queries\\rAuthors: Zishi Li, Xiaoxuan Ma, Qiuyan Shang, Wentao Zhu, Hai Ci, Yu Qiao, and\\r  Yizhou Wang\\rCategories: cs.CV\\rComments: code: https://github.com/lizishi/DeTRC, proj page:\\r  https://shirleymaxx.github.io/DeTRC/\\r\\\\\\\\\\r  Temporal repetition counting aims to quantify the repeated action cycles\\rwithin a video. The majority of existing methods rely on the similarity\\rcorrelation matrix to characterize the repetitiveness of actions, but their\\rscalability is hindered due to the quadratic computational complexity. In this\\rwork, we introduce a novel approach that employs an action query representation\\rto localize repeated action cycles with linear computational complexity. Based\\ron this representation, we further develop two key components to tackle the\\ressential challenges of temporal repetition counting. Firstly, to facilitate\\ropen-set action counting, we propose the dynamic update scheme on action\\rqueries. Unlike static action queries, this approach dynamically embeds video\\rfeatures into action queries, offering a more flexible and generalizable\\rrepresentation. Secondly, to distinguish between actions of interest and\\rbackground noise actions, we incorporate inter-query contrastive learning to\\rregularize the video representations corresponding to different action queries.\\rAs a result, our method significantly outperforms previous works, particularly\\rin terms of long video sequences, unseen actions, and actions at various\\rspeeds. On the challenging RepCountA benchmark, we outperform the\\rstate-of-the-art method TransRAC by 26.5% in OBO accuracy, with a 22.7% mean\\rerror decrease and 94.1% computational burden reduction. Code is available at\\rhttps://github.com/lizishi/DeTRC.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01543 ,  19890kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01546\\rDate: Sun, 3 Mar 2024 15:47:43 GMT   (32906kb,D)\\r\\rTitle: Hyperspectral Image Analysis in Single-Modal and Multimodal setting\\r  using Deep Learning Techniques\\rAuthors: Shivam Pande\\rCategories: cs.CV cs.LG\\rComments: 253 pages\\r\\\\\\\\\\r  Hyperspectral imaging provides precise classification for land use and cover\\rdue to its exceptional spectral resolution. However, the challenges of high\\rdimensionality and limited spatial resolution hinder its effectiveness. This\\rstudy addresses these challenges by employing deep learning techniques to\\refficiently process, extract features, and classify data in an integrated\\rmanner. To enhance spatial resolution, we integrate information from\\rcomplementary modalities such as LiDAR and SAR data through multimodal\\rlearning. Moreover, adversarial learning and knowledge distillation are\\rutilized to overcome issues stemming from domain disparities and missing\\rmodalities. We also tailor deep learning architectures to suit the unique\\rcharacteristics of HSI data, utilizing 1D convolutional and recurrent neural\\rnetworks to handle its continuous spectral dimension. Techniques like visual\\rattention and feedback connections within the architecture bolster the\\rrobustness of feature extraction. Additionally, we tackle the issue of limited\\rtraining samples through self-supervised learning methods, employing\\rautoencoders for dimensionality reduction and exploring semi-supervised\\rlearning techniques that leverage unlabeled data. Our proposed approaches are\\revaluated across various HSI datasets, consistently outperforming existing\\rstate-of-the-art techniques.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01546 ,  32906kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01549\\rDate: Sun, 3 Mar 2024 15:53:48 GMT   (218kb,D)\\r\\rTitle: Self-Supervised Representation Learning with Meta Comprehensive\\r  Regularization\\rAuthors: Huijie Guo, Ying Ba, Jie Hu, Lingyu Si, Wenwen Qiang, Lei Shi\\rCategories: cs.CV\\r\\\\\\\\\\r  Self-Supervised Learning (SSL) methods harness the concept of semantic\\rinvariance by utilizing data augmentation strategies to produce similar\\rrepresentations for different deformations of the same input. Essentially, the\\rmodel captures the shared information among multiple augmented views of\\rsamples, while disregarding the non-shared information that may be beneficial\\rfor downstream tasks. To address this issue, we introduce a module called\\rCompMod with Meta Comprehensive Regularization (MCR), embedded into existing\\rself-supervised frameworks, to make the learned representations more\\rcomprehensive. Specifically, we update our proposed model through a bi-level\\roptimization mechanism, enabling it to capture comprehensive features.\\rAdditionally, guided by the constrained extraction of features using maximum\\rentropy coding, the self-supervised learning model learns more comprehensive\\rfeatures on top of learning consistent features. In addition, we provide\\rtheoretical support for our proposed method from information theory and causal\\rcounterfactual perspective. Experimental results show that our method achieves\\rsignificant improvement in classification, object detection and instance\\rsegmentation tasks on multiple benchmark datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01549 ,  218kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01560\\rDate: Sun, 3 Mar 2024 16:48:16 GMT   (370kb,D)\\r\\rTitle: Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary\\r  Action Recognition\\rAuthors: Kun-Yu Lin, Henghui Ding, Jiaming Zhou, Yi-Xing Peng, Zhilin Zhao,\\r  Chen Change Loy, Wei-Shi Zheng\\rCategories: cs.CV\\r\\\\\\\\\\r  Contrastive Language-Image Pretraining (CLIP) has shown remarkable\\ropen-vocabulary abilities across various image understanding tasks. Building\\rupon this impressive success, recent pioneer works have proposed to adapt the\\rpowerful CLIP to video data, leading to efficient and effective video learners\\rfor open-vocabulary action recognition. Inspired by the fact that humans\\rperform actions in diverse environments, our work delves into an intriguing\\rquestion: Can CLIP-based video learners effectively generalize to video domains\\rthey have not encountered during training? To answer this, we establish a\\rCROSS-domain Open-Vocabulary Action recognition benchmark named XOV-Action, and\\rconduct a comprehensive evaluation of five state-of-the-art CLIP-based video\\rlearners under various types of domain gaps. Our evaluation demonstrates that\\rprevious methods exhibit limited action recognition performance in unseen video\\rdomains, revealing potential challenges of the cross-domain open-vocabulary\\raction recognition task. To address this task, our work focuses on a critical\\rchallenge, namely scene bias, and we accordingly contribute a novel scene-aware\\rvideo-text alignment method. Our key idea is to distinguish video\\rrepresentations apart from scene-encoded text representations, aiming to learn\\rscene-agnostic video representations for recognizing actions across domains.\\rExtensive experimental results demonstrate the effectiveness of our method. The\\rbenchmark and code will be available at\\rhttps://github.com/KunyuLin/XOV-Action/.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01560 ,  370kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01569\\rDate: Sun, 3 Mar 2024 17:29:03 GMT   (25054kb,D)\\r\\rTitle: Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV &\\r  CribsTV\\rAuthors: Jaime Spencer, Chris Russell, Simon Hadfield, Richard Bowden\\rCategories: cs.CV cs.AI cs.RO\\r\\\\\\\\\\r  Self-supervised learning is the key to unlocking generic computer vision\\rsystems. By eliminating the reliance on ground-truth annotations, it allows\\rscaling to much larger data quantities. Unfortunately, self-supervised\\rmonocular depth estimation (SS-MDE) has been limited by the absence of diverse\\rtraining data. Existing datasets have focused exclusively on urban driving in\\rdensely populated cities, resulting in models that fail to generalize beyond\\rthis domain.\\r  To address these limitations, this paper proposes two novel datasets: SlowTV\\rand CribsTV. These are large-scale datasets curated from publicly available\\rYouTube videos, containing a total of 2M training frames. They offer an\\rincredibly diverse set of environments, ranging from snowy forests to coastal\\rroads, luxury mansions and even underwater coral reefs. We leverage these\\rdatasets to tackle the challenging task of zero-shot generalization,\\routperforming every existing SS-MDE approach and even some state-of-the-art\\rsupervised methods.\\r  The generalization capabilities of our models are further enhanced by a range\\rof components and contributions: 1) learning the camera intrinsics, 2) a\\rstronger augmentation regime targeting aspect ratio changes, 3) support frame\\rrandomization, 4) flexible motion estimation, 5) a modern transformer-based\\rarchitecture. We demonstrate the effectiveness of each component in extensive\\rablation experiments. To facilitate the development of future research, we make\\rthe datasets, code and pretrained models available to the public at\\rhttps://github.com/jspenmar/slowtv_monodepth.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01569 ,  25054kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01599\\rDate: Sun, 3 Mar 2024 19:53:06 GMT   (1487kb,D)\\r\\rTitle: SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional\\r  Videos\\rAuthors: Yulei Niu, Wenliang Guo, Long Chen, Xudong Lin, Shih-Fu Chang\\rCategories: cs.CV cs.AI cs.CL cs.LG\\rComments: Accepted by ICLR 2024\\r\\\\\\\\\\r  We study the problem of procedure planning in instructional videos, which\\raims to make a goal-oriented sequence of action steps given partial visual\\rstate observations. The motivation of this problem is to learn a structured and\\rplannable state and action space. Recent works succeeded in sequence modeling\\rof steps with only sequence-level annotations accessible during training, which\\roverlooked the roles of states in the procedures. In this work, we point out\\rthat State CHangEs MAtter (SCHEMA) for procedure planning in instructional\\rvideos. We aim to establish a more structured state space by investigating the\\rcausal relations between steps and states in procedures. Specifically, we\\rexplicitly represent each step as state changes and track the state changes in\\rprocedures. For step representation, we leveraged the commonsense knowledge in\\rlarge language models (LLMs) to describe the state changes of steps via our\\rdesigned chain-of-thought prompting. For state change tracking, we align visual\\rstate observations with language state descriptions via cross-modal contrastive\\rlearning, and explicitly model the intermediate states of the procedure using\\rLLM-generated state descriptions. Experiments on CrossTask, COIN, and NIV\\rbenchmark datasets demonstrate that our proposed SCHEMA model achieves\\rstate-of-the-art performance and obtains explainable visualizations.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01599 ,  1487kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01606\\rDate: Sun, 3 Mar 2024 20:16:14 GMT   (1336kb,D)\\r\\rTitle: A Unified Model Selection Technique for Spectral Clustering Based Motion\\r  Segmentation\\rAuthors: Yuxiang Huang, John Zelek\\rCategories: cs.CV cs.AI\\rComments: Journal of Computational Vision and Imaging Systems\\r\\\\\\\\\\r  Motion segmentation is a fundamental problem in computer vision and is\\rcrucial in various applications such as robotics, autonomous driving and action\\rrecognition. Recently, spectral clustering based methods have shown impressive\\rresults on motion segmentation in dynamic environments. These methods perform\\rspectral clustering on motion affinity matrices to cluster objects or point\\rtrajectories in the scene into different motion groups. However, existing\\rmethods often need the number of motions present in the scene to be known,\\rwhich significantly reduces their practicality. In this paper, we propose a\\runified model selection technique to automatically infer the number of motion\\rgroups for spectral clustering based motion segmentation methods by combining\\rdifferent existing model selection techniques together. We evaluate our method\\ron the KT3DMoSeg dataset and achieve competitve results comparing to the\\rbaseline where the number of clusters is given as ground truth information.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01606 ,  1336kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01619\\rDate: Sun, 3 Mar 2024 21:35:00 GMT   (2169kb,D)\\r\\rTitle: Spectrum AUC Difference (SAUCD): Human-aligned 3D Shape Evaluation\\rAuthors: Tianyu Luan, Zhong Li, Lele Chen, Xuan Gong, Lichang Chen, Yi Xu, and\\r  Junsong Yuan\\rCategories: cs.CV cs.GR\\rComments: Accepted by CVPR 2024. Project page: https://bit.ly/saucd\\r\\\\\\\\\\r  Existing 3D mesh shape evaluation metrics mainly focus on the overall shape\\rbut are usually less sensitive to local details. This makes them inconsistent\\rwith human evaluation, as human perception cares about both overall and\\rdetailed shape. In this paper, we propose an analytic metric named Spectrum\\rArea Under the Curve Difference (SAUCD) that demonstrates better consistency\\rwith human evaluation. To compare the difference between two shapes, we first\\rtransform the 3D mesh to the spectrum domain using the discrete\\rLaplace-Beltrami operator and Fourier transform. Then, we calculate the Area\\rUnder the Curve (AUC) difference between the two spectrums, so that each\\rfrequency band that captures either the overall or detailed shape is equitably\\rconsidered. Taking human sensitivity across frequency bands into account, we\\rfurther extend our metric by learning suitable weights for each frequency band\\rwhich better aligns with human perception. To measure the performance of SAUCD,\\rwe build a 3D mesh evaluation dataset called Shape Grading, along with manual\\rannotations from more than 800 subjects. By measuring the correlation between\\rour metric and human evaluation, we demonstrate that SAUCD is well aligned with\\rhuman evaluation, and outperforms previous 3D mesh metrics.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01619 ,  2169kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01641\\rDate: Sun, 3 Mar 2024 23:35:49 GMT   (2640kb,D)\\r\\rTitle: AIO2: Online Correction of Object Labels for Deep Learning with\\r  Incomplete Annotation in Remote Sensing Image Segmentation\\rAuthors: Chenying Liu and Conrad M Albrecht and Yi Wang and Qingyu Li and Xiao\\r  Xiang Zhu\\rCategories: cs.CV\\rComments: This work has been accepted by IEEE Transactions on Geoscience and\\r  Remote Sensing (TGRS)\\r\\\\\\\\\\r  While the volume of remote sensing data is increasing daily, deep learning in\\rEarth Observation faces lack of accurate annotations for supervised\\roptimization. Crowdsourcing projects such as OpenStreetMap distribute the\\rannotation load to their community. However, such annotation inevitably\\rgenerates noise due to insufficient control of the label quality, lack of\\rannotators, frequent changes of the Earth's surface as a result of natural\\rdisasters and urban development, among many other factors. We present\\rAdaptively trIggered Online Object-wise correction (AIO2) to address annotation\\rnoise induced by incomplete label sets. AIO2 features an Adaptive Correction\\rTrigger (ACT) module that avoids label correction when the model training\\runder- or overfits, and an Online Object-wise Correction (O2C) methodology that\\remploys spatial information for automated label modification. AIO2 utilizes a\\rmean teacher model to enhance training robustness with noisy labels to both\\rstabilize the training accuracy curve for fitting in ACT and provide pseudo\\rlabels for correction in O2C. Moreover, O2C is implemented online without the\\rneed to store updated labels every training epoch. We validate our approach on\\rtwo building footprint segmentation datasets with different spatial\\rresolutions. Experimental results with varying degrees of building label noise\\rdemonstrate the robustness of AIO2. Source code will be available at\\rhttps://github.com/zhu-xlab/AIO2.git.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01641 ,  2640kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01644\\rDate: Sun, 3 Mar 2024 23:46:06 GMT   (40247kb,D)\\r\\rTitle: OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework\\r  for 3D Occupancy Prediction\\rAuthors: Zhenxing Ming, Julie Stephany Berrio, Mao Shan, and Stewart Worrall\\rCategories: cs.CV cs.RO\\r\\\\\\\\\\r  This paper introduces OccFusion, a straightforward and efficient sensor\\rfusion framework for predicting 3D occupancy. A comprehensive understanding of\\r3D scenes is crucial in autonomous driving, and recent models for 3D semantic\\roccupancy prediction have successfully addressed the challenge of describing\\rreal-world objects with varied shapes and classes. However, existing methods\\rfor 3D occupancy prediction heavily rely on surround-view camera images, making\\rthem susceptible to changes in lighting and weather conditions. By integrating\\rfeatures from additional sensors, such as lidar and surround view radars, our\\rframework enhances the accuracy and robustness of occupancy prediction,\\rresulting in top-tier performance on the nuScenes benchmark. Furthermore,\\rextensive experiments conducted on the nuScenes dataset, including challenging\\rnight and rainy scenarios, confirm the superior performance of our sensor\\rfusion strategy across various perception ranges. The code for this framework\\rwill be made available at https://github.com/DanielMing123/OCCFusion.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01644 ,  40247kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01647\\rDate: Mon, 4 Mar 2024 00:01:52 GMT   (23423kb)\\r\\rTitle: Neural Network Assisted Lifting Steps For Improved Fully Scalable Lossy\\r  Image Compression in JPEG 2000\\rAuthors: Xinyue Li, Aous Naman and David Taubman\\rCategories: cs.CV eess.IV\\rComments: This work has been submitted to the IEEE for possible publication.\\r  Copyright may be transferred without notice, after which this version may no\\r  longer be accessible\\r\\\\\\\\\\r  This work proposes to augment the lifting steps of the conventional wavelet\\rtransform with additional neural network assisted lifting steps. These\\radditional steps reduce residual redundancy (notably aliasing information)\\ramongst the wavelet subbands, and also improve the visual quality of\\rreconstructed images at reduced resolutions. The proposed approach involves two\\rsteps, a high-to-low step followed by a low-to-high step. The high-to-low step\\rsuppresses aliasing in the low-pass band by using the detail bands at the same\\rresolution, while the low-to-high step aims to further remove redundancy from\\rdetail bands, so as to achieve higher energy compaction. The proposed two\\rlifting steps are trained in an end-to-end fashion; we employ a backward\\rannealing approach to overcome the non-differentiability of the quantization\\rand cost functions during back-propagation. Importantly, the networks employed\\rin this paper are compact and with limited non-linearities, allowing a fully\\rscalable system; one pair of trained network parameters are applied for all\\rlevels of decomposition and for all bit-rates of interest. By employing the\\rproposed approach within the JPEG 2000 image coding standard, our method can\\rachieve up to 17.4% average BD bit-rate saving over a wide range of bit-rates,\\rwhile retaining quality and resolution scalability features of JPEG 2000.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01647 ,  23423kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01663\\rDate: Mon, 4 Mar 2024 01:18:29 GMT   (256kb,D)\\r\\rTitle: PillarGen: Enhancing Radar Point Cloud Density and Quality via\\r  Pillar-based Point Generation Network\\rAuthors: Jisong Kim, Geonho Bang, Kwangjin Choi, Minjae Seong, Jaechang Yoo,\\r  Eunjong Pyo, Jun Won Choi\\rCategories: cs.CV\\rComments: 6 pages, 3 figures\\r\\\\\\\\\\r  In this paper, we present a novel point generation model, referred to as\\rPillar-based Point Generation Network (PillarGen), which facilitates the\\rtransformation of point clouds from one domain into another. PillarGen can\\rproduce synthetic point clouds with enhanced density and quality based on the\\rprovided input point clouds. The PillarGen model performs the following three\\rsteps: 1) pillar encoding, 2) Occupied Pillar Prediction (OPP), and 3) Pillar\\rto Point Generation (PPG). The input point clouds are encoded using a pillar\\rgrid structure to generate pillar features. Then, OPP determines the active\\rpillars used for point generation and predicts the center of points and the\\rnumber of points to be generated for each active pillar. PPG generates the\\rsynthetic points for each active pillar based on the information provided by\\rOPP. We evaluate the performance of PillarGen using our proprietary radar\\rdataset, focusing on enhancing the density and quality of short-range radar\\rdata using the long-range radar data as supervision. Our experiments\\rdemonstrate that PillarGen outperforms traditional point upsampling methods in\\rquantitative and qualitative measures. We also confirm that when PillarGen is\\rincorporated into bird's eye view object detection, a significant improvement\\rin detection accuracy is achieved.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01663 ,  256kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01680\\rDate: Mon, 4 Mar 2024 02:25:41 GMT   (21959kb,D)\\r\\rTitle: Zero-shot Generalizable Incremental Learning for Vision-Language Object\\r  Detection\\rAuthors: Jieren Deng, Haojian Zhang, Kun Ding, Jianhua Hu, Xingxuan Zhang,\\r  Yunkuan Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  This paper presents Incremental Vision-Language Object Detection (IVLOD), a\\rnovel learning task designed to incrementally adapt pre-trained Vision-Language\\rObject Detection Models (VLODMs) to various specialized domains, while\\rsimultaneously preserving their zero-shot generalization capabilities for the\\rgeneralized domain. To address this new challenge, we present the\\rZero-interference Reparameterizable Adaptation (ZiRa), a novel method that\\rintroduces Zero-interference Loss and reparameterization techniques to tackle\\rIVLOD without incurring additional inference costs or a significant increase in\\rmemory usage. Comprehensive experiments on COCO and ODinW-13 datasets\\rdemonstrate that ZiRa effectively safeguards the zero-shot generalization\\rability of VLODMs while continuously adapting to new tasks. Specifically, after\\rtraining on ODinW-13 datasets, ZiRa exhibits superior performance compared to\\rCL-DETR and iDETR, boosting zero-shot generalizability by substantial 13.91 and\\r8.71 AP, respectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01680 ,  21959kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01683\\rDate: Mon, 4 Mar 2024 02:29:02 GMT   (8812kb,D)\\r\\rTitle: DD-VNB: A Depth-based Dual-Loop Framework for Real-time Visually\\r  Navigated Bronchoscopy\\rAuthors: Qingyao Tian, Huai Liao, Xinyan Huang, Jian Chen, Zihui Zhang, Bingyu\\r  Yang, Sebastien Ourselin and Hongbin Liu\\rCategories: cs.CV\\r\\\\\\\\\\r  Real-time 6 DOF localization of bronchoscopes is crucial for enhancing\\rintervention quality. However, current vision-based technologies struggle to\\rbalance between generalization to unseen data and computational speed. In this\\rstudy, we propose a Depth-based Dual-Loop framework for real-time Visually\\rNavigated Bronchoscopy (DD-VNB) that can generalize across patient cases\\rwithout the need of re-training. The DD-VNB framework integrates two key\\rmodules: depth estimation and dual-loop localization. To address the domain gap\\ramong patients, we propose a knowledge-embedded depth estimation network that\\rmaps endoscope frames to depth, ensuring generalization by eliminating\\rpatient-specific textures. The network embeds view synthesis knowledge into a\\rcycle adversarial architecture for scale-constrained monocular depth\\restimation. For real-time performance, our localization module embeds a fast\\rego-motion estimation network into the loop of depth registration. The\\rego-motion inference network estimates the pose change of the bronchoscope in\\rhigh frequency while depth registration against the pre-operative 3D model\\rprovides absolute pose periodically. Specifically, the relative pose changes\\rare fed into the registration process as the initial guess to boost its\\raccuracy and speed. Experiments on phantom and in-vivo data from patients\\rdemonstrate the effectiveness of our framework: 1) monocular depth estimation\\routperforms SOTA, 2) localization achieves an accuracy of Absolute Tracking\\rError (ATE) of 4.7 $\\\\pm$ 3.17 mm in phantom and 6.49 $\\\\pm$ 3.88 mm in patient\\rdata, 3) with a frame-rate approaching video capture speed, 4) without the\\rnecessity of case-wise network retraining. The framework's superior speed and\\raccuracy demonstrate its promising clinical potential for real-time\\rbronchoscopic navigation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01683 ,  8812kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01693\\rDate: Mon, 4 Mar 2024 03:00:22 GMT   (18665kb,D)\\r\\rTitle: HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances\\rAuthors: Supreeth Narasimhaswamy, Uttaran Bhattacharya, Xiang Chen, Ishita\\r  Dasgupta, Saayan Mitra, Minh Hoai\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Text-to-image generative models can generate high-quality humans, but realism\\ris lost when generating hands. Common artifacts include irregular hand poses,\\rshapes, incorrect numbers of fingers, and physically implausible finger\\rorientations. To generate images with realistic hands, we propose a novel\\rdiffusion-based architecture called HanDiffuser that achieves realism by\\rinjecting hand embeddings in the generative process. HanDiffuser consists of\\rtwo components: a Text-to-Hand-Params diffusion model to generate SMPL-Body and\\rMANO-Hand parameters from input text prompts, and a Text-Guided\\rHand-Params-to-Image diffusion model to synthesize images by conditioning on\\rthe prompts and hand parameters generated by the previous component. We\\rincorporate multiple aspects of hand representation, including 3D shapes and\\rjoint-level finger positions, orientations and articulations, for robust\\rlearning and reliable performance during inference. We conduct extensive\\rquantitative and qualitative experiments and perform user studies to\\rdemonstrate the efficacy of our method in generating images with high-quality\\rhands.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01693 ,  18665kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01713\\rDate: Mon, 4 Mar 2024 04:02:59 GMT   (341kb,D)\\r\\rTitle: MCA: Moment Channel Attention Networks\\rAuthors: Yangbo Jiang, Zhiwei Jiang, Le Han, Zenan Huang, Nenggan Zheng\\rCategories: cs.CV\\r\\\\\\\\\\r  Channel attention mechanisms endeavor to recalibrate channel weights to\\renhance representation abilities of networks. However, mainstream methods often\\rrely solely on global average pooling as the feature squeezer, which\\rsignificantly limits the overall potential of models. In this paper, we\\rinvestigate the statistical moments of feature maps within a neural network.\\rOur findings highlight the critical role of high-order moments in enhancing\\rmodel capacity. Consequently, we introduce a flexible and comprehensive\\rmechanism termed Extensive Moment Aggregation (EMA) to capture the global\\rspatial context. Building upon this mechanism, we propose the Moment Channel\\rAttention (MCA) framework, which efficiently incorporates multiple levels of\\rmoment-based information while minimizing additional computation costs through\\rour Cross Moment Convolution (CMC) module. The CMC module via channel-wise\\rconvolution layer to capture multiple order moment information as well as cross\\rchannel features. The MCA block is designed to be lightweight and easily\\rintegrated into a variety of neural network architectures. Experimental results\\ron classical image classification, object detection, and instance segmentation\\rtasks demonstrate that our proposed method achieves state-of-the-art results,\\routperforming existing channel attention methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01713 ,  341kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01731\\rDate: Mon, 4 Mar 2024 05:03:24 GMT   (1549kb,D)\\r\\rTitle: RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant\\r  Features\\rAuthors: Howard H. Qian, Yangxiao Lu, Kejia Ren, Gaotian Wang, Ninad\\r  Khargonkar, Yu Xiang, Kaiyu Hang\\rCategories: cs.CV cs.RO\\rComments: 7 pages, 5 figures, ICRA 2024\\r\\\\\\\\\\r  In order to successfully perform manipulation tasks in new environments, such\\ras grasping, robots must be proficient in segmenting unseen objects from the\\rbackground and/or other objects. Previous works perform unseen object instance\\rsegmentation (UOIS) by training deep neural networks on large-scale data to\\rlearn RGB/RGB-D feature embeddings, where cluttered environments often result\\rin inaccurate segmentations. We build upon these methods and introduce a novel\\rapproach to correct inaccurate segmentation, such as under-segmentation, of\\rstatic image-based UOIS masks by using robot interaction and a designed body\\rframe-invariant feature. We demonstrate that the relative linear and rotational\\rvelocities of frames randomly attached to rigid bodies due to robot\\rinteractions can be used to identify objects and accumulate corrected\\robject-level segmentation masks. By introducing motion to regions of\\rsegmentation uncertainty, we are able to drastically improve segmentation\\raccuracy in an uncertainty-driven manner with minimal, non-disruptive\\rinteractions (ca. 2-3 per scene). We demonstrate the effectiveness of our\\rproposed interactive perception pipeline in accurately segmenting cluttered\\rscenes by achieving an average object segmentation accuracy rate of 80.7%, an\\rincrease of 28.2% when compared with other state-of-the-art UOIS methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01731 ,  1549kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01733\\rDate: Mon, 4 Mar 2024 05:11:26 GMT   (2059kb,D)\\r\\rTitle: 3D Hand Reconstruction via Aggregating Intra and Inter Graphs Guided by\\r  Prior Knowledge for Hand-Object Interaction Scenario\\rAuthors: Feng Shuang, Wenbo He and Shaodong Li\\rCategories: cs.CV\\r\\\\\\\\\\r  Recently, 3D hand reconstruction has gained more attention in human-computer\\rcooperation, especially for hand-object interaction scenario. However, it still\\rremains huge challenge due to severe hand-occlusion caused by interaction,\\rwhich contain the balance of accuracy and physical plausibility, highly\\rnonlinear mapping of model parameters and occlusion feature enhancement. To\\rovercome these issues, we propose a 3D hand reconstruction network combining\\rthe benefits of model-based and model-free approaches to balance accuracy and\\rphysical plausibility for hand-object interaction scenario. Firstly, we present\\ra novel MANO pose parameters regression module from 2D joints directly, which\\ravoids the process of highly nonlinear mapping from abstract image feature and\\rno longer depends on accurate 3D joints. Moreover, we further propose a\\rvertex-joint mutual graph-attention model guided by MANO to jointly refine hand\\rmeshes and joints, which model the dependencies of vertex-vertex and\\rjoint-joint and capture the correlation of vertex-joint for aggregating\\rintra-graph and inter-graph node features respectively. The experimental\\rresults demonstrate that our method achieves a competitive performance on\\rrecently benchmark datasets HO3DV2 and Dex-YCB, and outperforms all only\\rmodel-base approaches and model-free approaches.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01733 ,  2059kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01736\\rDate: Mon, 4 Mar 2024 05:29:32 GMT   (358kb,D)\\r\\rTitle: Lightweight Object Detection: A Study Based on YOLOv7 Integrated with\\r  ShuffleNetv2 and Vision Transformer\\rAuthors: Wenkai Gong\\rCategories: cs.CV\\r\\\\\\\\\\r  As mobile computing technology rapidly evolves, deploying efficient object\\rdetection algorithms on mobile devices emerges as a pivotal research area in\\rcomputer vision. This study zeroes in on optimizing the YOLOv7 algorithm to\\rboost its operational efficiency and speed on mobile platforms while ensuring\\rhigh accuracy. Leveraging a synergy of advanced techniques such as Group\\rConvolution, ShuffleNetV2, and Vision Transformer, this research has\\reffectively minimized the model's parameter count and memory usage, streamlined\\rthe network architecture, and fortified the real-time object detection\\rproficiency on resource-constrained devices. The experimental outcomes reveal\\rthat the refined YOLO model demonstrates exceptional performance, markedly\\renhancing processing velocity while sustaining superior detection accuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01736 ,  358kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01740\\rDate: Mon, 4 Mar 2024 05:38:16 GMT   (17818kb,D)\\r\\rTitle: DEMOS: Dynamic Environment Motion Synthesis in 3D Scenes via Local\\r  Spherical-BEV Perception\\rAuthors: Jingyu Gong, Min Wang, Wentao Liu, Chen Qian, Zhizhong Zhang, Yuan\\r  Xie, Lizhuang Ma\\rCategories: cs.CV\\r\\\\\\\\\\r  Motion synthesis in real-world 3D scenes has recently attracted much\\rattention. However, the static environment assumption made by most current\\rmethods usually cannot be satisfied especially for real-time motion synthesis\\rin scanned point cloud scenes, if multiple dynamic objects exist, e.g., moving\\rpersons or vehicles. To handle this problem, we propose the first Dynamic\\rEnvironment MOtion Synthesis framework (DEMOS) to predict future motion\\rinstantly according to the current scene, and use it to dynamically update the\\rlatent motion for final motion synthesis. Concretely, we propose a\\rSpherical-BEV perception method to extract local scene features that are\\rspecifically designed for instant scene-aware motion prediction. Then, we\\rdesign a time-variant motion blending to fuse the new predicted motions into\\rthe latent motion, and the final motion is derived from the updated latent\\rmotions, benefitting both from motion-prior and iterative methods. We unify the\\rdata format of two prevailing datasets, PROX and GTA-IM, and take them for\\rmotion synthesis evaluation in 3D scenes. We also assess the effectiveness of\\rthe proposed method in dynamic environments from GTA-IM and Semantic3D to check\\rthe responsiveness. The results show our method outperforms previous works\\rsignificantly and has great performance in handling dynamic environments.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01740 ,  17818kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01753\\rDate: Mon, 4 Mar 2024 06:19:27 GMT   (528kb,D)\\r\\rTitle: Training-Free Pretrained Model Merging\\rAuthors: Zhengqi Xu, Ke Yuan, Huiqiong Wang, Yong Wang, Mingli Song, Jie Song\\rCategories: cs.CV\\rComments: CVPR2024 accepted\\r\\\\\\\\\\r  Recently, model merging techniques have surfaced as a solution to combine\\rmultiple single-talent models into a single multi-talent model. However,\\rprevious endeavors in this field have either necessitated additional training\\ror fine-tuning processes, or require that the models possess the same\\rpre-trained initialization. In this work, we identify a common drawback in\\rprior works w.r.t. the inconsistency of unit similarity in the weight space and\\rthe activation space. To address this inconsistency, we propose an innovative\\rmodel merging framework, coined as merging under dual-space constraints\\r(MuDSC). Specifically, instead of solely maximizing the objective of a single\\rspace, we advocate for the exploration of permutation matrices situated in a\\rregion with a unified high similarity in the dual space, achieved through the\\rlinear combination of activation and weight similarity matrices. In order to\\renhance usability, we have also incorporated adaptations for group structure,\\rincluding Multi-Head Attention and Group Normalization. Comprehensive\\rexperimental comparisons demonstrate that MuDSC can significantly boost the\\rperformance of merged models with various task combinations and architectures.\\rFurthermore, the visualization of the merged model within the multi-task loss\\rlandscape reveals that MuDSC enables the merged model to reside in the\\roverlapping segment, featuring a unified lower loss for each task. Our code is\\rpublicly available at https://github.com/zju-vipa/training_free_model_merging.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01753 ,  528kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01756\\rDate: Mon, 4 Mar 2024 06:22:17 GMT   (3165kb,D)\\r\\rTitle: Attention Guidance Mechanism for Handwritten Mathematical Expression\\r  Recognition\\rAuthors: Yutian Liu, Wenjun Ke, Jianguo Wei\\rCategories: cs.CV\\r\\\\\\\\\\r  Handwritten mathematical expression recognition (HMER) is challenging in OCR\\rtasks due to the complex layouts of mathematical expressions, suffering from\\rissues including over-parsing and under-parsing. To solve these, previous\\rmethods utilize historical attention weights to improve the attention\\rmechanism. However, this approach has limitations in addressing under-parsing\\rsince it cannot correct the erroneous attention on image regions that should be\\rparsed at subsequent decoding steps. When this happens, the attention module\\rincorporates future context into the current decoding step, thus confusing the\\ralignment process. To address this issue, we propose an attention guidance\\rmechanism to explicitly suppress attention weights in irrelevant regions and\\renhance ones in appropriate regions, thereby inhibiting access to information\\routside the intended context. Depending on the type of attention guidance, we\\rdevise two complementary approaches to refine attention weights: self-guidance\\rthat coordinates attention of multiple heads and neighbor-guidance that\\rintegrates attention from adjacent time steps. Experiments show that our method\\routperforms existing state-of-the-art methods, achieving expression recognition\\rrates of 60.75% / 61.81% / 63.30% on the CROHME 2014 / 2016 / 2019 datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01756 ,  3165kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01779\\rDate: Mon, 4 Mar 2024 07:17:44 GMT   (3847kb,D)\\r\\rTitle: OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable\\r  Virtual Try-on\\rAuthors: Yuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen\\rCategories: cs.CV\\r\\\\\\\\\\r  Image-based virtual try-on (VTON), which aims to generate an outfitted image\\rof a target human wearing an in-shop garment, is a challenging image-synthesis\\rtask calling for not only high fidelity of the outfitted human but also full\\rpreservation of garment details. To tackle this issue, we propose Outfitting\\rover Try-on Diffusion (OOTDiffusion), leveraging the power of pretrained latent\\rdiffusion models and designing a novel network architecture for realistic and\\rcontrollable virtual try-on. Without an explicit warping process, we propose an\\routfitting UNet to learn the garment detail features, and merge them with the\\rtarget human body via our proposed outfitting fusion in the denoising process\\rof diffusion models. In order to further enhance the controllability of our\\routfitting UNet, we introduce outfitting dropout to the training process, which\\renables us to adjust the strength of garment features through classifier-free\\rguidance. Our comprehensive experiments on the VITON-HD and Dress Code datasets\\rdemonstrate that OOTDiffusion efficiently generates high-quality outfitted\\rimages for arbitrary human and garment images, which outperforms other VTON\\rmethods in both fidelity and controllability, indicating an impressive\\rbreakthrough in virtual try-on. Our source code is available at\\rhttps://github.com/levihsu/OOTDiffusion.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01779 ,  3847kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01781\\rDate: Mon, 4 Mar 2024 07:21:07 GMT   (8658kb,D)\\r\\rTitle: Integrating Efficient Optimal Transport and Functional Maps For\\r  Unsupervised Shape Correspondence Learning\\rAuthors: Tung Le, Khai Nguyen, Shanlin Sun, Nhat Ho, Xiaohui Xie\\rCategories: cs.CV cs.AI\\rComments: accepted by CVPR 2024\\r\\\\\\\\\\r  In the realm of computer vision and graphics, accurately establishing\\rcorrespondences between geometric 3D shapes is pivotal for applications like\\robject tracking, registration, texture transfer, and statistical shape\\ranalysis. Moving beyond traditional hand-crafted and data-driven feature\\rlearning methods, we incorporate spectral methods with deep learning, focusing\\ron functional maps (FMs) and optimal transport (OT). Traditional OT-based\\rapproaches, often reliant on entropy regularization OT in learning-based\\rframework, face computational challenges due to their quadratic cost. Our key\\rcontribution is to employ the sliced Wasserstein distance (SWD) for OT, which\\ris a valid fast optimal transport metric in an unsupervised shape matching\\rframework. This unsupervised framework integrates functional map regularizers\\rwith a novel OT-based loss derived from SWD, enhancing feature alignment\\rbetween shapes treated as discrete probability measures. We also introduce an\\radaptive refinement process utilizing entropy regularized OT, further refining\\rfeature alignments for accurate point-to-point correspondences. Our method\\rdemonstrates superior performance in non-rigid shape matching, including\\rnear-isometric and non-isometric scenarios, and excels in downstream tasks like\\rsegmentation transfer. The empirical results on diverse datasets highlight our\\rframework's effectiveness and generalization capabilities, setting new\\rstandards in non-rigid shape matching with efficient OT metrics and an adaptive\\rrefinement module.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01781 ,  8658kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01786\\rDate: Mon, 4 Mar 2024 07:28:23 GMT   (23124kb,D)\\r\\rTitle: Exposing the Deception: Uncovering More Forgery Clues for Deepfake\\r  Detection\\rAuthors: Zhongjie Ba, Qingyu Liu, Zhenguang Liu, Shuang Wu, Feng Lin, Li Lu,\\r  Kui Ren\\rCategories: cs.CV cs.IT math.IT\\rComments: AAAI2024\\r\\\\\\\\\\r  Deepfake technology has given rise to a spectrum of novel and compelling\\rapplications. Unfortunately, the widespread proliferation of high-fidelity fake\\rvideos has led to pervasive confusion and deception, shattering our faith that\\rseeing is believing. One aspect that has been overlooked so far is that current\\rdeepfake detection approaches may easily fall into the trap of overfitting,\\rfocusing only on forgery clues within one or a few local regions. Moreover,\\rexisting works heavily rely on neural networks to extract forgery features,\\rlacking theoretical constraints guaranteeing that sufficient forgery clues are\\rextracted and superfluous features are eliminated. These deficiencies culminate\\rin unsatisfactory accuracy and limited generalizability in real-life scenarios.\\r  In this paper, we try to tackle these challenges through three designs: (1)\\rWe present a novel framework to capture broader forgery clues by extracting\\rmultiple non-overlapping local representations and fusing them into a global\\rsemantic-rich feature. (2) Based on the information bottleneck theory, we\\rderive Local Information Loss to guarantee the orthogonality of local\\rrepresentations while preserving comprehensive task-relevant information. (3)\\rFurther, to fuse the local representations and remove task-irrelevant\\rinformation, we arrive at a Global Information Loss through the theoretical\\ranalysis of mutual information. Empirically, our method achieves\\rstate-of-the-art performance on five benchmark datasets.Our code is available\\rat \\\\url{https://github.com/QingyuLiu/Exposing-the-Deception}, hoping to inspire\\rresearchers.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01786 ,  23124kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01795\\rDate: Mon, 4 Mar 2024 07:35:36 GMT   (8052kb,D)\\r\\rTitle: RankED: Addressing Imbalance and Uncertainty in Edge Detection Using\\r  Ranking-based Losses\\rAuthors: Bedrettin Cetinkaya, Sinan Kalkan, Emre Akbas\\rCategories: cs.CV\\rComments: accepted to CVPR 2024\\r\\\\\\\\\\r  Detecting edges in images suffers from the problems of (P1) heavy imbalance\\rbetween positive and negative classes as well as (P2) label uncertainty owing\\rto disagreement between different annotators. Existing solutions address P1\\rusing class-balanced cross-entropy loss and dice loss and P2 by only predicting\\redges agreed upon by most annotators. In this paper, we propose RankED, a\\runified ranking-based approach that addresses both the imbalance problem (P1)\\rand the uncertainty problem (P2). RankED tackles these two problems with two\\rcomponents: One component which ranks positive pixels over negative pixels, and\\rthe second which promotes high confidence edge pixels to have more label\\rcertainty. We show that RankED outperforms previous studies and sets a new\\rstate-of-the-art on NYUD-v2, BSDS500 and Multi-cue datasets. Code is available\\rat https://ranked-cvpr24.github.io.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01795 ,  8052kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01799\\rDate: Mon, 4 Mar 2024 07:40:55 GMT   (3225kb,D)\\r\\rTitle: Superpixel Graph Contrastive Clustering with Semantic-Invariant\\r  Augmentations for Hyperspectral Images\\rAuthors: Jianhan Qi, Yuheng Jia, Hui Liu, Junhui Hou\\rCategories: cs.CV\\r\\\\\\\\\\r  Hyperspectral images (HSI) clustering is an important but challenging task.\\rThe state-of-the-art (SOTA) methods usually rely on superpixels, however, they\\rdo not fully utilize the spatial and spectral information in HSI 3-D structure,\\rand their optimization targets are not clustering-oriented. In this work, we\\rfirst use 3-D and 2-D hybrid convolutional neural networks to extract the\\rhigh-order spatial and spectral features of HSI through pre-training, and then\\rdesign a superpixel graph contrastive clustering (SPGCC) model to learn\\rdiscriminative superpixel representations. Reasonable augmented views are\\rcrucial for contrastive clustering, and conventional contrastive learning may\\rhurt the cluster structure since different samples are pushed away in the\\rembedding space even if they belong to the same class. In SPGCC, we design two\\rsemantic-invariant data augmentations for HSI superpixels: pixel sampling\\raugmentation and model weight augmentation. Then sample-level alignment and\\rclustering-center-level contrast are performed for better intra-class\\rsimilarity and inter-class dissimilarity of superpixel embeddings. We perform\\rclustering and network optimization alternatively. Experimental results on\\rseveral HSI datasets verify the advantages of the proposed method, e.g., on\\rIndia Pines, our model improves the clustering accuracy from 58.79% to 67.59%\\rcompared to the SOTA method.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01799 ,  3225kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01800\\rDate: Mon, 4 Mar 2024 07:41:50 GMT   (18239kb,D)\\r\\rTitle: AtomoVideo: High Fidelity Image-to-Video Generation\\rAuthors: Litong Gong, Yiran Zhu, Weijie Li, Xiaoyang Kang, Biao Wang, Tiezheng\\r  Ge, Bo Zheng\\rCategories: cs.CV\\rComments: Technical report\\r\\\\\\\\\\r  Recently, video generation has achieved significant rapid development based\\ron superior text-to-image generation techniques. In this work, we propose a\\rhigh fidelity framework for image-to-video generation, named AtomoVideo. Based\\ron multi-granularity image injection, we achieve higher fidelity of the\\rgenerated video to the given image. In addition, thanks to high quality\\rdatasets and training strategies, we achieve greater motion intensity while\\rmaintaining superior temporal consistency and stability. Our architecture\\rextends flexibly to the video frame prediction task, enabling long sequence\\rprediction through iterative generation. Furthermore, due to the design of\\radapter training, our approach can be well combined with existing personalised\\rmodels and controllable modules. By quantitatively and qualitatively\\revaluation, AtomoVideo achieves superior results compared to popular methods,\\rmore examples can be found on our project website: https://atomo-\\rvideo.github.io/.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01800 ,  18239kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01802\\rDate: Mon, 4 Mar 2024 07:47:05 GMT   (2824kb,D)\\r\\rTitle: TNF: Tri-branch Neural Fusion for Multimodal Medical Data Classification\\rAuthors: Tong Zheng, Shusaku Sone, Yoshitaka Ushiku, Yuki Oba, Jiaxin Ma\\rCategories: cs.CV\\r\\\\\\\\\\r  This paper presents a Tri-branch Neural Fusion (TNF) approach designed for\\rclassifying multimodal medical images and tabular data. It also introduces two\\rsolutions to address the challenge of label inconsistency in multimodal\\rclassification. Traditional methods in multi-modality medical data\\rclassification often rely on single-label approaches, typically merging\\rfeatures from two distinct input modalities. This becomes problematic when\\rfeatures are mutually exclusive or labels differ across modalities, leading to\\rreduced accuracy. To overcome this, our TNF approach implements a tri-branch\\rframework that manages three separate outputs: one for image modality, another\\rfor tabular modality, and a third hybrid output that fuses both image and\\rtabular data. The final decision is made through an ensemble method that\\rintegrates likelihoods from all three branches. We validate the effectiveness\\rof TNF through extensive experiments, which illustrate its superiority over\\rtraditional fusion and ensemble methods in various convolutional neural\\rnetworks and transformer-based architectures across multiple datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01802 ,  2824kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01804\\rDate: Mon, 4 Mar 2024 07:51:46 GMT   (4768kb,D)\\r\\rTitle: PointCore: Efficient Unsupervised Point Cloud Anomaly Detector Using\\r  Local-Global Features\\rAuthors: Baozhu Zhao, Qiwei Xiong, Xiaohan Zhang, Jingfeng Guo, Qi Liu, Xiaofen\\r  Xing, Xiangmin Xu\\rCategories: cs.CV\\r\\\\\\\\\\r  Three-dimensional point cloud anomaly detection that aims to detect anomaly\\rdata points from a training set serves as the foundation for a variety of\\rapplications, including industrial inspection and autonomous driving. However,\\rexisting point cloud anomaly detection methods often incorporate multiple\\rfeature memory banks to fully preserve local and global representations, which\\rcomes at the high cost of computational complexity and mismatches between\\rfeatures. To address that, we propose an unsupervised point cloud anomaly\\rdetection framework based on joint local-global features, termed PointCore. To\\rbe specific, PointCore only requires a single memory bank to store local\\r(coordinate) and global (PointMAE) representations and different priorities are\\rassigned to these local-global features, thereby reducing the computational\\rcost and mismatching disturbance in inference. Furthermore, to robust against\\rthe outliers, a normalization ranking method is introduced to not only adjust\\rvalues of different scales to a notionally common scale, but also transform\\rdensely-distributed data into a uniform distribution. Extensive experiments on\\rReal3D-AD dataset demonstrate that PointCore achieves competitive inference\\rtime and the best performance in both detection and localization as compared to\\rthe state-of-the-art Reg3D-AD approach and several competitors.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01804 ,  4768kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01807\\rDate: Mon, 4 Mar 2024 07:57:05 GMT   (19053kb,D)\\r\\rTitle: ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models\\rAuthors: Lukas H\\\\ollein, Alja\\\\v{z} Bo\\\\v{z}i\\\\v{c}, Norman M\\\\uller, David\\r  Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollh\\\\ofer, Matthias\\r  Nie{\\\\ss}ner\\rCategories: cs.CV\\rComments: Accepted to CVPR 2024, project page:\\r  https://lukashoel.github.io/ViewDiff/, video:\\r  https://www.youtube.com/watch?v=SdjoCqHzMMk, code:\\r  https://github.com/facebookresearch/ViewDiff\\r\\\\\\\\\\r  3D asset generation is getting massive amounts of attention, inspired by the\\rrecent success of text-guided 2D content creation. Existing text-to-3D methods\\ruse pretrained text-to-image diffusion models in an optimization problem or\\rfine-tune them on synthetic data, which often results in non-photorealistic 3D\\robjects without backgrounds. In this paper, we present a method that leverages\\rpretrained text-to-image models as a prior, and learn to generate multi-view\\rimages in a single denoising process from real-world data. Concretely, we\\rpropose to integrate 3D volume-rendering and cross-frame-attention layers into\\reach block of the existing U-Net network of the text-to-image model. Moreover,\\rwe design an autoregressive generation that renders more 3D-consistent images\\rat any viewpoint. We train our model on real-world datasets of objects and\\rshowcase its capabilities to generate instances with a variety of high-quality\\rshapes and textures in authentic surroundings. Compared to the existing\\rmethods, the results generated by our method are consistent, and have favorable\\rvisual quality (-30% FID, -37% KID).\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01807 ,  19053kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01813\\rDate: Mon, 4 Mar 2024 08:00:20 GMT   (7142kb,D)\\r\\rTitle: A Simple Baseline for Efficient Hand Mesh Reconstruction\\rAuthors: Zhishan Zhou, Shihao.zhou, Zhi Lv, Minqiang Zou, Yao Tang, Jiajun\\r  Liang\\rCategories: cs.CV\\r\\\\\\\\\\r  3D hand pose estimation has found broad application in areas such as gesture\\rrecognition and human-machine interaction tasks. As performance improves, the\\rcomplexity of the systems also increases, which can limit the comparative\\ranalysis and practical implementation of these methods. In this paper, we\\rpropose a simple yet effective baseline that not only surpasses\\rstate-of-the-art (SOTA) methods but also demonstrates computational efficiency.\\rTo establish this baseline, we abstract existing work into two components: a\\rtoken generator and a mesh regressor, and then examine their core structures. A\\rcore structure, in this context, is one that fulfills intrinsic functions,\\rbrings about significant improvements, and achieves excellent performance\\rwithout unnecessary complexities. Our proposed approach is decoupled from any\\rmodifications to the backbone, making it adaptable to any modern models. Our\\rmethod outperforms existing solutions, achieving state-of-the-art (SOTA)\\rresults across multiple datasets. On the FreiHAND dataset, our approach\\rproduced a PA-MPJPE of 5.7mm and a PA-MPVPE of 6.0mm. Similarly, on the Dexycb\\rdataset, we observed a PA-MPJPE of 5.5mm and a PA-MPVPE of 5.0mm. As for\\rperformance speed, our method reached up to 33 frames per second (fps) when\\rusing HRNet and up to 70 fps when employing FastViT-MA36\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01813 ,  7142kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01818\\rDate: Mon, 4 Mar 2024 08:06:41 GMT   (14636kb,D)\\r\\rTitle: AllSpark: Reborn Labeled Features from Unlabeled in Transformer for\\r  Semi-Supervised Semantic Segmentation\\rAuthors: Haonan Wang, Qixiang Zhang, Yi Li, Xiaomeng Li\\rCategories: cs.CV cs.AI\\rComments: Accepted by CVPR 2024\\r\\\\\\\\\\r  Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate\\rthe burden of time-consuming pixel-level manual labeling, which leverages\\rlimited labeled data along with larger amounts of unlabeled data. Current\\rstate-of-the-art methods train the labeled data with ground truths and\\runlabeled data with pseudo labels. However, the two training flows are\\rseparate, which allows labeled data to dominate the training process, resulting\\rin low-quality pseudo labels and, consequently, sub-optimal results. To\\ralleviate this issue, we present AllSpark, which reborns the labeled features\\rfrom unlabeled ones with the channel-wise cross-attention mechanism. We further\\rintroduce a Semantic Memory along with a Channel Semantic Grouping strategy to\\rensure that unlabeled features adequately represent labeled features. The\\rAllSpark shed new light on the architecture level designs of SSSS rather than\\rframework level, which avoids increasingly complicated training pipeline\\rdesigns. It can also be regarded as a flexible bottleneck module that can be\\rseamlessly integrated into a general transformer-based segmentation model. The\\rproposed AllSpark outperforms existing methods across all evaluation protocols\\ron Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and\\rmodel weights are available at: https://github.com/xmed-lab/AllSpark.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01818 ,  14636kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01840\\rDate: Mon, 4 Mar 2024 08:38:15 GMT   (9843kb,D)\\r\\rTitle: FreeA: Human-object Interaction Detection using Free Annotation Labels\\rAuthors: Yuxiao Wang, Zhenao Wei, Xinyu Jiang, Yu Lei, Weiying Xue, Jinxiu Liu,\\r  Qi Liu\\rCategories: cs.CV cs.AI\\rComments: 11 pages, 7 figures, 6 tables\\r\\\\\\\\\\r  Recent human-object interaction (HOI) detection approaches rely on high cost\\rof manpower and require comprehensive annotated image datasets. In this paper,\\rwe propose a novel self-adaption language-driven HOI detection method, termed\\ras FreeA, without labeling by leveraging the adaptability of CLIP to generate\\rlatent HOI labels. To be specific, FreeA matches image features of human-object\\rpairs with HOI text templates, and a priori knowledge-based mask method is\\rdeveloped to suppress improbable interactions. In addition, FreeA utilizes the\\rproposed interaction correlation matching method to enhance the likelihood of\\ractions related to a specified action, further refine the generated HOI labels.\\rExperiments on two benchmark datasets show that FreeA achieves state-of-the-art\\rperformance among weakly supervised HOI models. Our approach is +8.58 mean\\rAverage Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in\\rlocalizing and classifying the interactive actions than the newest weakly\\rmodel, and +1.68 mAP and +7.28 mAP than the latest weakly+ model, respectively.\\rCode will be available at https://drliuqi.github.io/.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01840 ,  9843kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01849\\rDate: Mon, 4 Mar 2024 08:59:32 GMT   (4106kb,D)\\r\\rTitle: One Prompt Word is Enough to Boost Adversarial Robustness for\\r  Pre-trained Vision-Language Models\\rAuthors: Lin Li, Haoyan Guan, Jianing Qiu, Michael Spratling\\rCategories: cs.CV cs.AI cs.LG\\rComments: CVPR2024\\r\\\\\\\\\\r  Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having\\rremarkable generalization ability, are highly vulnerable to adversarial\\rexamples. This work studies the adversarial robustness of VLMs from the novel\\rperspective of the text prompt instead of the extensively studied model weights\\r(frozen in this work). We first show that the effectiveness of both adversarial\\rattack and defense are sensitive to the used text prompt. Inspired by this, we\\rpropose a method to improve resilience to adversarial attacks by learning a\\rrobust text prompt for VLMs. The proposed method, named Adversarial Prompt\\rTuning (APT), is effective while being both computationally and data efficient.\\rExtensive experiments are conducted across 15 datasets and 4 data sparsity\\rschemes (from 1-shot to full training data settings) to show APT's superiority\\rover hand-engineered prompts and other state-of-the-art adaption methods. APT\\rdemonstrated excellent abilities in terms of the in-distribution performance\\rand the generalization under input distribution shift and across datasets.\\rSurprisingly, by simply adding one learned word to the prompts, APT can\\rsignificantly boost the accuracy and robustness (epsilon=4/255) over the\\rhand-engineered prompts by +13% and +8.5% on average respectively. The\\rimprovement further increases, in our most effective setting, to +26.4% for\\raccuracy and +16.7% for robustness. Code is available at\\rhttps://github.com/TreeLLi/APT.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01849 ,  4106kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01852\\rDate: Mon, 4 Mar 2024 09:03:16 GMT   (14538kb,D)\\r\\rTitle: PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis\\rAuthors: Zhengyao Lv and Yuxiang Wei and Wangmeng Zuo and Kwan-Yee K. Wong\\rCategories: cs.CV\\r\\\\\\\\\\r  Recent advancements in large-scale pre-trained text-to-image models have led\\rto remarkable progress in semantic image synthesis. Nevertheless, synthesizing\\rhigh-quality images with consistent semantics and layout remains a challenge.\\rIn this paper, we propose the adaPtive LAyout-semantiC fusion modulE (PLACE)\\rthat harnesses pre-trained models to alleviate the aforementioned issues.\\rSpecifically, we first employ the layout control map to faithfully represent\\rlayouts in the feature space. Subsequently, we combine the layout and semantic\\rfeatures in a timestep-adaptive manner to synthesize images with realistic\\rdetails. During fine-tuning, we propose the Semantic Alignment (SA) loss to\\rfurther enhance layout alignment. Additionally, we introduce the Layout-Free\\rPrior Preservation (LFP) loss, which leverages unlabeled data to maintain the\\rpriors of pre-trained models, thereby improving the visual quality and semantic\\rconsistency of synthesized images. Extensive experiments demonstrate that our\\rapproach performs favorably in terms of visual quality, semantic consistency,\\rand layout alignment. The source code and model are available at\\rhttps://github.com/cszy98/PLACE/tree/main.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01852 ,  14538kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01859\\rDate: Mon, 4 Mar 2024 09:15:55 GMT   (5371kb,D)\\r\\rTitle: CSE: Surface Anomaly Detection with Contrastively Selected Embedding\\rAuthors: Simon Thomine and Hichem Snoussi\\rCategories: cs.CV\\rComments: 9 pages, VISAPP 2024 conference\\r\\\\\\\\\\r  Detecting surface anomalies of industrial materials poses a significant\\rchallenge within a myriad of industrial manufacturing processes. In recent\\rtimes, various methodologies have emerged, capitalizing on the advantages of\\remploying a network pre-trained on natural images for the extraction of\\rrepresentative features. Subsequently, these features are subjected to\\rprocessing through a diverse range of techniques including memory banks,\\rnormalizing flow, and knowledge distillation, which have exhibited exceptional\\raccuracy. This paper revisits approaches based on pre-trained features by\\rintroducing a novel method centered on target-specific embedding. To capture\\rthe most representative features of the texture under consideration, we employ\\ra variant of a contrastive training procedure that incorporates both\\rartificially generated defective samples and anomaly-free samples during\\rtraining. Exploiting the intrinsic properties of surfaces, we derived a\\rmeaningful representation from the defect-free samples during training,\\rfacilitating a straightforward yet effective calculation of anomaly scores. The\\rexperiments conducted on the MVTEC AD and TILDA datasets demonstrate the\\rcompetitiveness of our approach compared to state-of-the-art methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01859 ,  5371kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01898\\rDate: Mon, 4 Mar 2024 09:57:08 GMT   (6801kb,D)\\r\\rTitle: Revisiting Learning-based Video Motion Magnification for Real-time\\r  Processing\\rAuthors: Hyunwoo Ha, Oh Hyun-Bin, Kim Jun-Seong, Kwon Byung-Ki, Kim Sung-Bin,\\r  Linh-Tam Tran, Ji-Yun Kim, Sung-Ho Bae, Tae-Hyun Oh\\rCategories: cs.CV eess.IV\\rComments: 19 pages\\r\\\\\\\\\\r  Video motion magnification is a technique to capture and amplify subtle\\rmotion in a video that is invisible to the naked eye. The deep learning-based\\rprior work successfully demonstrates the modelling of the motion magnification\\rproblem with outstanding quality compared to conventional signal\\rprocessing-based ones. However, it still lags behind real-time performance,\\rwhich prevents it from being extended to various online applications. In this\\rpaper, we investigate an efficient deep learning-based motion magnification\\rmodel that runs in real time for full-HD resolution videos. Due to the\\rspecified network design of the prior art, i.e. inhomogeneous architecture, the\\rdirect application of existing neural architecture search methods is\\rcomplicated. Instead of automatic search, we carefully investigate the\\rarchitecture module by module for its role and importance in the motion\\rmagnification task. Two key findings are 1) Reducing the spatial resolution of\\rthe latent motion representation in the decoder provides a good trade-off\\rbetween computational efficiency and task quality, and 2) surprisingly, only a\\rsingle linear layer and a single branch in the encoder are sufficient for the\\rmotion magnification task. Based on these findings, we introduce a real-time\\rdeep learning-based motion magnification model with4.2X fewer FLOPs and is 2.7X\\rfaster than the prior art while maintaining comparable quality.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01898 ,  6801kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01901\\rDate: Mon, 4 Mar 2024 09:59:48 GMT   (2912kb,D)\\r\\rTitle: FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces\\r  from Disentangled Audio\\rAuthors: Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan,\\r  Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun\\rCategories: cs.CV\\r\\\\\\\\\\r  In this paper, we abstract the process of people hearing speech, extracting\\rmeaningful cues, and creating various dynamically audio-consistent talking\\rfaces, termed Listening and Imagining, into the task of high-fidelity diverse\\rtalking faces generation from a single audio. Specifically, it involves two\\rcritical challenges: one is to effectively decouple identity, content, and\\remotion from entangled audio, and the other is to maintain intra-video\\rdiversity and inter-video consistency. To tackle the issues, we first dig out\\rthe intricate relationships among facial factors and simplify the decoupling\\rprocess, tailoring a Progressive Audio Disentanglement for accurate facial\\rgeometry and semantics learning, where each stage incorporates a customized\\rtraining module responsible for a specific factor. Secondly, to achieve\\rvisually diverse and audio-synchronized animation solely from input audio\\rwithin a single model, we introduce the Controllable Coherent Frame generation,\\rwhich involves the flexible integration of three trainable adapters with frozen\\rLatent Diffusion Models (LDMs) to focus on maintaining facial geometry and\\rsemantics, as well as texture and temporal coherence between frames. In this\\rway, we inherit high-quality diverse generation from LDMs while significantly\\rimproving their controllability at a low training cost. Extensive experiments\\rdemonstrate the flexibility and effectiveness of our method in handling this\\rparadigm. The codes will be released at\\rhttps://github.com/modelscope/facechain.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01901 ,  2912kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01909\\rDate: Mon, 4 Mar 2024 10:18:38 GMT   (2459kb,D)\\r\\rTitle: Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey\\rAuthors: Lingyan Ran, Yali Li, Guoqiang Liang, and Yanning Zhang\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Semantic segmentation is an important and popular research area in computer\\rvision that focuses on classifying pixels in an image based on their semantics.\\rHowever, supervised deep learning requires large amounts of data to train\\rmodels and the process of labeling images pixel by pixel is time-consuming and\\rlaborious. This review aims to provide a first comprehensive and organized\\roverview of the state-of-the-art research results on pseudo-label methods in\\rthe field of semi-supervised semantic segmentation, which we categorize from\\rdifferent perspectives and present specific methods for specific application\\rareas. In addition, we explore the application of pseudo-label technology in\\rmedical and remote-sensing image segmentation. Finally, we also propose some\\rfeasible future research directions to address the existing challenges.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01909 ,  2459kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01915\\rDate: Mon, 4 Mar 2024 10:29:58 GMT   (2666kb,D)\\r\\rTitle: xT: Nested Tokenization for Larger Context in Large Images\\rAuthors: Ritwik Gupta, Shufan Li, Tyler Zhu, Jitendra Malik, Trevor Darrell,\\r  Karttikeya Mangalam\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Modern computer vision pipelines handle large images in one of two\\rsub-optimal ways: down-sampling or cropping. These two methods incur\\rsignificant losses in the amount of information and context present in an\\rimage. There are many downstream applications in which global context matters\\ras much as high frequency details, such as in real-world satellite imagery; in\\rsuch cases researchers have to make the uncomfortable choice of which\\rinformation to discard. We introduce xT, a simple framework for vision\\rtransformers which effectively aggregates global context with local details and\\rcan model large images end-to-end on contemporary GPUs. We select a set of\\rbenchmark datasets across classic vision tasks which accurately reflect a\\rvision model's ability to understand truly large images and incorporate fine\\rdetails over large scales and assess our method's improvement on them. By\\rintroducing a nested tokenization scheme for large images in conjunction with\\rlong-sequence length models normally used for natural language processing, we\\rare able to increase accuracy by up to 8.6% on challenging classification tasks\\rand $F_1$ score by 11.6 on context-dependent segmentation in large images.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01915 ,  2666kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01932\\rDate: Mon, 4 Mar 2024 11:02:17 GMT   (47288kb,D)\\r\\rTitle: Tree Counting by Bridging 3D Point Clouds with Imagery\\rAuthors: Lei Li, Tianfang Zhang, Zhongyu Jiang, Cheng-Yen Yang, Jenq-Neng\\r  Hwang, Stefan Oehmcke, Dimitri Pierre Johannes Gominski, Fabian Gieseke,\\r  Christian Igel\\rCategories: cs.CV\\rComments: Work in progress\\r\\\\\\\\\\r  Accurate and consistent methods for counting trees based on remote sensing\\rdata are needed to support sustainable forest management, assess climate change\\rmitigation strategies, and build trust in tree carbon credits. Two-dimensional\\rremote sensing imagery primarily shows overstory canopy, and it does not\\rfacilitate easy differentiation of individual trees in areas with a dense\\rcanopy and does not allow for easy separation of trees when the canopy is\\rdense. We leverage the fusion of three-dimensional LiDAR measurements and 2D\\rimagery to facilitate the accurate counting of trees. We compare a deep\\rlearning approach to counting trees in forests using 3D airborne LiDAR data and\\r2D imagery. The approach is compared with state-of-the-art algorithms, like\\roperating on 3D point cloud and 2D imagery. We empirically evaluate the\\rdifferent methods on the NeonTreeCount data set, which we use to define a\\rtree-counting benchmark. The experiments show that FuseCountNet yields more\\raccurate tree counts.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01932 ,  47288kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01944\\rDate: Mon, 4 Mar 2024 11:30:02 GMT   (4764kb,D)\\r\\rTitle: Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency\\r  Augmentation in Image Classification\\rAuthors: Puru Vaish, Shunxin Wang and Nicola Strisciuglio\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Computer vision models normally witness degraded performance when deployed in\\rreal-world scenarios, due to unexpected changes in inputs that were not\\raccounted for during training. Data augmentation is commonly used to address\\rthis issue, as it aims to increase data variety and reduce the distribution gap\\rbetween training and test data. However, common visual augmentations might not\\rguarantee extensive robustness of computer vision models. In this paper, we\\rpropose Auxiliary Fourier-basis Augmentation (AFA), a complementary technique\\rtargeting augmentation in the frequency domain and filling the augmentation gap\\rleft by visual augmentations. We demonstrate the utility of augmentation via\\rFourier-basis additive noise in a straightforward and efficient adversarial\\rsetting. Our results show that AFA benefits the robustness of models against\\rcommon corruptions, OOD generalization, and consistency of performance of\\rmodels against increasing perturbations, with negligible deficit to the\\rstandard performance of models. It can be seamlessly integrated with other\\raugmentation techniques to further boost performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01944 ,  4764kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01966\\rDate: Mon, 4 Mar 2024 12:10:24 GMT   (7511kb,D)\\r\\rTitle: Enhancing Information Maximization with Distance-Aware Contrastive\\r  Learning for Source-Free Cross-Domain Few-Shot Learning\\rAuthors: Huali Xu, Li Liu, Shuaifeng Zhi, Shaojing Fu, Zhuo Su, Ming-Ming\\r  Cheng, Yongxiang Liu\\rCategories: cs.CV\\rComments: Accepted by TIP, 16 pages, 11 figures, 8 tables\\r\\\\\\\\\\r  Existing Cross-Domain Few-Shot Learning (CDFSL) methods require access to\\rsource domain data to train a model in the pre-training phase. However, due to\\rincreasing concerns about data privacy and the desire to reduce data\\rtransmission and training costs, it is necessary to develop a CDFSL solution\\rwithout accessing source data. For this reason, this paper explores a\\rSource-Free CDFSL (SF-CDFSL) problem, in which CDFSL is addressed through the\\ruse of existing pretrained models instead of training a model with source data,\\ravoiding accessing source data. This paper proposes an Enhanced Information\\rMaximization with Distance-Aware Contrastive Learning (IM-DCL) method to\\raddress these challenges. Firstly, we introduce the transductive mechanism for\\rlearning the query set. Secondly, information maximization (IM) is explored to\\rmap target samples into both individual certainty and global diversity\\rpredictions, helping the source model better fit the target data distribution.\\rHowever, IM fails to learn the decision boundary of the target task. This\\rmotivates us to introduce a novel approach called Distance-Aware Contrastive\\rLearning (DCL), in which we consider the entire feature set as both positive\\rand negative sets, akin to Schrodinger's concept of a dual state. Instead of a\\rrigid separation between positive and negative sets, we employ a weighted\\rdistance calculation among features to establish a soft classification of the\\rpositive and negative sets for the entire feature set. Furthermore, we address\\rissues related to IM by incorporating contrastive constraints between object\\rfeatures and their corresponding positive and negative sets. Evaluations of the\\r4 datasets in the BSCD-FSL benchmark indicate that the proposed IM-DCL, without\\raccessing the source domain, demonstrates superiority over existing methods,\\respecially in the distant domain task.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01966 ,  7511kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01968\\rDate: Mon, 4 Mar 2024 12:11:07 GMT   (629kb,D)\\r\\rTitle: Explicit Motion Handling and Interactive Prompting for Video Camouflaged\\r  Object Detection\\rAuthors: Xin Zhang, Tao Xiao, Gepeng Ji, Xuan Wu, Keren Fu, Qijun Zhao\\rCategories: cs.CV\\rComments: 9 pages, 6 figures\\r\\\\\\\\\\r  Camouflage poses challenges in distinguishing a static target, whereas any\\rmovement of the target can break this disguise. Existing video camouflaged\\robject detection (VCOD) approaches take noisy motion estimation as input or\\rmodel motion implicitly, restricting detection performance in complex dynamic\\rscenes. In this paper, we propose a novel Explicit Motion handling and\\rInteractive Prompting framework for VCOD, dubbed EMIP, which handles motion\\rcues explicitly using a frozen pre-trained optical flow fundamental model. EMIP\\ris characterized by a two-stream architecture for simultaneously conducting\\rcamouflaged segmentation and optical flow estimation. Interactions across the\\rdual streams are realized in an interactive prompting way that is inspired by\\remerging visual prompt learning. Two learnable modules, i.e. the camouflaged\\rfeeder and motion collector, are designed to incorporate segmentation-to-motion\\rand motion-to-segmentation prompts, respectively, and enhance outputs of the\\rboth streams. The prompt fed to the motion stream is learned by supervising\\roptical flow in a self-supervised manner. Furthermore, we show that long-term\\rhistorical information can also be incorporated as a prompt into EMIP and\\rachieve more robust results with temporal consistency. Experimental results\\rdemonstrate that our EMIP achieves new state-of-the-art records on popular VCOD\\rbenchmarks. The code will be publicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01968 ,  629kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01978\\rDate: Mon, 4 Mar 2024 12:20:40 GMT   (3378kb,D)\\r\\rTitle: Leveraging Anchor-based LiDAR 3D Object Detection via Point Assisted\\r  Sample Selection\\rAuthors: Shitao Chen, Haolin Zhang, Nanning Zheng\\rCategories: cs.CV\\r\\\\\\\\\\r  3D object detection based on LiDAR point cloud and prior anchor boxes is a\\rcritical technology for autonomous driving environment perception and\\runderstanding. Nevertheless, an overlooked practical issue in existing methods\\ris the ambiguity in training sample allocation based on box Intersection over\\rUnion (IoU_box). This problem impedes further enhancements in the performance\\rof anchor-based LiDAR 3D object detectors. To tackle this challenge, this paper\\rintroduces a new training sample selection method that utilizes point cloud\\rdistribution for anchor sample quality measurement, named Point Assisted Sample\\rSelection (PASS). This method has undergone rigorous evaluation on two widely\\rutilized datasets. Experimental results demonstrate that the application of\\rPASS elevates the average precision of anchor-based LiDAR 3D object detectors\\rto a novel state-of-the-art, thereby proving the effectiveness of the proposed\\rapproach. The codes will be made available at\\rhttps://github.com/XJTU-Haolin/Point_Assisted_Sample_Selection.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01978 ,  3378kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01993\\rDate: Mon, 4 Mar 2024 12:37:52 GMT   (4198kb,D)\\r\\rTitle: Physics-Informed Learning for Time-Resolved Angiographic Contrast Agent\\r  Concentration Reconstruction\\rAuthors: Noah Maul, Annette Birkhold, Fabian Wagner, Mareike Thies, Maximilian\\r  Rohleder, Philipp Berg, Markus Kowarschik, Andreas Maier\\rCategories: cs.CV\\r\\\\\\\\\\r  Three-dimensional Digital Subtraction Angiography (3D-DSA) is a\\rwell-established X-ray-based technique for visualizing vascular anatomy.\\rRecently, four-dimensional DSA (4D-DSA) reconstruction algorithms have been\\rdeveloped to enable the visualization of volumetric contrast flow dynamics\\rthrough time-series of volumes. . This reconstruction problem is ill-posed\\rmainly due to vessel overlap in the projection direction and geometric vessel\\rforeshortening, which leads to information loss in the recorded projection\\rimages. However, knowledge about the underlying fluid dynamics can be leveraged\\rto constrain the solution space. In our work, we implicitly include this\\rinformation in a neural network-based model that is trained on a dataset of\\rimage-based blood flow simulations. The model predicts the spatially averaged\\rcontrast agent concentration for each centerline point of the vasculature over\\rtime, lowering the overall computational demand. The trained network enables\\rthe reconstruction of relative contrast agent concentrations with a mean\\rabsolute error of 0.02 $\\\\pm$ 0.02 and a mean absolute percentage error of 5.31\\r% $\\\\pm$ 9.25 %. Moreover, the network is robust to varying degrees of vessel\\roverlap and vessel foreshortening. Our approach demonstrates the potential of\\rthe integration of machine learning and blood flow simulations in time-resolved\\rangiographic flow reconstruction.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01993 ,  4198kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02037\\rDate: Mon, 4 Mar 2024 13:42:54 GMT   (28089kb,D)\\r\\rTitle: Scalable Vision-Based 3D Object Detection and Monocular Depth Estimation\\r  for Autonomous Driving\\rAuthors: Yuxuan Liu\\rCategories: cs.CV cs.RO\\rComments: HKUST PhD Thesis; https://github.com/Owen-Liuyuxuan/visionfactory\\r\\\\\\\\\\r  This dissertation is a multifaceted contribution to the advancement of\\rvision-based 3D perception technologies. In the first segment, the thesis\\rintroduces structural enhancements to both monocular and stereo 3D object\\rdetection algorithms. By integrating ground-referenced geometric priors into\\rmonocular detection models, this research achieves unparalleled accuracy in\\rbenchmark evaluations for monocular 3D detection. Concurrently, the work\\rrefines stereo 3D detection paradigms by incorporating insights and inferential\\rstructures gleaned from monocular networks, thereby augmenting the operational\\refficiency of stereo detection systems. The second segment is devoted to\\rdata-driven strategies and their real-world applications in 3D vision\\rdetection. A novel training regimen is introduced that amalgamates datasets\\rannotated with either 2D or 3D labels. This approach not only augments the\\rdetection models through the utilization of a substantially expanded dataset\\rbut also facilitates economical model deployment in real-world scenarios where\\ronly 2D annotations are readily available. Lastly, the dissertation presents an\\rinnovative pipeline tailored for unsupervised depth estimation in autonomous\\rdriving contexts. Extensive empirical analyses affirm the robustness and\\refficacy of this newly proposed pipeline. Collectively, these contributions lay\\ra robust foundation for the widespread adoption of vision-based 3D perception\\rtechnologies in autonomous driving applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02037 ,  28089kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02041\\rDate: Mon, 4 Mar 2024 13:47:30 GMT   (1343kb,D)\\r\\rTitle: A Generative Approach for Wikipedia-Scale Visual Entity Recognition\\rAuthors: Mathilde Caron, Ahmet Iscen, Alireza Fathi, Cordelia Schmid\\rCategories: cs.CV\\rComments: CVPR2024\\r\\\\\\\\\\r  In this paper, we address web-scale visual entity recognition, specifically\\rthe task of mapping a given query image to one of the 6 million existing\\rentities in Wikipedia. One way of approaching a problem of such scale is using\\rdual-encoder models (eg CLIP), where all the entity names and query images are\\rembedded into a unified space, paving the way for an approximate k-NN search.\\rAlternatively, it is also possible to re-purpose a captioning model to directly\\rgenerate the entity names for a given image. In contrast, we introduce a novel\\rGenerative Entity Recognition (GER) framework, which given an input image\\rlearns to auto-regressively decode a semantic and discriminative ``code''\\ridentifying the target entity. Our experiments demonstrate the efficacy of this\\rGER paradigm, showcasing state-of-the-art performance on the challenging OVEN\\rbenchmark. GER surpasses strong captioning, dual-encoder, visual matching and\\rhierarchical classification baselines, affirming its advantage in tackling the\\rcomplexities of web-scale recognition.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02041 ,  1343kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02059\\rDate: Mon, 4 Mar 2024 14:00:45 GMT   (1016kb,D)\\r\\rTitle: Multi-Spectral Remote Sensing Image Retrieval Using Geospatial\\r  Foundation Models\\rAuthors: Benedikt Blumenstiel, Viktoria Moor, Romeo Kienzler, Thomas\\r  Brunschwiler\\rCategories: cs.CV\\r\\\\\\\\\\r  Image retrieval enables an efficient search through vast amounts of satellite\\rimagery and returns similar images to a query. Deep learning models can\\ridentify images across various semantic concepts without the need for\\rannotations. This work proposes to use Geospatial Foundation Models, like\\rPrithvi, for remote sensing image retrieval with multiple benefits: i) the\\rmodels encode multi-spectral satellite data and ii) generalize without further\\rfine-tuning. We introduce two datasets to the retrieval task and observe a\\rstrong performance: Prithvi processes six bands and achieves a mean Average\\rPrecision of 97.62\\\\% on BigEarthNet-43 and 44.51\\\\% on ForestNet-12,\\routperforming other RGB-based models. Further, we evaluate three compression\\rmethods with binarized embeddings balancing retrieval speed and accuracy. They\\rmatch the retrieval speed of much shorter hash codes while maintaining the same\\raccuracy as floating-point embeddings but with a 32-fold compression. The code\\ris available at https://github.com/IBM/remote-sensing-image-retrieval.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02059 ,  1016kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02063\\rDate: Mon, 4 Mar 2024 14:06:51 GMT   (9972kb,D)\\r\\rTitle: Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input\\r  Views\\rAuthors: Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song\\rCategories: cs.CV\\r\\\\\\\\\\r  Novel-view synthesis with sparse input views is important for real-world\\rapplications like AR/VR and autonomous driving. Recent methods have integrated\\rdepth information into NeRFs for sparse input synthesis, leveraging depth prior\\rfor geometric and spatial understanding. However, most existing works tend to\\roverlook inaccuracies within depth maps and have low time efficiency. To\\raddress these issues, we propose a depth-guided robust and fast point cloud\\rfusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel\\rgrid of features. A point cloud is constructed for each input view,\\rcharacterized within the voxel grid using matrices and vectors. We accumulate\\rthe point cloud of each input view to construct the fused point cloud of the\\rentire scene. Each voxel determines its density and appearance by referring to\\rthe point cloud of the entire scene. Through point cloud fusion and voxel grid\\rfine-tuning, inaccuracies in depth values are refined or substituted by those\\rfrom other views. Moreover, our method can achieve faster reconstruction and\\rgreater compactness through effective vector-matrix decomposition. Experimental\\rresults underline the superior performance and time efficiency of our approach\\rcompared to state-of-the-art baselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02063 ,  9972kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02069\\rDate: Mon, 4 Mar 2024 14:17:30 GMT   (4720kb)\\r\\rTitle: HyperPredict: Estimating Hyperparameter Effects for Instance-Specific\\r  Regularization in Deformable Image Registration\\rAuthors: Aisha L. Shuaibu, Ivor J. A. Simpson\\rCategories: cs.CV\\r\\\\\\\\\\r  Methods for medical image registration infer geometric transformations that\\ralign pairs/groups of images by maximising an image similarity metric. This\\rproblem is ill-posed as several solutions may have equivalent likelihoods, also\\roptimising purely for image similarity can yield implausible transformations.\\rFor these reasons regularization terms are essential to obtain meaningful\\rregistration results. However, this requires the introduction of at least one\\rhyperparameter often termed {\\\\lambda}, that serves as a tradeoff between loss\\rterms. In some situations, the quality of the estimated transformation greatly\\rdepends on hyperparameter choice, and different choices may be required\\rdepending on the characteristics of the data. Analyzing the effect of these\\rhyperparameters requires labelled data, which is not commonly available at\\rtest-time. In this paper, we propose a method for evaluating the influence of\\rhyperparameters and subsequently selecting an optimal value for given image\\rpairs. Our approach which we call HyperPredict, implements a Multi-Layer\\rPerceptron that learns the effect of selecting particular hyperparameters for\\rregistering an image pair by predicting the resulting segmentation overlap and\\rmeasure of deformation smoothness. This approach enables us to select optimal\\rhyperparameters at test time without requiring labelled data, removing the need\\rfor a one-size-fits-all cross-validation approach. Furthermore, the criteria\\rused to define optimal hyperparameter is flexible post-training, allowing us to\\refficiently choose specific properties. We evaluate our proposed method on the\\rOASIS brain MR dataset using a recent deep learning approach(cLapIRN) and an\\ralgorithmic method(Niftyreg). Our results demonstrate good performance in\\rpredicting the effects of regularization hyperparameters and highlight the\\rbenefits of our image-pair specific approach to hyperparameter selection.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02069 ,  4720kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02074\\rDate: Mon, 4 Mar 2024 14:21:51 GMT   (1743kb,D)\\r\\rTitle: Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation\\rAuthors: Zhongzhen Huang, Linda Wei, Shaoting Zhang, Xiaofan Zhang\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Combining images from multi-modalities is beneficial to explore various\\rinformation in computer vision, especially in the medical domain. As an\\ressential part of clinical diagnosis, multi-modal brain tumor segmentation aims\\rto delineate the malignant entity involving multiple modalities. Although\\rexisting methods have shown remarkable performance in the task, the information\\rexchange for cross-scale and high-level representations fusion in spatial and\\rmodality are limited in these methods. In this paper, we present a novel\\rModality Aware and Shift Mixer that integrates intra-modality and\\rinter-modality dependencies of multi-modal images for effective and robust\\rbrain tumor segmentation. Specifically, we introduce a Modality-Aware module\\raccording to neuroimaging studies for modeling the specific modality pair\\rrelationships at low levels, and a Modality-Shift module with specific mosaic\\rpatterns is developed to explore the complex relationships across modalities at\\rhigh levels via the self-attention. Experimentally, we outperform previous\\rstate-of-the-art approaches on the public Brain Tumor Segmentation (BraTS 2021\\rsegmentation) dataset. Further qualitative experiments demonstrate the efficacy\\rand robustness of MASM.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02074 ,  1743kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02075\\rDate: Mon, 4 Mar 2024 14:21:51 GMT   (17133kb,D)\\r\\rTitle: DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with\\r  Non-linear Prediction\\rAuthors: Weiyi Lv and Yuhang Huang and Ning Zhang and Ruei-Sung Lin and Mei Han\\r  and Dan Zeng\\rCategories: cs.CV\\r\\\\\\\\\\r  In Multiple Object Tracking, objects often exhibit non-linear motion of\\racceleration and deceleration, with irregular direction changes.\\rTacking-by-detection (TBD) with Kalman Filter motion prediction works well in\\rpedestrian-dominant scenarios but falls short in complex situations when\\rmultiple objects perform non-linear and diverse motion simultaneously. To\\rtackle the complex non-linear motion, we propose a real-time diffusion-based\\rMOT approach named DiffMOT. Specifically, for the motion predictor component,\\rwe propose a novel Decoupled Diffusion-based Motion Predictor (D MP). It models\\rthe entire distribution of various motion presented by the data as a whole. It\\ralso predicts an individual object's motion conditioning on an individual's\\rhistorical motion information. Furthermore, it optimizes the diffusion process\\rwith much less sampling steps. As a MOT tracker, the DiffMOT is real-time at\\r22.7FPS, and also outperforms the state-of-the-art on DanceTrack and SportsMOT\\rdatasets with 63.4 and 76.2 in HOTA metrics, respectively. To the best of our\\rknowledge, DiffMOT is the first to introduce a diffusion probabilistic model\\rinto the MOT to tackle non-linear motion prediction.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02075 ,  17133kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02076\\rDate: Mon, 4 Mar 2024 14:22:02 GMT   (4513kb,D)\\r\\rTitle: VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT\\rAuthors: Yifang Xu, Yunzhuo Sun, Zien Xie, Benxiang Zhai, and Sidan Du\\rCategories: cs.CV cs.AI\\rComments: 15 pages, 7 figures\\rDOI: 10.3390/app14051894\\r\\\\\\\\\\r  Video temporal grounding (VTG) aims to locate specific temporal segments from\\ran untrimmed video based on a linguistic query. Most existing VTG models are\\rtrained on extensive annotated video-text pairs, a process that not only\\rintroduces human biases from the queries but also incurs significant\\rcomputational costs. To tackle these challenges, we propose VTG-GPT, a\\rGPT-based method for zero-shot VTG without training or fine-tuning. To reduce\\rprejudice in the original query, we employ Baichuan2 to generate debiased\\rqueries. To lessen redundant information in videos, we apply MiniGPT-v2 to\\rtransform visual content into more precise captions. Finally, we devise the\\rproposal generator and post-processing to produce accurate segments from\\rdebiased queries and image captions. Extensive experiments demonstrate that\\rVTG-GPT significantly outperforms SOTA methods in zero-shot settings and\\rsurpasses unsupervised approaches. More notably, it achieves competitive\\rperformance comparable to supervised methods. The code is available on\\rhttps://github.com/YoucanBaby/VTG-GPT\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02076 ,  4513kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02084\\rDate: Mon, 4 Mar 2024 14:36:56 GMT   (40703kb,D)\\r\\rTitle: ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models\\rAuthors: Jiaxiang Cheng, Pan Xie, Xin Xia, Jiashi Li, Jie Wu, Yuxi Ren, Huixia\\r  Li, Xuefeng Xiao, Min Zheng, Lean Fu\\rCategories: cs.CV\\rComments: 21 pages, 16 figures\\r\\\\\\\\\\r  Recent advancement in text-to-image models (e.g., Stable Diffusion) and\\rcorresponding personalized technologies (e.g., DreamBooth and LoRA) enables\\rindividuals to generate high-quality and imaginative images. However, they\\roften suffer from limitations when generating images with resolutions outside\\rof their trained domain. To overcome this limitation, we present the Resolution\\rAdapter (ResAdapter), a domain-consistent adapter designed for diffusion models\\rto generate images with unrestricted resolutions and aspect ratios. Unlike\\rother multi-resolution generation methods that process images of static\\rresolution with complex post-process operations, ResAdapter directly generates\\rimages with the dynamical resolution. Especially, after learning a deep\\runderstanding of pure resolution priors, ResAdapter trained on the general\\rdataset, generates resolution-free images with personalized diffusion models\\rwhile preserving their original style domain. Comprehensive experiments\\rdemonstrate that ResAdapter with only 0.5M can process images with flexible\\rresolutions for arbitrary diffusion models. More extended experiments\\rdemonstrate that ResAdapter is compatible with other modules (e.g., ControlNet,\\rIP-Adapter and LCM-LoRA) for image generation across a broad range of\\rresolutions, and can be integrated into other multi-resolution model (e.g.,\\rElasticDiffusion) for efficiently generating higher-resolution images. Project\\rlink is https://res-adapter.github.io\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02084 ,  40703kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02090\\rDate: Mon, 4 Mar 2024 14:46:58 GMT   (1352kb,D)\\r\\rTitle: Modeling Multimodal Social Interactions: New Challenges and Baselines\\r  with Densely Aligned Representations\\rAuthors: Sangmin Lee, Bolin Lai, Fiona Ryan, Bikram Boote, James M. Rehg\\rCategories: cs.CV cs.CL cs.LG\\rComments: CVPR 2024\\r\\\\\\\\\\r  Understanding social interactions involving both verbal and non-verbal cues\\ris essential to effectively interpret social situations. However, most prior\\rworks on multimodal social cues focus predominantly on single-person behaviors\\ror rely on holistic visual representations that are not densely aligned to\\rutterances in multi-party environments. They are limited in modeling the\\rintricate dynamics of multi-party interactions. In this paper, we introduce\\rthree new challenging tasks to model the fine-grained dynamics between multiple\\rpeople: speaking target identification, pronoun coreference resolution, and\\rmentioned player prediction. We contribute extensive data annotations to curate\\rthese new challenges in social deduction game settings. Furthermore, we propose\\ra novel multimodal baseline that leverages densely aligned language-visual\\rrepresentations by synchronizing visual features with their corresponding\\rutterances. This facilitates concurrently capturing verbal and non-verbal cues\\rpertinent to social reasoning. Experiments demonstrate the effectiveness of the\\rproposed approach with densely aligned multimodal representations in modeling\\rsocial interactions. We will release our benchmarks and source code to\\rfacilitate further research.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02090 ,  1352kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02112\\rDate: Mon, 4 Mar 2024 15:15:57 GMT   (1199kb,D)\\r\\rTitle: A New Perspective on Smiling and Laughter Detection: Intensity Levels\\r  Matter\\rAuthors: Hugo Bohy, Kevin El Haddad and Thierry Dutoit\\rCategories: cs.CV\\rJournal-ref: In 2022 10th International Conference on Affective Computing and\\r  Intelligent Interaction (ACII) (pp. 1-8). IEEE\\rDOI: 10.1109/ACII55700.2022.9953896\\r\\\\\\\\\\r  Smiles and laughs detection systems have attracted a lot of attention in the\\rpast decade contributing to the improvement of human-agent interaction systems.\\rBut very few considered these expressions as distinct, although no prior work\\rclearly proves them to belong to the same category or not. In this work, we\\rpresent a deep learning-based multimodal smile and laugh classification system,\\rconsidering them as two different entities. We compare the use of audio and\\rvision-based models as well as a fusion approach. We show that, as expected,\\rthe fusion leads to a better generalization on unseen data. We also present an\\rin-depth analysis of the behavior of these models on the smiles and laughs\\rintensity levels. The analyses on the intensity levels show that the\\rrelationship between smiles and laughs might not be as simple as a binary one\\ror even grouping them in a single category, and so, a more complex approach\\rshould be taken when dealing with them. We also tackle the problem of limited\\rresources by showing that transfer learning allows the models to improve the\\rdetection of confusing intensity levels.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02112 ,  1199kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02127\\rDate: Mon, 4 Mar 2024 15:34:12 GMT   (16415kb,D)\\r\\rTitle: LOCR: Location-Guided Transformer for Optical Character Recognition\\rAuthors: Yu Sun, Dongzhan Zhou, Chen Lin, Conghui He, Wanli Ouyang, Han-Sen\\r  Zhong\\rCategories: cs.CV cs.AI cs.CL\\r\\\\\\\\\\r  Academic documents are packed with texts, equations, tables, and figures,\\rrequiring comprehensive understanding for accurate Optical Character\\rRecognition (OCR). While end-to-end OCR methods offer improved accuracy over\\rlayout-based approaches, they often grapple with significant repetition issues,\\respecially with complex layouts in Out-Of-Domain (OOD) documents.To tackle this\\rissue, we propose LOCR, a model that integrates location guiding into the\\rtransformer architecture during autoregression. We train the model on a dataset\\rcomprising over 77M text-location pairs from 125K academic document pages,\\rincluding bounding boxes for words, tables and mathematical symbols. LOCR\\radeptly handles various formatting elements and generates content in Markdown\\rlanguage. It outperforms all existing methods in our test set constructed from\\rarXiv, as measured by edit distance, BLEU, METEOR and F-measure.LOCR also\\rreduces repetition frequency from 4.4% of pages to 0.5% in the arXiv dataset,\\rfrom 13.2% to 1.3% in OOD quantum physics documents and from 8.1% to 1.8% in\\rOOD marketing documents. Additionally, LOCR features an interactive OCR mode,\\rfacilitating the generation of complex documents through a few location prompts\\rfrom human.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02127 ,  16415kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02132\\rDate: Mon, 4 Mar 2024 15:40:31 GMT   (15245kb,D)\\r\\rTitle: UB-FineNet: Urban Building Fine-grained Classification Network for\\r  Open-access Satellite Images\\rAuthors: Zhiyi He, Wei Yao, Jie Shao, Puzuo Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  Fine classification of city-scale buildings from satellite remote sensing\\rimagery is a crucial research area with significant implications for urban\\rplanning, infrastructure development, and population distribution analysis.\\rHowever, the task faces big challenges due to low-resolution overhead images\\racquired from high altitude space-borne platforms and the long-tail sample\\rdistribution of fine-grained urban building categories, leading to severe class\\rimbalance problem. To address these issues, we propose a deep network approach\\rto fine-grained classification of urban buildings using open-access satellite\\rimages. A Denoising Diffusion Probabilistic Model (DDPM) based super-resolution\\rmethod is first introduced to enhance the spatial resolution of satellite\\rimages, which benefits from domain-adaptive knowledge distillation. Then, a new\\rfine-grained classification network with Category Information Balancing Module\\r(CIBM) and Contrastive Supervision (CS) technique is proposed to mitigate the\\rproblem of class imbalance and improve the classification robustness and\\raccuracy. Experiments on Hong Kong data set with 11 fine building types\\rrevealed promising classification results with a mean Top-1 accuracy of\\r60.45\\\\%, which is on par with street-view image based approaches. Extensive\\rablation study shows that CIBM and CS improve Top-1 accuracy by 2.6\\\\% and 3.5\\\\%\\rcompared to the baseline method, respectively. And both modules can be easily\\rinserted into other classification networks and similar enhancements have been\\rachieved. Our research contributes to the field of urban analysis by providing\\ra practical solution for fine classification of buildings in challenging mega\\rcity scenarios solely using open-access satellite images. The proposed method\\rcan serve as a valuable tool for urban planners, aiding in the understanding of\\reconomic, industrial, and population distribution.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02132 ,  15245kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02136\\rDate: Mon, 4 Mar 2024 15:46:50 GMT   (34093kb,D)\\r\\rTitle: Point2Building: Reconstructing Buildings from Airborne LiDAR Point\\r  Clouds\\rAuthors: Yujia Liu, Anton Obukhov, Jan Dirk Wegner, Konrad Schindler\\rCategories: cs.CV\\r\\\\\\\\\\r  We present a learning-based approach to reconstruct buildings as 3D polygonal\\rmeshes from airborne LiDAR point clouds. What makes 3D building reconstruction\\rfrom airborne LiDAR hard is the large diversity of building designs and\\respecially roof shapes, the low and varying point density across the scene, and\\rthe often incomplete coverage of building facades due to occlusions by\\rvegetation or to the viewing angle of the sensor. To cope with the diversity of\\rshapes and inhomogeneous and incomplete object coverage, we introduce a\\rgenerative model that directly predicts 3D polygonal meshes from input point\\rclouds. Our autoregressive model, called Point2Building, iteratively builds up\\rthe mesh by generating sequences of vertices and faces. This approach enables\\rour model to adapt flexibly to diverse geometries and building structures.\\rUnlike many existing methods that rely heavily on pre-processing steps like\\rexhaustive plane detection, our model learns directly from the point cloud\\rdata, thereby reducing error propagation and increasing the fidelity of the\\rreconstruction. We experimentally validate our method on a collection of\\rairborne LiDAR data of Zurich, Berlin and Tallinn. Our method shows good\\rgeneralization to diverse urban styles.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02136 ,  34093kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02138\\rDate: Mon, 4 Mar 2024 15:48:56 GMT   (121kb,D)\\r\\rTitle: Self-Supervised Facial Representation Learning with Facial Region\\r  Awareness\\rAuthors: Zheng Gao, Ioannis Patras\\rCategories: cs.CV\\r\\\\\\\\\\r  Self-supervised pre-training has been proved to be effective in learning\\rtransferable representations that benefit various visual tasks. This paper asks\\rthis question: can self-supervised pre-training learn general facial\\rrepresentations for various facial analysis tasks? Recent efforts toward this\\rgoal are limited to treating each face image as a whole, i.e., learning\\rconsistent facial representations at the image-level, which overlooks the\\rconsistency of local facial representations (i.e., facial regions like eyes,\\rnose, etc). In this work, we make a first attempt to propose a novel\\rself-supervised facial representation learning framework to learn consistent\\rglobal and local facial representations, Facial Region Awareness (FRA).\\rSpecifically, we explicitly enforce the consistency of facial regions by\\rmatching the local facial representations across views, which are extracted\\rwith learned heatmaps highlighting the facial regions. Inspired by the mask\\rprediction in supervised semantic segmentation, we obtain the heatmaps via\\rcosine similarity between the per-pixel projection of feature maps and facial\\rmask embeddings computed from learnable positional embeddings, which leverage\\rthe attention mechanism to globally look up the facial image for facial\\rregions. To learn such heatmaps, we formulate the learning of facial mask\\rembeddings as a deep clustering problem by assigning the pixel features from\\rthe feature maps to them. The transfer learning results on facial\\rclassification and regression tasks show that our FRA outperforms previous\\rpre-trained models and more importantly, using ResNet as the unified backbone\\rfor various tasks, our FRA achieves comparable or even better performance\\rcompared with SOTA methods in facial analysis tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02138 ,  121kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02148\\rDate: Mon, 4 Mar 2024 15:57:29 GMT   (3471kb,D)\\r\\rTitle: MiM-ISTD: Mamba-in-Mamba for Efficient Infrared Small Target Detection\\rAuthors: Tianxiang Chen, Zhentao Tan, Tao Gong, Qi Chu, Yue Wu, Bin Liu,\\r  Jieping Ye, Nenghai Yu\\rCategories: cs.CV\\rComments: The first Mamba-based model for infrared small target detection\\r\\\\\\\\\\r  Thanks to the development of basic models, infrared small target detection\\r(ISTD) algorithms have made significant progress. Specifically, the structures\\rcombining convolutional networks with transformers can well extract both local\\rand global features. At the same time, they also inherit defects from the basic\\rmodel, e.g., the quadratic computational complexity of transformers, which\\rimpacts efficiency. Inspired by a recent basic model with linear complexity for\\rlong-distance modeling, called Mamba, we explore the potential of this state\\rspace model in ISTD in this paper. However, direct application is unsuitable\\rsince local features, which are critical to detecting small targets, cannot be\\rfully exploited. Instead, we tailor a Mamba-in-Mamba (MiM-ISTD) structure for\\refficient ISTD. For example, we treat the local patches as visual sentences\\rand further decompose them into sub-patches as visual words to further\\rexplore the locality. The interactions among each word in a given visual\\rsentence will be calculated with negligible computational costs. By aggregating\\rthe word and sentence features, the representation ability of MiM-ISTD can be\\rsignificantly bolstered. Experiments on NUAA-SIRST and IRSTD-1k prove the\\rsuperior accuracy and efficiency of our method. Specifically, MiM-ISTD is $10\\r\\\\times$ faster than the SOTA and reduces GPU memory usage by 73.4$\\\\%$ per $2048\\r\\\\times 2048$ image during inference, overcoming the computation$\\\\&$memory\\rconstraints on performing Mamba-based understanding on high-resolution infrared\\rimages.Source code is available at https://github.com/txchen-USTC/MiM-ISTD.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02148 ,  3471kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02151\\rDate: Mon, 4 Mar 2024 16:00:56 GMT   (1818kb,D)\\r\\rTitle: TripoSR: Fast 3D Object Reconstruction from a Single Image\\rAuthors: Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam\\r  Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, Yan-Pei\\r  Cao\\rCategories: cs.CV\\rComments: Model: https://huggingface.co/stabilityai/TripoSR Code:\\r  https://github.com/VAST-AI-Research/TripoSR Demo:\\r  https://huggingface.co/spaces/stabilityai/TripoSR\\r\\\\\\\\\\r  This technical report introduces TripoSR, a 3D reconstruction model\\rleveraging transformer architecture for fast feed-forward 3D generation,\\rproducing 3D mesh from a single image in under 0.5 seconds. Building upon the\\rLRM network architecture, TripoSR integrates substantial improvements in data\\rprocessing, model design, and training techniques. Evaluations on public\\rdatasets show that TripoSR exhibits superior performance, both quantitatively\\rand qualitatively, compared to other open-source alternatives. Released under\\rthe MIT license, TripoSR is intended to empower researchers, developers, and\\rcreatives with the latest advancements in 3D generative AI.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02151 ,  1818kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02211\\rDate: Mon, 4 Mar 2024 16:59:43 GMT   (3124kb,D)\\r\\rTitle: Perceptive self-supervised learning network for noisy image watermark\\r  removal\\rAuthors: Chunwei Tian, Menghua Zheng, Bo Li, Yanning Zhang, Shichao Zhang,\\r  David Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  Popular methods usually use a degradation model in a supervised way to learn\\ra watermark removal model. However, it is true that reference images are\\rdifficult to obtain in the real world, as well as collected images by cameras\\rsuffer from noise. To overcome these drawbacks, we propose a perceptive\\rself-supervised learning network for noisy image watermark removal (PSLNet) in\\rthis paper. PSLNet depends on a parallel network to remove noise and\\rwatermarks. The upper network uses task decomposition ideas to remove noise and\\rwatermarks in sequence. The lower network utilizes the degradation model idea\\rto simultaneously remove noise and watermarks. Specifically, mentioned paired\\rwatermark images are obtained in a self supervised way, and paired noisy images\\r(i.e., noisy and reference images) are obtained in a supervised way. To enhance\\rthe clarity of obtained images, interacting two sub-networks and fusing\\robtained clean images are used to improve the effects of image watermark\\rremoval in terms of structural information and pixel enhancement. Taking into\\rtexture information account, a mixed loss uses obtained images and features to\\rachieve a robust model of noisy image watermark removal. Comprehensive\\rexperiments show that our proposed method is very effective in comparison with\\rpopular convolutional neural networks (CNNs) for noisy image watermark removal.\\rCodes can be obtained at https://github.com/hellloxiaotian/PSLNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02211 ,  3124kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02217\\rDate: Mon, 4 Mar 2024 17:05:01 GMT   (4651kb,D)\\r\\rTitle: DragTex: Generative Point-Based Texture Editing on 3D Mesh\\rAuthors: Yudi Zhang, Qi Xu, Lei Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  Creating 3D textured meshes using generative artificial intelligence has\\rgarnered significant attention recently. While existing methods support\\rtext-based generative texture generation or editing on 3D meshes, they often\\rstruggle to precisely control pixels of texture images through more intuitive\\rinteraction. While 2D images can be edited generatively using drag interaction,\\rapplying this type of methods directly to 3D mesh textures still leads to\\rissues such as the lack of local consistency among multiple views, error\\raccumulation and long training times. To address these challenges, we propose a\\rgenerative point-based 3D mesh texture editing method called DragTex. This\\rmethod utilizes a diffusion model to blend locally inconsistent textures in the\\rregion near the deformed silhouette between different views, enabling locally\\rconsistent texture editing. Besides, we fine-tune a decoder to reduce\\rreconstruction errors in the non-drag region, thereby mitigating overall error\\raccumulation. Moreover, we train LoRA using multi-view images instead of\\rtraining each view individually, which significantly shortens the training\\rtime. The experimental results show that our method effectively achieves\\rdragging textures on 3D meshes and generates plausible textures that align with\\rthe desired intent of drag interaction.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02217 ,  4651kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02234\\rDate: Mon, 4 Mar 2024 17:26:28 GMT   (6814kb,D)\\r\\rTitle: 3DTopia: Large Text-to-3D Generation Model with Hybrid Diffusion Priors\\rAuthors: Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi\\r  Chen, Tengfei Wang, Liang Pan, Dahua Lin, Ziwei Liu\\rCategories: cs.CV\\rComments: Code available at https://github.com/3DTopia/3DTopia\\r\\\\\\\\\\r  We present a two-stage text-to-3D generation system, namely 3DTopia, which\\rgenerates high-quality general 3D assets within 5 minutes using hybrid\\rdiffusion priors. The first stage samples from a 3D diffusion prior directly\\rlearned from 3D data. Specifically, it is powered by a text-conditioned\\rtri-plane latent diffusion model, which quickly generates coarse 3D samples for\\rfast prototyping. The second stage utilizes 2D diffusion priors to further\\rrefine the texture of coarse 3D models from the first stage. The refinement\\rconsists of both latent and pixel space optimization for high-quality texture\\rgeneration. To facilitate the training of the proposed system, we clean and\\rcaption the largest open-source 3D dataset, Objaverse, by combining the power\\rof vision language models and large language models. Experiment results are\\rreported qualitatively and quantitatively to show the performance of the\\rproposed system. Our codes and models are available at\\rhttps://github.com/3DTopia/3DTopia\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02234 ,  6814kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02249\\rDate: Mon, 4 Mar 2024 17:34:59 GMT   (12043kb,D)\\r\\rTitle: Non-autoregressive Sequence-to-Sequence Vision-Language Models\\rAuthors: Kunyu Shi, Qi Dong, Luis Goncalves, Zhuowen Tu, Stefano Soatto\\rCategories: cs.CV cs.AI\\rComments: Accepted to CVPR 2024\\r\\\\\\\\\\r  Sequence-to-sequence vision-language models are showing promise, but their\\rapplicability is limited by their inference latency due to their autoregressive\\rway of generating predictions. We propose a parallel decoding\\rsequence-to-sequence vision-language model, trained with a Query-CTC loss, that\\rmarginalizes over multiple inference paths in the decoder. This allows us to\\rmodel the joint distribution of tokens, rather than restricting to conditional\\rdistribution as in an autoregressive model. The resulting model, NARVL,\\rachieves performance on-par with its state-of-the-art autoregressive\\rcounterpart, but is faster at inference time, reducing from the linear\\rcomplexity associated with the sequential generation of tokens to a paradigm of\\rconstant time joint inference.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02249 ,  12043kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02265\\rDate: Mon, 4 Mar 2024 17:54:33 GMT   (28161kb,D)\\r\\rTitle: DaReNeRF: Direction-aware Representation for Dynamic Scenes\\rAuthors: Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao\\r  Ding, Terrence Chen, Jack Noble, Ziyan Wu\\rCategories: cs.CV cs.GR\\rComments: Accepted at CVPR 2024. Paper + supplementary material\\r\\\\\\\\\\r  Addressing the intricate challenge of modeling and re-rendering dynamic\\rscenes, most recent approaches have sought to simplify these complexities using\\rplane-based explicit representations, overcoming the slow training time issues\\rassociated with methods like Neural Radiance Fields (NeRF) and implicit\\rrepresentations. However, the straightforward decomposition of 4D dynamic\\rscenes into multiple 2D plane-based representations proves insufficient for\\rre-rendering high-fidelity scenes with complex motions. In response, we present\\ra novel direction-aware representation (DaRe) approach that captures scene\\rdynamics from six different directions. This learned representation undergoes\\ran inverse dual-tree complex wavelet transformation (DTCWT) to recover\\rplane-based information. DaReNeRF computes features for each space-time point\\rby fusing vectors from these recovered planes. Combining DaReNeRF with a tiny\\rMLP for color regression and leveraging volume rendering in training yield\\rstate-of-the-art performance in novel view synthesis for complex dynamic\\rscenes. Notably, to address redundancy introduced by the six real and six\\rimaginary direction-aware wavelet coefficients, we introduce a trainable\\rmasking approach, mitigating storage issues without significant performance\\rdecline. Moreover, DaReNeRF maintains a 2x reduction in training time compared\\rto prior art while delivering superior performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02265 ,  28161kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02302\\rDate: Mon, 4 Mar 2024 18:32:12 GMT   (17793kb,D)\\r\\rTitle: Beyond Specialization: Assessing the Capabilities of MLLMs in Age and\\r  Gender Estimation\\rAuthors: Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh\\rCategories: cs.CV cs.AI cs.LG\\rACM-class: I.2.0; I.4.0; I.4.9\\r\\\\\\\\\\r  Multimodal Large Language Models (MLLMs) have recently gained immense\\rpopularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as\\ropen-source ones such as LLaVA, are essentially general-purpose models and are\\rapplied to solve a wide variety of tasks, including those in computer vision.\\rThese neural networks possess such strong general knowledge and reasoning\\rabilities that they have proven capable of working even on tasks for which they\\rwere not specifically trained. We compared the capabilities of the most\\rpowerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task\\rof age and gender estimation with our state-of-the-art specialized model,\\rMiVOLO. We also updated MiVOLO and provide details and new metrics in this\\rarticle. This comparison has yielded some interesting results and insights\\rabout the strengths and weaknesses of the participating models. Furthermore, we\\rattempted various ways to fine-tune the ShareGPT4V model for this specific\\rtask, aiming to achieve state-of-the-art results in this particular challenge.\\rAlthough such a model would not be practical in production, as it is incredibly\\rexpensive compared to a specialized model like MiVOLO, it could be very useful\\rin some tasks, like data annotation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02302 ,  17793kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02308\\rDate: Mon, 4 Mar 2024 18:46:20 GMT   (179kb,D)\\r\\rTitle: Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like\\r  Architectures\\rAuthors: Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu\\r  Qiao, Hongsheng Li, Jifeng Dai, Wenhai Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  Transformers have revolutionized computer vision and natural language\\rprocessing, but their high computational complexity limits their application in\\rhigh-resolution image processing and long-context analysis. This paper\\rintroduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the\\rNLP field with necessary modifications for vision tasks. Similar to the Vision\\rTransformer (ViT), our model is designed to efficiently handle sparse inputs\\rand demonstrate robust global processing capabilities, while also scaling up\\reffectively, accommodating both large-scale parameters and extensive datasets.\\rIts distinctive advantage lies in its reduced spatial aggregation complexity,\\rwhich renders it exceptionally adept at processing high-resolution images\\rseamlessly, eliminating the necessity for windowing operations. Our evaluations\\rin image classification demonstrate that VRWKV matches ViT's classification\\rperformance with significantly faster speeds and lower memory usage. In dense\\rprediction tasks, it outperforms window-based models, maintaining comparable\\rspeeds. These results highlight VRWKV's potential as a more efficient\\ralternative for visual perception tasks. Code is released at\\r\\\\url{https://github.com/OpenGVLab/Vision-RWKV}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02308 ,  179kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02325\\rDate: Mon, 4 Mar 2024 18:55:30 GMT   (7484kb,D)\\r\\rTitle: Contrastive Region Guidance: Improving Grounding in Vision-Language\\r  Models without Training\\rAuthors: David Wan, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal\\rCategories: cs.CV cs.AI cs.CL cs.LG\\rComments: Project website: https://contrastive-region-guidance.github.io/\\r\\\\\\\\\\r  Highlighting particularly relevant regions of an image can improve the\\rperformance of vision-language models (VLMs) on various vision-language (VL)\\rtasks by guiding the model to attend more closely to these regions of interest.\\rFor example, VLMs can be given a visual prompt, where visual markers such as\\rbounding boxes delineate key image regions. However, current VLMs that can\\rincorporate visual guidance are either proprietary and expensive or require\\rcostly training on curated data that includes visual prompts. We introduce\\rContrastive Region Guidance (CRG), a training-free guidance method that enables\\ropen-source VLMs to respond to visual prompts. CRG contrasts model outputs\\rproduced with and without visual prompts, factoring out biases revealed by the\\rmodel when answering without the information required to produce a correct\\ranswer (i.e., the model's prior). CRG achieves substantial improvements in a\\rwide variety of VL tasks: When region annotations are provided, CRG increases\\rabsolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse\\rregion-based tasks such as recognition, math, and object relationship\\rreasoning. We also show CRG's applicability to spatial reasoning, with 10%\\rimprovement on What'sUp, as well as to compositional generalization --\\rimproving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe\\r-- and to image-text alignment for generated images, where we improve by up to\\r8.4 AUROC and 6.8 F1 points on SeeTRUE. When reference regions are absent, CRG\\rallows us to re-rank proposed regions in referring expression comprehension and\\rphrase grounding benchmarks like RefCOCO/+/g and Flickr30K Entities, with an\\raverage gain of 3.2% in accuracy. Our analysis explores alternative masking\\rstrategies for CRG, quantifies CRG's probability shift, and evaluates the role\\rof region guidance strength, empirically validating CRG's design choices.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02325 ,  7484kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02330\\rDate: Mon, 4 Mar 2024 18:58:08 GMT   (19351kb,D)\\r\\rTitle: RegionGPT: Towards Region Understanding Vision Language Model\\rAuthors: Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun\\r  Cheung, Yizhou Yu, Ping Luo, Sifei Liu\\rCategories: cs.CV\\rComments: Accepted by CVPR 2024\\r\\\\\\\\\\r  Vision language models (VLMs) have experienced rapid advancements through the\\rintegration of large language models (LLMs) with image-text pairs, yet they\\rstruggle with detailed regional visual understanding due to limited spatial\\rawareness of the vision encoder, and the use of coarse-grained training data\\rthat lacks detailed, region-specific captions. To address this, we introduce\\rRegionGPT (short as RGPT), a novel framework designed for complex region-level\\rcaptioning and understanding. RGPT enhances the spatial awareness of regional\\rrepresentation with simple yet effective modifications to existing visual\\rencoders in VLMs. We further improve performance on tasks requiring a specific\\routput scope by integrating task-guided instruction prompts during both\\rtraining and inference phases, while maintaining the model's versatility for\\rgeneral-purpose tasks. Additionally, we develop an automated region caption\\rdata generation pipeline, enriching the training set with detailed region-level\\rcaptions. We demonstrate that a universal RGPT model can be effectively applied\\rand significantly enhancing performance across a range of region-level tasks,\\rincluding but not limited to complex region descriptions, reasoning, object\\rclassification, and referring expressions comprehension.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02330 ,  19351kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02332\\rDate: Mon, 4 Mar 2024 18:58:11 GMT   (6671kb,D)\\r\\rTitle: UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video\\r  Diffusion Models via Training-Free Unified Attention Control\\rAuthors: Xuweiyi Chen, Tian Xia, and Sihan Xu\\rCategories: cs.CV\\r\\\\\\\\\\r  Video Diffusion Models have been developed for video generation, usually\\rintegrating text and image conditioning to enhance control over the generated\\rcontent. Despite the progress, ensuring consistency across frames remains a\\rchallenge, particularly when using text prompts as control conditions. To\\raddress this problem, we introduce UniCtrl, a novel, plug-and-play method that\\ris universally applicable to improve the spatiotemporal consistency and motion\\rdiversity of videos generated by text-to-video models without additional\\rtraining. UniCtrl ensures semantic consistency across different frames through\\rcross-frame self-attention control, and meanwhile, enhances the motion quality\\rand spatiotemporal consistency through motion injection and spatiotemporal\\rsynchronization. Our experimental results demonstrate UniCtrl's efficacy in\\renhancing various text-to-video models, confirming its effectiveness and\\runiversality.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02332 ,  6671kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02336\\rDate: Mon, 4 Mar 2024 18:58:53 GMT   (8109kb,D)\\r\\rTitle: Brand Visibility in Packaging: A Deep Learning Approach for Logo\\r  Detection, Saliency-Map Prediction, and Logo Placement Analysis\\rAuthors: Alireza Hosseini, Kiana Hooshanfar, Pouria Omrani, Reza Toosi, Ramin\\r  Toosi, Zahra Ebrahimian, Mohammad Ali Akhaee\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  In the highly competitive area of product marketing, the visibility of brand\\rlogos on packaging plays a crucial role in shaping consumer perception,\\rdirectly influencing the success of the product. This paper introduces a\\rcomprehensive framework to measure the brand logo's attention on a packaging\\rdesign. The proposed method consists of three steps. The first step leverages\\rYOLOv8 for precise logo detection across prominent datasets, FoodLogoDet-1500\\rand LogoDet-3K. The second step involves modeling the user's visual attention\\rwith a novel saliency prediction model tailored for the packaging context. The\\rproposed saliency model combines the visual elements with text maps employing a\\rtransformers-based architecture to predict user attention maps. In the third\\rstep, by integrating logo detection with a saliency map generation, the\\rframework provides a comprehensive brand attention score. The effectiveness of\\rthe proposed method is assessed module by module, ensuring a thorough\\revaluation of each component. Comparing logo detection and saliency map\\rprediction with state-of-the-art models shows the superiority of the proposed\\rmethods. To investigate the robustness of the proposed brand attention score,\\rwe collected a unique dataset to examine previous psychophysical hypotheses\\rrelated to brand visibility. the results show that the brand attention score is\\rin line with all previous studies. Also, we introduced seven new hypotheses to\\rcheck the impact of position, orientation, presence of person, and other visual\\relements on brand attention. This research marks a significant stride in the\\rintersection of cognitive psychology, computer vision, and marketing, paving\\rthe way for advanced, consumer-centric packaging designs.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02336 ,  8109kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01629\\rDate: Sun, 3 Mar 2024 22:29:37 GMT   (9202kb,D)\\r\\rTitle: VR Research at Fraunofer IGD, Darmstadt, Germany\\rAuthors: Wolfgang Felger, Martin G\\\\obel, Dirk Reiners, Gabriel Zachmann\\rCategories: cs.GR\\rComments: IEEE VR 2024 Workshop Archiving VR\\rACM-class: K.2; I.3.7; I.3.8\\r\\\\\\\\\\r  We present a historical outline of the research and developments\\r  of Virtual Reality at the Fraunhofer Institute for Computer Graphics (IGD)\\r  in Darmstadt, Germany, from 1990 through 2000.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01629 ,  9202kb)\\r%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00774 (*cross-listing*)\\rDate: Wed, 14 Feb 2024 02:33:17 GMT   (712kb)\\r\\rTitle: Regional inflation analysis using social network data\\rAuthors: Vasilii Chsherbakov Ilia Karpov\\rCategories: q-fin.ST cs.CL cs.SI\\r\\\\\\\\\\r  Inflation is one of the most important macroeconomic indicators that have a\\rgreat impact on the population of any country and region. Inflation is\\rinfluenced by range of factors, one of which is inflation expectations. Many\\rcentral banks take this factor into consideration while implementing monetary\\rpolicy within the inflation targeting regime. Nowadays, a lot of people are\\ractive users of the Internet, especially social networks. There is a hypothesis\\rthat people search, read, and discuss mainly only those issues that are of\\rparticular interest to them. It is logical to assume that the dynamics of\\rprices may also be in the focus of user discussions. So, such discussions could\\rbe regarded as an alternative source of more rapid information about inflation\\rexpectations. This study is based on unstructured data from Vkontakte social\\rnetwork to analyze upward and downward inflationary trends (on the example of\\rthe Omsk region). The sample of more than 8.5 million posts was collected\\rbetween January 2010 and May 2022. The authors used BERT neural networks to\\rsolve the problem. These models demonstrated better results than the benchmarks\\r(e.g., logistic regression, decision tree classifier, etc.). It makes possible\\rto define pro-inflationary and disinflationary types of keywords in different\\rcontexts and get their visualization with SHAP method. This analysis provides\\radditional operational information about inflationary processes at the regional\\rlevel The proposed approach can be scaled for other regions. At the same time\\rthe limitation of the work is the time and power costs for the initial training\\rof similar models for all regions of Russia.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00774 ,  712kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00782 (*cross-listing*)\\rDate: Sun, 18 Feb 2024 10:28:18 GMT   (2042kb,D)\\r\\rTitle: Ploutos: Towards interpretable stock movement prediction with financial\\r  large language model\\rAuthors: Hanshuang Tong, Jun Li, Ning Wu, Ming Gong, Dongmei Zhang, Qi Zhang\\rCategories: q-fin.ST cs.AI cs.CL\\rComments: 8 pages, 4 figures\\r\\\\\\\\\\r  Recent advancements in large language models (LLMs) have opened new pathways\\rfor many domains. However, the full potential of LLMs in financial investments\\rremains largely untapped. There are two main challenges for typical deep\\rlearning-based methods for quantitative finance. First, they struggle to fuse\\rtextual and numerical information flexibly for stock movement prediction.\\rSecond, traditional methods lack clarity and interpretability, which impedes\\rtheir application in scenarios where the justification for predictions is\\ressential. To solve the above challenges, we propose Ploutos, a novel financial\\rLLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen\\rcontains multiple primary experts that can analyze different modal data, such\\ras text and numbers, and provide quantitative strategies from different\\rperspectives. Then PloutosGPT combines their insights and predictions and\\rgenerates interpretable rationales. To generate accurate and faithful\\rrationales, the training strategy of PloutosGPT leverage rearview-mirror\\rprompting mechanism to guide GPT-4 to generate rationales, and a dynamic token\\rweighting mechanism to finetune LLM by increasing key tokens weight. Extensive\\rexperiments show our framework outperforms the state-of-the-art methods on both\\rprediction accuracy and interpretability.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00782 ,  2042kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00784 (*cross-listing*)\\rDate: Sun, 18 Feb 2024 23:22:40 GMT   (1134kb,D)\\r\\rTitle: Utilizing BERT for Information Retrieval: Survey, Applications,\\r  Resources, and Challenges\\rAuthors: Jiajia Wang, Jimmy X. Huang, Xinhui Tu, Junmei Wang, Angela J. Huang,\\r  Md Tahmid Rahman Laskar, Amran Bhuiyan\\rCategories: cs.IR cs.AI cs.CL\\r\\\\\\\\\\r  Recent years have witnessed a substantial increase in the use of deep\\rlearning to solve various natural language processing (NLP) problems. Early\\rdeep learning models were constrained by their sequential or unidirectional\\rnature, such that they struggled to capture the contextual relationships across\\rtext inputs. The introduction of bidirectional encoder representations from\\rtransformers (BERT) leads to a robust encoder for the transformer model that\\rcan understand the broader context and deliver state-of-the-art performance\\racross various NLP tasks. This has inspired researchers and practitioners to\\rapply BERT to practical problems, such as information retrieval (IR). A survey\\rthat focuses on a comprehensive analysis of prevalent approaches that apply\\rpretrained transformer encoders like BERT to IR can thus be useful for academia\\rand the industry. In light of this, we revisit a variety of BERT-based methods\\rin this survey, cover a wide range of techniques of IR, and group them into six\\rhigh-level categories: (i) handling long documents, (ii) integrating semantic\\rinformation, (iii) balancing effectiveness and efficiency, (iv) predicting the\\rweights of terms, (v) query expansion, and (vi) document expansion. We also\\rprovide links to resources, including datasets and toolkits, for BERT-based IR\\rsystems. A key highlight of our survey is the comparison between BERT's\\rencoder-based models and the latest generative Large Language Models (LLMs),\\rsuch as ChatGPT, which rely on decoders. Despite the popularity of LLMs, we\\rfind that for specific tasks, finely tuned BERT encoders still outperform, and\\rat a lower deployment cost. Finally, we summarize the comprehensive outcomes of\\rthe survey and suggest directions for future research in the area.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00784 ,  1134kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00801 (*cross-listing*)\\rDate: Fri, 23 Feb 2024 18:45:35 GMT   (516kb,D)\\r\\rTitle: Self-Retrieval: Building an Information Retrieval System with One Large\\r  Language Model\\rAuthors: Qiaoyu Tang, Jiawei Chen, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu,\\r  Hongyu Lin, Fei Huang, Ben He, Xianpei Han, Le Sun, Yongbin Li\\rCategories: cs.IR cs.CL\\r\\\\\\\\\\r  The rise of large language models (LLMs) has transformed the role of\\rinformation retrieval (IR) systems in the way to humans accessing information.\\rDue to the isolated architecture and the limited interaction, existing IR\\rsystems are unable to fully accommodate the shift from directly providing\\rinformation to humans to indirectly serving large language models. In this\\rpaper, we propose Self-Retrieval, an end-to-end, LLM-driven information\\rretrieval architecture that can fully internalize the required abilities of IR\\rsystems into a single LLM and deeply leverage the capabilities of LLMs during\\rIR process. Specifically, Self-retrieval internalizes the corpus to retrieve\\rinto a LLM via a natural language indexing architecture. Then the entire\\rretrieval process is redefined as a procedure of document generation and\\rself-assessment, which can be end-to-end executed using a single large language\\rmodel. Experimental results demonstrate that Self-Retrieval not only\\rsignificantly outperforms previous retrieval approaches by a large margin, but\\ralso can significantly boost the performance of LLM-driven downstream\\rapplications like retrieval augumented generation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00801 ,  516kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00806 (*cross-listing*)\\rDate: Sat, 24 Feb 2024 12:17:06 GMT   (989kb)\\r\\rTitle: Enhanced User Interaction in Operating Systems through Machine Learning\\r  Language Models\\rAuthors: Chenwei Zhang, Wenran Lu, Chunhe Ni, Hongbo Wang, Jiang Wu\\rCategories: cs.IR cs.CE cs.CL cs.CV\\r\\\\\\\\\\r  With the large language model showing human-like logical reasoning and\\runderstanding ability, whether agents based on the large language model can\\rsimulate the interaction behavior of real users, so as to build a reliable\\rvirtual recommendation A/B test scene to help the application of recommendation\\rresearch is an urgent, important and economic value problem. The combination of\\rinteraction design and machine learning can provide a more efficient and\\rpersonalized user experience for products and services. This personalized\\rservice can meet the specific needs of users and improve user satisfaction and\\rloyalty. Second, the interactive system can understand the user's views and\\rneeds for the product by providing a good user interface and interactive\\rexperience, and then use machine learning algorithms to improve and optimize\\rthe product. This iterative optimization process can continuously improve the\\rquality and performance of the product to meet the changing needs of users. At\\rthe same time, designers need to consider how these algorithms and tools can be\\rcombined with interactive systems to provide a good user experience. This paper\\rexplores the potential applications of large language models, machine learning\\rand interaction design for user interaction in recommendation systems and\\roperating systems. By integrating these technologies, more intelligent and\\rpersonalized services can be provided to meet user needs and promote continuous\\rimprovement and optimization of products. This is of great value for both\\rrecommendation research and user experience applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00806 ,  989kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00807 (*cross-listing*)\\rDate: Sat, 24 Feb 2024 12:31:22 GMT   (587kb)\\r\\rTitle: Enhancing Cloud-Based Large Language Model Processing with Elasticsearch\\r  and Transformer Models\\rAuthors: Chunhe Ni, Jiang Wu, Hongbo Wang, Wenran Lu, Chenwei Zhang\\rCategories: cs.IR cs.CL cs.DC cs.DL\\r\\\\\\\\\\r  Large Language Models (LLMs) are a class of generative AI models built using\\rthe Transformer network, capable of leveraging vast datasets to identify,\\rsummarize, translate, predict, and generate language. LLMs promise to\\rrevolutionize society, yet training these foundational models poses immense\\rchallenges. Semantic vector search within large language models is a potent\\rtechnique that can significantly enhance search result accuracy and relevance.\\rUnlike traditional keyword-based search methods, semantic search utilizes the\\rmeaning and context of words to grasp the intent behind queries and deliver\\rmore precise outcomes. Elasticsearch emerges as one of the most popular tools\\rfor implementing semantic search an exceptionally scalable and robust search\\rengine designed for indexing and searching extensive datasets. In this article,\\rwe delve into the fundamentals of semantic search and explore how to harness\\rElasticsearch and Transformer models to bolster large language model processing\\rparadigms. We gain a comprehensive understanding of semantic search principles\\rand acquire practical skills for implementing semantic search in real-world\\rmodel application scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00807 ,  587kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00810 (*cross-listing*)\\rDate: Sun, 25 Feb 2024 01:40:30 GMT   (2702kb,D)\\r\\rTitle: Bootstrapping Cognitive Agents with a Large Language Model\\rAuthors: Feiyu Zhu, Reid Simmons\\rCategories: cs.AI cs.CL\\r\\\\\\\\\\r  Large language models contain noisy general knowledge of the world, yet are\\rhard to train or fine-tune. On the other hand cognitive architectures have\\rexcellent interpretability and are flexible to update but require a lot of\\rmanual work to instantiate. In this work, we combine the best of both worlds:\\rbootstrapping a cognitive-based model with the noisy knowledge encoded in large\\rlanguage models. Through an embodied agent doing kitchen tasks, we show that\\rour proposed framework yields better efficiency compared to an agent based\\rentirely on large language models. Our experiments indicate that large language\\rmodels are a good source of information for cognitive architectures, and the\\rcognitive architecture in turn can verify and update the knowledge of large\\rlanguage models to a specific domain.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00810 ,  2702kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00811 (*cross-listing*)\\rDate: Sun, 25 Feb 2024 02:35:56 GMT   (7208kb,D)\\r\\rTitle: Cognitive Bias in High-Stakes Decision-Making with LLMs\\rAuthors: Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, Zexue He\\rCategories: cs.AI cs.CL\\r\\\\\\\\\\r  Large language models (LLMs) offer significant potential as tools to support\\ran expanding range of decision-making tasks. However, given their training on\\rhuman (created) data, LLMs can inherit both societal biases against protected\\rgroups, as well as be subject to cognitive bias. Such human-like bias can\\rimpede fair and explainable decisions made with LLM assistance. Our work\\rintroduces BiasBuster, a framework designed to uncover, evaluate, and mitigate\\rcognitive bias in LLMs, particularly in high-stakes decision-making tasks.\\rInspired by prior research in psychology and cognitive sciences, we develop a\\rdataset containing 16,800 prompts to evaluate different cognitive biases (e.g.,\\rprompt-induced, sequential, inherent). We test various bias mitigation\\rstrategies, amidst proposing a novel method using LLMs to debias their own\\rprompts. Our analysis provides a comprehensive picture on the presence and\\reffects of cognitive bias across different commercial and open-source models.\\rWe demonstrate that our self-help debiasing effectively mitigate cognitive bias\\rwithout having to manually craft examples for each bias type.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00811 ,  7208kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00820 (*cross-listing*)\\rDate: Mon, 26 Feb 2024 12:56:17 GMT   (578kb,D)\\r\\rTitle: Retrieval Augmented Generation Systems: Automatic Dataset Creation,\\r  Evaluation and Boolean Agent Setup\\rAuthors: Tristan Kenneweg and Philip Kenneweg and Barbara Hammer\\rCategories: cs.IR cs.CL\\rComments: Was handed in to IJCNN prior to preprint publication here. Was\\r  neither accepted nor rejected at date of publication here\\r\\\\\\\\\\r  Retrieval Augmented Generation (RAG) systems have seen huge popularity in\\raugmenting Large-Language Model (LLM) outputs with domain specific and time\\rsensitive data. Very recently a shift is happening from simple RAG setups that\\rquery a vector database for additional information with every user input to\\rmore sophisticated forms of RAG. However, different concrete approaches compete\\ron mostly anecdotal evidence at the moment. In this paper we present a rigorous\\rdataset creation and evaluation workflow to quantitatively compare different\\rRAG strategies. We use a dataset created this way for the development and\\revaluation of a boolean agent RAG setup: A system in which a LLM can decide\\rwhether to query a vector database or not, thus saving tokens on questions that\\rcan be answered with internal knowledge. We publish our code and generated\\rdataset online.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00820 ,  578kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00823 (*cross-listing*)\\rDate: Mon, 26 Feb 2024 23:15:07 GMT   (276kb,D)\\r\\rTitle: Adapting to Teammates in a Cooperative Language Game\\rAuthors: Christopher Archibald and Spencer Brosnahan\\rCategories: cs.AI cs.CL\\r\\\\\\\\\\r  The game of Codenames has recently emerged as a domain of interest for\\rintelligent agent design. The game is unique due to the way that language and\\rcoordination between teammates play important roles. Previous approaches to\\rdesigning agents for this game have utilized a single internal language model\\rto determine action choices. This often leads to good performance with some\\rteammates and inferior performance with other teammates, as the agent cannot\\radapt to any specific teammate. In this paper we present the first adaptive\\ragent for playing Codenames. We adopt an ensemble approach with the goal of\\rdetermining, during the course of interacting with a specific teammate, which\\rof our internal expert agents, each potentially with its own language model, is\\rthe best match. One difficulty faced in this approach is the lack of a single\\rnumerical metric that accurately captures the performance of a Codenames team.\\rPrior Codenames research has utilized a handful of different metrics to\\revaluate agent teams. We propose a novel single metric to evaluate the\\rperformance of a Codenames team, whether playing a single team (solitaire)\\rgame, or a competitive game against another team. We then present and analyze\\ran ensemble agent which selects an internal expert on each turn in order to\\rmaximize this proposed metric. Experimental analysis shows that this ensemble\\rapproach adapts to individual teammates and often performs nearly as well as\\rthe best internal expert with a teammate. Crucially, this success does not\\rdepend on any previous knowledge about the teammates, the ensemble agents, or\\rtheir compatibility. This research represents an important step to making\\rlanguage-based agents for cooperative language settings like Codenames more\\radaptable to individual teammates.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00823 ,  276kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00829 (*cross-listing*)\\rDate: Wed, 28 Feb 2024 03:40:46 GMT   (538kb,D)\\r\\rTitle: TroubleLLM: Align to Red Team Expert\\rAuthors: Zhuoer Xu, Jianping Zhang, Shiwen Cui, Changhua Meng, Weiqiang Wang\\rCategories: cs.AI cs.CL\\r\\\\\\\\\\r  Large Language Models (LLMs) become the start-of-the-art solutions for a\\rvariety of natural language tasks and are integrated into real-world\\rapplications. However, LLMs can be potentially harmful in manifesting\\rundesirable safety issues like social biases and toxic content. It is\\rimperative to assess its safety issues before deployment. However, the quality\\rand diversity of test prompts generated by existing methods are still far from\\rsatisfactory. Not only are these methods labor-intensive and require large\\rbudget costs, but the controllability of test prompt generation is lacking for\\rthe specific testing domain of LLM applications. With the idea of LLM for LLM\\rtesting, we propose the first LLM, called TroubleLLM, to generate controllable\\rtest prompts on LLM safety issues. Extensive experiments and human evaluation\\rillustrate the superiority of TroubleLLM on generation quality and generation\\rcontrollability.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00829 ,  538kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00830 (*cross-listing*)\\rDate: Wed, 28 Feb 2024 08:30:49 GMT   (8433kb,D)\\r\\rTitle: MedAide: Leveraging Large Language Models for On-Premise Medical\\r  Assistance on Edge Devices\\rAuthors: Abdul Basit, Khizar Hussain, Muhammad Abdullah Hanif, Muhammad\\r  Shafique\\rCategories: cs.AI cs.CL\\rComments: 7 pages, 11 figures, ACM conference paper, 33 references\\rACM-class: I.2.7\\r\\\\\\\\\\r  Large language models (LLMs) are revolutionizing various domains with their\\rremarkable natural language processing (NLP) abilities. However, deploying LLMs\\rin resource-constrained edge computing and embedded systems presents\\rsignificant challenges. Another challenge lies in delivering medical assistance\\rin remote areas with limited healthcare facilities and infrastructure. To\\raddress this, we introduce MedAide, an on-premise healthcare chatbot. It\\rleverages tiny-LLMs integrated with LangChain, providing efficient edge-based\\rpreliminary medical diagnostics and support. MedAide employs model\\roptimizations for minimal memory footprint and latency on embedded edge devices\\rwithout server infrastructure. The training process is optimized using low-rank\\radaptation (LoRA). Additionally, the model is trained on diverse medical\\rdatasets, employing reinforcement learning from human feedback (RLHF) to\\renhance its domain-specific capabilities. The system is implemented on various\\rconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77\\\\%\\raccuracy in medical consultations and scores 56 in USMLE benchmark, enabling an\\renergy-efficient healthcare assistance platform that alleviates privacy\\rconcerns due to edge-based deployment, thereby empowering the community.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00830 ,  8433kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00839 (*cross-listing*)\\rDate: Thu, 29 Feb 2024 02:04:00 GMT   (1757kb,D)\\r\\rTitle: ToolNet: Connecting Large Language Models with Massive Tools via Tool\\r  Graph\\rAuthors: Xukun Liu, Zhiyuan Peng, Xiaoyuan Yi, Xing Xie, Lirong Xiang, Yuchen\\r  Liu, Dongkuan Xu\\rCategories: cs.AI cs.CL\\r\\\\\\\\\\r  While achieving remarkable progress in a broad range of tasks, large language\\rmodels (LLMs) remain significantly limited in properly using massive external\\rtools. Existing in-context learning approaches simply format tools into a list\\rof plain text descriptions and input them to LLMs, from which, LLMs generate a\\rsequence of tool calls to solve problems step by step. Such a paradigm ignores\\rthe intrinsic dependency between tools and offloads all reasoning loads to\\rLLMs, making them restricted to a limited number of specifically designed\\rtools. It thus remains challenging for LLMs to operate on a library of massive\\rtools, casting a great limitation when confronted with real-world scenarios.\\rThis paper proposes ToolNet, a plug-and-play framework that scales up the\\rnumber of tools to thousands with a moderate increase in token consumption.\\rToolNet organizes tools into a directed graph. Each node represents a tool, and\\rweighted edges denote tool transition. Starting from an initial tool node, an\\rLLM navigates in the graph by iteratively choosing the next one from its\\rsuccessors until the task is resolved. Extensive experiments show that ToolNet\\rcan achieve impressive results in challenging multi-hop tool learning datasets\\rand is resilient to tool failures.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00839 ,  1757kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00843 (*cross-listing*)\\rDate: Thu, 29 Feb 2024 13:49:56 GMT   (2358kb,D)\\r\\rTitle: Enhancing Long-Term Recommendation with Bi-level Learnable Large\\r  Language Model Planning\\rAuthors: Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi\\r  Zhang, Qifan Wang, Fuli Feng\\rCategories: cs.IR cs.AI cs.CL cs.LG\\rComments: 11 pages, 5 figures\\r\\\\\\\\\\r  Traditional recommendation setting tends to excessively cater to users'\\rimmediate interests and neglect their long-term engagement. To address it, it\\ris crucial to incorporate planning capabilities into the recommendation\\rdecision-making process to develop policies that take into account both\\rimmediate interests and long-term engagement. Despite Reinforcement Learning\\r(RL) can learn planning capacity by maximizing cumulative reward, the scarcity\\rof recommendation data presents challenges such as instability and\\rsusceptibility to overfitting when training RL models from scratch.\\r  In this context, we propose to leverage the remarkable planning capabilities\\rover sparse data of Large Language Models (LLMs) for long-term recommendation.\\rThe key lies in enabling a language model to understand and apply task-solving\\rprinciples effectively in personalized recommendation scenarios, as the model's\\rpre-training may not naturally encompass these principles, necessitating the\\rneed to inspire or teach the model. To achieve this, we propose a Bi-level\\rLearnable LLM Planner framework, which combines macro-learning and\\rmicro-learning through a hierarchical mechanism. The framework includes a\\rPlanner and Reflector for acquiring high-level guiding principles and an\\rActor-Critic component for planning personalization. Extensive experiments\\rvalidate the superiority of the framework in learning to plan for long-term\\rrecommendations.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00843 ,  2358kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00854 (*cross-listing*)\\rDate: Thu, 29 Feb 2024 18:30:52 GMT   (671kb,D)\\r\\rTitle: Speaker-Independent Dysarthria Severity Classification using\\r  Self-Supervised Transformers and Multi-Task Learning\\rAuthors: Lauren Stumpf and Balasundaram Kadirvelu and Sigourney Waibel and A.\\r  Aldo Faisal\\rCategories: q-bio.NC cs.AI cs.CL cs.LG cs.SD eess.AS\\rComments: 17 pages, 2 tables, 4 main figures, 2 supplemental figures, prepared\\r  for journal submission\\rACM-class: I.2.7; I.2.1; J.3\\r\\\\\\\\\\r  Dysarthria, a condition resulting from impaired control of the speech muscles\\rdue to neurological disorders, significantly impacts the communication and\\rquality of life of patients. The condition's complexity, human scoring and\\rvaried presentations make its assessment and management challenging. This study\\rpresents a transformer-based framework for automatically assessing dysarthria\\rseverity from raw speech data. It can offer an objective, repeatable,\\raccessible, standardised and cost-effective and compared to traditional methods\\rrequiring human expert assessors. We develop a transformer framework, called\\rSpeaker-Agnostic Latent Regularisation (SALR), incorporating a multi-task\\rlearning objective and contrastive learning for speaker-independent multi-class\\rdysarthria severity classification. The multi-task framework is designed to\\rreduce reliance on speaker-specific characteristics and address the intrinsic\\rintra-class variability of dysarthric speech. We evaluated on the Universal\\rAccess Speech dataset using leave-one-speaker-out cross-validation, our model\\rdemonstrated superior performance over traditional machine learning approaches,\\rwith an accuracy of $70.48\\\\%$ and an F1 score of $59.23\\\\%$. Our SALR model also\\rexceeded the previous benchmark for AI-based classification, which used support\\rvector machines, by $16.58\\\\%$. We open the black box of our model by\\rvisualising the latent space where we can observe how the model substantially\\rreduces speaker-specific cues and amplifies task-specific ones, thereby showing\\rits robustness. In conclusion, SALR establishes a new benchmark in\\rspeaker-independent multi-class dysarthria severity classification using\\rgenerative AI. The potential implications of our findings for broader clinical\\rapplications in automated dysarthria severity assessments.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00854 ,  671kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00858 (*cross-listing*)\\rDate: Thu, 29 Feb 2024 19:55:06 GMT   (865kb,D)\\r\\rTitle: Direct Alignment of Draft Model for Speculative Decoding with\\r  Chat-Fine-Tuned LLMs\\rAuthors: Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee,\\r  Christopher Lott\\rCategories: cs.LG cs.AI cs.CL\\rComments: 8 pages, 3 figures\\r\\\\\\\\\\r  Text generation with Large Language Models (LLMs) is known to be memory bound\\rdue to the combination of their auto-regressive nature, huge parameter counts,\\rand limited memory bandwidths, often resulting in low token rates. Speculative\\rdecoding has been proposed as a solution for LLM inference acceleration.\\rHowever, since draft models are often unavailable in the modern open-source LLM\\rfamilies, e.g., for Llama 2 7B, training a high-quality draft model is required\\rto enable inference acceleration via speculative decoding. In this paper, we\\rpropose a simple draft model training framework for direct alignment to\\rchat-capable target models. With the proposed framework, we train Llama 2 Chat\\rDrafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\\\\% of\\rthe original size. Our training framework only consists of pretraining,\\rdistillation dataset generation, and finetuning with knowledge distillation,\\rwith no additional alignment procedure. For the finetuning step, we use\\rinstruction-response pairs generated by target model for distillation in\\rplausible data distribution, and propose a new Total Variation Distance++\\r(TVD++) loss that incorporates variance reduction techniques inspired from the\\rpolicy gradient method in reinforcement learning. Our empirical results show\\rthat Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3\\rblock efficiency and 2.4$\\\\times$ speed-up relative to autoregressive decoding\\ron various tasks with no further task-specific fine-tuning.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00858 ,  865kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00863 (*cross-listing*)\\rDate: Thu, 29 Feb 2024 23:03:19 GMT   (1161kb,D)\\r\\rTitle: LLM-Ensemble: Optimal Large Language Model Ensemble Method for\\r  E-commerce Product Attribute Value Extraction\\rAuthors: Chenhao Fang, Xiaohan Li, Zezhong Fan, Jianpeng Xu, Kaushiki Nag,\\r  Evren Korpeoglu, Sushant Kumar, Kannan Achan\\rCategories: cs.IR cs.AI cs.CL\\r\\\\\\\\\\r  Product attribute value extraction is a pivotal component in Natural Language\\rProcessing (NLP) and the contemporary e-commerce industry. The provision of\\rprecise product attribute values is fundamental in ensuring high-quality\\rrecommendations and enhancing customer satisfaction. The recently emerging\\rLarge Language Models (LLMs) have demonstrated state-of-the-art performance in\\rnumerous attribute extraction tasks, without the need for domain-specific\\rtraining data. Nevertheless, varying strengths and weaknesses are exhibited by\\rdifferent LLMs due to the diversity in data, architectures, and\\rhyperparameters. This variation makes them complementary to each other, with no\\rsingle LLM dominating all others. Considering the diverse strengths and\\rweaknesses of LLMs, it becomes necessary to develop an ensemble method that\\rleverages their complementary potentials. In this paper, we propose a novel\\ralgorithm called LLM-ensemble to ensemble different LLMs' outputs for attribute\\rvalue extraction. We iteratively learn the weights for different LLMs to\\raggregate the labels with weights to predict the final attribute value. Not\\ronly can our proposed method be proven theoretically optimal, but it also\\rensures efficient computation, fast convergence, and safe deployment. We have\\ralso conducted extensive experiments with various state-of-the-art LLMs,\\rincluding Llama2-13B, Llama2-70B, PaLM-2, GPT-3.5, and GPT-4, on Walmart's\\rinternal data. Our offline metrics demonstrate that the LLM-ensemble method\\routperforms all the state-of-the-art single LLMs on Walmart's internal dataset.\\rThis method has been launched in several production models, leading to improved\\rGross Merchandise Volume (GMV), Click-Through Rate (CTR), Conversion Rate\\r(CVR), and Add-to-Cart Rate (ATC).\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00863 ,  1161kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00867 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 03:29:54 GMT   (1041kb,D)\\r\\rTitle: Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by\\r  Exploring Refusal Loss Landscapes\\rAuthors: Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho\\rCategories: cs.CR cs.AI cs.CL cs.LG\\rComments: Project page:\\r  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense\\r\\\\\\\\\\r  Large Language Models (LLMs) are becoming a prominent generative AI tool,\\rwhere the user enters a query and the LLM generates an answer. To reduce harm\\rand misuse, efforts have been made to align these LLMs to human values using\\radvanced training techniques such as Reinforcement Learning from Human Feedback\\r(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\\radversarial jailbreak attempts aiming at subverting the embedded safety\\rguardrails. To address this challenge, this paper defines and investigates the\\rRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\\rjailbreak attempts. Gradient Cuff exploits the unique properties observed in\\rthe refusal loss landscape, including functional values and its smoothness, to\\rdesign an effective two-step detection strategy. Experimental results on two\\raligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\\rattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\\rsignificantly improve the LLM's rejection capability for malicious jailbreak\\rqueries, while maintaining the model's performance for benign user queries by\\radjusting the detection threshold.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00867 ,  1041kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00871 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 06:15:07 GMT   (616kb,D)\\r\\rTitle: Teach LLMs to Phish: Stealing Private Information from Language Models\\rAuthors: Ashwinee Panda, Christopher A. Choquette-Choo, Zhengming Zhang,\\r  Yaoqing Yang, Prateek Mittal\\rCategories: cs.CR cs.AI cs.CL cs.LG\\rComments: ICLR 2024\\r\\\\\\\\\\r  When large language models are trained on private data, it can be a\\rsignificant privacy risk for them to memorize and regurgitate sensitive\\rinformation. In this work, we propose a new practical data extraction attack\\rthat we call neural phishing. This attack enables an adversary to target and\\rextract sensitive or personally identifiable information (PII), e.g., credit\\rcard numbers, from a model trained on user data with upwards of 10% attack\\rsuccess rates, at times, as high as 50%. Our attack assumes only that an\\radversary can insert as few as 10s of benign-appearing sentences into the\\rtraining dataset using only vague priors on the structure of the user data.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00871 ,  616kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00887 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 11:28:37 GMT   (672kb,D)\\r\\rTitle: SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in\\r  Speech\\rAuthors: Aron R, Indra Sigicharla, Chirag Periwal, Mohanaprasad K, Nithya\\r  Darisini P S, Sourabh Tiwari, Shivani Arora\\rCategories: eess.AS cs.AI cs.CL cs.LG cs.SD\\r\\\\\\\\\\r  The interpretation of human voices holds importance across various\\rapplications. This study ventures into predicting age, gender, and emotion from\\rvocal cues, a field with vast applications. Voice analysis tech advancements\\rspan domains, from improving customer interactions to enhancing healthcare and\\rretail experiences. Discerning emotions aids mental health, while age and\\rgender detection are vital in various contexts. Exploring deep learning models\\rfor these predictions involves comparing single, multi-output, and sequential\\rmodels highlighted in this paper. Sourcing suitable data posed challenges,\\rresulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work\\rshowed promise in individual predictions, but limited research considered all\\rthree variables simultaneously. This paper identifies flaws in an individual\\rmodel approach and advocates for our novel multi-output learning architecture\\rSpeech-based Emotion Gender and Age Analysis (SEGAA) model. The experiments\\rsuggest that Multi-output models perform comparably to individual models,\\refficiently capturing the intricate relationships between variables and speech\\rinputs, all while achieving improved runtime.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00887 ,  672kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00891 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 13:04:12 GMT   (1821kb,D)\\r\\rTitle: A Regularization-based Transfer Learning Method for Information\\r  Extraction via Instructed Graph Decoder\\rAuthors: Kedi Chen and Jie Zhou and Qin Chen and Shunyu Liu and Liang He\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\\\r  Information extraction (IE) aims to extract complex structured information\\rfrom the text. Numerous datasets have been constructed for various IE tasks,\\rleading to time-consuming and labor-intensive data annotations. Nevertheless,\\rmost prevailing methods focus on training task-specific models, while the\\rcommon knowledge among different IE tasks is not explicitly modeled. Moreover,\\rthe same phrase may have inconsistent labels in different tasks, which poses a\\rbig challenge for knowledge transfer using a unified model. In this study, we\\rpropose a regularization-based transfer learning method for IE (TIE) via an\\rinstructed graph decoder. Specifically, we first construct an instruction pool\\rfor datasets from all well-known IE tasks, and then present an instructed graph\\rdecoder, which decodes various complex structures into a graph uniformly based\\ron corresponding instructions. In this way, the common knowledge shared with\\rexisting datasets can be learned and transferred to a new dataset with new\\rlabels. Furthermore, to alleviate the label inconsistency problem among various\\rIE tasks, we introduce a task-specific regularization strategy, which does not\\rupdate the gradients of two tasks with 'opposite direction'. We conduct\\rextensive experiments on 12 datasets spanning four IE tasks, and the results\\rdemonstrate the great advantages of our proposed method\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00891 ,  1821kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00894 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 14:43:06 GMT   (7365kb,D)\\r\\rTitle: A systematic evaluation of large language models for generating\\r  programming code\\rAuthors: Wenpin Hou and Zhicheng Ji\\rCategories: cs.SE cs.AI cs.CL cs.PL\\r\\\\\\\\\\r  We systematically evaluated the performance of seven large language models in\\rgenerating programming code using various prompt strategies, programming\\rlanguages, and task difficulties. GPT-4 substantially outperforms other large\\rlanguage models, including Gemini Ultra and Claude 2. The coding performance of\\rGPT-4 varies considerably with different prompt strategies. In most LeetCode\\rand GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the\\roptimal prompt strategy outperforms 85 percent of human participants.\\rAdditionally, GPT-4 demonstrates strong capabilities in translating code\\rbetween different programming languages and in learning from past errors. The\\rcomputational efficiency of the code generated by GPT-4 is comparable to that\\rof human programmers. These results suggest that GPT-4 has the potential to\\rserve as a reliable assistant in programming code generation and software\\rdevelopment.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00894 ,  7365kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00923 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 19:08:25 GMT   (806kb,D)\\r\\rTitle: An Interpretable Ensemble of Graph and Language Models for Improving\\r  Search Relevance in E-Commerce\\rAuthors: Nurendra Choudhary, Edward W Huang, Karthik Subbian, Chandan K. Reddy\\rCategories: cs.IR cs.CL\\rComments: Accepted to The Web Conference 2024 (Industry)\\rACM-class: H.3.3; I.2.7; J.7\\r\\\\\\\\\\r  The problem of search relevance in the E-commerce domain is a challenging one\\rsince it involves understanding the intent of a user's short nuanced query and\\rmatching it with the appropriate products in the catalog. This problem has\\rtraditionally been addressed using language models (LMs) and graph neural\\rnetworks (GNNs) to capture semantic and inter-product behavior signals,\\rrespectively. However, the rapid development of new architectures has created a\\rgap between research and the practical adoption of these techniques. Evaluating\\rthe generalizability of these models for deployment requires extensive\\rexperimentation on complex, real-world datasets, which can be non-trivial and\\rexpensive. Furthermore, such models often operate on latent space\\rrepresentations that are incomprehensible to humans, making it difficult to\\revaluate and compare the effectiveness of different models. This lack of\\rinterpretability hinders the development and adoption of new techniques in the\\rfield. To bridge this gap, we propose Plug and Play Graph LAnguage Model\\r(PP-GLAM), an explainable ensemble of plug and play models. Our approach uses a\\rmodular framework with uniform data processing pipelines. It employs additive\\rexplanation metrics to independently decide whether to include (i) language\\rmodel candidates, (ii) GNN model candidates, and (iii) inter-product behavioral\\rsignals. For the task of search relevance, we show that PP-GLAM outperforms\\rseveral state-of-the-art baselines as well as a proprietary model on real-world\\rmultilingual, multi-regional e-commerce datasets. To promote better model\\rcomprehensibility and adoption, we also provide an analysis of the\\rexplainability and computational complexity of our model. We also provide the\\rpublic codebase and provide a deployment strategy for practical implementation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00923 ,  806kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00932 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 19:22:24 GMT   (306kb,D)\\r\\rTitle: Differentially Private Knowledge Distillation via Synthetic Text\\r  Generation\\rAuthors: James Flemings and Murali Annavaram\\rCategories: cs.LG cs.CL cs.CR\\r\\\\\\\\\\r  Large Language models (LLMs) are achieving state-of-the-art performance in\\rmany different downstream tasks. However, the increasing urgency of data\\rprivacy requires LLMs to train with Differential Privacy (DP) on private data.\\rConcurrently it is also necessary to compress LLMs for real-life deployments on\\rresource-constrained devices or latency-sensitive applications. Differential\\rprivacy and model compression generally must trade off utility loss to achieve\\rtheir objectives. Moreover, concurrently achieving both can result in even more\\rutility loss. To this end, we propose a novel differentially private knowledge\\rdistillation algorithm that exploits synthetic data generated by a\\rdifferentially private LLM. The knowledge of a teacher model is transferred\\ronto the student in two ways: one way from the synthetic data itself, the hard\\rlabels, and the other way by the output distribution of the teacher model\\revaluated on the synthetic data, the soft labels. Furthermore, if the teacher\\rand student share a similar architectural structure, we can further distill\\rknowledge by exploiting hidden representations. Our results show that our\\rframework substantially improves the utility over existing baselines with\\rstrong privacy parameters, {\\\\epsilon} = 2, validating that we can successfully\\rcompress autoregressive LLMs while preserving the privacy of training data.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00932 ,  306kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00994 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 21:29:32 GMT   (2551kb,D)\\r\\rTitle: Leveraging Prompt-Based Large Language Models: Predicting Pandemic\\r  Health Decisions and Outcomes Through Social Media Language\\rAuthors: Xiaohan Ding, Buse Carik, Uma Sushmitha Gunturi, Valerie Reyna, and\\r  Eugenia H. Rho\\rCategories: cs.HC cs.AI cs.CL cs.SI\\rComments: 20 pages, 4 figures\\rDOI: 10.1145/3613904.3642117\\r\\\\\\\\\\r  We introduce a multi-step reasoning framework using prompt-based LLMs to\\rexamine the relationship between social media language patterns and trends in\\rnational health outcomes. Grounded in fuzzy-trace theory, which emphasizes the\\rimportance of gists of causal coherence in effective health communication, we\\rintroduce Role-Based Incremental Coaching (RBIC), a prompt-based LLM framework,\\rto identify gists at-scale. Using RBIC, we systematically extract gists from\\rsubreddit discussions opposing COVID-19 health measures (Study 1). We then\\rtrack how these gists evolve across key events (Study 2) and assess their\\rinfluence on online engagement (Study 3). Finally, we investigate how the\\rvolume of gists is associated with national health trends like vaccine uptake\\rand hospitalizations (Study 4). Our work is the first to empirically link\\rsocial media linguistic patterns to real-world public health trends,\\rhighlighting the potential of prompt-based LLMs in identifying critical online\\rdiscussion patterns that can form the basis of public health communication\\rstrategies.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00994 ,  2551kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01131 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 08:21:59 GMT   (4006kb,D)\\r\\rTitle: LLaMoCo: Instruction Tuning of Large Language Models for Optimization\\r  Code Generation\\rAuthors: Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao,\\r  Yining Ma, Yue-Jiao Gong\\rCategories: math.OC cs.AI cs.CL cs.LG cs.NE cs.SE\\r\\\\\\\\\\r  Recent research explores optimization using large language models (LLMs) by\\reither iteratively seeking next-step solutions from LLMs or directly prompting\\rLLMs for an optimizer. However, these approaches exhibit inherent limitations,\\rincluding low operational efficiency, high sensitivity to prompt design, and a\\rlack of domain-specific knowledge. We introduce LLaMoCo, the first\\rinstruction-tuning framework designed to adapt LLMs for solving optimization\\rproblems in a code-to-code manner. Specifically, we establish a comprehensive\\rinstruction set containing well-described problem prompts and effective\\roptimization codes. We then develop a novel two-phase learning strategy that\\rincorporates a contrastive learning-based warm-up procedure before the\\rinstruction-tuning phase to enhance the convergence behavior during model\\rfine-tuning. The experiment results demonstrate that a CodeGen (350M) model\\rfine-tuned by our LLaMoCo achieves superior optimization performance compared\\rto GPT-4 Turbo and the other competitors across both synthetic and realistic\\rproblem sets. The fine-tuned model and the usage instructions are available at\\rhttps://anonymous.4open.science/r/LLaMoCo-722A.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01131 ,  4006kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01203 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 12:44:59 GMT   (2004kb,D)\\r\\rTitle: Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment\\rAuthors: Luyao Wang and Pengnian Qi and Xigang Bao and Chunlai Zhou and Biao\\r  Qin\\rCategories: cs.LG cs.CL cs.DB\\rComments: accepted by AAAI2024\\r\\\\\\\\\\r  Multi-modal entity alignment (MMEA) aims to identify equivalent entities\\rbetween two multi-modal knowledge graphs for integration. Unfortunately, prior\\rarts have attempted to improve the interaction and fusion of multi-modal\\rinformation, which have overlooked the influence of modal-specific noise and\\rthe usage of labeled and unlabeled data in semi-supervised settings. In this\\rwork, we introduce a Pseudo-label Calibration Multi-modal Entity Alignment\\r(PCMEA) in a semi-supervised way. Specifically, in order to generate holistic\\rentity representations, we first devise various embedding modules and attention\\rmechanisms to extract visual, structural, relational, and attribute features.\\rDifferent from the prior direct fusion methods, we next propose to exploit\\rmutual information maximization to filter the modal-specific noise and to\\raugment modal-invariant commonality. Then, we combine pseudo-label calibration\\rwith momentum-based contrastive learning to make full use of the labeled and\\runlabeled data, which improves the quality of pseudo-label and pulls aligned\\rentities closer. Finally, extensive experiments on two MMEA datasets\\rdemonstrate the effectiveness of our PCMEA, which yields state-of-the-art\\rperformance.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01203 ,  2004kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01242 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 16:06:03 GMT   (1609kb,D)\\r\\rTitle: Augmenting Automation: Intent-Based User Instruction Classification with\\r  Machine Learning\\rAuthors: Lochan Basyal, Bijay Gaudel\\rCategories: cs.LG cs.AI cs.CL cs.HC\\rComments: 7 pages, 14 figures\\r\\\\\\\\\\r  Electric automation systems offer convenience and efficiency in controlling\\relectrical circuits and devices. Traditionally, these systems rely on\\rpredefined commands for control, limiting flexibility and adaptability. In this\\rpaper, we propose a novel approach to augment automation by introducing\\rintent-based user instruction classification using machine learning techniques.\\rOur system represents user instructions as intents, allowing for dynamic\\rcontrol of electrical circuits without relying on predefined commands. Through\\ra machine learning model trained on a labeled dataset of user instructions, our\\rsystem classifies intents from user input, enabling a more intuitive and\\radaptable control scheme. We present the design and implementation of our\\rintent-based electric automation system, detailing the development of the\\rmachine learning model for intent classification. Experimental results\\rdemonstrate the effectiveness of our approach in enhancing user experience and\\rexpanding the capabilities of electric automation systems. Our work contributes\\rto the advancement of smart technologies by providing a more seamless\\rinteraction between users and their environments.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01242 ,  1609kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01267 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 17:10:44 GMT   (14202kb,D)\\r\\rTitle: Dissecting Language Models: Machine Unlearning via Selective Pruning\\rAuthors: Nicholas Pochinkov and Nandi Schoots\\rCategories: cs.LG cs.CL\\r\\\\\\\\\\r  Understanding and shaping the behaviour of Large Language Models (LLMs) is\\rincreasingly important as applications become more powerful and more frequently\\radopted. This paper introduces a machine unlearning method specifically\\rdesigned for LLMs. We introduce a selective pruning method for LLMs that\\rremoves neurons based on their relative importance on a targeted capability\\rcompared to overall network performance. This approach is a compute- and\\rdata-efficient method for identifying and removing neurons that enable specific\\rbehaviours. Our findings reveal that both feed-forward and attention neurons in\\rLLMs are specialized; that is, for specific tasks, certain neurons are more\\rcrucial than others.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01267 ,  14202kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01273 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 17:29:22 GMT   (2266kb,D)\\r\\rTitle: NoMAD-Attention: Efficient LLM Inference on CPUs Through\\r  Multiply-add-free Attention\\rAuthors: Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, Anshumali\\r  Shrivastava\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\\\r  Large language model inference on Central Processing Units (CPU) is\\rchallenging due to the vast quantities of expensive Multiply-Add (MAD) matrix\\roperations in the attention computations. In this paper, we argue that there is\\ra rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers,\\rwhich allow for ultra-low-latency lookups in batch. We leverage this unique\\rcapability of CPUs to propose NoMAD-Attention, an efficient attention algorithm\\rthat replaces MAD operations with in-register lookups. Through hardware-aware\\ralgorithmic designs, NoMAD-Attention achieves the computation of attention\\rscores using repeated fast accesses to SIMD registers despite their highly\\rlimited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based\\rLLMs without model finetuning. Empirical evaluations demonstrate that\\rNoMAD-Attention maintains the quality of the original LLMs well, and speeds up\\rthe 4-bit quantized LLaMA-7B-based model by up to 2$\\\\times$ at 16k context\\rlength. Our results are reproducible at\\rhttps://github.com/tonyzhang617/nomad-dist.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01273 ,  2266kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01384 (*cross-listing*)\\rDate: Sun, 3 Mar 2024 03:27:07 GMT   (738kb)\\r\\rTitle: On the Compressibility of Quantized Large Language Models\\rAuthors: Yu Mao, Weilan Wang, Hongchao Du, Nan Guan, and Chun Jason Xue\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\\\r  Deploying Large Language Models (LLMs) on edge or mobile devices offers\\rsignificant benefits, such as enhanced data privacy and real-time processing\\rcapabilities. However, it also faces critical challenges due to the substantial\\rmemory requirement of LLMs. Quantization is an effective way of reducing the\\rmodel size while maintaining good performance. However, even after\\rquantization, LLMs may still be too big to fit entirely into the limited memory\\rof edge or mobile devices and have to be partially loaded from the storage to\\rcomplete the inference. In this case, the I/O latency of model loading becomes\\rthe bottleneck of the LLM inference latency. In this work, we take a\\rpreliminary step of studying applying data compression techniques to reduce\\rdata movement and thus speed up the inference of quantized LLM on\\rmemory-constrained devices. In particular, we discussed the compressibility of\\rquantized LLMs, the trade-off between the compressibility and performance of\\rquantized LLMs, and opportunities to optimize both of them jointly.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01384 ,  738kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01457 (*cross-listing*)\\rDate: Sun, 3 Mar 2024 09:22:21 GMT   (1167kb,D)\\r\\rTitle: Logic Rules as Explanations for Legal Case Retrieval\\rAuthors: Zhongxiang Sun, Kepu Zhang, Weijie Yu, Haoyu Wang, Jun Xu\\rCategories: cs.IR cs.CL\\rComments: accepted by lrec-coling 2024\\r\\\\\\\\\\r  In this paper, we address the issue of using logic rules to explain the\\rresults from legal case retrieval. The task is critical to legal case retrieval\\rbecause the users (e.g., lawyers or judges) are highly specialized and require\\rthe system to provide logical, faithful, and interpretable explanations before\\rmaking legal decisions. Recently, research efforts have been made to learn\\rexplainable legal case retrieval models. However, these methods usually select\\rrationales (key sentences) from the legal cases as explanations, failing to\\rprovide faithful and logically correct explanations. In this paper, we propose\\rNeural-Symbolic enhanced Legal Case Retrieval (NS-LCR), a framework that\\rexplicitly conducts reasoning on the matching of legal cases through learning\\rcase-level and law-level logic rules. The learned rules are then integrated\\rinto the retrieval process in a neuro-symbolic manner. Benefiting from the\\rlogic and interpretable nature of the logic rules, NS-LCR is equipped with\\rbuilt-in faithful explainability. We also show that NS-LCR is a model-agnostic\\rframework that can be plugged in for multiple legal retrieval models. To\\rshowcase NS-LCR's superiority, we enhance existing benchmarks by adding\\rmanually annotated logic rules and introducing a novel explainability metric\\rusing Large Language Models (LLMs). Our comprehensive experiments reveal\\rNS-LCR's effectiveness for ranking, alongside its proficiency in delivering\\rreliable explanations for legal case retrieval.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01457 ,  1167kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01472 (*cross-listing*)\\rDate: Sun, 3 Mar 2024 10:39:27 GMT   (15553kb,D)\\r\\rTitle: WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service\\r  Copyright Protection\\rAuthors: Anudeex Shetty, Yue Teng, Ke He, Qiongkai Xu\\rCategories: cs.CR cs.CL cs.LG\\rComments: Work in Progress\\r\\\\\\\\\\r  Embedding as a Service (EaaS) has become a widely adopted solution, which\\roffers feature extraction capabilities for addressing various downstream tasks\\rin Natural Language Processing (NLP). Prior studies have shown that EaaS can be\\rprone to model extraction attacks; nevertheless, this concern could be\\rmitigated by adding backdoor watermarks to the text embeddings and subsequently\\rverifying the attack models post-publication. Through the analysis of the\\rrecent watermarking strategy for EaaS, EmbMarker, we design a novel CSE\\r(Clustering, Selection, Elimination) attack that removes the backdoor watermark\\rwhile maintaining the high utility of embeddings, indicating that the previous\\rwatermarking approach can be breached. In response to this new threat, we\\rpropose a new protocol to make the removal of watermarks more challenging by\\rincorporating multiple possible watermark directions. Our defense approach,\\rWARDEN, notably increases the stealthiness of watermarks and empirically has\\rbeen shown effective against CSE attack.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01472 ,  15553kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01643 (*cross-listing*)\\rDate: Sun, 3 Mar 2024 23:40:35 GMT   (78kb,D)\\r\\rTitle: You Need to Pay Better Attention\\rAuthors: Mehran Hosseini, Peyman Hosseini\\rCategories: cs.LG cs.AI cs.CL cs.CV\\rMSC-class: 68T07 (Primary) 68T45, 68T50, 68T10, 15A03, 15A04 (Secondary)\\rACM-class: I.2.6; I.2.7; I.2.10; I.4.0; I.5.0; I.7.0\\r\\\\\\\\\\r  We introduce three new attention mechanisms that outperform standard\\rmulti-head attention in terms of efficiency and learning capabilities, thereby\\rimproving the performance and broader deployability of Transformer models. Our\\rfirst contribution is Optimised Attention, which performs similarly to standard\\rattention, but has 3/4 as many parameters and one matrix multiplication fewer\\rper head. Next, we introduce Efficient Attention, which performs on par with\\rstandard attention with only 1/2 as many parameters as many parameters and two\\rmatrix multiplications fewer per head and is up to twice as fast as standard\\rattention. Lastly, we introduce Super Attention, which surpasses standard\\rattention by a significant margin in both vision and natural language\\rprocessing tasks while having fewer parameters and matrix multiplications. In\\raddition to providing rigorous mathematical comparisons, we evaluate the\\rpresented attention mechanisms on MNIST, CIFAR100, IMDB Movie Reviews, and\\rAmazon Reviews datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01643 ,  78kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01747 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 05:52:41 GMT   (3473kb,D)\\r\\rTitle: Towards Self-Contained Answers: Entity-Based Answer Rewriting in\\r  Conversational Search\\rAuthors: Ivan Sekuli\\\\'c, Krisztian Balog, Fabio Crestani\\rCategories: cs.IR cs.CL\\rDOI: 10.1145/3627508.3638300\\r\\\\\\\\\\r  Conversational information-seeking (CIS) is an emerging paradigm for\\rknowledge acquisition and exploratory search. Traditional web search interfaces\\renable easy exploration of entities, but this is limited in conversational\\rsettings due to the limited-bandwidth interface. This paper explore ways to\\rrewrite answers in CIS, so that users can understand them without having to\\rresort to external services or sources. Specifically, we focus on salient\\rentities -- entities that are central to understanding the answer. As our first\\rcontribution, we create a dataset of conversations annotated with entities for\\rsaliency. Our analysis of the collected data reveals that the majority of\\ranswers contain salient entities. As our second contribution, we propose two\\ranswer rewriting strategies aimed at improving the overall user experience in\\rCIS. One approach expands answers with inline definitions of salient entities,\\rmaking the answer self-contained. The other approach complements answers with\\rfollow-up questions, offering users the possibility to learn more about\\rspecific entities. Results of a crowdsourcing-based study indicate that\\rrewritten answers are clearly preferred over the original ones. We also find\\rthat inline definitions tend to be favored over follow-up questions, but this\\rchoice is highly subjective, thereby providing a promising future direction for\\rpersonalization.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01747 ,  3473kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01757 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 06:24:21 GMT   (12412kb,D)\\r\\rTitle: How Multimodal Integration Boost the Performance of LLM for\\r  Optimization: Case Study on Capacitated Vehicle Routing Problems\\rAuthors: Yuxiao Huang, Wenjie Zhang, Liang Feng, Xingyu Wu, Kay Chen Tan\\rCategories: cs.AI cs.CL cs.LG cs.NE math.OC\\rComments: 8pages,3 figures, 2 tables\\r\\\\\\\\\\r  Recently, large language models (LLMs) have notably positioned them as\\rcapable tools for addressing complex optimization challenges. Despite this\\rrecognition, a predominant limitation of existing LLM-based optimization\\rmethods is their struggle to capture the relationships among decision variables\\rwhen relying exclusively on numerical text prompts, especially in\\rhigh-dimensional problems. Keeping this in mind, we first propose to enhance\\rthe optimization performance using multimodal LLM capable of processing both\\rtextual and visual prompts for deeper insights of the processed optimization\\rproblem. This integration allows for a more comprehensive understanding of\\roptimization problems, akin to human cognitive processes. We have developed a\\rmultimodal LLM-based optimization framework that simulates human\\rproblem-solving workflows, thereby offering a more nuanced and effective\\ranalysis. The efficacy of this method is evaluated through extensive empirical\\rstudies focused on a well-known combinatorial optimization problem, i.e.,\\rcapacitated vehicle routing problem. The results are compared against those\\robtained from the LLM-based optimization algorithms that rely solely on textual\\rprompts, demonstrating the significant advantages of our multimodal approach.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01757 ,  12412kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01832 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 08:29:15 GMT   (177kb)\\r\\rTitle: Model-Based Data-Centric AI: Bridging the Divide Between Academic Ideals\\r  and Industrial Pragmatism\\rAuthors: Chanjun Park, Minsoo Khang, Dahyun Kim\\rCategories: cs.AI cs.CL\\rComments: Accepted for Data-centric Machine Learning Research (DMLR) Workshop\\r  at ICLR 2024\\r\\\\\\\\\\r  This paper delves into the contrasting roles of data within academic and\\rindustrial spheres, highlighting the divergence between Data-Centric AI and\\rModel-Agnostic AI approaches. We argue that while Data-Centric AI focuses on\\rthe primacy of high-quality data for model performance, Model-Agnostic AI\\rprioritizes algorithmic flexibility, often at the expense of data quality\\rconsiderations. This distinction reveals that academic standards for data\\rquality frequently do not meet the rigorous demands of industrial applications,\\rleading to potential pitfalls in deploying academic models in real-world\\rsettings. Through a comprehensive analysis, we address these disparities,\\rpresenting both the challenges they pose and strategies for bridging the gap.\\rFurthermore, we propose a novel paradigm: Model-Based Data-Centric AI, which\\raims to reconcile these differences by integrating model considerations into\\rdata optimization processes. This approach underscores the necessity for\\revolving data requirements that are sensitive to the nuances of both academic\\rresearch and industrial deployment. By exploring these discrepancies, we aim to\\rfoster a more nuanced understanding of data's role in AI development and\\rencourage a convergence of academic and industrial standards to enhance AI's\\rreal-world applicability.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01832 ,  177kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02167 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 16:13:39 GMT   (3620kb,D)\\r\\rTitle: Speech emotion recognition from voice messages recorded in the wild\\rAuthors: Luc\\\\'ia G\\\\'omez-Zaragoz\\\\'a, \\\\'Oscar Valls, Roc\\\\'io del Amor, Mar\\\\'ia\\r  Jos\\\\'e Castro-Bleda, Valery Naranjo, Mariano Alca\\\\~niz Raya, Javier\\r  Mar\\\\'in-Morales\\rCategories: eess.AS cs.AI cs.CL cs.SD\\rComments: This work has been submitted to the IEEE for possible publication.\\r  Copyright may be transferred without notice, after which this version may no\\r  longer be accessible\\rACM-class: I.5.1; I.5.4\\r\\\\\\\\\\r  Emotion datasets used for Speech Emotion Recognition (SER) often contain\\racted or elicited speech, limiting their applicability in real-world scenarios.\\rIn this work, we used the Emotional Voice Messages (EMOVOME) database,\\rincluding spontaneous voice messages from conversations of 100 Spanish speakers\\ron a messaging app, labeled in continuous and discrete emotions by expert and\\rnon-expert annotators. We created speaker-independent SER models using the\\reGeMAPS features, transformer-based models and their combination. We compared\\rthe results with reference databases and analyzed the influence of annotators\\rand gender fairness. The pre-trained Unispeech-L model and its combination with\\reGeMAPS achieved the highest results, with 61.64% and 55.57% Unweighted\\rAccuracy (UA) for 3-class valence and arousal prediction respectively, a 10%\\rimprovement over baseline models. For the emotion categories, 42.58% UA was\\robtained. EMOVOME performed lower than the acted RAVDESS database. The elicited\\rIEMOCAP database also outperformed EMOVOME in the prediction of emotion\\rcategories, while similar results were obtained in valence and arousal.\\rAdditionally, EMOVOME outcomes varied with annotator labels, showing superior\\rresults and better fairness when combining expert and non-expert annotations.\\rThis study significantly contributes to the evaluation of SER models in\\rreal-life situations, advancing in the development of applications for\\ranalyzing spontaneous voice messages.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02167 ,  3620kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02185 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 16:27:21 GMT   (1047kb,D)\\r\\rTitle: Distilled ChatGPT Topic & Sentiment Modeling with Applications in\\r  Finance\\rAuthors: Olivier Gandouet, Mouloud Belbahri, Armelle Jezequel, Yuriy Bodjov\\rCategories: cs.LG cs.CE cs.CL\\rComments: Edge Intelligence Workshop at AAAI24\\r\\\\\\\\\\r  In this study, ChatGPT is utilized to create streamlined models that generate\\reasily interpretable features. These features are then used to evaluate\\rfinancial outcomes from earnings calls. We detail a training approach that\\rmerges knowledge distillation and transfer learning, resulting in lightweight\\rtopic and sentiment classification models without significant loss in accuracy.\\rThese models are assessed through a dataset annotated by experts. The paper\\ralso delves into two practical case studies, highlighting how the generated\\rfeatures can be effectively utilized in quantitative investing scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02185 ,  1047kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02253 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 17:38:32 GMT   (3872kb,D)\\r\\rTitle: KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for\\r  Enhancing Reference-Based Phishing Detection\\rAuthors: Yuexin Li, Chengyu Huang, Shumin Deng, Mei Lin Lock, Tri Cao, Nay Oo,\\r  Bryan Hooi, Hoon Wei Lim\\rCategories: cs.CR cs.AI cs.CL cs.LG\\r\\\\\\\\\\r  Phishing attacks have inflicted substantial losses on individuals and\\rbusinesses alike, necessitating the development of robust and efficient\\rautomated phishing detection approaches. Reference-based phishing detectors\\r(RBPDs), which compare the logos on a target webpage to a known set of logos,\\rhave emerged as the state-of-the-art approach. However, a major limitation of\\rexisting RBPDs is that they rely on a manually constructed brand knowledge\\rbase, making it infeasible to scale to a large number of brands, which results\\rin false negative errors due to the insufficient brand coverage of the\\rknowledge base. To address this issue, we propose an automated knowledge\\rcollection pipeline, using which we collect and release a large-scale\\rmultimodal brand knowledge base, KnowPhish, containing 20k brands with rich\\rinformation about each brand. KnowPhish can be used to boost the performance of\\rexisting RBPDs in a plug-and-play manner. A second limitation of existing RBPDs\\ris that they solely rely on the image modality, ignoring useful textual\\rinformation present in the webpage HTML. To utilize this textual information,\\rwe propose a Large Language Model (LLM)-based approach to extract brand\\rinformation of webpages from text. Our resulting multimodal phishing detection\\rapproach, KnowPhish Detector (KPD), can detect phishing webpages with or\\rwithout logos. We evaluate KnowPhish and KPD on a manually validated dataset,\\rand on a field study under Singapore's local context, showing substantial\\rimprovements in effectiveness and efficiency compared to state-of-the-art\\rbaselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02253 ,  3872kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00771 (*cross-listing*)\\rDate: Sun, 11 Feb 2024 21:57:49 GMT   (417kb)\\r\\rTitle: XProspeCT: CT Volume Generation from Paired X-Rays\\rAuthors: Benjamin Paulson, Joshua Goldshteyn, Sydney Balboni, John Cisler,\\r  Andrew Crisler, Natalia Bukowski, Julia Kalish, Theodore Colwell\\rCategories: eess.IV cs.CV cs.LG physics.med-ph\\rComments: Originally submitted as part of the MICS 2023 Undergraduate Paper\\r  Competition\\r\\\\\\\\\\r  Computed tomography (CT) is a beneficial imaging tool for diagnostic\\rpurposes. CT scans provide detailed information concerning the internal\\ranatomic structures of a patient, but present higher radiation dose and costs\\rcompared to X-ray imaging. In this paper, we build on previous research to\\rconvert orthogonal X-ray images into simulated CT volumes by exploring larger\\rdatasets and various model structures. Significant model variations include\\rUNet architectures, custom connections, activation functions, loss functions,\\roptimizers, and a novel back projection approach.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00771 ,  417kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00816 (*cross-listing*)\\rDate: Mon, 26 Feb 2024 01:17:50 GMT   (21506kb,D)\\r\\rTitle: CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document\\r  Visual Question Answering\\rAuthors: Jinxu Zhang, Yongqi Yu, Yu Zhang\\rCategories: cs.IR cs.AI cs.CV\\r\\\\\\\\\\r  Document Visual Question Answering (DVQA) is a task that involves responding\\rto queries based on the content of images. Existing work is limited to locating\\rinformation within a single page and does not facilitate cross-page\\rquestion-and-answer interaction. Furthermore, the token length limitation\\rimposed on inputs to the model may lead to truncation of segments pertinent to\\rthe answer. In this study, we introduce a simple but effective methodology\\rcalled CFRet-DVQA, which focuses on retrieval and efficient tuning to address\\rthis critical issue effectively. For that, we initially retrieve multiple\\rsegments from the document that correlate with the question at hand.\\rSubsequently, we leverage the advanced reasoning abilities of the large\\rlanguage model (LLM), further augmenting its performance through instruction\\rtuning. This approach enables the generation of answers that align with the\\rstyle of the document labels. The experiments demonstrate that our methodology\\rachieved state-of-the-art or competitive results with both single-page and\\rmulti-page documents in various fields.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00816 ,  21506kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00865 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 02:20:04 GMT   (1312kb,D)\\r\\rTitle: Fast and Efficient Local Search for Genetic Programming Based Loss\\r  Function Learning\\rAuthors: Christian Raymond, Qi Chen, Bing Xue, and Mengjie Zhang\\rCategories: cs.NE cs.AI cs.CV cs.LG\\rComments: arXiv admin note: substantial text overlap with arXiv:2209.08907\\r\\\\\\\\\\r  In this paper, we develop upon the topic of loss function learning, an\\remergent meta-learning paradigm that aims to learn loss functions that\\rsignificantly improve the performance of the models trained under them.\\rSpecifically, we propose a new meta-learning framework for task and\\rmodel-agnostic loss function learning via a hybrid search approach. The\\rframework first uses genetic programming to find a set of symbolic loss\\rfunctions. Second, the set of learned loss functions is subsequently\\rparameterized and optimized via unrolled differentiation. The versatility and\\rperformance of the proposed framework are empirically validated on a diverse\\rset of supervised learning tasks. Results show that the learned loss functions\\rbring improved convergence, sample efficiency, and inference performance on\\rtabulated, computer vision, and natural language processing problems, using a\\rvariety of task-specific neural network architectures.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00865 ,  1312kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00897 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 16:27:33 GMT   (3833kb,D)\\r\\rTitle: VisRec: A Semi-Supervised Approach to Radio Interferometric Data\\r  Reconstruction\\rAuthors: Ruoqi Wang, Haitao Wang, Qiong Luo, Feng Wang and Hejun Wu\\rCategories: eess.IV astro-ph.GA cs.AI cs.CV cs.LG\\r\\\\\\\\\\r  Radio telescopes produce visibility data about celestial objects, but these\\rdata are sparse and noisy. As a result, images created on raw visibility data\\rare of low quality. Recent studies have used deep learning models to\\rreconstruct visibility data to get cleaner images. However, these methods rely\\ron a substantial amount of labeled training data, which requires significant\\rlabeling effort from radio astronomers. Addressing this challenge, we propose\\rVisRec, a model-agnostic semi-supervised learning approach to the\\rreconstruction of visibility data. Specifically, VisRec consists of both a\\rsupervised learning module and an unsupervised learning module. In the\\rsupervised learning module, we introduce a set of data augmentation functions\\rto produce diverse training examples. In comparison, the unsupervised learning\\rmodule in VisRec augments unlabeled data and uses reconstructions from\\rnon-augmented visibility data as pseudo-labels for training. This hybrid\\rapproach allows VisRec to effectively leverage both labeled and unlabeled data.\\rThis way, VisRec performs well even when labeled data is scarce. Our evaluation\\rresults show that VisRec outperforms all baseline methods in reconstruction\\rquality, robustness against common observation perturbation, and\\rgeneralizability to different telescope configurations.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00897 ,  3833kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00946 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 19:50:22 GMT   (6186kb,D)\\r\\rTitle: Fine-tuning with Very Large Dropout\\rAuthors: Jianyu Zhang, L\\\\'eon Bottou\\rCategories: cs.LG cs.CV\\rComments: 13 pages\\r\\\\\\\\\\r  It is impossible today to pretend that the practice of machine learning is\\rcompatible with the idea that training and testing data follow the same\\rdistribution. Several authors have recently used ensemble techniques to show\\rhow scenarios involving multiple data distributions are best served by\\rrepresentations that are both richer than those obtained by regularizing for\\rthe best in-distribution performance, and richer than those obtained under the\\rinfluence of the implicit sparsity bias of common stochastic gradient\\rprocedures.\\r  This contribution investigates the use of very high dropout rates instead of\\rensembles to obtain such rich representations. Although training a deep network\\rfrom scratch using such dropout rates is virtually impossible, fine-tuning a\\rlarge pre-trained model under such conditions is not only possible but also\\rachieves out-of-distribution performances that exceed those of both ensembles\\rand weight averaging methods such as model soups. This result has practical\\rsignificance because the importance of the fine-tuning scenario has\\rconsiderably grown in recent years. This result also provides interesting\\rinsights on the nature of rich representations and on the intrinsically linear\\rnature of fine-tuning a large network using a comparatively small dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00946 ,  6186kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00976 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 20:56:14 GMT   (2908kb,D)\\r\\rTitle: Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor\\rAuthors: Junlin Song, Antoine Richard, Miguel Olivares-Mendez\\rCategories: cs.RO cs.CV\\rComments: Accepted by 3DV 2024\\r\\\\\\\\\\r  In robotics, motion capture systems have been widely used to measure the\\raccuracy of localization algorithms. Moreover, this infrastructure can also be\\rused for other computer vision tasks, such as the evaluation of Visual\\r(-Inertial) SLAM dynamic initialization, multi-object tracking, or automatic\\rannotation. Yet, to work optimally, these functionalities require having\\raccurate and reliable spatial-temporal calibration parameters between the\\rcamera and the global pose sensor. In this study, we provide two novel\\rsolutions to estimate these calibration parameters. Firstly, we design an\\roffline target-based method with high accuracy and consistency.\\rSpatial-temporal parameters, camera intrinsic, and trajectory are optimized\\rsimultaneously. Then, we propose an online target-less method, eliminating the\\rneed for a calibration target and enabling the estimation of time-varying\\rspatial-temporal parameters. Additionally, we perform detailed observability\\ranalysis for the target-less method. Our theoretical findings regarding\\robservability are validated by simulation experiments and provide explainable\\rguidelines for calibration. Finally, the accuracy and consistency of two\\rproposed methods are evaluated with hand-held real-world datasets where\\rtraditional hand-eye calibration method do not work.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00976 ,  2908kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00991 (*cross-listing*)\\rDate: Fri, 1 Mar 2024 21:27:03 GMT   (7300kb,D)\\r\\rTitle: SELFI: Autonomous Self-Improvement with Reinforcement Learning for\\r  Social Navigation\\rAuthors: Noriaki Hirose, Dhruv Shah, Kyle Stachowicz, Ajay Sridhar and Sergey\\r  Levine\\rCategories: cs.RO cs.CV cs.LG\\rComments: 11pages, 13 figures, 2 tables\\r\\\\\\\\\\r  Autonomous self-improving robots that interact and improve with experience\\rare key to the real-world deployment of robotic systems. In this paper, we\\rpropose an online learning method, SELFI, that leverages online robot\\rexperience to rapidly fine-tune pre-trained control policies efficiently. SELFI\\rapplies online model-free reinforcement learning on top of offline model-based\\rlearning to bring out the best parts of both learning paradigms. Specifically,\\rSELFI stabilizes the online learning process by incorporating the same\\rmodel-based learning objective from offline pre-training into the Q-values\\rlearned with online model-free reinforcement learning. We evaluate SELFI in\\rmultiple real-world environments and report improvements in terms of collision\\ravoidance, as well as more socially compliant behavior, measured by a human\\ruser study. SELFI enables us to quickly learn useful robotic behaviors with\\rless human interventions such as pre-emptive behavior for the pedestrians,\\rcollision avoidance for small and transparent objects, and avoiding travel on\\runeven floor surfaces. We provide supplementary videos to demonstrate the\\rperformance of our fine-tuned policy on our project page.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00991 ,  7300kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01053 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 00:56:05 GMT   (1223kb,D)\\r\\rTitle: Seeing Unseen: Discover Novel Biomedical Concepts via\\r  GeometryConstrained Probabilistic Modeling\\rAuthors: Jianan Fan, Dongnan Liu, Hang Chang, Heng Huang, Mei Chen, and Weidong\\r  Cai\\rCategories: cs.LG cs.AI cs.CV\\rComments: CVPR 2024\\r\\\\\\\\\\r  Machine learning holds tremendous promise for transforming the fundamental\\rpractice of scientific discovery by virtue of its data-driven nature. With the\\rever-increasing stream of research data collection, it would be appealing to\\rautonomously explore patterns and insights from observational data for\\rdiscovering novel classes of phenotypes and concepts. However, in the\\rbiomedical domain, there are several challenges inherently presented in the\\rcumulated data which hamper the progress of novel class discovery. The\\rnon-i.i.d. data distribution accompanied by the severe imbalance among\\rdifferent groups of classes essentially leads to ambiguous and biased semantic\\rrepresentations. In this work, we present a geometry-constrained probabilistic\\rmodeling treatment to resolve the identified issues. First, we propose to\\rparameterize the approximated posterior of instance embedding as a marginal von\\rMisesFisher distribution to account for the interference of distributional\\rlatent bias. Then, we incorporate a suite of critical geometric properties to\\rimpose proper constraints on the layout of constructed embedding space, which\\rin turn minimizes the uncontrollable risk for unknown class learning and\\rstructuring. Furthermore, a spectral graph-theoretic method is devised to\\restimate the number of potential novel classes. It inherits two intriguing\\rmerits compared to existent approaches, namely high computational efficiency\\rand flexibility for taxonomy-adaptive estimation. Extensive experiments across\\rvarious biomedical scenarios substantiate the effectiveness and general\\rapplicability of our method.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01053 ,  1223kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01087 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 04:07:24 GMT   (3384kb,D)\\r\\rTitle: Towards Accurate Lip-to-Speech Synthesis in-the-Wild\\rAuthors: Sindhu Hegde, Rudrabha Mukhopadhyay, C.V. Jawahar, Vinay Namboodiri\\rCategories: cs.MM cs.CV cs.SD eess.AS\\rComments: 8 pages of content, 1 page of references and 4 figures\\rJournal-ref: In Proceedings of the 31st ACM International Conference on\\r  Multimedia, 2023\\rDOI: 10.1145/3581783.3611787\\r\\\\\\\\\\r  In this paper, we introduce a novel approach to address the task of\\rsynthesizing speech from silent videos of any in-the-wild speaker solely based\\ron lip movements. The traditional approach of directly generating speech from\\rlip videos faces the challenge of not being able to learn a robust language\\rmodel from speech alone, resulting in unsatisfactory outcomes. To overcome this\\rissue, we propose incorporating noisy text supervision using a state-of-the-art\\rlip-to-text network that instills language information into our model. The\\rnoisy text is generated using a pre-trained lip-to-text model, enabling our\\rapproach to work without text annotations during inference. We design a visual\\rtext-to-speech network that utilizes the visual stream to generate accurate\\rspeech, which is in-sync with the silent input video. We perform extensive\\rexperiments and ablation studies, demonstrating our approach's superiority over\\rthe current state-of-the-art methods on various benchmark datasets. Further, we\\rdemonstrate an essential practical application of our method in assistive\\rtechnology by generating speech for an ALS patient who has lost the voice but\\rcan make mouth movements. Our demo video, code, and additional details can be\\rfound at\\r\\\\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01087 ,  3384kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01189 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 12:06:42 GMT   (43875kb,D)\\r\\rTitle: Training Unbiased Diffusion Models From Biased Dataset\\rAuthors: Yeongmin Kim, Byeonghu Na, Minsang Park, JoonHo Jang, Dongjun Kim,\\r  Wanmo Kang, Il-Chul Moon\\rCategories: cs.LG cs.CV\\rComments: International Conference on Learning Representations (ICLR 2024)\\r\\\\\\\\\\r  With significant advancements in diffusion models, addressing the potential\\rrisks of dataset bias becomes increasingly important. Since generated outputs\\rdirectly suffer from dataset bias, mitigating latent bias becomes a key factor\\rin improving sample quality and proportion. This paper proposes time-dependent\\rimportance reweighting to mitigate the bias for the diffusion models. We\\rdemonstrate that the time-dependent density ratio becomes more precise than\\rprevious approaches, thereby minimizing error propagation in generative\\rlearning. While directly applying it to score-matching is intractable, we\\rdiscover that using the time-dependent density ratio both for reweighting and\\rscore correction can lead to a tractable form of the objective function to\\rregenerate the unbiased data density. Furthermore, we theoretically establish a\\rconnection with traditional score-matching, and we demonstrate its convergence\\rto an unbiased distribution. The experimental evidence supports the usefulness\\rof the proposed method, which outperforms baselines including time-independent\\rimportance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various\\rbias settings. Our code is available at https://github.com/alsdudrla10/TIW-DSM.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01189 ,  43875kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01306 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 20:36:10 GMT   (55404kb,D)\\r\\rTitle: ICC: Quantifying Image Caption Concreteness for Multimodal Dataset\\r  Curation\\rAuthors: Moran Yanuka, Morris Alper, Hadar Averbuch-Elor and Raja Giryes\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Web-scale training on paired text-image data is becoming increasingly central\\rto multimodal learning, but is challenged by the highly noisy nature of\\rdatasets in the wild. Standard data filtering approaches succeed in removing\\rmismatched text-image pairs, but permit semantically related but highly\\rabstract or subjective text. These approaches lack the fine-grained ability to\\risolate the most concrete samples that provide the strongest signal for\\rlearning in a noisy dataset. In this work, we propose a new metric, image\\rcaption concreteness, that evaluates caption text without an image reference to\\rmeasure its concreteness and relevancy for use in multimodal learning. Our\\rapproach leverages strong foundation models for measuring visual-semantic\\rinformation loss in multimodal representations. We demonstrate that this\\rstrongly correlates with human evaluation of concreteness in both single-word\\rand sentence-level texts. Moreover, we show that curation using ICC complements\\rexisting approaches: It succeeds in selecting the highest quality samples from\\rmultimodal web-scale datasets to allow for efficient training in\\rresource-constrained settings.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01306 ,  55404kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01329 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 22:27:44 GMT   (26234kb,D)\\r\\rTitle: Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow\\r  Models\\rAuthors: Neta Shaul, Uriel Singer, Ricky T. Q. Chen, Matthew Le, Ali Thabet,\\r  Albert Pumarola, Yaron Lipman\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\\\r  This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver\\rdistillation approach to improve sample efficiency of Diffusion and Flow\\rmodels. BNS solvers are based on a family of non-stationary solvers that\\rprovably subsumes existing numerical ODE solvers and consequently demonstrate\\rconsiderable improvement in sample approximation (PSNR) over these baselines.\\rCompared to model distillation, BNS solvers benefit from a tiny parameter space\\r($<$200 parameters), fast optimization (two orders of magnitude faster),\\rmaintain diversity of samples, and in contrast to previous solver distillation\\rapproaches nearly close the gap from standard distillation methods such as\\rProgressive Distillation in the low-medium NFE regime. For example, BNS solver\\rachieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We\\rexperimented with BNS solvers for conditional image generation, text-to-image\\rgeneration, and text-2-audio generation showing significant improvement in\\rsample approximation (PSNR) in all.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01329 ,  26234kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01344 (*cross-listing*)\\rDate: Sat, 2 Mar 2024 23:37:16 GMT   (3988kb,D)\\r\\rTitle: Mitigating the Bias in the Model for Continual Test-Time Adaptation\\rAuthors: Inseop Chung, Kyomin Hwang, Jayeon Yoo, Nojun Kwak\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt\\ra source pre-trained model to continually changing target domains. In the CTA\\rsetting, a model does not know when the target domain changes, thus facing a\\rdrastic change in the distribution of streaming inputs during the test-time.\\rThe key challenge is to keep adapting the model to the continually changing\\rtarget domains in an online manner. We find that a model shows highly biased\\rpredictions as it constantly adapts to the chaining distribution of the target\\rdata. It predicts certain classes more often than other classes, making\\rinaccurate over-confident predictions. This paper mitigates this issue to\\rimprove performance in the CTA scenario. To alleviate the bias issue, we make\\rclass-wise exponential moving average target prototypes with reliable target\\rsamples and exploit them to cluster the target features class-wisely. Moreover,\\rwe aim to align the target distributions to the source distribution by\\ranchoring the target feature to its corresponding source prototype. With\\rextensive experiments, our proposed method achieves noteworthy performance gain\\rwhen applied on top of existing CTA methods without substantial adaptation time\\roverhead.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01344 ,  3988kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01362 (*cross-listing*)\\rDate: Sun, 3 Mar 2024 01:36:11 GMT   (1929kb)\\r\\rTitle: Enhancing Retinal Vascular Structure Segmentation in Images With a Novel\\r  Design Two-Path Interactive Fusion Module Model\\rAuthors: Rui Yang and Shunpu Zhang\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Precision in identifying and differentiating micro and macro blood vessels in\\rthe retina is crucial for the diagnosis of retinal diseases, although it poses\\ra significant challenge. Current autoencoding-based segmentation approaches\\rencounter limitations as they are constrained by the encoder and undergo a\\rreduction in resolution during the encoding stage. The inability to recover\\rlost information in the decoding phase further impedes these approaches.\\rConsequently, their capacity to extract the retinal microvascular structure is\\rrestricted. To address this issue, we introduce Swin-Res-Net, a specialized\\rmodule designed to enhance the precision of retinal vessel segmentation.\\rSwin-Res-Net utilizes the Swin transformer which uses shifted windows with\\rdisplacement for partitioning, to reduce network complexity and accelerate\\rmodel convergence. Additionally, the model incorporates interactive fusion with\\ra functional module in the Res2Net architecture. The Res2Net leverages\\rmulti-scale techniques to enlarge the receptive field of the convolutional\\rkernel, enabling the extraction of additional semantic information from the\\rimage. This combination creates a new module that enhances the localization and\\rseparation of micro vessels in the retina. To improve the efficiency of\\rprocessing vascular information, we've added a module to eliminate redundant\\rinformation between the encoding and decoding steps.\\r  Our proposed architecture produces outstanding results, either meeting or\\rsurpassing those of other published models. The AUC reflects significant\\renhancements, achieving values of 0.9956, 0.9931, and 0.9946 in pixel-wise\\rsegmentation of retinal vessels across three widely utilized datasets:\\rCHASE-DB1, DRIVE, and STARE, respectively. Moreover, Swin-Res-Net outperforms\\ralternative architectures, demonstrating superior performance in both IOU and\\rF1 measure metrics.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01362 ,  1929kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01449 (*cross-listing*)\\rDate: Sun, 3 Mar 2024 09:07:16 GMT   (7401kb,D)\\r\\rTitle: DUFOMap: Efficient Dynamic Awareness Mapping\\rAuthors: Daniel Duberg, Qingwen Zhang, MingKai Jia, Patric Jensfelt\\rCategories: cs.RO cs.CV\\rComments: The first two authors hold equal contribution. 8 pages, 7 figures,\\r  project page https://kin-zhang.github.io/dufomap\\r\\\\\\\\\\r  The dynamic nature of the real world is one of the main challenges in\\rrobotics. The first step in dealing with it is to detect which parts of the\\rworld are dynamic. A typical benchmark task is to create a map that contains\\ronly the static part of the world to support, for example, localization and\\rplanning. Current solutions are often applied in post-processing, where\\rparameter tuning allows the user to adjust the setting for a specific dataset.\\rIn this paper, we propose DUFOMap, a novel dynamic awareness mapping framework\\rdesigned for efficient online processing. Despite having the same parameter\\rsettings for all scenarios, it performs better or is on par with\\rstate-of-the-art methods. Ray casting is utilized to identify and classify\\rfully observed empty regions. Since these regions have been observed empty, it\\rfollows that anything inside them at another time must be dynamic. Evaluation\\ris carried out in various scenarios, including outdoor environments in KITTI\\rand Argoverse 2, open areas on the KTH campus, and with different sensor types.\\rDUFOMap outperforms the state of the art in terms of accuracy and computational\\refficiency. The source code, benchmarks, and links to the datasets utilized are\\rprovided. See https://kin-zhang.github.io/dufomap for more details.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01449 ,  7401kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01485 (*cross-listing*)\\rDate: Sun, 3 Mar 2024 11:36:35 GMT   (2782kb,D)\\r\\rTitle: Approximations to the Fisher Information Metric of Deep Generative\\r  Models for Out-Of-Distribution Detection\\rAuthors: Sam Dauncey, Chris Holmes, Christopher Williams and Fabian Falck\\rCategories: stat.ML cs.CV cs.LG\\r\\\\\\\\\\r  Likelihood-based deep generative models such as score-based diffusion models\\rand variational autoencoders are state-of-the-art machine learning models\\rapproximating high-dimensional distributions of data such as images, text, or\\raudio. One of many downstream tasks they can be naturally applied to is\\rout-of-distribution (OOD) detection. However, seminal work by Nalisnick et al.\\rwhich we reproduce showed that deep generative models consistently infer higher\\rlog-likelihoods for OOD data than data they were trained on, marking an open\\rproblem. In this work, we analyse using the gradient of a data point with\\rrespect to the parameters of the deep generative model for OOD detection, based\\ron the simple intuition that OOD data should have larger gradient norms than\\rtraining data. We formalise measuring the size of the gradient as approximating\\rthe Fisher information metric. We show that the Fisher information matrix (FIM)\\rhas large absolute diagonal values, motivating the use of chi-square\\rdistributed, layer-wise gradient norms as features. We combine these features\\rto make a simple, model-agnostic and hyperparameter-free method for OOD\\rdetection which estimates the joint density of the layer-wise gradient norms\\rfor a given data point. We find that these layer-wise gradient norms are weakly\\rcorrelated, rendering their combined usage informative, and prove that the\\rlayer-wise gradient norms satisfy the principle of (data representation)\\rinvariance. Our empirical results indicate that this method outperforms the\\rTypicality test for most deep generative models and image dataset pairings.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01485 ,  2782kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01513 (*cross-listing*)\\rDate: Sun, 3 Mar 2024 13:36:07 GMT   (757kb)\\r\\rTitle: CDSE-UNet: Enhancing COVID-19 CT Image Segmentation with Canny Edge\\r  Detection and Dual-Path SENet Feature Fusion\\rAuthors: Jiao Ding, Jie Chang, Renrui Han, Li Yang\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Accurate segmentation of COVID-19 CT images is crucial for reducing the\\rseverity and mortality rates associated with COVID-19 infections. In response\\rto blurred boundaries and high variability characteristic of lesion areas in\\rCOVID-19 CT images, we introduce CDSE-UNet: a novel UNet-based segmentation\\rmodel that integrates Canny operator edge detection and a dual-path SENet\\rfeature fusion mechanism. This model enhances the standard UNet architecture by\\remploying the Canny operator for edge detection in sample images, paralleling\\rthis with a similar network structure for semantic feature extraction. A key\\rinnovation is the Double SENet Feature Fusion Block, applied across\\rcorresponding network layers to effectively combine features from both image\\rpaths. Moreover, we have developed a Multiscale Convolution approach, replacing\\rthe standard Convolution in UNet, to adapt to the varied lesion sizes and\\rshapes. This addition not only aids in accurately classifying lesion edge\\rpixels but also significantly improves channel differentiation and expands the\\rcapacity of the model. Our evaluations on public datasets demonstrate\\rCDSE-UNet's superior performance over other leading models, particularly in\\rsegmenting large and small lesion areas, accurately delineating lesion edges,\\rand effectively suppressing noise\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01513 ,  757kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01598 (*cross-listing*)\\rDate: Sun, 3 Mar 2024 19:52:43 GMT   (20353kb,D)\\r\\rTitle: APISR: Anime Production Inspired Real-World Anime Super-Resolution\\rAuthors: Boyang Wang, Fengyu Yang, Xihang Yu, Chao Zhang, Hanbin Zhao\\rCategories: eess.IV cs.AI cs.CV\\r\\\\\\\\\\r  While real-world anime super-resolution (SR) has gained increasing attention\\rin the SR community, existing methods still adopt techniques from the\\rphotorealistic domain. In this paper, we analyze the anime production workflow\\rand rethink how to use characteristics of it for the sake of the real-world\\ranime SR. First, we argue that video networks and datasets are not necessary\\rfor anime SR due to the repetition use of hand-drawing frames. Instead, we\\rpropose an anime image collection pipeline by choosing the least compressed and\\rthe most informative frames from the video sources. Based on this pipeline, we\\rintroduce the Anime Production-oriented Image (API) dataset. In addition, we\\ridentify two anime-specific challenges of distorted and faint hand-drawn lines\\rand unwanted color artifacts. We address the first issue by introducing a\\rprediction-oriented compression module in the image degradation model and a\\rpseudo-ground truth preparation with enhanced hand-drawn lines. In addition, we\\rintroduce the balanced twin perceptual loss combining both anime and\\rphotorealistic high-level features to mitigate unwanted color artifacts and\\rincrease visual clarity. We evaluate our method through extensive experiments\\ron the public benchmark, showing our method outperforms state-of-the-art\\rapproaches by a large margin.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01598 ,  20353kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01633 (*cross-listing*)\\rDate: Sun, 3 Mar 2024 22:43:47 GMT   (14254kb,D)\\r\\rTitle: Critical windows: non-asymptotic theory for feature emergence in\\r  diffusion models\\rAuthors: Marvin Li and Sitan Chen\\rCategories: cs.LG cs.CV stat.ML\\r\\\\\\\\\\r  We develop theory to understand an intriguing property of diffusion models\\rfor image generation that we term critical windows. Empirically, it has been\\robserved that there are narrow time intervals in sampling during which\\rparticular features of the final image emerge, e.g. the image class or\\rbackground color (Ho et al., 2020b; Georgiev et al., 2023; Raya & Ambrogioni,\\r2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous\\rfor interpretability as it implies one can localize properties of the\\rgeneration to a small segment of the trajectory, it seems at odds with the\\rcontinuous nature of the diffusion. We propose a formal framework for studying\\rthese windows and show that for data coming from a mixture of strongly\\rlog-concave densities, these windows can be provably bounded in terms of\\rcertain measures of inter- and intra-group separation. We also instantiate\\rthese bounds for concrete examples like well-conditioned Gaussian mixtures.\\rFinally, we use our bounds to give a rigorous interpretation of diffusion\\rmodels as hierarchical samplers that progressively decide output features\\rover a discrete sequence of times. We validate our bounds with synthetic\\rexperiments. Additionally, preliminary experiments on Stable Diffusion suggest\\rcritical windows may serve as a useful tool for diagnosing fairness and privacy\\rviolations in real-world diffusion models.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01633 ,  14254kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01666 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 01:33:53 GMT   (6702kb,D)\\r\\rTitle: Improving Adversarial Energy-Based Model via Diffusion Process\\rAuthors: Cong Geng, Tian Han, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, S{\\\\o}ren\\r  Hauberg, Bo Li\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Generative models have shown strong generation ability while efficient\\rlikelihood estimation is less explored. Energy-based models~(EBMs) define a\\rflexible energy function to parameterize unnormalized densities efficiently but\\rare notorious for being difficult to train. Adversarial EBMs introduce a\\rgenerator to form a minimax training game to avoid expensive MCMC sampling used\\rin traditional EBMs, but a noticeable gap between adversarial EBMs and other\\rstrong generative models still exists. Inspired by diffusion-based models, we\\rembedded EBMs into each denoising step to split a long-generated process into\\rseveral smaller steps. Besides, we employ a symmetric Jeffrey divergence and\\rintroduce a variational posterior distribution for the generator's training to\\raddress the main challenges that exist in adversarial EBMs. Our experiments\\rshow significant improvement in generation compared to existing adversarial\\rEBMs, while also providing a useful energy function for efficient density\\restimation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01666 ,  6702kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01692 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 02:52:29 GMT   (7402kb,D)\\r\\rTitle: PI-AstroDeconv: A Physics-Informed Unsupervised Learning Method for\\r  Astronomical Image Deconvolution\\rAuthors: Shulei Ni, Yisheng Qiu, Yunchun Chen, Zihao Song, Hao Chen, Xuejian\\r  Jiang, and Huaxi Chen\\rCategories: astro-ph.IM astro-ph.GA cs.CV eess.IV\\r\\\\\\\\\\r  In the imaging process of an astronomical telescope, the deconvolution of its\\rbeam or Point Spread Function (PSF) is a crucial task. However, deconvolution\\rpresents a classical and challenging inverse computation problem. In scenarios\\rwhere the beam or PSF is complex or inaccurately measured, such as in\\rinterferometric arrays and certain radio telescopes, the resultant blurry\\rimages are often challenging to interpret visually or analyze using traditional\\rphysical detection methods. We argue that traditional methods frequently lack\\rspecific prior knowledge, thereby leading to suboptimal performance. To address\\rthis issue and achieve image deconvolution and reconstruction, we propose an\\runsupervised network architecture that incorporates prior physical information.\\rThe network adopts an encoder-decoder structure while leveraging the\\rtelescope's PSF as prior knowledge. During network training, we introduced\\raccelerated Fast Fourier Transform (FFT) convolution to enable efficient\\rprocessing of high-resolution input images and PSFs. We explored various\\rclassic regression networks, including autoencoder (AE) and U-Net, and\\rconducted a comprehensive performance evaluation through comparative analysis.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01692 ,  7402kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01758 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 06:24:24 GMT   (1389kb,D)\\r\\rTitle: AFBT GAN: enhanced explainability and diagnostic performance for\\r  cognitive decline by counterfactual generative adversarial network\\rAuthors: Xiongri Shen, Zhenxi Song, Zhiguo Zhang\\rCategories: eess.IV cs.CV cs.LG\\rComments: 10 pages, 5 figures\\r\\\\\\\\\\r  Existing explanation results of functional connectivity (FC) are normally\\rgenerated by using classification result labels and correlation analysis\\rmethods such as Pearson's correlation or gradient backward. However, the\\rdiagnostic model is still trained on the black box model and might lack the\\rattention of FCs in important regions during the training. To enhance the\\rexplainability and improve diagnostic performance, providing prior knowledge on\\rneurodegeneration-related regions when healthy subjects (HC) develop into\\rsubject cognitive decline (SCD) and mild cognitive impairment (MCI) for the\\rdiagnostic model is a key step. To better determine the\\rneurodegeneration-related regions, we employ counterfactual reasoning to\\rgenerate the target label FC matrices derived from source label FC and then\\rsubtract source label FC with target label FC. The counterfactual reasoning\\rarchitecture is constructed by adaptive forward and backward transformer\\rgenerative adversarial network (AFBT GAN), which is specifically designed by\\rnetwork property in FC and inverse patch embedding operation in the\\rtransformer. The specific design can make the model focus more on the current\\rnetwork correlation and employ the global insight of the transformer to\\rreconstruct FC, which both help the generation of high-quality target label FC.\\rThe validation experiments are conducted on both clinical and public datasets,\\rthe generated attention map are both vital correlated to cognitive function and\\rthe diagnostic performance is also significant. The code is available at\\rhttps://github.com/SXR3015/AFBT-GAN.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01758 ,  1389kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01759 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 06:25:26 GMT   (3406kb,D)\\r\\rTitle: Open-world Machine Learning: A Review and New Outlooks\\rAuthors: Fei Zhu, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang,\\r  Cheng-Lin Liu\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Machine learning has achieved remarkable success in many applications.\\rHowever, existing studies are largely based on the closed-world assumption,\\rwhich assumes that the environment is stationary, and the model is fixed once\\rdeployed. In many real-world applications, this fundamental and rather naive\\rassumption may not hold because an open environment is complex, dynamic, and\\rfull of unknowns. In such cases, rejecting unknowns, discovering novelties, and\\rthen incrementally learning them, could enable models to be safe and evolve\\rcontinually as biological systems do. This paper provides a holistic view of\\ropen-world machine learning by investigating unknown rejection, novel class\\rdiscovery, and class-incremental learning in a unified paradigm. The\\rchallenges, principles, and limitations of current methodologies are discussed\\rin detail. Finally, we discuss several potential directions for future\\rresearch. This paper aims to provide a comprehensive introduction to the\\remerging open-world machine learning paradigm, to help researchers build more\\rpowerful AI systems in their respective fields, and to promote the development\\rof artificial general intelligence.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01759 ,  3406kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01766 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 06:47:06 GMT   (21639kb,D)\\r\\rTitle: Improving Visual Perception of a Social Robot for Controlled and\\r  In-the-wild Human-robot Interaction\\rAuthors: Wangjie Zhong, Leimin Tian, Duy Tho Le, Hamid Rezatofighi\\rCategories: cs.RO cs.CV\\rComments: accepted to HRI 2023 (LBR track)\\rDOI: 10.1145/3610978.3640648\\r\\\\\\\\\\r  Social robots often rely on visual perception to understand their users and\\rthe environment. Recent advancements in data-driven approaches for computer\\rvision have demonstrated great potentials for applying deep-learning models to\\renhance a social robot's visual perception. However, the high computational\\rdemands of deep-learning methods, as opposed to the more resource-efficient\\rshallow-learning models, bring up important questions regarding their effects\\ron real-world interaction and user experience. It is unclear how will the\\robjective interaction performance and subjective user experience be influenced\\rwhen a social robot adopts a deep-learning based visual perception model. We\\remployed state-of-the-art human perception and tracking models to improve the\\rvisual perception function of the Pepper robot and conducted a controlled lab\\rstudy and an in-the-wild human-robot interaction study to evaluate this novel\\rperception function for following a specific user with other people present in\\rthe scene.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01766 ,  21639kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01845 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 08:51:38 GMT   (1159kb,D)\\r\\rTitle: NASH: Neural Architecture Search for Hardware-Optimized Machine Learning\\r  Models\\rAuthors: Mengfei Ji, Zaid Al-Ars\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\\\r  As machine learning (ML) algorithms get deployed in an ever-increasing number\\rof applications, these algorithms need to achieve better trade-offs between\\rhigh accuracy, high throughput and low latency. This paper introduces NASH, a\\rnovel approach that applies neural architecture search to machine learning\\rhardware. Using NASH, hardware designs can achieve not only high throughput and\\rlow latency but also superior accuracy performance. We present four versions of\\rthe NASH strategy in this paper, all of which show higher accuracy than the\\roriginal models. The strategy can be applied to various convolutional neural\\rnetworks, selecting specific model operations among many to guide the training\\rprocess toward higher accuracy. Experimental results show that applying NASH on\\rResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top\\r5 accuracy increase of up to 2.2% compared to the non-NASH version when tested\\ron the ImageNet data set. We also integrated this approach into the FINN\\rhardware model synthesis tool to automate the application of our approach and\\rthe generation of the hardware model. Results show that using FINN can achieve\\ra maximum throughput of 324.5 fps. In addition, NASH models can also result in\\ra better trade-off between accuracy and hardware resource utilization. The\\raccuracy-hardware (HW) Pareto curve shows that the models with the four NASH\\rversions represent the best trade-offs achieving the highest accuracy for a\\rgiven HW utilization. The code for our implementation is open-source and\\rpublicly available on GitHub at https://github.com/MFJI/NASH.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01845 ,  1159kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01861 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 09:18:13 GMT   (9335kb,D)\\r\\rTitle: AiSDF: Structure-aware Neural Signed Distance Fields in Indoor Scenes\\rAuthors: Jaehoon Jang, Inha Lee, Minje Kim, Kyungdon Joo\\rCategories: cs.RO cs.AI cs.CV\\rComments: 8 pages, 6 figures, Accepted to IEEE RA-L (First two authors\\r  contributed equally)\\r\\\\\\\\\\r  Indoor scenes we are living in are visually homogenous or textureless, while\\rthey inherently have structural forms and provide enough structural priors for\\r3D scene reconstruction. Motivated by this fact, we propose a structure-aware\\ronline signed distance fields (SDF) reconstruction framework in indoor scenes,\\respecially under the Atlanta world (AW) assumption. Thus, we dub this\\rincremental SDF reconstruction for AW as AiSDF. Within the online framework, we\\rinfer the underlying Atlanta structure of a given scene and then estimate\\rplanar surfel regions supporting the Atlanta structure. This Atlanta-aware\\rsurfel representation provides an explicit planar map for a given scene. In\\raddition, based on these Atlanta planar surfel regions, we adaptively sample\\rand constrain the structural regularity in the SDF reconstruction, which\\renables us to improve the reconstruction quality by maintaining a high-level\\rstructure while enhancing the details of a given scene. We evaluate the\\rproposed AiSDF on the ScanNet and ReplicaCAD datasets, where we demonstrate\\rthat the proposed framework is capable of reconstructing fine details of\\robjects implicitly, as well as structures explicitly in room-scale scenes.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01861 ,  9335kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01868 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 09:23:11 GMT   (8702kb,D)\\r\\rTitle: Map-aided annotation for pole base detection\\rAuthors: Benjamin Missaoui (Heudiasyc), Maxime Noizet (Heudiasyc), Philippe Xu\\r  (Heudiasyc)\\rCategories: eess.IV cs.CV\\rJournal-ref: 35th IEEE Intelligent Vehicles Symposium (IV 2023), Jun 2023,\\r  Anchorage, AK, United States\\rDOI: 10.1109/IV55152.2023.10186774\\r\\\\\\\\\\r  For autonomous navigation, high definition maps are a widely used source of\\rinformation. Pole-like features encoded in HD maps such as traffic signs,\\rtraffic lights or street lights can be used as landmarks for localization. For\\rthis purpose, they first need to be detected by the vehicle using its embedded\\rsensors. While geometric models can be used to process 3D point clouds\\rretrieved by lidar sensors, modern image-based approaches rely on deep neural\\rnetwork and therefore heavily depend on annotated training data. In this paper,\\ra 2D HD map is used to automatically annotate pole-like features in images. In\\rthe absence of height information, the map features are represented as pole\\rbases at the ground level. We show how an additional lidar sensor can be used\\rto filter out occluded features and refine the ground projection. We also\\rdemonstrate how an object detector can be trained to detect a pole base. To\\revaluate our methodology, it is first validated with data manually annotated\\rfrom semantic segmentation and then compared to our own automatically generated\\rannotated data recorded in the city of Compi{\\\\`e}gne, France. Erratum: In the\\roriginal version [1], an error occurred in the accuracy evaluation of the\\rdifferent models studied and the evaluation method applied on the detection\\rresults was not clearly defined. In this revision, we offer a rectification to\\rthis segment, presenting updated results, especially in terms of Mean Absolute\\rErrors (MAE).\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01868 ,  8702kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01927 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 10:44:57 GMT   (917kb,D)\\r\\rTitle: Advancing Gene Selection in Oncology: A Fusion of Deep Learning and\\r  Sparsity for Precision Gene Selection\\rAuthors: Akhila Krishna, Ravi Kant Gupta, Pranav Jeevan, Amit Sethi\\rCategories: q-bio.GN cs.CV q-bio.QM q-bio.TO\\r\\\\\\\\\\r  Gene selection plays a pivotal role in oncology research for improving\\routcome prediction accuracy and facilitating cost-effective genomic profiling\\rfor cancer patients. This paper introduces two gene selection strategies for\\rdeep learning-based survival prediction models. The first strategy uses a\\rsparsity-inducing method while the second one uses importance based gene\\rselection for identifying relevant genes. Our overall approach leverages the\\rpower of deep learning to model complex biological data structures, while\\rsparsity-inducing methods ensure the selection process focuses on the most\\rinformative genes, minimizing noise and redundancy. Through comprehensive\\rexperimentation on diverse genomic and survival datasets, we demonstrate that\\rour strategy not only identifies gene signatures with high predictive power for\\rsurvival outcomes but can also streamlines the process for low-cost genomic\\rprofiling. The implications of this research are profound as it offers a\\rscalable and effective tool for advancing personalized medicine and targeted\\rcancer therapies. By pushing the boundaries of gene selection methodologies,\\rour work contributes significantly to the ongoing efforts in cancer genomics,\\rpromising improved diagnostic and prognostic capabilities in clinical settings.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01927 ,  917kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01977 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 12:20:29 GMT   (5551kb,D)\\r\\rTitle: TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation\\r  under Visual Corruptions\\rAuthors: Maytus Piriyajitakonkij, Mingfei Sun, Mengmi Zhang, Wei Pan\\rCategories: cs.RO cs.AI cs.CV\\rComments: Under review. Code will be available soon\\r\\\\\\\\\\r  Robot navigation under visual corruption presents a formidable challenge. To\\raddress this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,\\rfor point-goal navigation under visual corruptions. Our plug-and-play method\\rincorporates a top-down decoder to a pre-trained navigation model. Firstly, the\\rpre-trained navigation model gets a corrupted image and extracts features.\\rSecondly, the top-down decoder produces the reconstruction given the high-level\\rfeatures extracted by the pre-trained model. Then, it feeds the reconstruction\\rof a corrupted image back to the pre-trained model. Finally, the pre-trained\\rmodel does forward pass again to output action. Despite being trained solely on\\rclean images, the top-down decoder can reconstruct cleaner images from\\rcorrupted ones without the need for gradient-based adaptation. The pre-trained\\rnavigation model with our top-down decoder significantly enhances navigation\\rperformance across almost all visual corruptions in our benchmarks. Our method\\rimproves the success rate of point-goal navigation from the state-of-the-art\\rresult of 46% to 94% on the most severe corruption. This suggests its potential\\rfor broader application in robotic visual navigation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01977 ,  5551kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02043 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 13:47:49 GMT   (1929kb,D)\\r\\rTitle: Iterative Occlusion-Aware Light Field Depth Estimation using 4D\\r  Geometrical Cues\\rAuthors: Rui Louren\\\\c{c}o, Lucas Thomaz, Eduardo A. B. Silva, Sergio M. M.\\r  Faria\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Light field cameras and multi-camera arrays have emerged as promising\\rsolutions for accurately estimating depth by passively capturing light\\rinformation. This is possible because the 3D information of a scene is embedded\\rin the 4D light field geometry. Commonly, depth estimation methods extract this\\rinformation relying on gradient information, heuristic-based optimisation\\rmodels, or learning-based approaches. This paper focuses mainly on explicitly\\runderstanding and exploiting 4D geometrical cues for light field depth\\restimation. Thus, a novel method is proposed, based on a non-learning-based\\roptimisation approach for depth estimation that explicitly considers surface\\rnormal accuracy and occlusion regions by utilising a fully explainable 4D\\rgeometric model of the light field. The 4D model performs depth/disparity\\restimation by determining the orientations and analysing the intersections of\\rkey 2D planes in 4D space, which are the images of 3D-space points in the 4D\\rlight field. Experimental results show that the proposed method outperforms\\rboth learning-based and non-learning-based state-of-the-art methods in terms of\\rsurface normal angle accuracy, achieving a Median Angle Error on planar\\rsurfaces, on average, 26.3\\\\% lower than the state-of-the-art, and still being\\rcompetitive with state-of-the-art methods in terms of Mean Squared Error\\r$\\\\vc{\\\\times}$ 100 and Badpix 0.07.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02043 ,  1929kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02118 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 15:21:51 GMT   (20900kb,D)\\r\\rTitle: Position Paper: Towards Implicit Prompt For Text-To-Image Models\\rAuthors: Yue Yang, Yuqi lin, Hong Liu, Wenqi Shao, Runjian Chen, Hailong Shang,\\r  Yu Wang, Yu Qiao, Kaipeng Zhang, Ping Luo\\rCategories: cs.CY cs.AI cs.CV\\r\\\\\\\\\\r  Recent text-to-image (T2I) models have had great success, and many benchmarks\\rhave been proposed to evaluate their performance and safety. However, they only\\rconsider explicit prompts while neglecting implicit prompts (hint at a target\\rwithout explicitly mentioning it). These prompts may get rid of safety\\rconstraints and pose potential threats to the applications of these models.\\rThis position paper highlights the current state of T2I models toward implicit\\rprompts. We present a benchmark named ImplicitBench and conduct an\\rinvestigation on the performance and impacts of implicit prompts with popular\\rT2I models. Specifically, we design and collect more than 2,000 implicit\\rprompts of three aspects: General Symbols, Celebrity Privacy, and\\rNot-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models'\\rcapabilities under these implicit prompts. Experiment results show that (1) T2I\\rmodels are able to accurately create various target symbols indicated by\\rimplicit prompts; (2) Implicit prompts bring potential risks of privacy leakage\\rfor T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can\\rbe bypassed with implicit prompts. We call for increased attention to the\\rpotential and risks of implicit prompts in the T2I community and further\\rinvestigation into the capabilities and impacts of implicit prompts, advocating\\rfor a balanced approach that harnesses their benefits while mitigating their\\rrisks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02118 ,  20900kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02163 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 16:11:41 GMT   (6022kb,D)\\r\\rTitle: REAL-Colon: A dataset for developing real-world AI applications in\\r  colonoscopy\\rAuthors: Carlo Biffi, Giulio Antonelli, Sebastian Bernhofer, Cesare Hassan,\\r  Daizen Hirata, Mineo Iwatate, Andreas Maieron, Pietro Salvagnini and Andrea\\r  Cherubini\\rCategories: eess.IV cs.CV\\rComments: 12 pages, 5 tables, 7 figures\\r\\\\\\\\\\r  Detection and diagnosis of colon polyps are key to preventing colorectal\\rcancer. Recent evidence suggests that AI-based computer-aided detection (CADe)\\rand computer-aided diagnosis (CADx) systems can enhance endoscopists'\\rperformance and boost colonoscopy effectiveness. However, most available public\\rdatasets primarily consist of still images or video clips, often at a\\rdown-sampled resolution, and do not accurately represent real-world colonoscopy\\rprocedures. We introduce the REAL-Colon (Real-world multi-center Endoscopy\\rAnnotated video Library) dataset: a compilation of 2.7M native video frames\\rfrom sixty full-resolution, real-world colonoscopy recordings across multiple\\rcenters. The dataset contains 350k bounding-box annotations, each created under\\rthe supervision of expert gastroenterologists. Comprehensive patient clinical\\rdata, colonoscopy acquisition information, and polyp histopathological\\rinformation are also included in each video. With its unprecedented size,\\rquality, and heterogeneity, the REAL-Colon dataset is a unique resource for\\rresearchers and developers aiming to advance AI research in colonoscopy. Its\\ropenness and transparency facilitate rigorous and reproducible research,\\rfostering the development and benchmarking of more accurate and reliable\\rcolonoscopy-related algorithms and models.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02163 ,  6022kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02192 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 16:31:58 GMT   (7715kb,D)\\r\\rTitle: Domain adaptation, Explainability & Fairness in AI for Medical Image\\r  Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans\\rAuthors: Dimitrios Kollias and Anastasios Arsenos and Stefanos Kollias\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  The paper presents the DEF-AI-MIA COV19D Competition, which is organized in\\rthe framework of the 'Domain adaptation, Explainability, Fairness in AI for\\rMedical Image Analysis (DEF-AI-MIA)' Workshop of the 2024 Computer Vision and\\rPattern Recognition (CVPR) Conference. The Competition is the 4th in the\\rseries, following the first three Competitions held in the framework of ICCV\\r2021, ECCV 2022 and ICASSP 2023 International Conferences respectively. It\\rincludes two Challenges on: i) Covid-19 Detection and ii) Covid-19 Domain\\rAdaptation. The Competition use data from COV19-CT-DB database, which is\\rdescribed in the paper and includes a large number of chest CT scan series.\\rEach chest CT scan series consists of a sequence of 2-D CT slices, the number\\rof which is between 50 and 700. Training, validation and test datasets have\\rbeen extracted from COV19-CT-DB and provided to the participants in both\\rChallenges. The paper presents the baseline models used in the Challenges and\\rthe performance which was obtained respectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02192 ,  7715kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02236 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 17:29:03 GMT   (103kb,D)\\r\\rTitle: Interpretable Models for Detecting and Monitoring Elevated Intracranial\\r  Pressure\\rAuthors: Darryl Hannan, Steven C. Nesbit, Ximing Wen, Glen Smith, Qiao Zhang,\\r  Alberto Goffi, Vincent Chan, Michael J. Morris, John C. Hunninghake, Nicholas\\r  E. Villalobos, Edward Kim, Rosina O. Weber, Christopher J. MacLellan\\rCategories: eess.IV cs.CV\\rComments: 5 pages, 2 figures, ISBI 2024\\r\\\\\\\\\\r  Detecting elevated intracranial pressure (ICP) is crucial in diagnosing and\\rmanaging various neurological conditions. These fluctuations in pressure are\\rtransmitted to the optic nerve sheath (ONS), resulting in changes to its\\rdiameter, which can then be detected using ultrasound imaging devices. However,\\rinterpreting sonographic images of the ONS can be challenging. In this work, we\\rpropose two systems that actively monitor the ONS diameter throughout an\\rultrasound video and make a final prediction as to whether ICP is elevated. To\\rconstruct our systems, we leverage subject matter expert (SME) guidance,\\rstructuring our processing pipeline according to their collection procedure,\\rwhile also prioritizing interpretability and computational efficiency. We\\rconduct a number of experiments, demonstrating that our proposed systems are\\rable to outperform various baselines. One of our SMEs then manually validates\\rour top system's performance, lending further credibility to our approach while\\rdemonstrating its potential utility in a clinical setting.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02236 ,  103kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02241 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 17:33:20 GMT   (6018kb,D)\\r\\rTitle: Neural Redshift: Random Networks are not Random Functions\\rAuthors: Damien Teney, Armand Nicolicioiu, Valentin Hartmann, Ehsan Abbasnejad\\rCategories: cs.LG cs.AI cs.CV\\rJournal-ref: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\\r  2024\\r\\\\\\\\\\r  Our understanding of the generalization capabilities of neural networks (NNs)\\ris still incomplete. Prevailing explanations are based on implicit biases of\\rgradient descent (GD) but they cannot account for the capabilities of models\\rfrom gradient-free methods nor the simplicity bias recently observed in\\runtrained networks. This paper seeks other sources of generalization in NNs.\\r  Findings. To understand the inductive biases provided by architectures\\rindependently from GD, we examine untrained, random-weight networks. Even\\rsimple MLPs show strong inductive biases: uniform sampling in weight space\\ryields a very biased distribution of functions in terms of complexity. But\\runlike common wisdom, NNs do not have an inherent simplicity bias. This\\rproperty depends on components such as ReLUs, residual connections, and layer\\rnormalizations. Alternative architectures can be built with a bias for any\\rlevel of complexity. Transformers also inherit all these properties from their\\rbuilding blocks.\\r  Implications. We provide a fresh explanation for the success of deep learning\\rindependent from gradient-based training. It points at promising avenues for\\rcontrolling the solutions implemented by trained models.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02241 ,  6018kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02307 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 18:44:30 GMT   (402kb,D)\\r\\rTitle: Harnessing Intra-group Variations Via a Population-Level Context for\\r  Pathology Detection\\rAuthors: P. Bilha Githinji, Xi Yuan, Zhenglin Chen, Ijaz Gul, Dingqi Shang, Wen\\r  Liang, Jianming Deng, Dan Zeng, Dongmei yu, Chenggang Yan, Peiwu Qin\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Realizing sufficient separability between the distributions of healthy and\\rpathological samples is a critical obstacle for pathology detection\\rconvolutional models. Moreover, these models exhibit a bias for contrast-based\\rimages, with diminished performance on texture-based medical images. This study\\rintroduces the notion of a population-level context for pathology detection and\\remploys a graph theoretic approach to model and incorporate it into the latent\\rcode of an autoencoder via a refinement module we term PopuSense. PopuSense\\rseeks to capture additional intra-group variations inherent in biomedical data\\rthat a local or global context of the convolutional model might miss or smooth\\rout. Experiments on contrast-based and texture-based images, with minimal\\radaptation, encounter the existing preference for intensity-based input.\\rNevertheless, PopuSense demonstrates improved separability in contrast-based\\rimages, presenting an additional avenue for refining representations learned by\\ra model.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02307 ,  402kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02311 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 18:47:56 GMT   (10045kb,D)\\r\\rTitle: Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications\\r  to Cardiac MRI Segmentation\\rAuthors: Yidong Zhao, Joao Tourais, Iain Pierce, Christian Nitsche, Thomas A.\\r  Treibel, Sebastian Weing\\\\artner, Artur M. Schweidtmann, Qian Tao\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Deep learning (DL)-based methods have achieved state-of-the-art performance\\rfor a wide range of medical image segmentation tasks. Nevertheless, recent\\rstudies show that deep neural networks (DNNs) can be miscalibrated and\\roverconfident, leading to silent failures that are risky} for clinical\\rapplications. Bayesian statistics provide an intuitive approach to DL failure\\rdetection, based on posterior probability estimation. However, Bayesian DL, and\\rin particular the posterior estimation, is intractable for large medical image\\rsegmentation DNNs. To tackle this challenge, we propose a Bayesian learning\\rframework by Hamiltonian Monte Carlo (HMC), tempered by cold posterior (CP) to\\raccommodate medical data augmentation, named HMC-CP. For HMC computation, we\\rfurther propose a cyclical annealing strategy, which captures both local and\\rglobal geometries of the posterior distribution, enabling highly efficient\\rBayesian DNN training with the same computational budget requirements as\\rtraining a single DNN. The resulting Bayesian DNN outputs an ensemble\\rsegmentation along with the segmentation uncertainty. We evaluate the proposed\\rHMC-CP extensively on cardiac magnetic resonance image (MRI) segmentation,\\rusing in-domain steady-state free precession (SSFP) cine images as well as\\rout-of-domain datasets of quantitative $T_1$ and $T_2$ mapping.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02311 ,  10045kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02329 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 18:57:11 GMT   (25026kb,D)\\r\\rTitle: COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against\\r  Semantic Attacks\\rAuthors: Zijian Huang, Wenda Chu, Linyi Li, Chejian Xu, Bo Li\\rCategories: cs.LG cs.CR cs.CV\\r\\\\\\\\\\r  Multi-sensor fusion systems (MSFs) play a vital role as the perception module\\rin modern autonomous vehicles (AVs). Therefore, ensuring their robustness\\ragainst common and realistic adversarial semantic transformations, such as\\rrotation and shifting in the physical world, is crucial for the safety of AVs.\\rWhile empirical evidence suggests that MSFs exhibit improved robustness\\rcompared to single-modal models, they are still vulnerable to adversarial\\rsemantic transformations. Despite the proposal of empirical defenses, several\\rworks show that these defenses can be attacked again by new adaptive attacks.\\rSo far, there is no certified defense proposed for MSFs. In this work, we\\rpropose the first robustness certification framework COMMIT certify robustness\\rof multi-sensor fusion systems against semantic attacks. In particular, we\\rpropose a practical anisotropic noise mechanism that leverages randomized\\rsmoothing with multi-modal data and performs a grid-based splitting method to\\rcharacterize complex semantic transformations. We also propose efficient\\ralgorithms to compute the certification in terms of object detection accuracy\\rand IoU for large-scale MSF models. Empirically, we evaluate the efficacy of\\rCOMMIT in different settings and provide a comprehensive benchmark of certified\\rrobustness for different MSF models using the CARLA simulation platform. We\\rshow that the certification for MSF models is at most 48.39% higher than that\\rof single-modal models, which validates the advantages of MSF models. We\\rbelieve our certification framework and benchmark will contribute an important\\rstep towards certifiably robust AVs in practice.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02329 ,  25026kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02334 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 18:58:46 GMT   (426kb,D)\\r\\rTitle: Gradient Correlation Subspace Learning against Catastrophic Forgetting\\rAuthors: Tammuz Dubnov, Vishal Thengane\\rCategories: cs.LG cs.AI cs.CV\\rComments: 5 figures; Code will be available here:\\r  https://github.com/vgthengane/GCSL\\r\\\\\\\\\\r  Efficient continual learning techniques have been a topic of significant\\rresearch over the last few years. A fundamental problem with such learning is\\rsevere degradation of performance on previously learned tasks, known also as\\rcatastrophic forgetting. This paper introduces a novel method to reduce\\rcatastrophic forgetting in the context of incremental class learning called\\rGradient Correlation Subspace Learning (GCSL). The method detects a subspace of\\rthe weights that is least affected by previous tasks and projects the weights\\rto train for the new task into said subspace. The method can be applied to one\\ror more layers of a given network architectures and the size of the subspace\\rused can be altered from layer to layer and task to task. Code will be\\ravailable at\\r\\\\href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02334 ,  426kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.02338 (*cross-listing*)\\rDate: Mon, 4 Mar 2024 18:59:30 GMT   (15272kb,D)\\r\\rTitle: Twisting Lids Off with Two Hands\\rAuthors: Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter Abbeel, Jitendra Malik\\rCategories: cs.RO cs.AI cs.CV cs.LG\\rComments: Project page can be found at https://toruowo.github.io/bimanual-twist\\r\\\\\\\\\\r  Manipulating objects with two multi-fingered hands has been a long-standing\\rchallenge in robotics, attributed to the contact-rich nature of many\\rmanipulation tasks and the complexity inherent in coordinating a\\rhigh-dimensional bimanual system. In this work, we consider the problem of\\rtwisting lids of various bottle-like objects with two hands, and demonstrate\\rthat policies trained in simulation using deep reinforcement learning can be\\reffectively transferred to the real world. With novel engineering insights into\\rphysical modeling, real-time perception, and reward design, the policy\\rdemonstrates generalization capabilities across a diverse set of unseen\\robjects, showcasing dynamic and dexterous behaviors. Our findings serve as\\rcompelling evidence that deep reinforcement learning combined with sim-to-real\\rtransfer remains a promising approach for addressing manipulation problems of\\runprecedented complexity.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.02338 ,  15272kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00834 (*cross-listing*)\\rDate: Tue, 20 Feb 2024 17:48:01 GMT   (12146kb,D)\\r\\rTitle: Virtual Reality for Understanding Artificial-Intelligence-driven\\r  Scientific Discovery with an Application in Quantum Optics\\rAuthors: Philipp Schmidt, S\\\\oren Arlt, Carlos Ruiz-Gonzalez, Xuemei Gu, Carla\\r  Rodr\\\\'iguez, Mario Krenn\\rCategories: cs.HC cs.AI cs.GR quant-ph\\rComments: 12 pages, 6 figures, comments welcome\\r\\\\\\\\\\r  Generative Artificial Intelligence (AI) models can propose solutions to\\rscientific problems beyond human capability. To truly make conceptual\\rcontributions, researchers need to be capable of understanding the AI-generated\\rstructures and extracting the underlying concepts and ideas. When algorithms\\rprovide little explanatory reasoning alongside the output, scientists have to\\rreverse-engineer the fundamental insights behind proposals based solely on\\rexamples. This task can be challenging as the output is often highly complex\\rand thus not immediately accessible to humans. In this work we show how\\rtransferring part of the analysis process into an immersive Virtual Reality\\r(VR) environment can assist researchers in developing an understanding of\\rAI-generated solutions. We demonstrate the usefulness of VR in finding\\rinterpretable configurations of abstract graphs, representing Quantum Optics\\rexperiments. Thereby, we can manually discover new generalizations of\\rAI-discoveries as well as new understanding in experimental quantum optics.\\rFurthermore, it allows us to customize the search space in an informed way - as\\ra human-in-the-loop - to achieve significantly faster subsequent discovery\\riterations. As concrete examples, with this technology, we discover a new\\rresource-efficient 3-dimensional entanglement swapping scheme, as well as a\\r3-dimensional 4-particle Greenberger-Horne-Zeilinger-state analyzer. Our\\rresults show the potential of VR for increasing a human researcher's ability to\\rderive knowledge from graph-based generative AI that, which is a common\\rabstract data representation used in diverse fields of science.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00834 ,  12146kb)\\r%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.00463\\rreplaced with revised version Sun, 3 Mar 2024 11:27:26 GMT   (343kb,D)\\r\\rTitle: Mismatching-Aware Unsupervised Translation Quality Estimation For\\r  Low-Resource Languages\\rAuthors: Fatemeh Azadi, Heshaam Faili, Mohammad Javad Dousti\\rCategories: cs.CL\\rComments: Submitted to Language Resources and Evaluation\\r\\\\\\\\ ( https://arxiv.org/abs/2208.00463 ,  343kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.04870\\rreplaced with revised version Mon, 4 Mar 2024 03:38:14 GMT   (408kb,D)\\r\\rTitle: SMiLE: Schema-augmented Multi-level Contrastive Learning for Knowledge\\r  Graph Link Prediction\\rAuthors: Miao Peng, Ben Liu, Qianqian Xie, Wenjie Xu, Hua Wang, Min Peng\\rCategories: cs.CL cs.AI\\rComments: Findings of EMNLP 2022\\r\\\\\\\\ ( https://arxiv.org/abs/2210.04870 ,  408kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.03827\\rreplaced with revised version Sat, 2 Mar 2024 21:33:53 GMT   (5120kb,D)\\r\\rTitle: Discovering Latent Knowledge in Language Models Without Supervision\\rAuthors: Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt\\rCategories: cs.CL cs.AI cs.LG\\rComments: ICLR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2212.03827 ,  5120kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.07196\\rreplaced with revised version Sat, 2 Mar 2024 19:19:44 GMT   (151kb,D)\\r\\rTitle: A Comprehensive Empirical Evaluation of Existing Word Embedding\\r  Approaches\\rAuthors: Obaidullah Zaland, Muhammad Abulaish, Mohd. Fazil\\rCategories: cs.CL cs.NE\\rComments: 28 pages, 3 figures and 10 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2303.07196 ,  151kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.02313\\rreplaced with revised version Mon, 4 Mar 2024 12:33:46 GMT   (31009kb,D)\\r\\rTitle: Personality-aware Human-centric Multimodal Reasoning: A New Task,\\r  Dataset and Baselines\\rAuthors: Yaochen Zhu, Xiangqing Shen, Rui Xia\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2304.02313 ,  31009kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.02483\\rreplaced with revised version Fri, 1 Mar 2024 23:41:24 GMT   (10699kb,D)\\r\\rTitle: Personalized Abstractive Summarization by Tri-agent Generation Pipeline\\rAuthors: Wen Xiao, Yujia Xie, Giuseppe Carenini, Pengcheng He\\rCategories: cs.CL\\rComments: Accepted at EACL 2024 Findings\\r\\\\\\\\ ( https://arxiv.org/abs/2305.02483 ,  10699kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.07912\\rreplaced with revised version Mon, 4 Mar 2024 03:14:25 GMT   (7152kb,D)\\r\\rTitle: Pre-trained Language Model with Prompts for Temporal Knowledge Graph\\r  Completion\\rAuthors: Wenjie Xu, Ben Liu, Miao Peng, Xu Jia, Min Peng\\rCategories: cs.CL cs.AI\\rComments: Accepted to Findings of ACL 2023\\rACM-class: I.2.4; I.2.7\\r\\\\\\\\ ( https://arxiv.org/abs/2305.07912 ,  7152kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.15337\\rreplaced with revised version Sat, 2 Mar 2024 02:45:33 GMT   (1913kb,D)\\r\\rTitle: Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation\\rAuthors: Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, Yu Wang\\rCategories: cs.CL cs.AI\\rComments: In ICLR'24\\r\\\\\\\\ ( https://arxiv.org/abs/2307.15337 ,  1913kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.05576\\rreplaced with revised version Mon, 4 Mar 2024 14:24:10 GMT   (158kb,D)\\r\\rTitle: Do Language Models' Words Refer?\\rAuthors: Matthew Mandelkern and Tal Linzen\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2308.05576 ,  158kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.04550\\rreplaced with revised version Sun, 3 Mar 2024 18:48:02 GMT   (346kb,D)\\r\\rTitle: Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges\\rAuthors: Hiba Ahsan, Denis Jered McInerney, Jisoo Kim, Christopher Potter,\\r  Geoffrey Young, Silvio Amir, Byron C. Wallace\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2309.04550 ,  346kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.04739\\rreplaced with revised version Sat, 2 Mar 2024 23:14:47 GMT   (73kb)\\r\\rTitle: Data Augmentation for Conversational AI\\rAuthors: Heydar Soudani, Evangelos Kanoulas and Faegheh Hasibi\\rCategories: cs.CL cs.IR\\rDOI: 10.1145/3583780.3615291\\r\\\\\\\\ ( https://arxiv.org/abs/2309.04739 ,  73kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.04823\\rreplaced with revised version Sat, 2 Mar 2024 14:46:27 GMT   (784kb,D)\\r\\rTitle: FaNS: a Facet-based Narrative Similarity Metric\\rAuthors: Mousumi Akter, Shubhra Kanti Karmaker Santu\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2309.04823 ,  784kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.11852\\rreplaced with revised version Sat, 2 Mar 2024 08:50:40 GMT   (6707kb,D)\\r\\rTitle: Knowledge Sanitization of Large Language Models\\rAuthors: Yoichi Ishibashi, Hidetoshi Shimodaira\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2309.11852 ,  6707kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.02954\\rreplaced with revised version Sat, 2 Mar 2024 14:38:24 GMT   (2523kb,D)\\r\\rTitle: DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for\\r  In-Context Learning\\rAuthors: Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze\\r  Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang,\\r  Chengming Li, Xiaodan Liang\\rCategories: cs.CL\\rComments: Accepted in ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.02954 ,  2523kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.06474\\rreplaced with revised version Mon, 4 Mar 2024 04:03:54 GMT   (703kb,D)\\r\\rTitle: Multilingual Jailbreak Challenges in Large Language Models\\rAuthors: Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing\\rCategories: cs.CL\\rComments: ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.06474 ,  703kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.09680\\rreplaced with revised version Mon, 4 Mar 2024 04:37:35 GMT   (1599kb,D)\\r\\rTitle: Improved Contextual Recognition In Automatic Speech Recognition Systems\\r  By Semantic Lattice Rescoring\\rAuthors: Ankitha Sudarshan, Vinay Samuel, Parth Patwa, Ibtihel Amara, Aman\\r  Chadha\\rCategories: cs.CL cs.AI cs.SD eess.AS\\r\\\\\\\\ ( https://arxiv.org/abs/2310.09680 ,  1599kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.11053\\rreplaced with revised version Mon, 4 Mar 2024 07:14:10 GMT   (1481kb,D)\\r\\rTitle: Denevil: Towards Deciphering and Navigating the Ethical Values of Large\\r  Language Models via Instruction Learning\\rAuthors: Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu\\rCategories: cs.CL cs.AI cs.CY\\rComments: Accepted by ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.11053 ,  1481kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.05112\\rreplaced with revised version Sun, 3 Mar 2024 01:15:36 GMT   (652kb,D)\\r\\rTitle: A Survey of Large Language Models in Medicine: Progress, Application,\\r  and Challenge\\rAuthors: Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge\\r  Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng\\r  Mao, Chenyu You, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo,\\r  David A. Clifton\\rCategories: cs.CL cs.AI\\rComments: Preprint. Version 4. 7 figures; 13 tables; 49 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2311.05112 ,  652kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.06534\\rreplaced with revised version Sat, 2 Mar 2024 17:18:34 GMT   (295kb,D)\\r\\rTitle: Translating Legalese: Enhancing Public Understanding of Court Opinions\\r  with Legal Summarizers\\rAuthors: Elliott Ash and Aniket Kesari and Suresh Naidu and Lena Song and\\r  Dominik Stammbach\\rCategories: cs.CL\\rComments: published in proceedings of CSLAW 2024: Symposium on Computer Science\\r  and Law\\rDOI: 10.1145/3614407.3643700\\r\\\\\\\\ ( https://arxiv.org/abs/2311.06534 ,  295kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09033\\rreplaced with revised version Sun, 3 Mar 2024 06:37:11 GMT   (677kb,D)\\r\\rTitle: MELA: Multilingual Evaluation of Linguistic Acceptability\\rAuthors: Ziyin Zhang and Yikang Liu and Weifang Huang and Junyu Mao and Rui\\r  Wang and Hai Hu\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09033 ,  677kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09800\\rreplaced with revised version Mon, 4 Mar 2024 14:35:59 GMT   (8543kb,D)\\r\\rTitle: $\\\\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of\\r  Information-Seeking Dialogue via Behavioural Fine-Tuning\\rAuthors: Evgeniia Razumovskaia, Ivan Vuli\\\\'c, Pavle Markovi\\\\'c, Tomasz Cichy,\\r  Qian Zheng, Tsung-Hsien Wen, Pawe{\\\\l} Budzianowski\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09800 ,  8543kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.01050\\rreplaced with revised version Sat, 2 Mar 2024 21:53:37 GMT   (5649kb,D)\\r\\rTitle: Detection and Analysis of Stress-Related Posts in Reddit Acamedic\\r  Communities\\rAuthors: Nazzere Oryngozha and Pakizar Shamoi and Ayan Igali\\rCategories: cs.CL\\rComments: This work has been submitted to the IEEE for possible publication.\\r  Copyright may be transferred without notice, after which this version may no\\r  longer be accessible\\rJournal-ref: IEEE Access, vol. 12, pp. 14932-14948, 2024\\rDOI: 10.1109/ACCESS.2024.3357662\\r\\\\\\\\ ( https://arxiv.org/abs/2312.01050 ,  5649kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.01714\\rreplaced with revised version Sun, 3 Mar 2024 06:12:44 GMT   (3841kb,D)\\r\\rTitle: Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large\\r  Language Models\\rAuthors: Bingshuai Liu, Chenyang Lyu, Zijun Min, Zhanyu Wang, Jinsong Su,\\r  Longyue Wang\\rCategories: cs.CL\\rComments: Work in progress\\r\\\\\\\\ ( https://arxiv.org/abs/2312.01714 ,  3841kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.02436\\rreplaced with revised version Mon, 4 Mar 2024 04:12:02 GMT   (2374kb,D)\\r\\rTitle: MUFFIN: Curating Multi-Faceted Instructions for Improving\\r  Instruction-Following\\rAuthors: Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu\\r  Su, Wenpeng Yin\\rCategories: cs.CL cs.AI\\rComments: ICLR 2024. Data, model, and code are available at:\\r  https://renzelou.github.io/Muffin/\\r\\\\\\\\ ( https://arxiv.org/abs/2312.02436 ,  2374kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11361\\rreplaced with revised version Mon, 4 Mar 2024 16:32:10 GMT   (477kb,D)\\r\\rTitle: NoMIRACL: Knowing When You Don't Know for Robust Multilingual\\r  Retrieval-Augmented Generation\\rAuthors: Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan\\r  Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi\\r  Rezagholizadeh, Jimmy Lin\\rCategories: cs.CL cs.IR\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11361 ,  477kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13103\\rreplaced with revised version Sun, 3 Mar 2024 21:06:33 GMT   (711kb)\\r\\rTitle: Exploring Multimodal Large Language Models for Radiology Report\\r  Error-checking\\rAuthors: Jinge Wu, Yunsoo Kim, Eva C. Keller, Jamie Chow, Adam P. Levine,\\r  Nikolas Pontikos, Zina Ibrahim, Paul Taylor, Michelle C. Williams, Honghan Wu\\rCategories: cs.CL cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13103 ,  711kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.17242\\rreplaced with revised version Mon, 4 Mar 2024 16:00:23 GMT   (951kb,D)\\r\\rTitle: Learning to Generate Text in Arbitrary Writing Styles\\rAuthors: Aleem Khan, Andrew Wang, Sophia Hager, Nicholas Andrews\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2312.17242 ,  951kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.06509\\rreplaced with revised version Mon, 4 Mar 2024 13:16:53 GMT   (10742kb,D)\\r\\rTitle: AntEval: Quantitatively Evaluating Informativeness and Expressiveness of\\r  Agent Social Interactions\\rAuthors: Yuanzhi Liang, Linchao Zhu, Yi Yang\\rCategories: cs.CL\\rComments: Preliminary version of an ongoing work\\r\\\\\\\\ ( https://arxiv.org/abs/2401.06509 ,  10742kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.06915\\rreplaced with revised version Thu, 29 Feb 2024 19:55:14 GMT   (7703kb,D)\\r\\rTitle: DocFinQA: A Long-Context Financial Reasoning Dataset\\rAuthors: Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Michael Krumdick,\\r  Charles Lovering, Chris Tanner\\rCategories: cs.CL cs.AI\\rComments: 13 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2401.06915 ,  7703kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.02564\\rreplaced with revised version Sat, 2 Mar 2024 16:33:32 GMT   (440kb,D)\\r\\rTitle: A Truly Joint Neural Architecture for Segmentation and Parsing\\rAuthors: Danit Yshaayahu Levi and Reut Tsarfaty\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2402.02564 ,  440kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.03223\\rreplaced with revised version Mon, 4 Mar 2024 15:31:48 GMT   (220kb,D)\\r\\rTitle: English Prompts are Better for NLI-based Zero-Shot Emotion\\r  Classification than Target-Language Prompts\\rAuthors: Patrick Barrei{\\\\ss} and Roman Klinger and Jeremy Barnes\\rCategories: cs.CL\\rComments: accepted to the PromptEng workshop at The Web Conf\\r\\\\\\\\ ( https://arxiv.org/abs/2402.03223 ,  220kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.05044\\rreplaced with revised version Mon, 4 Mar 2024 07:20:31 GMT   (12089kb,D)\\r\\rTitle: SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large\\r  Language Models\\rAuthors: Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin,\\r  Yu Qiao, Jing Shao\\rCategories: cs.CL cs.AI cs.CR cs.LG\\rComments: fix institution typo\\r\\\\\\\\ ( https://arxiv.org/abs/2402.05044 ,  12089kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.09282\\rreplaced with revised version Sun, 3 Mar 2024 15:01:55 GMT   (1048kb,D)\\r\\rTitle: Distilling Large Language Models into Tiny Models for Named Entity\\r  Recognition\\rAuthors: Yining Huang\\rCategories: cs.CL\\rComments: 16 pages, 3 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2402.09282 ,  1048kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.11094\\rreplaced with revised version Sat, 2 Mar 2024 14:01:04 GMT   (8846kb,D)\\r\\rTitle: Word Embeddings Revisited: Do LLMs Offer Something New?\\rAuthors: Matthew Freestone and Shubhra Kanti Karmaker Santu\\rCategories: cs.CL\\rComments: 7 pages, 4 figures\\rACM-class: I.2.7\\r\\\\\\\\ ( https://arxiv.org/abs/2402.11094 ,  8846kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.11924\\rreplaced with revised version Sun, 3 Mar 2024 02:23:19 GMT   (1642kb,D)\\r\\rTitle: MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition\\rAuthors: Jian Wu, Linyi Yang, Manabu Okumura, Yue Zhang\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2402.11924 ,  1642kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.12151\\rreplaced with revised version Sun, 3 Mar 2024 20:06:24 GMT   (11260kb,D)\\r\\rTitle: Transformer-based Causal Language Models Perform Clustering\\rAuthors: Xinbo Wu, Lav R. Varshney\\rCategories: cs.CL cs.AI\\rComments: Added new experimental results and fixed some errors\\r\\\\\\\\ ( https://arxiv.org/abs/2402.12151 ,  11260kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.15302\\rreplaced with revised version Mon, 4 Mar 2024 18:50:19 GMT   (616kb,D)\\r\\rTitle: How (un)ethical are instruction-centric responses of LLMs? Unveiling the\\r  vulnerabilities of safety guardrails to harmful queries\\rAuthors: Somnath Banerjee, Sayan Layek, Rima Hazra, Animesh Mukherjee\\rCategories: cs.CL cs.CR\\rComments: Under review.\\r  {https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA}\\r\\\\\\\\ ( https://arxiv.org/abs/2402.15302 ,  616kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.15708\\rreplaced with revised version Sun, 3 Mar 2024 09:18:07 GMT   (3507kb,D)\\r\\rTitle: Query Augmentation by Decoding Semantics from Brain Signals\\rAuthors: Ziyi Ye, Jingtao Zhan, Qingyao Ai, Yiqun Liu, Maarten de Rijke,\\r  Christina Lioma, Tuukka Ruotsalo\\rCategories: cs.CL cs.AI cs.IR\\r\\\\\\\\ ( https://arxiv.org/abs/2402.15708 ,  3507kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.16061\\rreplaced with revised version Mon, 4 Mar 2024 13:37:48 GMT   (1348kb,D)\\r\\rTitle: How Large Language Models Encode Context Knowledge? A Layer-Wise Probing\\r  Study\\rAuthors: Tianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan, Zhaochun Ren, Gongshen\\r  Liu\\rCategories: cs.CL\\rComments: Accepted at LREC-COLING 2024 (Long Paper)\\r\\\\\\\\ ( https://arxiv.org/abs/2402.16061 ,  1348kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.16063\\rreplaced with revised version Mon, 4 Mar 2024 01:53:35 GMT   (731kb,D)\\r\\rTitle: Citation-Enhanced Generation for LLM-based Chatbots\\rAuthors: Weitao Li, Junkai Li, Weizhi Ma, Yang Liu\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2402.16063 ,  731kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.16107\\rreplaced with revised version Sun, 3 Mar 2024 07:21:36 GMT   (243kb,D)\\r\\rTitle: FuseChat: Knowledge Fusion of Chat Models\\rAuthors: Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang,\\r  Wei Bi\\rCategories: cs.CL\\rComments: Technical Report, work in progress\\r\\\\\\\\ ( https://arxiv.org/abs/2402.16107 ,  243kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.16379\\rreplaced with revised version Mon, 4 Mar 2024 03:14:11 GMT   (5006kb,D)\\r\\rTitle: Improving LLM-based Machine Translation with Systematic Self-Correction\\rAuthors: Zhaopeng Feng, Yan Zhang, Hao Li, Wenqiang Liu, Jun Lang, Yang Feng,\\r  Jian Wu, Zuozhu Liu\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2402.16379 ,  5006kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17256\\rreplaced with revised version Mon, 4 Mar 2024 06:04:32 GMT   (1228kb,D)\\r\\rTitle: Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent\\r  Detection\\rAuthors: Pei Wang, Keqing He, Yejie Wang, Xiaoshuai Song, Yutao Mou, Jingang\\r  Wang, Yunsen Xian, Xunliang Cai, Weiran Xu\\rCategories: cs.CL\\rJournal-ref: LREC-COLING 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17256 ,  1228kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17411\\rreplaced with revised version Sat, 2 Mar 2024 14:08:06 GMT   (0kb,I)\\r\\rTitle: Consistency Matters: Explore LLMs Consistency From a Black-Box\\r  Perspective\\rAuthors: Fufangchen Zhao, Guoqiang Jin, Jiaheng Huang, Rui Zhao and Fei Tan\\rCategories: cs.CL\\rComments: This paper is not ready\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17411 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17512\\rreplaced with revised version Mon, 4 Mar 2024 12:21:52 GMT   (862kb,D)\\r\\rTitle: Latent Attention for Linear Time Transformers\\rAuthors: Rares Dolga, Marius Cobzarenco, David Barber\\rCategories: cs.CL stat.ML\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17512 ,  862kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17759\\rreplaced with revised version Sun, 3 Mar 2024 15:07:50 GMT   (2172kb,D)\\r\\rTitle: Towards Optimal Learning of Language Models\\rAuthors: Yuxian Gu, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, Furu Wei\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17759 ,  2172kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17887\\rreplaced with revised version Sat, 2 Mar 2024 09:03:18 GMT   (1792kb,D)\\r\\rTitle: JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning\\r  and Professional Question Answering Capability\\rAuthors: Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu\\rCategories: cs.CL cs.IR\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17887 ,  1792kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17897\\rreplaced with revised version Mon, 4 Mar 2024 14:06:42 GMT   (300kb,D)\\r\\rTitle: A Language Model based Framework for New Concept Placement in Ontologies\\rAuthors: Hang Dong, Jiaoyan Chen, Yuan He, Yongsheng Gao, Ian Horrocks\\rCategories: cs.CL cs.IR\\rComments: 20 pages, 3 figures, accepted for ESWC 2024\\rACM-class: I.2.7; I.2.4\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17897 ,  300kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.18243\\rreplaced with revised version Sat, 2 Mar 2024 08:28:14 GMT   (483kb,D)\\r\\rTitle: Learning or Self-aligning? Rethinking Instruction Fine-tuning\\rAuthors: Mengjie Ren, Boxi Cao, Hongyu Lin, Cao Liu, Xianpei Han, Ke Zeng,\\r  Guanglu Wan, Xunliang Cai, Le Sun\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2402.18243 ,  483kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.18284\\rreplaced with revised version Sat, 2 Mar 2024 23:19:27 GMT   (6813kb,D)\\r\\rTitle: Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of\\r  Pre-trained Language Models with Proximal Policy Optimization\\rAuthors: Shuo Yang and Gjergji Kasneci\\rCategories: cs.CL cs.AI\\rComments: 12 pages, 2 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2402.18284 ,  6813kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.19116\\rreplaced with revised version Mon, 4 Mar 2024 08:42:56 GMT   (1116kb,D)\\r\\rTitle: How to Understand Support? An Implicit-enhanced Causal Inference\\r  Approach for Weakly-supervised Phrase Grounding\\rAuthors: Jiamin Luo, Jianing Zhao, Jingjing Wang, Guodong Zhou\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2402.19116 ,  1116kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.19248\\rreplaced with revised version Sat, 2 Mar 2024 04:37:37 GMT   (8119kb,D)\\r\\rTitle: Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question\\r  Answering Benchmark\\rAuthors: Zhikun Xu, Yinghui Li, Ruixue Ding, Xinyu Wang, Boli Chen, Yong Jiang,\\r  Hai-Tao Zheng, Wenlian Lu, Pengjun Xie, Fei Huang\\rCategories: cs.CL\\rComments: Work in progress!\\r\\\\\\\\ ( https://arxiv.org/abs/2402.19248 ,  8119kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.19282\\rreplaced with revised version Mon, 4 Mar 2024 12:30:10 GMT   (124kb,D)\\r\\rTitle: WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset\\rAuthors: Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia\\r  Yu, ChaoBin Zhang, Pei Chu, Yuan Qu, Jin Shi, Lindong Lu, Runyu Peng, Zhiyuan\\r  Zeng, Huanze Tang, Zhikai Lei, Jiawei Hong, Keyu Chen, Zhaoye Fei, Ruiliang\\r  Xu, Wei Li, Zhongyin Tu, Hang Yan and Conghui He\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2402.19282 ,  124kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.19406\\rreplaced with revised version Mon, 4 Mar 2024 11:35:02 GMT   (10140kb,D)\\r\\rTitle: On the Scaling Laws of Geographical Representation in Language Models\\rAuthors: Nathan Godey, \\\\'Eric de la Clergerie, Beno\\\\^it Sagot\\rCategories: cs.CL cs.AI\\rComments: Accepted at LREC-COLING 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2402.19406 ,  10140kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00510\\rreplaced with revised version Mon, 4 Mar 2024 06:36:01 GMT   (773kb,D)\\r\\rTitle: ROME: Memorization Insights from Text, Probability and Hidden State in\\r  Large Language Models\\rAuthors: Bo Li and Qinghua Zhao and Lijie Wen\\rCategories: cs.CL cs.AI\\rComments: Submitted to ACL, 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00510 ,  773kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:1910.04935\\rreplaced with revised version Sun, 3 Mar 2024 12:08:37 GMT   (4740kb,D)\\r\\rTitle: FetusMap: Fetal Pose Estimation in 3D Ultrasound\\rAuthors: Xin Yang, Wenlong Shi, Haoran Dou, Jikuan Qian, Yi Wang, Wufeng Xue,\\r  Shengli Li, Dong Ni, Pheng-Ann Heng\\rCategories: cs.CV cs.LG eess.IV\\rComments: 9 pages, 6 figures, 2 tables. Accepted by MICCAI 2019\\r\\\\\\\\ ( https://arxiv.org/abs/1910.04935 ,  4740kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2106.14490\\rreplaced with revised version Sat, 2 Mar 2024 08:17:24 GMT   (14536kb,D)\\r\\rTitle: Making Images Real Again: A Comprehensive Survey on Deep Image\\r  Composition\\rAuthors: Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, Liqing\\r  Zhang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2106.14490 ,  14536kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2204.05798\\rreplaced with revised version Mon, 4 Mar 2024 16:43:14 GMT   (11534kb,D)\\r\\rTitle: Multi-View Hypercomplex Learning for Breast Cancer Screening\\rAuthors: Eleonora Lopez, Eleonora Grassucci, Martina Valleriani, Danilo\\r  Comminiello\\rCategories: cs.CV cs.AI cs.LG\\rComments: This paper has been submitted to IEEE Transactions on Pattern\\r  Analysis and Machine Intelligence\\r\\\\\\\\ ( https://arxiv.org/abs/2204.05798 ,  11534kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.09500\\rreplaced with revised version Mon, 4 Mar 2024 16:39:15 GMT   (6535kb,D)\\r\\rTitle: Causality-Inspired Taxonomy for Explainable Artificial Intelligence\\rAuthors: Pedro C. Neto, Tiago Gon\\\\c{c}alves, Jo\\\\~ao Ribeiro Pinto, Wilson\\r  Silva, Ana F. Sequeira, Arun Ross, Jaime S. Cardoso\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2208.09500 ,  6535kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.01412\\rreplaced with revised version Sun, 3 Mar 2024 10:41:29 GMT   (3034kb,D)\\r\\rTitle: CAMANet: Class Activation Map Guided Attention Network for Radiology\\r  Report Generation\\rAuthors: Jun Wang, Abhir Bhalerao, Terry Yin, Simon See, Yulan He\\rCategories: cs.CV\\rComments: Accepted to IEEE Journal of Biomedical and Health Informatics\\r  (IJBHI). 13 pages, 8 figures\\rDOI: 10.1109/JBHI.2024.3354712\\r\\\\\\\\ ( https://arxiv.org/abs/2211.01412 ,  3034kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.14298\\rreplaced with revised version Sun, 3 Mar 2024 10:51:36 GMT   (21431kb,D)\\r\\rTitle: PIP: Positional-encoding Image Prior\\rAuthors: Nimrod Shabtay, Eli Schwartz and Raja Giryes\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2211.14298 ,  21431kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00556\\rreplaced with revised version Mon, 4 Mar 2024 10:58:44 GMT   (9105kb,D)\\r\\rTitle: Correspondence-free online human motion retargeting\\rAuthors: Rim Rekik, Mathieu Marsot, Anne-H\\\\'el\\\\`ene Olivier, Jean-S\\\\'ebastien\\r  Franco and Stefanie Wuhrer\\rCategories: cs.CV\\rComments: Published in International Conference on 3D Vision (3DV), 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00556 ,  9105kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.10970\\rreplaced with revised version Sat, 2 Mar 2024 00:31:18 GMT   (31926kb,D)\\r\\rTitle: Differentiable Rendering with Reparameterized Volume Sampling\\rAuthors: Nikita Morozov, Denis Rakitin, Oleg Desheulin, Dmitry Vetrov, Kirill\\r  Struminsky\\rCategories: cs.CV\\rComments: Accepted at AISTATS 2024. Short version of this paper appeared in\\r  ICLR 2023 Neural Fields workshop\\r\\\\\\\\ ( https://arxiv.org/abs/2302.10970 ,  31926kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.09105\\rreplaced with revised version Mon, 4 Mar 2024 11:30:06 GMT   (1758kb,D)\\r\\rTitle: Rethinking Model Ensemble in Transfer-based Adversarial Attacks\\rAuthors: Huanran Chen, Yichi Zhang, Yinpeng Dong, Xiao Yang, Hang Su, Jun Zhu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2303.09105 ,  1758kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.12194\\rreplaced with revised version Sat, 2 Mar 2024 22:18:12 GMT   (9884kb,D)\\r\\rTitle: LiDARFormer: A Unified Transformer-based Multi-task Network for LiDAR\\r  Perception\\rAuthors: Zixiang Zhou, Dongqiangzi Ye, Weijia Chen, Yufei Xie, Yu Wang, Panqu\\r  Wang, Hassan Foroosh\\rCategories: cs.CV\\rComments: ICRA 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2303.12194 ,  9884kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.17386\\rreplaced with revised version Mon, 4 Mar 2024 18:06:33 GMT   (47531kb,D)\\r\\rTitle: Complementary Random Masking for RGB-Thermal Semantic Segmentation\\rAuthors: Ukcheol Shin, Kyunghyun Lee, In So Kweon, Jean Oh\\rCategories: cs.CV cs.AI cs.RO\\rComments: ICRA 2024, Our source code is available at\\r  https://github.com/UkcheolShin/CRM_RGBTSeg\\r\\\\\\\\ ( https://arxiv.org/abs/2303.17386 ,  47531kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.02867\\rreplaced with revised version Sun, 3 Mar 2024 15:15:05 GMT   (2514kb,D)\\r\\rTitle: Voxel or Pillar: Exploring Efficient Point Cloud Representation for 3D\\r  Object Detection\\rAuthors: Yuhao Huang, Sanping Zhou, Junjie Zhang, Jinpeng Dong, Nanning Zheng\\rCategories: cs.CV\\rComments: Accepted by AAAI-2024\\r\\\\\\\\ ( https://arxiv.org/abs/2304.02867 ,  2514kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.14614\\rreplaced with revised version Sat, 2 Mar 2024 17:56:07 GMT   (17289kb,D)\\r\\rTitle: Fusion is Not Enough: Single Modal Attacks on Fusion Models for 3D\\r  Object Detection\\rAuthors: Zhiyuan Cheng, Hongjun Choi, James Liang, Shiwei Feng, Guanhong Tao,\\r  Dongfang Liu, Michael Zuzak, Xiangyu Zhang\\rCategories: cs.CV cs.CR\\rComments: Accepted at ICLR'2024\\r\\\\\\\\ ( https://arxiv.org/abs/2304.14614 ,  17289kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.04440\\rreplaced with revised version Mon, 4 Mar 2024 01:34:33 GMT   (2176kb,D)\\r\\rTitle: Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot\\r  Class-Agnostic Counting\\rAuthors: Zhicheng Wang, Liwen Xiao, Zhiguo Cao, Hao Lu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2305.04440 ,  2176kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.11468\\rreplaced with revised version Mon, 4 Mar 2024 13:29:18 GMT   (1152kb,D)\\r\\rTitle: Overcoming Topology Agnosticism: Enhancing Skeleton-Based Action\\r  Recognition through Redefined Skeletal Topology Awareness\\rAuthors: Yuxuan Zhou, Zhi-Qi Cheng, Jun-Yan He, Bin Luo, Yifeng Geng, Xuansong\\r  Xie\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2305.11468 ,  1152kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.11577\\rreplaced with revised version Sat, 2 Mar 2024 12:03:56 GMT   (11091kb,D)\\r\\rTitle: LeftRefill: Filling Right Canvas based on Left Reference through\\r  Generalized Text-to-Image Diffusion Model\\rAuthors: Chenjie Cao, Yunuo Cai, Qiaole Dong, Yikai Wang, Yanwei Fu\\rCategories: cs.CV\\rComments: Accepted by CVPR2024. Codes and models are released at\\r  https://github.com/ewrfcas/LeftRefill, Project page:\\r  https://ewrfcas.github.io/LeftRefill\\r\\\\\\\\ ( https://arxiv.org/abs/2305.11577 ,  11091kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.13655\\rreplaced with revised version Mon, 4 Mar 2024 18:43:49 GMT   (46414kb,D)\\r\\rTitle: LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image\\r  Diffusion Models with Large Language Models\\rAuthors: Long Lian, Boyi Li, Adam Yala, Trevor Darrell\\rCategories: cs.CV\\rComments: Transactions on Machine Learning Research (TMLR) 2024, with Featured\\r  Certification\\r\\\\\\\\ ( https://arxiv.org/abs/2305.13655 ,  46414kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.15086\\rreplaced with revised version Sat, 2 Mar 2024 12:47:22 GMT   (35486kb,D)\\r\\rTitle: Unpaired Image-to-Image Translation via Neural Schr\\\\odinger Bridge\\rAuthors: Beomsu Kim, Gihyun Kwon, Kwanyoung Kim, Jong Chul Ye\\rCategories: cs.CV cs.AI cs.LG stat.ML\\rComments: ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2305.15086 ,  35486kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.00973\\rreplaced with revised version Mon, 4 Mar 2024 10:53:18 GMT   (16807kb,D)\\r\\rTitle: Intelligent Grimm -- Open-ended Visual Storytelling via Latent Diffusion\\r  Models\\rAuthors: Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, Weidi\\r  Xie\\rCategories: cs.CV\\rComments: Accepted by CVPR 2024. Project Page:\\r  https://haoningwu3639.github.io/StoryGen_Webpage/\\r\\\\\\\\ ( https://arxiv.org/abs/2306.00973 ,  16807kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.03424\\rreplaced with revised version Sat, 2 Mar 2024 13:37:25 GMT   (9479kb,D)\\r\\rTitle: GCD-DDPM: A Generative Change Detection Model Based on\\r  Difference-Feature Guided DDPM\\rAuthors: Yihan Wen, Xianping Ma, Xiaokang Zhang, Man-On Pun\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.03424 ,  9479kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.07490\\rreplaced with revised version Sat, 2 Mar 2024 15:10:16 GMT   (9334kb,D)\\r\\rTitle: Top-Down Framework for Weakly-supervised Grounded Image Captioning\\rAuthors: Chen Cai, Suchen Wang, Kim-hui Yap, Yi Wang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.07490 ,  9334kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.08625\\rreplaced with revised version Fri, 1 Mar 2024 21:10:52 GMT   (7717kb,D)\\r\\rTitle: RRSIS: Referring Remote Sensing Image Segmentation\\rAuthors: Zhenghang Yuan, Lichao Mou, Yuansheng Hua, Xiao Xiang Zhu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.08625 ,  7717kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.09348\\rreplaced with revised version Sat, 2 Mar 2024 16:48:59 GMT   (40272kb,D)\\r\\rTitle: Seeing the World through Your Eyes\\rAuthors: Hadi Alzayer, Kevin Zhang, Brandon Feng, Christopher Metzler, Jia-Bin\\r  Huang\\rCategories: cs.CV\\rComments: CVPR 2024. First two authors contributed equally. Project page:\\r  https://world-from-eyes.github.io/\\r\\\\\\\\ ( https://arxiv.org/abs/2306.09348 ,  40272kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.12525\\rreplaced with revised version Sat, 2 Mar 2024 22:36:04 GMT   (1995kb,D)\\r\\rTitle: LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network\\rAuthors: Dongqiangzi Ye, Yufei Xie, Weijia Chen, Zixiang Zhou, Lingting Ge,\\r  Hassan Foroosh\\rCategories: cs.CV\\rComments: ICRA 2024. Top solution for the Waymo Open Dataset Challenges 2023 -\\r  Pose Estimation. CVPR 2023 Workshop on Autonomous Driving\\r\\\\\\\\ ( https://arxiv.org/abs/2306.12525 ,  1995kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.00212\\rreplaced with revised version Mon, 4 Mar 2024 05:12:26 GMT   (3404kb,D)\\r\\rTitle: Internal-External Boundary Attention Fusion for Glass Surface\\r  Segmentation\\rAuthors: Dongshen Han and Seungkyu Lee and Chaoning Zhang and Heechan Yoon and\\r  Hyukmin Kwon and Hyun-Cheol Kim and Hyon-Gon Choo\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2307.00212 ,  3404kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.11543\\rreplaced with revised version Mon, 4 Mar 2024 10:49:10 GMT   (17491kb,D)\\r\\rTitle: KVN: Keypoints Voting Network with Differentiable RANSAC for Stereo Pose\\r  Estimation\\rAuthors: Ivano Donadi and Alberto Pretto\\rCategories: cs.CV cs.RO\\rComments: Published in IEEE Robotics and Automation Letters\\rJournal-ref: IEEE Robotics and Automation Letters, vol. 9, no. 4, pp.\\r  3498-3505, April 2024\\rDOI: 10.1109/LRA.2024.3367508\\r\\\\\\\\ ( https://arxiv.org/abs/2307.11543 ,  17491kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.14288\\rreplaced with revised version Mon, 4 Mar 2024 14:33:05 GMT   (42902kb,D)\\r\\rTitle: US \\\\& MRI Image Fusion Based on Markerless Skin Registration\\rAuthors: Martina Paccini, Giacomo Paschina, Stefano De Beni, Andrei Stefanov,\\r  Velizar Kolev, Giuseppe Patan\\\\`e\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2307.14288 ,  42902kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.13888\\rreplaced with revised version Sat, 2 Mar 2024 14:01:08 GMT   (47459kb,D)\\r\\rTitle: Neural Implicit Morphing of Face Images\\rAuthors: Guilherme Schardong, Tiago Novello, Hallison Paz, Iurii Medvedev,\\r  Vin\\\\'icius da Silva, Luiz Velho, Nuno Gon\\\\c{c}alves\\rCategories: cs.CV cs.LG\\rComments: 14 pages, 18 figures\\rACM-class: I.4.8; I.4.10\\r\\\\\\\\ ( https://arxiv.org/abs/2308.13888 ,  47459kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.15109\\rreplaced with revised version Sat, 2 Mar 2024 12:34:42 GMT   (3204kb,D)\\r\\rTitle: DiffusionVMR: Diffusion Model for Joint Video Moment Retrieval and\\r  Highlight Detection\\rAuthors: Henghao Zhao, Kevin Qinghong Lin, Rui Yan and Zechao Li\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2308.15109 ,  3204kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.03750\\rreplaced with revised version Sat, 2 Mar 2024 20:54:01 GMT   (3282kb,D)\\r\\rTitle: PBP: Path-based Trajectory Prediction for Autonomous Driving\\rAuthors: Sepideh Afshar, Nachiket Deo, Akshay Bhagat, Titas Chakraborty,\\r  Yunming Shao, Balarama Raju Buddharaju, Adwait Deshpande, Henggang Cui\\rCategories: cs.CV\\rComments: Published at ICRA 2024; Sepideh Afshar and Nachiket Deo contributed\\r  equally\\r\\\\\\\\ ( https://arxiv.org/abs/2309.03750 ,  3282kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.05334\\rreplaced with revised version Sat, 2 Mar 2024 16:41:03 GMT   (7753kb,D)\\r\\rTitle: MultIOD: Rehearsal-free Multihead Incremental Object Detector\\rAuthors: Eden Belouadah, Arnaud Dapogny, Kevin Bailly\\rCategories: cs.CV\\rComments: Under review at the Workshop on Continual Learning in Computer Vision\\r  (CVPR 2024)\\r\\\\\\\\ ( https://arxiv.org/abs/2309.05334 ,  7753kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.06747\\rreplaced with revised version Mon, 4 Mar 2024 09:36:04 GMT   (2296kb)\\r\\rTitle: Integrating GAN and Texture Synthesis for Enhanced Road Damage Detection\\rAuthors: Tengyang Chen and Jiangtao Ren\\rCategories: cs.CV\\rComments: 10 pages, 13 figures, 2 Tables\\r\\\\\\\\ ( https://arxiv.org/abs/2309.06747 ,  2296kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.09256\\rreplaced with revised version Mon, 4 Mar 2024 07:37:55 GMT   (5924kb,D)\\r\\rTitle: LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models\\rAuthors: Kazuto Nakashima, Ryo Kurazume\\rCategories: cs.CV cs.RO\\rComments: ICRA 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2309.09256 ,  5924kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.09502\\rreplaced with revised version Mon, 4 Mar 2024 15:16:27 GMT   (3822kb,D)\\r\\rTitle: RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering\\r  Supervision\\rAuthors: Mingjie Pan, Jiaming Liu, Renrui Zhang, Peixiang Huang, Xiaoqi Li,\\r  Bing Wang, Hongwei Xie, Li Liu, Shanghang Zhang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.09502 ,  3822kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.17187\\rreplaced with revised version Sun, 3 Mar 2024 20:54:36 GMT   (4230kb,D)\\r\\rTitle: TBD Pedestrian Data Collection: Towards Rich, Portable, and Large-Scale\\r  Natural Pedestrian Data\\rAuthors: Allan Wang, Daisuke Sato, Yasser Corzo, Sonya Simkin, Abhijat Biswas,\\r  Aaron Steinfeld\\rCategories: cs.CV cs.HC cs.RO\\rComments: This work has been accepted by IEEE ICRA 2024. Copyright may be\\r  transferred without notice, after which this version may no longer be\\r  accessible. arXiv admin note: substantial text overlap with arXiv:2203.01974\\r\\\\\\\\ ( https://arxiv.org/abs/2309.17187 ,  4230kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.05207\\rreplaced with revised version Mon, 4 Mar 2024 12:42:07 GMT   (1715kb,D)\\r\\rTitle: Boosting Facial Action Unit Detection Through Jointly Learning Facial\\r  Landmark Detection and Domain Separation and Reconstruction\\rAuthors: Ziqiao Shang, Li Yu\\rCategories: cs.CV cs.AI cs.LG\\rComments: 5 pages, 1 figure\\r\\\\\\\\ ( https://arxiv.org/abs/2310.05207 ,  1715kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.06836\\rreplaced with revised version Mon, 4 Mar 2024 11:25:37 GMT   (7530kb,D)\\r\\rTitle: What Does Stable Diffusion Know about the 3D Scene?\\rAuthors: Guanqi Zhan, Chuanxia Zheng, Weidi Xie, Andrew Zisserman\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2310.06836 ,  7530kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.07855\\rreplaced with revised version Sun, 3 Mar 2024 09:57:57 GMT   (6188kb,D)\\r\\rTitle: CrIBo: Self-Supervised Learning via Cross-Image Object-Level\\r  Bootstrapping\\rAuthors: Tim Lebailly, Thomas Stegm\\\\uller, Behzad Bozorgtabar, Jean-Philippe\\r  Thiran, Tinne Tuytelaars\\rCategories: cs.CV cs.LG\\rComments: ICLR 2024 (spotlight)\\r\\\\\\\\ ( https://arxiv.org/abs/2310.07855 ,  6188kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.08929\\rreplaced with revised version Sat, 2 Mar 2024 02:51:18 GMT   (15007kb,D)\\r\\rTitle: Leveraging Image Augmentation for Object Manipulation: Towards\\r  Interpretable Controllability in Object-Centric Learning\\rAuthors: Jinwoo Kim, Janghyuk Choi, Jaehyun Kang, Changyeon Lee, Ho-Jin Choi,\\r  Seon Joo Kim\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2310.08929 ,  15007kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.11142\\rreplaced with revised version Mon, 4 Mar 2024 09:07:44 GMT   (10638kb,D)\\r\\rTitle: BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian\\r  Inference\\rAuthors: Siqi Kou, Lei Gan, Dequan Wang, Chongxuan Li, Zhijie Deng\\rCategories: cs.CV cs.LG\\rComments: ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.11142 ,  10638kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.00187\\rreplaced with revised version Sat, 2 Mar 2024 17:12:29 GMT   (4695kb,D)\\r\\rTitle: Decodable and Sample Invariant Continuous Object Encoder\\rAuthors: Dehao Yuan, Furong Huang, Cornelia Ferm\\\\uller, Yiannis Aloimonos\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2311.00187 ,  4695kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.00453\\rreplaced with revised version Sat, 2 Mar 2024 13:54:31 GMT   (6707kb,D)\\r\\rTitle: CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly\\r  Detection\\rAuthors: Xuhai Chen, Jiangning Zhang, Guanzhong Tian, Haoyang He, Wuhao Zhang,\\r  Yabiao Wang, Chengjie Wang, Yong Liu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2311.00453 ,  6707kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.01092\\rreplaced with revised version Mon, 4 Mar 2024 04:28:11 GMT   (25035kb,D)\\r\\rTitle: Learning A Multi-Task Transformer Via Unified And Customized Instruction\\r  Tuning For Chest Radiograph Interpretation\\rAuthors: Lijian Xu, Ziyu Ni, Xinglong Liu, Xiaosong Wang, Hongsheng Li, and\\r  Shaoting Zhang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2311.01092 ,  25035kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.10329\\rreplaced with revised version Mon, 4 Mar 2024 07:28:07 GMT   (10955kb,D)\\r\\rTitle: High-fidelity Person-centric Subject-to-Image Synthesis\\rAuthors: Yibin Wang and Weizhong Zhang and Jianwei Zheng and Cheng Jin\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2311.10329 ,  10955kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.12075\\rreplaced with revised version Mon, 4 Mar 2024 13:59:22 GMT   (1092kb,D)\\r\\rTitle: BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive\\r  Learning\\rAuthors: Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao,\\r  Ee-Chien Chang\\rCategories: cs.CV\\rComments: The paper lacks some work that needs to be cited\\rJournal-ref: CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.12075 ,  1092kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.13602\\rreplaced with revised version Mon, 4 Mar 2024 07:55:33 GMT   (16463kb,D)\\r\\rTitle: Retrieval-Augmented Layout Transformer for Content-Aware Layout\\r  Generation\\rAuthors: Daichi Horita, Naoto Inoue, Kotaro Kikuchi, Kota Yamaguchi, Kiyoharu\\r  Aizawa\\rCategories: cs.CV\\rComments: Accepted to CVPR 2024, Project website:\\r  https://udonda.github.io/RALF/\\r\\\\\\\\ ( https://arxiv.org/abs/2311.13602 ,  16463kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.15836\\rreplaced with revised version Sun, 3 Mar 2024 23:42:57 GMT   (29699kb,D)\\r\\rTitle: Syn3DWound: A Synthetic Dataset for 3D Wound Bed Analysis\\rAuthors: L\\\\'eo Lebrat, Rodrigo Santa Cruz, Remi Chierchia, Yulia Arzhaeva,\\r  Mohammad Ali Armin, Joshua Goldsmith, Jeremy Oorloff, Prithvi Reddy, Chuong\\r  Nguyen, Lars Petersson, Michelle Barakat-Johnson, Georgina Luscombe, Clinton\\r  Fookes, Olivier Salvado, David Ahmedt-Aristizabal\\rCategories: cs.CV\\rComments: In the IEEE International Symposium on Biomedical Imaging (ISBI) 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.15836 ,  29699kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.17119\\rreplaced with revised version Sat, 2 Mar 2024 13:07:56 GMT   (43493kb,D)\\r\\rTitle: Continuous Pose for Monocular Cameras in Neural Implicit Representation\\rAuthors: Qi Ma, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2311.17119 ,  43493kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.00786\\rreplaced with revised version Mon, 4 Mar 2024 17:24:31 GMT   (15299kb,D)\\r\\rTitle: Dense Optical Tracking: Connecting the Dots\\rAuthors: Guillaume Le Moing, Jean Ponce, Cordelia Schmid\\rCategories: cs.CV\\rComments: Accepted to CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.00786 ,  15299kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.04265\\rreplaced with revised version Mon, 4 Mar 2024 14:25:32 GMT   (1584kb,D)\\r\\rTitle: Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for\\r  Domain Generalized Semantic Segmentation\\rAuthors: Zhixiang Wei, Lin Chen, Yi Jin, Xiaoxiao Ma, Tianle Liu, Pengyang\\r  Ling, Ben Wang, Huaian Chen, Jinjin Zheng\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2312.04265 ,  1584kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.05476\\rreplaced with revised version Mon, 4 Mar 2024 13:30:44 GMT   (22851kb,D)\\r\\rTitle: Exploring the Naturalness of AI-Generated Images\\rAuthors: Zijian Chen, Wei Sun, Haoning Wu, Zicheng Zhang, Jun Jia, Zhongpeng\\r  Ji, Fengyu Sun, Shangling Jui, Xiongkuo Min, Guangtao Zhai, Wenjun Zhang\\rCategories: cs.CV\\rComments: 33 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2312.05476 ,  22851kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.05490\\rreplaced with revised version Mon, 4 Mar 2024 06:40:01 GMT   (12486kb,D)\\r\\rTitle: Shapley Values-enabled Progressive Pseudo Bag Augmentation for Whole\\r  Slide Image Classification\\rAuthors: Renao Yan, Qiehe Sun, Cheng Jin, Yiqing Liu, Yonghong He, Tian Guan,\\r  Hao Chen\\rCategories: cs.CV\\rComments: submitted to IEEE TRANSACTIONS ON MEDICAL IMAGING\\r\\\\\\\\ ( https://arxiv.org/abs/2312.05490 ,  12486kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.07472\\rreplaced with revised version Mon, 4 Mar 2024 17:41:52 GMT   (21294kb,D)\\r\\rTitle: MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active\\r  Perception\\rAuthors: Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao\\r  Zhang, Yu Qiao, Jing Shao\\rCategories: cs.CV\\rComments: Accepted to CVPR2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.07472 ,  21294kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.07806\\rreplaced with revised version Mon, 4 Mar 2024 13:39:52 GMT   (1991kb,D)\\r\\rTitle: Contextually Affinitive Neighborhood Refinery for Deep Clustering\\rAuthors: Chunlin Yu, Ye Shi, Jingya Wang\\rCategories: cs.CV\\rComments: Accepted to NeurIPS 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2312.07806 ,  1991kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.08664\\rreplaced with revised version Sun, 3 Mar 2024 08:33:01 GMT   (1643kb,D)\\r\\rTitle: SPEAL: Skeletal Prior Embedded Attention Learning for Cross-Source Point\\r  Cloud Registration\\rAuthors: Kezheng Xiong, Maoji Zheng, Qingshan Xu, Chenglu Wen, Siqi Shen, Cheng\\r  Wang\\rCategories: cs.CV\\rComments: Accepted by AAAI2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.08664 ,  1643kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.08886\\rreplaced with revised version Sun, 3 Mar 2024 08:08:03 GMT   (2595kb,D)\\r\\rTitle: Diffusion-based Blind Text Image Super-Resolution\\rAuthors: Yuzhe Zhang, Jiawei Zhang, Hao Li, Zhouxia Wang, Luwei Hou, Dongqing\\r  Zou, Liheng Bian\\rCategories: cs.CV\\rComments: Accepted by CVPR2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.08886 ,  2595kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.10930\\rreplaced with revised version Mon, 4 Mar 2024 06:12:17 GMT   (4450kb,D)\\r\\rTitle: Deep Learning Approaches for Seizure Video Analysis: A Review\\rAuthors: David Ahmedt-Aristizabal, Mohammad Ali Armin, Zeeshan Hayder, Norberto\\r  Garcia-Cairasco, Lars Petersson, Clinton Fookes, Simon Denman, Aileen\\r  McGonigal\\rCategories: cs.CV\\rComments: Accepted in Epilepsy & Behavior\\r\\\\\\\\ ( https://arxiv.org/abs/2312.10930 ,  4450kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11954\\rreplaced with revised version Fri, 1 Mar 2024 19:15:05 GMT   (1303kb,D)\\r\\rTitle: Adversarial AutoMixup\\rAuthors: Huafeng Qin, Xin Jin, Yun Jiang, Mounim A. El-Yacoubi, Xinbo Gao\\rCategories: cs.CV\\rComments: ICLR 2024 Camera Ready.(19 pages) with the source code at\\r  https://github.com/JinXins/Adversarial-AutoMixup\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11954 ,  1303kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.14404\\rreplaced with revised version Mon, 4 Mar 2024 05:55:08 GMT   (9232kb,D)\\r\\rTitle: Cross-Covariate Gait Recognition: A Benchmark\\rAuthors: Shinan Zou, Chao Fan, Jianbo Xiong, Chuanfu Shen, Shiqi Yu, Jin Tang\\rCategories: cs.CV\\rComments: AAAI2024\\rACM-class: I.5\\rJournal-ref: AAAI2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.14404 ,  9232kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.01749\\rreplaced with revised version Sun, 3 Mar 2024 03:00:33 GMT   (26883kb,D)\\r\\rTitle: Few-shot Image Generation via Information Transfer from the Built\\r  Geodesic Surface\\rAuthors: Yuexing Han and Liheng Ruan and Bing Wang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.01749 ,  26883kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.04330\\rreplaced with revised version Sun, 3 Mar 2024 08:39:20 GMT   (10368kb,D)\\r\\rTitle: BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method\\r  guided by multi-scale feature information aggregation\\rAuthors: Yonghui Tan, Xiaolong Li, Yishu Chen and Jinquan Ai\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2401.04330 ,  10368kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.05638\\rreplaced with revised version Sat, 2 Mar 2024 10:23:23 GMT   (29978kb,D)\\r\\rTitle: MatSAM: Efficient Extraction of Microstructures of Materials via Visual\\r  Large Model\\rAuthors: Changtai Li, Xu Han, Chao Yao, Xiaojuan Ban\\rCategories: cs.CV\\rComments: 18 pages, 8 figures, and 5 tables. Updated with revision and code\\r  repository\\r\\\\\\\\ ( https://arxiv.org/abs/2401.05638 ,  29978kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.10731\\rreplaced with revised version Sat, 2 Mar 2024 05:20:54 GMT   (4421kb,D)\\r\\rTitle: Removal and Selection: Improving RGB-Infrared Object Detection via\\r  Coarse-to-Fine Fusion\\rAuthors: Tianyi Zhao, Maoxun Yuan, Xingxing Wei\\rCategories: cs.CV\\rComments: 9pages, 7figures\\r\\\\\\\\ ( https://arxiv.org/abs/2401.10731 ,  4421kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.00321\\rreplaced with revised version Mon, 4 Mar 2024 05:37:29 GMT   (1418kb,D)\\r\\rTitle: SmartCooper: Vehicular Collaborative Perception with Adaptive Fusion and\\r  Judger Mechanism\\rAuthors: Yuang Zhang, Haonan An, Zhengru Fang, Guowen Xu, Yuan Zhou, Xianhao\\r  Chen and Yuguang Fang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2402.00321 ,  1418kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.00712\\rreplaced with revised version Fri, 1 Mar 2024 21:53:36 GMT   (20244kb,D)\\r\\rTitle: ChaosBench: A Multi-Channel, Physics-Based Benchmark for\\r  Subseasonal-to-Seasonal Climate Prediction\\rAuthors: Juan Nathaniel, Yongquan Qu, Tung Nguyen, Sungduk Yu, Julius Busecke,\\r  Aditya Grover, Pierre Gentine\\rCategories: cs.CV cs.CY\\rComments: 45 pages, 39 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2402.00712 ,  20244kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.02733\\rreplaced with revised version Mon, 4 Mar 2024 08:21:48 GMT   (12463kb,D)\\r\\rTitle: ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer\\rAuthors: Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo\\rCategories: cs.CV cs.AI cs.GR cs.LG cs.MM\\rComments: 8 pages, 9 figures, 1 table\\r\\\\\\\\ ( https://arxiv.org/abs/2402.02733 ,  12463kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.03246\\rreplaced with revised version Sat, 2 Mar 2024 13:49:10 GMT   (39505kb,D)\\r\\rTitle: SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM\\rAuthors: Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang\\rCategories: cs.CV cs.AI cs.RO\\r\\\\\\\\ ( https://arxiv.org/abs/2402.03246 ,  39505kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.05423\\rreplaced with revised version Mon, 4 Mar 2024 05:45:20 GMT   (2995kb,D)\\r\\rTitle: MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking\\r  Neural Network\\rAuthors: Chengzhi Liu, Zheng Tao, Zihong Luo, Chenghao Liu\\rCategories: cs.CV\\rComments: 6 pages, 6 figures, published to International Conference on Computer\\r  Supported Cooperative Work in Design\\rJournal-ref: International Conference on Computer Supported Cooperative Work in\\r  Design 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2402.05423 ,  2995kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.05448\\rreplaced with revised version Sun, 3 Mar 2024 10:02:54 GMT   (7283kb,D)\\r\\rTitle: Minecraft-ify: Minecraft Style Image Generation with Text-guided Image\\r  Editing for In-Game Application\\rAuthors: Bumsoo Kim, Sanghyun Byun, Yonghoon Jung, Wonseop Shin, Sareer UI\\r  Amin, Sanghyun Seo\\rCategories: cs.CV cs.AI cs.GR cs.LG cs.MM\\rComments: 2 pages, 2 figures. Accepted as Spotlight to NeurIPS 2023 Workshop on\\r  Machine Learning for Creativity and Design\\r\\\\\\\\ ( https://arxiv.org/abs/2402.05448 ,  7283kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.12721\\rreplaced with revised version Sat, 2 Mar 2024 10:31:40 GMT   (3676kb,D)\\r\\rTitle: PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for\\r  Recognizing Low-Quality Images\\rAuthors: Jinsung Jeon, Hyundong Jin, Jonghyun Choi, Sanghyun Hong, Dongeun Lee,\\r  Kookjin Lee, Noseong Park\\rCategories: cs.CV cs.AI\\rComments: Accepted at ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2402.12721 ,  3676kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.12728\\rreplaced with revised version Sun, 3 Mar 2024 04:51:28 GMT   (402kb,D)\\r\\rTitle: Modality-Aware Integration with Large Language Models for\\r  Knowledge-based Visual Question Answering\\rAuthors: Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao\\r  Huang\\rCategories: cs.CV cs.AI cs.CL cs.IR cs.LG\\rComments: 8 pages,3 figures and 1 page appendix; The processed graphs and codes\\r  will be avalibale\\r\\\\\\\\ ( https://arxiv.org/abs/2402.12728 ,  402kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.13172\\rreplaced with revised version Mon, 4 Mar 2024 00:10:12 GMT   (6761kb,D)\\r\\rTitle: 3D Kinematics Estimation from Video with a Biomechanical Model and\\r  Synthetic Training Data\\rAuthors: Zhi-Yi Lin, Bofan Lyu, Judith Cueto Fernandez, Eline van der Kruk,\\r  Ajay Seth, Xucong Zhang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2402.13172 ,  6761kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.13929\\rreplaced with revised version Sat, 2 Mar 2024 09:09:32 GMT   (16120kb,D)\\r\\rTitle: SDXL-Lightning: Progressive Adversarial Diffusion Distillation\\rAuthors: Shanchuan Lin, Anran Wang, Xiao Yang\\rCategories: cs.CV cs.AI cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2402.13929 ,  16120kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.16338\\rreplaced with revised version Sat, 2 Mar 2024 00:22:06 GMT   (5384kb,D)\\r\\rTitle: BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning\\r  of SAM\\rAuthors: Li Zhang, Youwei Liang, Ruiyi Zhang, Pengtao Xie\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2402.16338 ,  5384kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.16627\\rreplaced with revised version Mon, 4 Mar 2024 14:17:04 GMT   (35014kb,D)\\r\\rTitle: Cross-Modal Contextualized Diffusion Models for Text-Guided Visual\\r  Generation and Editing\\rAuthors: Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano\\r  Ermon, Bin Cui\\rCategories: cs.CV\\rComments: ICLR 2024. Project: https://github.com/YangLing0818/ContextDiff\\r\\\\\\\\ ( https://arxiv.org/abs/2402.16627 ,  35014kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.16641\\rreplaced with revised version Mon, 4 Mar 2024 07:06:22 GMT   (7564kb,D)\\r\\rTitle: Towards Open-ended Visual Quality Comparison\\rAuthors: Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,\\r  Liang Liao, Chunyi Li, Annan Wang, Wenxiu Sun, Qiong Yan, Xiaohong Liu,\\r  Guangtao Zhai, Shiqi Wang, and Weisi Lin\\rCategories: cs.CV\\rComments: Fix typos\\r\\\\\\\\ ( https://arxiv.org/abs/2402.16641 ,  7564kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17113\\rreplaced with revised version Fri, 1 Mar 2024 21:36:19 GMT   (25100kb,D)\\r\\rTitle: Transparent Image Layer Diffusion using Latent Transparency\\rAuthors: Lvmin Zhang, Maneesh Agrawala\\rCategories: cs.CV cs.GR\\rComments: 44 pages, 37 figures, github.com/layerdiffusion/LayerDiffuse\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17113 ,  25100kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17204\\rreplaced with revised version Fri, 1 Mar 2024 21:02:29 GMT   (7882kb,D)\\r\\rTitle: Advancing Generative Model Evaluation: A Novel Algorithm for Realistic\\r  Image Synthesis and Comparison in OCR System\\rAuthors: Majid Memari, Khaled R. Ahmed, Shahram Rahimi, Noorbakhsh Amiri\\r  Golilarz\\rCategories: cs.CV\\rComments: The manuscript was submitted to IEEE Access on 29-Jan-2024 and is\\r  currently under review for publication in IEEE Access\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17204 ,  7882kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17563\\rreplaced with revised version Mon, 4 Mar 2024 14:51:40 GMT   (7124kb,D)\\r\\rTitle: Structure-Guided Adversarial Training of Diffusion Models\\rAuthors: Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui\\rCategories: cs.CV\\rComments: Accepted by CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17563 ,  7124kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17664\\rreplaced with revised version Mon, 4 Mar 2024 14:35:25 GMT   (44199kb,D)\\r\\rTitle: Bayesian Differentiable Physics for Cloth Digitalization\\rAuthors: Deshan Gong, Ningtao Mao, He Wang\\rCategories: cs.CV\\rComments: 9 pages, 8 figures, to be published in CVPR\\rACM-class: F.4.8; I.6.8\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17664 ,  44199kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17862\\rreplaced with revised version Mon, 4 Mar 2024 05:59:58 GMT   (957kb,D)\\r\\rTitle: REPrune: Channel Pruning via Kernel Representative Selection\\rAuthors: Mincheol Park, Dongjin Kim, Cheonjun Park, Yuna Park, Gyeong Eun Gong,\\r  Won Woo Ro, Suhyun Kim\\rCategories: cs.CV cs.AI\\rComments: Published at AAAI2024\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17862 ,  957kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.18028\\rreplaced with revised version Mon, 4 Mar 2024 02:22:58 GMT   (942kb,D)\\r\\rTitle: OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models\\r  in Medicine\\rAuthors: Xiaosong Wang and Xiaofan Zhang and Guotai Wang and Junjun He and\\r  Zhongyu Li and Wentao Zhu and Yi Guo and Qi Dou and Xiaoxiao Li and Dequan\\r  Wang and Liang Hong and Qicheng Lao and Tong Ruan and Yukun Zhou and Yixue Li\\r  and Jie Zhao and Kang Li and Xin Sun and Lifeng Zhu and Shaoting Zhang\\rCategories: cs.CV\\rComments: Technical Report. Visit https://github.com/openmedlab for more\\r  details\\r\\\\\\\\ ( https://arxiv.org/abs/2402.18028 ,  942kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.18086\\rreplaced with revised version Sun, 3 Mar 2024 14:58:50 GMT   (877kb,D)\\r\\rTitle: Generalizable Two-Branch Framework for Image Class-Incremental Learning\\rAuthors: Chao Wu, Xiaobin Chang, Ruixuan Wang\\rCategories: cs.CV\\rComments: 5 pages,3 figures,accepted by ICASSP 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2402.18086 ,  877kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.18122\\rreplaced with revised version Sat, 2 Mar 2024 14:47:58 GMT   (4712kb,D)\\r\\rTitle: G4G:A Generic Framework for High Fidelity Talking Face Generation with\\r  Fine-grained Intra-modal Alignment\\rAuthors: Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu\\rCategories: cs.CV cs.MM\\r\\\\\\\\ ( https://arxiv.org/abs/2402.18122 ,  4712kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.18351\\rreplaced with revised version Sat, 2 Mar 2024 14:08:03 GMT   (48891kb,D)\\r\\rTitle: LatentSwap: An Efficient Latent Code Mapping Framework for Face Swapping\\rAuthors: Changho Choi, Minho Kim, Junhyeok Lee, Hyoung-Kyu Song, Younggeun Kim,\\r  Seungryong Kim\\rCategories: cs.CV\\rComments: 9 pages, 11 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2402.18351 ,  48891kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.18919\\rreplaced with revised version Sat, 2 Mar 2024 14:57:12 GMT   (42058kb,D)\\r\\rTitle: Decompose-and-Compose: A Compositional Approach to Mitigating Spurious\\r  Correlation\\rAuthors: Fahimeh Hosseini Noohdani, Parsa Hosseini, Aryan Yazdan Parast,\\r  Hamidreza Yaghoubi Araghi, Mahdieh Soleymani Baghshah\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2402.18919 ,  42058kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.18920\\rreplaced with revised version Mon, 4 Mar 2024 15:43:55 GMT   (43303kb,D)\\r\\rTitle: Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation\\rAuthors: Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers,\\r  Florian Bernard\\rCategories: cs.CV cs.AI cs.CG\\rComments: accepted by CVPR2024\\r\\\\\\\\ ( https://arxiv.org/abs/2402.18920 ,  43303kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.19405\\rreplaced with revised version Sun, 3 Mar 2024 09:19:44 GMT   (28056kb,D)\\r\\rTitle: Navigating Hallucinations for Reasoning of Unintentional Activities\\rAuthors: Shresth Grover, Vibhav Vineet, Yogesh S Rawat\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2402.19405 ,  28056kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00231\\rreplaced with revised version Mon, 4 Mar 2024 07:01:59 GMT   (5378kb,D)\\r\\rTitle: Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of\\r  Large Vision-Language Models\\rAuthors: Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng\\r  Kong, Qi Liu\\rCategories: cs.CV cs.CL\\rComments: Project page: https://mm-arxiv.github.io Fix typos\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00231 ,  5378kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00326\\rreplaced with revised version Mon, 4 Mar 2024 13:33:49 GMT   (37019kb,D)\\r\\rTitle: DAMS-DETR: Dynamic Adaptive Multispectral Detection Transformer with\\r  Competitive Query Selection and Adaptive Feature Fusion\\rAuthors: Junjie Guo, Chenqiang Gao, Fangcen Liu and Deyu Meng\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00326 ,  37019kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.00459\\rreplaced with revised version Mon, 4 Mar 2024 10:22:38 GMT   (17282kb,D)\\r\\rTitle: Deformable One-shot Face Stylization via DINO Semantic Guidance\\rAuthors: Yang Zhou and Zichong Chen and Hui Huang\\rCategories: cs.CV\\rComments: Accepted to CVPR 2024. Project page:\\r  https://github.com/zichongc/DoesFS\\r\\\\\\\\ ( https://arxiv.org/abs/2403.00459 ,  17282kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.05499\\rreplaced with revised version Sat, 2 Mar 2024 09:12:23 GMT   (729kb,D)\\r\\rTitle: Prompt Injection attack against LLM-integrated Applications\\rAuthors: Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng\\r  Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng and Yang Liu\\rCategories: cs.CR cs.AI cs.CL cs.SE\\r\\\\\\\\ ( https://arxiv.org/abs/2306.05499 ,  729kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.08018 (*cross-listing*)\\rreplaced with revised version Mon, 4 Mar 2024 12:49:31 GMT   (17523kb,D)\\r\\rTitle: Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for\\r  Large Language Models\\rAuthors: Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo\\r  Chen, Xiaohui Fan, Huajun Chen\\rCategories: q-bio.QM cs.AI cs.CE cs.CL cs.IR cs.LG\\rComments: ICLR 2024. Project homepage:\\r  https://github.com/zjunlp/Mol-Instructions\\r\\\\\\\\ ( https://arxiv.org/abs/2306.08018 ,  17523kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.09729\\rreplaced with revised version Sat, 2 Mar 2024 15:02:33 GMT   (1445kb,D)\\r\\rTitle: MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large\\r  Language Models\\rAuthors: Yilin Wen, Zifeng Wang, Jimeng Sun\\rCategories: cs.AI cs.CL cs.LG\\rComments: 8 pages, 8 figures, 12 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2308.09729 ,  1445kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.02207\\rreplaced with revised version Mon, 4 Mar 2024 18:25:29 GMT   (6793kb,D)\\r\\rTitle: Language Models Represent Space and Time\\rAuthors: Wes Gurnee, Max Tegmark\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2310.02207 ,  6793kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.11984\\rreplaced with revised version Sun, 3 Mar 2024 09:19:51 GMT   (1672kb,D)\\r\\rTitle: From Interpolation to Extrapolation: Complete Length Generalization for\\r  Arithmetic Transformers\\rAuthors: Shaoxiong Duan, Yining Shi, Wei Xu\\rCategories: cs.LG cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2310.11984 ,  1672kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.16119\\rreplaced with revised version Sun, 3 Mar 2024 00:12:16 GMT   (4756kb,D)\\r\\rTitle: Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of\\r  LLMs through a Global Scale Prompt Hacking Competition\\rAuthors: Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-Fran\\\\c{c}ois\\r  Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost,\\r  Christopher Carnahan, Jordan Boyd-Graber\\rCategories: cs.CR cs.AI cs.CL\\rComments: 34 pages, 8 figures Codebase:\\r  https://github.com/PromptLabs/hackaprompt Dataset:\\r  https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset/blob/main/README.md\\r  Playground: https://huggingface.co/spaces/hackaprompt/playground\\r\\\\\\\\ ( https://arxiv.org/abs/2311.16119 ,  4756kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.15068\\rreplaced with revised version Mon, 4 Mar 2024 17:03:42 GMT   (345kb,D)\\r\\rTitle: Refining GPT-3 Embeddings with a Siamese Structure for Technical Post\\r  Duplicate Detection\\rAuthors: Xingfang Wu, Heng Li, Nobukazu Yoshioka, Hironori Washizaki, Foutse\\r  Khomh\\rCategories: cs.SE cs.CL cs.LG\\rComments: SANER 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.15068 ,  345kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.13782\\rreplaced with revised version Sun, 3 Mar 2024 08:16:52 GMT   (7567kb,D)\\r\\rTitle: Tweets to Citations: Unveiling the Impact of Social Media Influencers on\\r  AI Research Visibility\\rAuthors: Iain Xie Weissburg, Mehir Arora, Xinyi Wang, Liangming Pan, William\\r  Yang Wang\\rCategories: cs.DL cs.AI cs.CL cs.CV cs.LG cs.SI\\rComments: 10 Pages, 14 Figures\\r\\\\\\\\ ( https://arxiv.org/abs/2401.13782 ,  7567kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.15422\\rreplaced with revised version Mon, 4 Mar 2024 16:55:15 GMT   (540kb,D)\\r\\rTitle: A Survey on Data Augmentation in Large Model Era\\rAuthors: Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, and Yuan Wu\\rCategories: cs.LG cs.CL cs.CV\\rComments: 33 pages; https://github.com/MLGroup-JLU/LLM-data-aug-survey\\r\\\\\\\\ ( https://arxiv.org/abs/2401.15422 ,  540kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.18018\\rreplaced with revised version Mon, 4 Mar 2024 06:31:21 GMT   (3015kb,D)\\r\\rTitle: On Prompt-Driven Safeguarding for Large Language Models\\rAuthors: Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei\\r  Chang, Minlie Huang, Nanyun Peng\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2401.18018 ,  3015kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.02446\\rreplaced with revised version Mon, 4 Mar 2024 12:18:47 GMT   (396kb,D)\\r\\rTitle: LQER: Low-Rank Quantization Error Reconstruction for LLMs\\rAuthors: Cheng Zhang, Jianyi Cheng, George A. Constantinides, and Yiren Zhao\\rCategories: cs.LG cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2402.02446 ,  396kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.03181\\rreplaced with revised version Sun, 3 Mar 2024 18:13:54 GMT   (14509kb,D)\\r\\rTitle: C-RAG: Certified Generation Risks for Retrieval-Augmented Language\\r  Models\\rAuthors: Mintong Kang, Nezihe Merve G\\\\urel, Ning Yu, Dawn Song, Bo Li\\rCategories: cs.AI cs.CL cs.IR\\r\\\\\\\\ ( https://arxiv.org/abs/2402.03181 ,  14509kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.07787\\rreplaced with revised version Mon, 4 Mar 2024 08:42:32 GMT   (8194kb,D)\\r\\rTitle: Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment\\r  Analysis\\rAuthors: Xiaowei Zhao, Yong Zhou, Xiujuan Xu, Yu Liu\\rCategories: cs.AI cs.CL\\rComments: 8 pages, 4 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2402.07787 ,  8194kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.08939\\rreplaced with revised version Mon, 4 Mar 2024 09:21:16 GMT   (7955kb,D)\\r\\rTitle: Premise Order Matters in Reasoning with Large Language Models\\rAuthors: Xinyun Chen, Ryan A. Chi, Xuezhi Wang, Denny Zhou\\rCategories: cs.AI cs.CL\\rComments: Xinyun and Ryan contribute equally\\r\\\\\\\\ ( https://arxiv.org/abs/2402.08939 ,  7955kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.11253\\rreplaced with revised version Sun, 3 Mar 2024 21:37:16 GMT   (370kb,D)\\r\\rTitle: Aligning Large Language Models by On-Policy Self-Judgment\\rAuthors: Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min\\r  Yoo, Youngjae Yu\\rCategories: cs.LG cs.AI cs.CL\\rComments: Preprint; Under review\\r\\\\\\\\ ( https://arxiv.org/abs/2402.11253 ,  370kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.14547\\rreplaced with revised version Mon, 4 Mar 2024 16:54:25 GMT   (1889kb,D)\\r\\rTitle: OmniPred: Language Models as Universal Regressors\\rAuthors: Xingyou Song, Oscar Li, Chansoo Lee, Bangding Yang, Daiyi Peng, Sagi\\r  Perel, Yutian Chen\\rCategories: cs.LG cs.AI cs.CL cs.DB\\rComments: 24 pages, 10 figures. Code can be found in\\r  https://github.com/google-research/optformer/tree/main/optformer/omnipred\\r\\\\\\\\ ( https://arxiv.org/abs/2402.14547 ,  1889kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.16906\\rreplaced with revised version Sat, 2 Mar 2024 00:04:53 GMT   (782kb,D)\\r\\rTitle: LDB: A Large Language Model Debugger via Verifying Runtime Execution\\r  Step-by-step\\rAuthors: Li Zhong, Zilong Wang, Jingbo Shang\\rCategories: cs.SE cs.AI cs.CL\\rComments: Preprint\\r\\\\\\\\ ( https://arxiv.org/abs/2402.16906 ,  782kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:1907.01743 (*cross-listing*)\\rreplaced with revised version Sun, 3 Mar 2024 11:57:48 GMT   (5731kb,D)\\r\\rTitle: Deep Attentive Features for Prostate Segmentation in 3D Transrectal\\r  Ultrasound\\rAuthors: Yi Wang, Haoran Dou, Xiaowei Hu, Lei Zhu, Xin Yang, Ming Xu, Jing Qin,\\r  Pheng-Ann Heng, Tianfu Wang, and Dong Ni\\rCategories: eess.IV cs.AI cs.CV cs.LG\\rComments: 11 pages, 10 figures, 2 tables. Accepted by IEEE transactions on\\r  Medical Imaging\\rDOI: 10.1109/TMI.2019.2913184\\r\\\\\\\\ ( https://arxiv.org/abs/1907.01743 ,  5731kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:1910.04331 (*cross-listing*)\\rreplaced with revised version Sun, 3 Mar 2024 12:01:18 GMT   (4946kb,D)\\r\\rTitle: Agent with Warm Start and Active Termination for Plane Localization in\\r  3D Ultrasound\\rAuthors: Haoran Dou, Xin Yang, Jikuan Qian, Wufeng Xue, Hao Qin, Xu Wang,\\r  Lequan Yu, Shujun Wang, Yi Xiong, Pheng-Ann Heng, Dong Ni\\rCategories: eess.IV cs.CV cs.LG\\rComments: 9 pages, 5 figures, 1 table. Accepted by MICCAI 2019 (oral)\\r\\\\\\\\ ( https://arxiv.org/abs/1910.04331 ,  4946kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2207.07827\\rreplaced with revised version Mon, 4 Mar 2024 05:48:45 GMT   (7453kb,D)\\r\\rTitle: CLMFormer: Mitigating Data Redundancy to Revitalize Transformer-based\\r  Long-Term Time Series Forecasting System\\rAuthors: Mingjie Li, Rui Liu, Guangsi Shi, Mingfei Han, Changling Li, Lina Yao,\\r  Xiaojun Chang, and Ling Chen\\rCategories: cs.LG cs.CV\\rComments: Tech report\\r\\\\\\\\ ( https://arxiv.org/abs/2207.07827 ,  7453kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.01607 (*cross-listing*)\\rreplaced with revised version Mon, 4 Mar 2024 17:52:04 GMT   (1623kb,D)\\r\\rTitle: A Generative Shape Compositional Framework to Synthesise Populations of\\r  Virtual Chimaeras\\rAuthors: Haoran Dou, Seppo Virtanen, Nishant Ravikumar, Alejandro F. Frangi\\rCategories: eess.IV cs.CV cs.LG\\rComments: 15 pages, 4 figures, 4 tables. Accepted by IEEE Transactions on\\r  Neural Networks and Learning Systems\\r\\\\\\\\ ( https://arxiv.org/abs/2210.01607 ,  1623kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.11517 (*cross-listing*)\\rreplaced with revised version Sat, 2 Mar 2024 07:09:37 GMT   (9264kb,D)\\r\\rTitle: A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate\\r  Detection\\rAuthors: Wei Tang, Kangning Cui, and Raymond H. Chan\\rCategories: eess.IV cs.CV cs.LG\\rComments: 8 pages, 3 figures, 2 tables. To appear in ISBI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2302.11517 ,  9264kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.09910\\rreplaced with revised version Fri, 1 Mar 2024 22:38:16 GMT   (561kb,D)\\r\\rTitle: LabelBench: A Comprehensive Framework for Benchmarking Adaptive\\r  Label-Efficient Learning\\rAuthors: Jifan Zhang, Yifang Chen, Gregory Canal, Stephen Mussmann, Arnav M.\\r  Das, Gantavya Bhatt, Yinglun Zhu, Jeffrey Bilmes, Simon Shaolei Du, Kevin\\r  Jamieson, Robert D Nowak\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.09910 ,  561kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.02561\\rreplaced with revised version Sun, 3 Mar 2024 08:12:36 GMT   (12155kb,D)\\r\\rTitle: Physically Grounded Vision-Language Models for Robotic Manipulation\\rAuthors: Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian\\r  Ichter, Anirudha Majumdar, Dorsa Sadigh\\rCategories: cs.RO cs.AI cs.CV\\rComments: Updated version for ICRA 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2309.02561 ,  12155kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.03891\\rreplaced with revised version Sun, 3 Mar 2024 18:53:09 GMT   (21275kb,D)\\r\\rTitle: ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous\\r  Grasping and Articulation\\rAuthors: Hui Zhang, Sammy Christen, Zicong Fan, Luocheng Zheng, Jemin Hwangbo,\\r  Jie Song, Otmar Hilliges\\rCategories: cs.RO cs.CV cs.LG\\rComments: 3DV-2024 camera ready. Project page:\\r  https://eth-ait.github.io/artigrasp/\\r\\\\\\\\ ( https://arxiv.org/abs/2309.03891 ,  21275kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.10314\\rreplaced with revised version Mon, 4 Mar 2024 01:43:21 GMT   (1503kb,D)\\r\\rTitle: Dive Deeper into Rectifying Homography for Stereo Camera Online\\r  Self-Calibration\\rAuthors: Hongbo Zhao, Yikang Zhang, Qijun Chen, Rui Fan\\rCategories: cs.RO cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.10314 ,  1503kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.13150\\rreplaced with revised version Sun, 3 Mar 2024 02:54:34 GMT   (4592kb,D)\\r\\rTitle: Pixel-wise Smoothing for Certified Robustness against Camera Motion\\r  Perturbations\\rAuthors: Hanjiang Hu, Zuxin Liu, Linyi Li, Jiacheng Zhu, Ding Zhao\\rCategories: cs.LG cs.CV cs.RO\\rComments: Camera-ready version of AISTATS 2024, 30 pages, 5 figures, 13 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2309.13150 ,  4592kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.15135\\rreplaced with revised version Mon, 4 Mar 2024 05:05:24 GMT   (15652kb,D)\\r\\rTitle: Contrastive Continual Multi-view Clustering with Filtered Structural\\r  Fusion\\rAuthors: Xinhang Wan, Jiyuan Liu, Hao Yu, Ao Li, Xinwang Liu, Ke Liang, Zhibin\\r  Dong, En Zhu\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.15135 ,  15652kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.15459\\rreplaced with revised version Sat, 2 Mar 2024 09:50:24 GMT   (8686kb,D)\\r\\rTitle: GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on\\r  Online Grasping Pose Fusion\\rAuthors: Jiazhao Zhang, Nandiraju Gireesh, Jilong Wang, Xiaomeng Fang, Chaoyi\\r  Xu, Weiguang Chen, Liu Dai, and He Wang\\rCategories: cs.RO cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.15459 ,  8686kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.12574 (*cross-listing*)\\rreplaced with revised version Mon, 4 Mar 2024 05:33:15 GMT   (420kb)\\r\\rTitle: A reproducible 3D convolutional neural network with dual attention\\r  module (3D-DAM) for Alzheimer's disease classification\\rAuthors: Thanh Phuong Vu, Tien Nhat Nguyen, N. Minh Nhat Hoang, and Gia Minh\\r  Hoang\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2310.12574 ,  420kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.05588\\rreplaced with revised version Sun, 3 Mar 2024 09:11:38 GMT   (20421kb,D)\\r\\rTitle: Language-assisted Vision Model Debugger: A Sample-Free Approach to\\r  Finding and Fixing Bugs\\rAuthors: Chaoquan Jiang, Jinqiang Wang, Rui Hu, Jitao Sang\\rCategories: cs.AI cs.CV\\rComments: 10 pages,8 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2312.05588 ,  20421kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.09854 (*cross-listing*)\\rreplaced with revised version Mon, 4 Mar 2024 15:21:18 GMT   (3941kb)\\r\\rTitle: Q-Segment: Segmenting Images In-Sensor for Vessel-Based Medical\\r  Diagnosis\\rAuthors: Pietro Bonazzi, Yawei Li, Sizhen Bian, Michele Magno\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2312.09854 ,  3941kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11436 (*cross-listing*)\\rreplaced with revised version Sun, 3 Mar 2024 16:31:58 GMT   (14340kb,D)\\r\\rTitle: Layerwise complexity-matched learning yields an improved model of\\r  cortical area V2\\rAuthors: Nikhil Parthasarathy, Olivier J. H\\\\'enaff, Eero P. Simoncelli\\rCategories: q-bio.NC cs.CV cs.LG\\rComments: 28 pages, 12 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11436 ,  14340kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00873\\rreplaced with revised version Mon, 4 Mar 2024 09:24:35 GMT   (4843kb,D)\\r\\rTitle: A Bayesian Unification of Self-Supervised Clustering and Energy-Based\\r  Models\\rAuthors: Emanuele Sansone and Robin Manhaeve\\rCategories: cs.LG cs.CV\\rComments: Changes from previous version: added mean and standard deviations in\\r  experiments. Integral version of workshop paper arXiv:2309.15420. Improved\\r  GEDI version (from two stages to single stage training) arxiv:2212.13425\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00873 ,  4843kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.06182 (*cross-listing*)\\rreplaced with revised version Sat, 2 Mar 2024 17:59:41 GMT   (5331kb,D)\\r\\rTitle: Prediction of Cellular Identities from Trajectory and Cell Fate\\r  Information\\rAuthors: Baiyang Dai, Jiamin Yang, Hari Shroff, Patrick La Riviere\\rCategories: q-bio.QM cs.CV cs.LG eess.IV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.06182 ,  5331kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.09630 (*cross-listing*)\\rreplaced with revised version Sat, 2 Mar 2024 00:08:16 GMT   (1557kb,D)\\r\\rTitle: CT Liver Segmentation via PVT-based Encoding and Refined Decoding\\rAuthors: Debesh Jha, Nikhil Kumar Tomar, Koushik Biswas, Gorkem Durak, Alpay\\r  Medetalibeyoglu, Matthew Antalek, Yury Velichko, Daniela Ladner, Amir\\r  Borhani, Ulas Bagci\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.09630 ,  1557kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.12938 (*cross-listing*)\\rreplaced with revised version Mon, 4 Mar 2024 09:05:04 GMT   (17131kb,D)\\r\\rTitle: Neural deformation fields for template-based reconstruction of cortical\\r  surfaces from MRI\\rAuthors: Fabian Bongratz, Anne-Marie Rickmann, Christian Wachinger\\rCategories: eess.IV cs.CV\\rComments: To appear in Medical Image Analysis\\r\\\\\\\\ ( https://arxiv.org/abs/2401.12938 ,  17131kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.15380\\rreplaced with revised version Sat, 2 Mar 2024 11:23:57 GMT   (12052kb,D)\\r\\rTitle: Open-RadVLAD: Fast and Robust Radar Place Recognition\\rAuthors: Matthew Gadd, Paul Newman\\rCategories: cs.RO cs.CV\\rComments: accepted at 2024 IEEE Radar Conference\\r\\\\\\\\ ( https://arxiv.org/abs/2401.15380 ,  12052kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.16549\\rreplaced with revised version Sun, 3 Mar 2024 19:47:58 GMT   (1345kb)\\r\\rTitle: Deep Learning for Multi-Label Learning: A Comprehensive Survey\\rAuthors: Adane Nega Tarekegn, Mohib Ullah, Faouzi Alaya Cheikh\\rCategories: cs.LG cs.CV\\rComments: 21 pages, 12 figures, 5 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2401.16549 ,  1345kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.04584 (*cross-listing*)\\rreplaced with revised version Sun, 3 Mar 2024 03:48:48 GMT   (26254kb,D)\\r\\rTitle: Troublemaker Learning for Low-Light Image Enhancement\\rAuthors: Yinghao Song, Zhiyuan Cao, Wanhong Xiang, Sifan Long, Bo Yang, Hongwei\\r  Ge, Yanchun Liang, Chunguo Wu\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2402.04584 ,  26254kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.04921 (*cross-listing*)\\rreplaced with revised version Mon, 4 Mar 2024 02:41:21 GMT   (10991kb,D)\\r\\rTitle: Is Two-shot All You Need? A Label-efficient Approach for Video\\r  Segmentation in Breast Ultrasound\\rAuthors: Jiajun Zeng, Dong Ni, Ruobing Huang\\rCategories: eess.IV cs.CV\\rComments: 5 pages, 4 figure, 2 tables, accepted by ISBI 2024\\rACM-class: I.4.6\\r\\\\\\\\ ( https://arxiv.org/abs/2402.04921 ,  10991kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.13809\\rreplaced with revised version Sat, 2 Mar 2024 05:17:51 GMT   (0kb,I)\\r\\rTitle: NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual\\r  Feature Guided Diffusion\\rAuthors: Haoyu Li, Hao Wu, Badong Chen\\rCategories: cs.NE cs.AI cs.CV\\rComments: The implementation error lead to incorrect results in experiment\\r\\\\\\\\ ( https://arxiv.org/abs/2402.13809 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.14795\\rreplaced with revised version Fri, 1 Mar 2024 19:53:57 GMT   (5193kb,D)\\r\\rTitle: CyberDemo: Augmenting Simulated Human Demonstration for Real-World\\r  Dexterous Manipulation\\rAuthors: Jun Wang, Yuzhe Qin, Kaiming Kuang, Yigit Korkmaz, Akhilan\\r  Gurumoorthy, Hao Su, Xiaolong Wang\\rCategories: cs.RO cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2402.14795 ,  5193kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.16991 (*cross-listing*)\\rreplaced with revised version Mon, 4 Mar 2024 14:04:51 GMT   (6540kb,D)\\r\\rTitle: A Phase Transition in Diffusion Models Reveals the Hierarchical Nature\\r  of Data\\rAuthors: Antonio Sclocchi, Alessandro Favero, Matthieu Wyart\\rCategories: stat.ML cond-mat.dis-nn cs.CV cs.LG\\rComments: 21 pages, 16 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2402.16991 ,  6540kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.17797 (*cross-listing*)\\rreplaced with revised version Sat, 2 Mar 2024 18:31:36 GMT   (6422kb,D)\\r\\rTitle: Neural Radiance Fields in Medical Imaging: Challenges and Next Steps\\rAuthors: Xin Wang, Shu Hu, Heng Fan, Hongtu Zhu, Xin Li\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2402.17797 ,  6422kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.19341\\rreplaced with revised version Sun, 3 Mar 2024 15:21:03 GMT   (18205kb,D)\\r\\rTitle: RoadRunner - Learning Traversability Estimation for Autonomous Off-road\\r  Driving\\rAuthors: Jonas Frey and Shehryar Khattak and Manthan Patel and Deegan Atha and\\r  Julian Nubert and Curtis Padgett and Marco Hutter and Patrick Spieler\\rCategories: cs.RO cs.CV\\rComments: under review for Field Robotics\\r\\\\\\\\ ( https://arxiv.org/abs/2402.19341 ,  18205kb)\\r%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%--- ------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\rSend any comments regarding submissions directly to submitter.\\r------------------------------------------------------------------------------\\rArchives at http://arxiv.org/\\rTo unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel\\r------------------------------------------------------------------------------\\r Submissions to:\\rComputation and Language\\rComputer Vision and Pattern Recognition\\rGraphics\\r received from  Fri 29 Dec 23 19:00:00 GMT  to  Mon  1 Jan 24 19:00:00 GMT\\r------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00095\\rDate: Fri, 29 Dec 2023 23:05:18 GMT   (30kb,D)\\r\\rTitle: Automatic Essay Scoring in a Brazilian Scenario\\rAuthors: Felipe Akio Matsuoka\\rCategories: cs.CL\\rComments: 6 pages, 3 figures and 4 tables\\r\\\\\\\\\\r  This paper presents a novel Automatic Essay Scoring (AES) algorithm tailored\\rfor the Portuguese-language essays of Brazil's Exame Nacional do Ensino M\\\\'edio\\r(ENEM), addressing the challenges in traditional human grading systems. Our\\rapproach leverages advanced deep learning techniques to align closely with\\rhuman grading criteria, targeting efficiency and scalability in evaluating\\rlarge volumes of student essays. This research not only responds to the\\rlogistical and financial constraints of manual grading in Brazilian educational\\rassessments but also promises to enhance fairness and consistency in scoring,\\rmarking a significant step forward in the application of AES in large-scale\\racademic settings.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00095 ,  30kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00158\\rDate: Sat, 30 Dec 2023 07:18:54 GMT   (296kb,D)\\r\\rTitle: ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained\\r  Language Models for Question Answering over Knowledge Graph\\rAuthors: Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yaliang Li, Ji-Rong Wen\\rCategories: cs.CL cs.AI\\rComments: EMNLP-23-Main; simple but effective SOTA on CWQ under a\\r  weak-supervised setting\\r\\\\\\\\\\r  Question Answering over Knowledge Graph (KGQA) aims to seek answer entities\\rfor the natural language question from a large-scale Knowledge Graph~(KG). To\\rbetter perform reasoning on KG, recent work typically adopts a pre-trained\\rlanguage model~(PLM) to model the question, and a graph neural network~(GNN)\\rbased module to perform multi-hop reasoning on the KG. Despite the\\reffectiveness, due to the divergence in model architecture, the PLM and GNN are\\rnot closely integrated, limiting the knowledge sharing and fine-grained feature\\rinteractions. To solve it, we aim to simplify the above two-module approach,\\rand develop a more capable PLM that can directly support subgraph reasoning for\\rKGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware\\rself-attention mechanism to imitate the GNN for performing structured\\rreasoning, and also adopt an adaptation tuning strategy to adapt the model\\rparameters with 20,000 subgraphs with synthesized questions. After adaptation,\\rthe PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments\\rshow that ReasoningLM surpasses state-of-the-art models by a large margin, even\\rwith fewer updated parameters and less training data. Our codes and data are\\rpublicly available at~\\\\url{https://github.com/RUCAIBox/ReasoningLM}.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00158 ,  296kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00165\\rDate: Sat, 30 Dec 2023 08:01:57 GMT   (3112kb,D)\\r\\rTitle: Mitigating the Impact of False Negatives in Dense Retrieval with\\r  Contrastive Confidence Regularization\\rAuthors: Shiqi Wang, Yeqin Zhang and Cam-Tu Nguyen\\rCategories: cs.CL\\rComments: Accepted by AAAI24\\r\\\\\\\\\\r  In open-domain Question Answering (QA), dense retrieval is crucial for\\rfinding relevant passages for answer generation. Typically, contrastive\\rlearning is used to train a retrieval model that maps passages and queries to\\rthe same semantic space. The objective is to make similar ones closer and\\rdissimilar ones further apart. However, training such a system is challenging\\rdue to the false negative issue, where relevant passages may be missed during\\rdata annotation. Hard negative sampling, which is commonly used to improve\\rcontrastive learning, can introduce more noise in training. This is because\\rhard negatives are those closer to a given query, and thus more likely to be\\rfalse negatives. To address this issue, we propose a novel contrastive\\rconfidence regularizer for Noise Contrastive Estimation (NCE) loss, a commonly\\rused loss for dense retrieval. Our analysis shows that the regularizer helps\\rdense retrieval models be more robust against false negatives with a\\rtheoretical guarantee. Additionally, we propose a model-agnostic method to\\rfilter out noisy negative passages in the dataset, improving any downstream\\rdense retrieval models. Through experiments on three datasets, we demonstrate\\rthat our method achieves better retrieval performance in comparison to existing\\rstate-of-the-art dense retrieval systems.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00165 ,  3112kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00170\\rDate: Sat, 30 Dec 2023 08:30:24 GMT   (1031kb)\\r\\rTitle: L3Cube-MahaSocialNER: A Social Media based Marathi NER Dataset and BERT\\r  models\\rAuthors: Harsh Chaudhari, Anuja Patil, Dhanashree Lavekar, Pranav Khairnar,\\r  Raviraj Joshi\\rCategories: cs.CL cs.LG\\rComments: Accepted at Forum for Information Retrieval Evaluation (FIRE 2023)\\r\\\\\\\\\\r  This work introduces the L3Cube-MahaSocialNER dataset, the first and largest\\rsocial media dataset specifically designed for Named Entity Recognition (NER)\\rin the Marathi language. The dataset comprises 18,000 manually labeled\\rsentences covering eight entity classes, addressing challenges posed by social\\rmedia data, including non-standard language and informal idioms. Deep learning\\rmodels, including CNN, LSTM, BiLSTM, and Transformer models, are evaluated on\\rthe individual dataset with IOB and non-IOB notations. The results demonstrate\\rthe effectiveness of these models in accurately recognizing named entities in\\rMarathi informal text. The L3Cube-MahaSocialNER dataset offers user-centric\\rinformation extraction and supports real-time applications, providing a\\rvaluable resource for public opinion analysis, news, and marketing on social\\rmedia platforms. We also show that the zero-shot results of the regular NER\\rmodel are poor on the social NER test set thus highlighting the need for more\\rsocial NER datasets. The datasets and models are publicly available at\\rhttps://github.com/l3cube-pune/MarathiNLP\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00170 ,  1031kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00177\\rDate: Sat, 30 Dec 2023 09:04:30 GMT   (175kb)\\r\\rTitle: Principle Interference in Technical and Scientific Translation\\rAuthors: Mohammad Ibrahim Qani\\rCategories: cs.CL physics.hist-ph\\r\\\\\\\\\\r  In this article, I will explore the nature of interference in translation,\\respecially in technical and scientific texts, using a descriptivist approach. I\\rwill have a brief overview of the historical excursion of interference in\\rtechnical and scientific translation. My aim is to explain this phenomenon and\\rits causes with all its paradoxes, instead of simply condemning it as an\\rexample of supposedly bad translation. Thus, I will focus on its status in the\\rbibliography of translation, on the motives for and consequences of\\rinterference in specialized translation, as well as on the nature of the\\rarguments given for and against this phenomenon. Therefore the relationship\\rbetween different societies has always been possible with the act of\\rtranslation. When civilizations are examined throughout history, it is seen\\rthat the dissemination of knowledge among different societies has been achieved\\rby translation. These societies have often become aware of the advancements in\\rtechnology and science by means of translation. Therefore; translation becomes\\rvery significant in technical contact between societies and humans. Since the\\rtranslation of technical texts is the preliminary scope of this thesis, it will\\rbe beneficial to have a brief look at the history of technical translation in\\rthe world.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00177 ,  175kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00210\\rDate: Sat, 30 Dec 2023 11:44:59 GMT   (202kb,D)\\r\\rTitle: The Problem of Alignment\\rAuthors: Tsvetelina Hristova, Liam Magee, Karen Soldatic\\rCategories: cs.CL cs.CY\\rComments: 23 pages, 1 figure\\rACM-class: K.4.2\\r\\\\\\\\\\r  Large Language Models produce sequences learned as statistical patterns from\\rlarge corpora. In order not to reproduce corpus biases, after initial training\\rmodels must be aligned with human values, preferencing certain continuations\\rover others. Alignment, which can be viewed as the superimposition of normative\\rstructure onto a statistical model, reveals a conflicted and complex\\rinterrelationship between language and technology. This relationship shapes\\rtheories of language, linguistic practice and subjectivity, which are\\respecially relevant to the current sophistication in artificially produced\\rtext. We examine this practice of structuration as a two-way interaction\\rbetween users and models by analysing how ChatGPT4 redacts perceived\\r`anomalous' language in fragments of Joyce's Ulysses and the new linguistic\\rpractice of prompt engineering. We then situate this alignment problem\\rhistorically, revisiting earlier postwar linguistic debates which counterposed\\rtwo views of meaning: as discrete structures, and as continuous probability\\rdistributions. We discuss the largely occluded work of the Moscow Linguistic\\rSchool, which sought to reconcile this opposition. Our attention to the Moscow\\rSchool and later related arguments by Searle and Kristeva casts the problem of\\ralignment in a new light: as one involving attention to the social\\rstructuration of linguistic practice, including structuration of anomalies\\rthat, like the Joycean text, exist in defiance of expressive conventions. These\\rdebates around the communicative orientation toward language can help explain\\rsome of the contemporary behaviours and interdependencies that take place\\rbetween users and LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00210 ,  202kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00238\\rDate: Sat, 30 Dec 2023 14:02:36 GMT   (106kb,D)\\r\\rTitle: How to Evaluate Coreference in Literary Texts?\\rAuthors: Ana-Isabel Duron-Tejedor and Pascal Amsili and Thierry Poibeau\\rCategories: cs.CL cs.AI\\rComments: Presented as a poster at the conference CHR2023 (non archival)\\r\\\\\\\\\\r  In this short paper, we examine the main metrics used to evaluate textual\\rcoreference and we detail some of their limitations. We show that a unique\\rscore cannot represent the full complexity of the problem at stake, and is thus\\runinformative, or even misleading. We propose a new way of evaluating\\rcoreference, taking into account the context (in our case, the analysis of\\rfictions, esp. novels). More specifically, we propose to distinguish long\\rcoreference chains (corresponding to main characters), from short ones\\r(corresponding to secondary characters), and singletons (isolated elements).\\rThis way, we hope to get more interpretable and thus more informative results\\rthrough evaluation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00238 ,  106kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00246\\rDate: Sat, 30 Dec 2023 14:20:04 GMT   (162kb,D)\\r\\rTitle: Boosting Large Language Model for Speech Synthesis: An Empirical Study\\rAuthors: Hongkun Hao, Long Zhou, Shujie Liu, Jinyu Li, Shujie Hu, Rui Wang,\\r  Furu Wei\\rCategories: cs.CL cs.SD eess.AS\\r\\\\\\\\\\r  Large language models (LLMs) have made significant advancements in natural\\rlanguage processing and are concurrently extending the language ability to\\rother modalities, such as speech and vision. Nevertheless, most of the previous\\rwork focuses on prompting LLMs with perception abilities like auditory\\rcomprehension, and the effective approach for augmenting LLMs with speech\\rsynthesis capabilities remains ambiguous. In this paper, we conduct a\\rcomprehensive empirical exploration of boosting LLMs with the ability to\\rgenerate speech, by combining pre-trained LLM LLaMA/OPT and text-to-speech\\rsynthesis model VALL-E. We compare three integration methods between LLMs and\\rspeech synthesis models, including directly fine-tuned LLMs, superposed layers\\rof LLMs and VALL-E, and coupled LLMs and VALL-E using LLMs as a powerful text\\rencoder. Experimental results show that, using LoRA method to fine-tune LLMs\\rdirectly to boost the speech synthesis capability does not work well, and\\rsuperposed LLMs and VALL-E can improve the quality of generated speech both in\\rspeaker similarity and word error rate (WER). Among these three methods,\\rcoupled methods leveraging LLMs as the text encoder can achieve the best\\rperformance, making it outperform original speech synthesis models with a\\rconsistently better speaker similarity and a significant (10.9%) WER reduction.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00246 ,  162kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00284\\rDate: Sat, 30 Dec 2023 17:22:01 GMT   (448kb,D)\\r\\rTitle: Evaluation is all you need. Prompting Generative Large Language Models\\r  for Annotation Tasks in the Social Sciences. A Primer using Open Models\\rAuthors: Maximilian Weber, Merle Reichardt\\rCategories: cs.CL\\r\\\\\\\\\\r  This paper explores the use of open generative Large Language Models (LLMs)\\rfor annotation tasks in the social sciences. The study highlights the\\rchallenges associated with proprietary models, such as limited reproducibility\\rand privacy concerns, and advocates for the adoption of open (source) models\\rthat can be operated on independent devices. Two examples of annotation tasks,\\rsentiment analysis in tweets and identification of leisure activities in\\rchildhood aspirational essays are provided. The study evaluates the performance\\rof different prompting strategies and models (neural-chat-7b-v3-2,\\rStarling-LM-7B-alpha, openchat_3.5, zephyr-7b-alpha and zephyr-7b-beta). The\\rresults indicate the need for careful validation and tailored prompt\\rengineering. The study highlights the advantages of open models for data\\rprivacy and reproducibility.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00284 ,  448kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00287\\rDate: Sat, 30 Dec 2023 17:37:06 GMT   (363kb,D)\\r\\rTitle: The Art of Defending: A Systematic Evaluation and Analysis of LLM\\r  Defense Strategies on Safety and Over-Defensiveness\\rAuthors: Neeraj Varshney, Pavel Dolin, Agastya Seth, Chitta Baral\\rCategories: cs.CL\\r\\\\\\\\\\r  As Large Language Models (LLMs) play an increasingly pivotal role in natural\\rlanguage processing applications, their safety concerns become critical areas\\rof NLP research. This paper presents Safety and Over-Defensiveness Evaluation\\r(SODE) benchmark: a collection of diverse safe and unsafe prompts with\\rcarefully designed evaluation methods that facilitate systematic evaluation,\\rcomparison, and analysis over 'safety' and 'over-defensiveness.' With SODE, we\\rstudy a variety of LLM defense strategies over multiple state-of-the-art LLMs,\\rwhich reveals several interesting and important findings, such as (a) the\\rwidely popular 'self-checking' techniques indeed improve the safety against\\runsafe inputs, but this comes at the cost of extreme over-defensiveness on the\\rsafe inputs, (b) providing a safety instruction along with in-context exemplars\\r(of both safe and unsafe inputs) consistently improves safety and also\\rmitigates undue over-defensiveness of the models, (c) providing contextual\\rknowledge easily breaks the safety guardrails and makes the models more\\rvulnerable to generating unsafe responses. Overall, our work reveals numerous\\rsuch critical findings that we believe will pave the way and facilitate further\\rresearch in improving the safety of LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00287 ,  363kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00290\\rDate: Sat, 30 Dec 2023 17:59:12 GMT   (18kb,D)\\r\\rTitle: Red Teaming for Large Language Models At Scale: Tackling Hallucinations\\r  on Mathematics Tasks\\rAuthors: Aleksander Buszydlik, Karol Dobiczek, Micha{\\\\l} Teodor Oko\\\\'n, Konrad\\r  Skublicki, Philip Lippmann, Jie Yang\\rCategories: cs.CL cs.AI\\rComments: Accepted to The ART of Safety: Workshop on Adversarial testing and\\r  Red-Teaming for generative AI (IJCNLP-AACL 2023)\\rACM-class: I.2.7\\r\\\\\\\\\\r  We consider the problem of red teaming LLMs on elementary calculations and\\ralgebraic tasks to evaluate how various prompting techniques affect the quality\\rof outputs. We present a framework to procedurally generate numerical questions\\rand puzzles, and compare the results with and without the application of\\rseveral red teaming techniques. Our findings suggest that even though\\rstructured reasoning and providing worked-out examples slow down the\\rdeterioration of the quality of answers, the gpt-3.5-turbo and gpt-4 models are\\rnot well suited for elementary calculations and reasoning tasks, also when\\rbeing red teamed.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00290 ,  18kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00366\\rDate: Sun, 31 Dec 2023 01:51:43 GMT   (213kb)\\r\\rTitle: Argumentation in Waltz's Emerging Structure of International Politics''\\rAuthors: Magdalena Wolska, Bernd Fr\\\\ohlich, Katrin Girgensohn, Sassan\\r  Gholiagha, Dora Kiesel, J\\\\urgen Neyer, Patrick Riehmann, Mitja Sienknecht,\\r  Benno Stein\\rCategories: cs.CL\\rComments: 9 pages\\r\\\\\\\\\\r  We present an annotation scheme for argumentative and domain-specific aspects\\rof scholarly articles on the theory of International Relations. At\\rargumentation level we identify Claims and Support/Attack relations. At domain\\rlevel we model discourse content in terms of Theory and Data-related\\rstatements. We annotate Waltz's 1993 text on structural realism and show that\\rour scheme can be reliably applied by domain experts enables insights on two\\rresearch questions on justifications of claims.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00366 ,  213kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00368\\rDate: Sun, 31 Dec 2023 02:13:18 GMT   (129kb,D)\\r\\rTitle: Improving Text Embeddings with Large Language Models\\rAuthors: Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder,\\r  Furu Wei\\rCategories: cs.CL cs.IR\\rComments: 15 pages, 8 tables\\r\\\\\\\\\\r  In this paper, we introduce a novel and simple method for obtaining\\rhigh-quality text embeddings using only synthetic data and less than 1k\\rtraining steps. Unlike existing methods that often depend on multi-stage\\rintermediate pre-training with billions of weakly-supervised text pairs,\\rfollowed by fine-tuning with a few labeled datasets, our method does not\\rrequire building complex training pipelines or relying on manually collected\\rdatasets that are often constrained by task diversity and language coverage. We\\rleverage proprietary LLMs to generate diverse synthetic data for hundreds of\\rthousands of text embedding tasks across nearly 100 languages. We then\\rfine-tune open-source decoder-only LLMs on the synthetic data using standard\\rcontrastive loss. Experiments demonstrate that our method achieves strong\\rperformance on highly competitive text embedding benchmarks without using any\\rlabeled data. Furthermore, when fine-tuned with a mixture of synthetic and\\rlabeled data, our model sets new state-of-the-art results on the BEIR and MTEB\\rbenchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00368 ,  129kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00383\\rDate: Sun, 31 Dec 2023 03:30:42 GMT   (9907kb,D)\\r\\rTitle: Predicting Evoked Emotions in Conversations\\rAuthors: Enas Altarawneh, Ameeta Agrawal, Michael Jenkin, Manos Papagelis\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  Understanding and predicting the emotional trajectory in multi-party\\rmulti-turn conversations is of great significance. Such information can be\\rused, for example, to generate empathetic response in human-machine interaction\\ror to inform models of pre-emptive toxicity detection. In this work, we\\rintroduce the novel problem of Predicting Emotions in Conversations (PEC) for\\rthe next turn (n+1), given combinations of textual and/or emotion input up to\\rturn n. We systematically approach the problem by modeling three dimensions\\rinherently connected to evoked emotions in dialogues, including (i) sequence\\rmodeling, (ii) self-dependency modeling, and (iii) recency modeling. These\\rmodeling dimensions are then incorporated into two deep neural network\\rarchitectures, a sequence model and a graph convolutional network model. The\\rformer is designed to capture the sequence of utterances in a dialogue, while\\rthe latter captures the sequence of utterances and the network formation of\\rmulti-party dialogues. We perform a comprehensive empirical evaluation of the\\rvarious proposed models for addressing the PEC problem. The results indicate\\r(i) the importance of the self-dependency and recency model dimensions for the\\rprediction task, (ii) the quality of simpler sequence models in short\\rdialogues, (iii) the importance of the graph neural models in improving the\\rpredictions in long dialogues.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00383 ,  9907kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00388\\rDate: Sun, 31 Dec 2023 03:51:31 GMT   (1039kb,D)\\r\\rTitle: FusionMind -- Improving question and answering with external context\\r  fusion\\rAuthors: Shreyas Verma, Manoj Parmar, Palash Choudhary, Sanchita Porwal\\rCategories: cs.CL\\rComments: 5 pages, 4 figures, 4 tables\\r\\\\\\\\\\r  Answering questions using pre-trained language models (LMs) and knowledge\\rgraphs (KGs) presents challenges in identifying relevant knowledge and\\rperforming joint reasoning.We compared LMs (fine-tuned for the task) with the\\rpreviously published QAGNN method for the Question-answering (QA) objective and\\rfurther measured the impact of additional factual context on the QAGNN\\rperformance. The QAGNN method employs LMs to encode QA context and estimate KG\\rnode importance, and effectively update the question choice entity\\rrepresentations using Graph Neural Networks (GNNs). We further experimented\\rwith enhancing the QA context encoding by incorporating relevant knowledge\\rfacts for the question stem. The models are trained on the OpenbookQA dataset,\\rwhich contains ~6000 4-way multiple choice questions and is widely used as a\\rbenchmark for QA tasks. Through our experimentation, we found that\\rincorporating knowledge facts context led to a significant improvement in\\rperformance. In contrast, the addition of knowledge graphs to language models\\rresulted in only a modest increase. This suggests that the integration of\\rcontextual knowledge facts may be more impactful for enhancing question\\ranswering performance compared to solely adding knowledge graphs.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00388 ,  1039kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00396\\rDate: Sun, 31 Dec 2023 04:43:45 GMT   (7284kb,D)\\r\\rTitle: RAGTruth: A Hallucination Corpus for Developing Trustworthy\\r  Retrieval-Augmented Language Models\\rAuthors: Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu, Randy Zhong,\\r  Juntong Song, Tong Zhang\\rCategories: cs.CL\\r\\\\\\\\\\r  Retrieval-augmented generation (RAG) has become a main technique for\\ralleviating hallucinations in large language models (LLMs). Despite the\\rintegration of RAG, LLMs may still present unsupported or contradictory claims\\rto the retrieved contents. In order to develop effective hallucination\\rprevention strategies under RAG, it is important to create benchmark datasets\\rthat can measure the extent of hallucination. This paper presents RAGTruth, a\\rcorpus tailored for analyzing word-level hallucinations in various domains and\\rtasks within the standard RAG frameworks for LLM applications. RAGTruth\\rcomprises nearly 18,000 naturally generated responses from diverse LLMs using\\rRAG. These responses have undergone meticulous manual annotations at both the\\rindividual cases and word levels, incorporating evaluations of hallucination\\rintensity. We not only benchmark hallucination frequencies across different\\rLLMs, but also critically assess the effectiveness of several existing\\rhallucination detection methodologies. Furthermore, we show that using a\\rhigh-quality dataset such as RAGTruth, it is possible to finetune a relatively\\rsmall LLM and achieve a competitive level of performance in hallucination\\rdetection when compared to the existing prompt-based approaches using\\rstate-of-the-art large language models such as GPT-4.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00396 ,  7284kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00424\\rDate: Sun, 31 Dec 2023 08:33:37 GMT   (595kb,D)\\r\\rTitle: SDIF-DA: A Shallow-to-Deep Interaction Framework with Data Augmentation\\r  for Multi-modal Intent Detection\\rAuthors: Shijue Huang, Libo Qin, Bingbing Wang, Geng Tu, Ruifeng Xu\\rCategories: cs.CL\\rComments: Accepted by ICASSP 2024\\r\\\\\\\\\\r  Multi-modal intent detection aims to utilize various modalities to understand\\rthe user's intentions, which is essential for the deployment of dialogue\\rsystems in real-world scenarios. The two core challenges for multi-modal intent\\rdetection are (1) how to effectively align and fuse different features of\\rmodalities and (2) the limited labeled multi-modal intent training data. In\\rthis work, we introduce a shallow-to-deep interaction framework with data\\raugmentation (SDIF-DA) to address the above challenges. Firstly, SDIF-DA\\rleverages a shallow-to-deep interaction module to progressively and effectively\\ralign and fuse features across text, video, and audio modalities. Secondly, we\\rpropose a ChatGPT-based data augmentation approach to automatically augment\\rsufficient training data. Experimental results demonstrate that SDIF-DA can\\reffectively align and fuse multi-modal features by achieving state-of-the-art\\rperformance. In addition, extensive analyses show that the introduced data\\raugmentation approach can successfully distill knowledge from the large\\rlanguage model.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00424 ,  595kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00426\\rDate: Sun, 31 Dec 2023 08:39:04 GMT   (2267kb,D)\\r\\rTitle: keqing: knowledge-based question answering is a nature chain-of-thought\\r  mentor of LLM\\rAuthors: Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen, Xinrun\\r  Wang, Lei Feng, Bo An\\rCategories: cs.CL cs.AI\\rComments: 12 pages, 6 figures\\r\\\\\\\\\\r  Large language models (LLMs) have exhibited remarkable performance on various\\rnatural language processing (NLP) tasks, especially for question answering.\\rHowever, in the face of problems beyond the scope of knowledge, these LLMs tend\\rto talk nonsense with a straight face, where the potential solution could be\\rincorporating an Information Retrieval (IR) module and generating response\\rbased on these retrieved knowledge. In this paper, we present a novel framework\\rto assist LLMs, such as ChatGPT, to retrieve question-related structured\\rinformation on the knowledge graph, and demonstrate that Knowledge-based\\rquestion answering (Keqing) could be a nature Chain-of-Thought (CoT) mentor to\\rguide the LLM to sequentially find the answer entities of a complex question\\rthrough interpretable logical chains. Specifically, the workflow of Keqing will\\rexecute decomposing a complex question according to predefined templates,\\rretrieving candidate entities on knowledge graph, reasoning answers of\\rsub-questions, and finally generating response with reasoning paths, which\\rgreatly improves the reliability of LLM's response. The experimental results on\\rKBQA datasets show that Keqing can achieve competitive performance and\\rillustrate the logic of answering each question.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00426 ,  2267kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00434\\rDate: Sun, 31 Dec 2023 09:22:54 GMT   (14695kb,D)\\r\\rTitle: GeoGalactica: A Scientific Large Language Model in Geoscience\\rAuthors: Zhouhan Lin, Cheng Deng, Le Zhou, Tianhang Zhang, Yi Xu, Yutong Xu,\\r  Zhongmou He, Yuanyuan Shi, Beiya Dai, Yunchong Song, Boyi Zeng, Qiyuan Chen,\\r  Tao Shi, Tianyu Huang, Yiwei Xu, Shu Wang, Luoyi Fu, Weinan Zhang, Junxian\\r  He, Chao Ma, Yunqiang Zhu, Xinbing Wang, Chenghu Zhou\\rCategories: cs.CL\\rACM-class: I.2.7; F.4.1\\r\\\\\\\\\\r  Large language models (LLMs) have achieved huge success for their general\\rknowledge and ability to solve a wide spectrum of tasks in natural language\\rprocessing (NLP). Due to their impressive abilities, LLMs have shed light on\\rpotential inter-discipline applications to foster scientific discoveries of a\\rspecific domain by using artificial intelligence (AI for science, AI4S). In the\\rmeantime, utilizing NLP techniques in geoscience research and practice is wide\\rand convoluted, contributing from knowledge extraction and document\\rclassification to question answering and knowledge discovery. In this work, we\\rtake the initial step to leverage LLM for science, through a rather\\rstraightforward approach. We try to specialize an LLM into geoscience, by\\rfurther pre-training the model with a vast amount of texts in geoscience, as\\rwell as supervised fine-tuning (SFT) the resulting model with our custom\\rcollected instruction tuning dataset. These efforts result in a model\\rGeoGalactica consisting of 30 billion parameters. To our best knowledge, it is\\rthe largest language model for the geoscience domain. More specifically,\\rGeoGalactica is from further pre-training of Galactica. We train GeoGalactica\\rover a geoscience-related text corpus containing 65 billion tokens curated from\\rextensive data sources in the big science project Deep-time Digital Earth\\r(DDE), preserving as the largest geoscience-specific text corpus. Then we\\rfine-tune the model with 1 million pairs of instruction-tuning data consisting\\rof questions that demand professional geoscience knowledge to answer. In this\\rtechnical report, we will illustrate in detail all aspects of GeoGalactica,\\rincluding data collection, data cleaning, base model selection, pre-training,\\rSFT, and evaluation. We open-source our data curation tools and the checkpoints\\rof GeoGalactica during the first 3/4 of pre-training.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00434 ,  14695kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00437\\rDate: Sun, 31 Dec 2023 09:34:51 GMT   (1227kb,D)\\r\\rTitle: BatchEval: Towards Human-like Text Evaluation\\rAuthors: Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Boyuan Pan, Heda\\r  Wang, Kan Li\\rCategories: cs.CL\\rComments: 19 pages, 9 figures\\r\\\\\\\\\\r  Significant progress has been made in automatic text evaluation with the\\rintroduction of large language models (LLMs) as evaluators. However, current\\rsample-wise evaluation paradigm suffers from the following issues: (1)\\rSensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble\\rperformance with static reference. Inspired by the fact that humans treat both\\rcriterion definition and inter sample comparison as references for evaluation,\\rwe propose BatchEval, a paradigm that conducts batch-wise evaluation\\riteratively to alleviate the above problems. We explore variants under this\\rparadigm and confirm the optimal settings are two stage procedure with\\rheterogeneous batch composition strategy and decimal scoring format.\\rComprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate\\rthat BatchEval outperforms state-of-the-art methods by 10.5% on Pearson\\rcorrelations with only 64% API cost on average. Further analyses have been\\rconducted to verify the robustness, generalization, and working mechanism of\\rBatchEval.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00437 ,  1227kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00504\\rDate: Sun, 31 Dec 2023 13:56:15 GMT   (239kb)\\r\\rTitle: HSC-GPT: A Large Language Model for Human Settlements Construction\\rAuthors: Chen Ran, Yao Xueqi, Jiang Xuhui, Han Zhengqi, Guo Jingze, Zhang\\r  Xianyue, Lin Chunyu, Liu Chumin, Zhao Jing, Lian Zeke, Zhang Jingjing, Li\\r  Keke\\rCategories: cs.CL\\r\\\\\\\\\\r  The field of human settlement construction encompasses a range of spatial\\rdesigns and management tasks, including urban planning and landscape\\rarchitecture design. These tasks involve a plethora of instructions and\\rdescriptions presented in natural language, which are essential for\\runderstanding design requirements and producing effective design solutions.\\rRecent research has sought to integrate natural language processing (NLP) and\\rgenerative artificial intelligence (AI) into human settlement construction\\rtasks. Due to the efficient processing and analysis capabilities of AI with\\rdata, significant successes have been achieved in design within this domain.\\rHowever, this task still faces several fundamental challenges. The semantic\\rinformation involved includes complex spatial details, diverse data source\\rformats, high sensitivity to regional culture, and demanding requirements for\\rinnovation and rigor in work scenarios. These factors lead to limitations when\\rapplying general generative AI in this field, further exacerbated by a lack of\\rhigh-quality data for model training. To address these challenges, this paper\\rfirst proposes HSC-GPT, a large-scale language model framework specifically\\rdesigned for tasks in human settlement construction, considering the unique\\rcharacteristics of this domain.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00504 ,  239kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00536\\rDate: Sun, 31 Dec 2023 16:48:03 GMT   (558kb,D)\\r\\rTitle: A Multi-Task, Multi-Modal Approach for Predicting Categorical and\\r  Dimensional Emotions\\rAuthors: Alex-R\\\\u{a}zvan Ispas, Th\\\\'eo Deschamps-Berger, Laurence Devillers\\rCategories: cs.CL cs.AI\\rComments: Companion Publication of the 25th International Conference on\\r  Multimodal Interaction (pp. 311-317)\\rACM-class: I.2.7\\rDOI: 10.1145/3610661.3616190\\r\\\\\\\\\\r  Speech emotion recognition (SER) has received a great deal of attention in\\rrecent years in the context of spontaneous conversations. While there have been\\rnotable results on datasets like the well known corpus of naturalistic dyadic\\rconversations, IEMOCAP, for both the case of categorical and dimensional\\remotions, there are few papers which try to predict both paradigms at the same\\rtime. Therefore, in this work, we aim to highlight the performance contribution\\rof multi-task learning by proposing a multi-task, multi-modal system that\\rpredicts categorical and dimensional emotions. The results emphasise the\\rimportance of cross-regularisation between the two types of emotions. Our\\rapproach consists of a multi-task, multi-modal architecture that uses parallel\\rfeature refinement through self-attention for the feature of each modality. In\\rorder to fuse the features, our model introduces a set of learnable bridge\\rtokens that merge the acoustic and linguistic features with the help of\\rcross-attention. Our experiments for categorical emotions on 10-fold validation\\ryield results comparable to the current state-of-the-art. In our configuration,\\rour multi-task approach provides better results compared to learning each\\rparadigm separately. On top of that, our best performing model achieves a high\\rresult for valence compared to the previous multi-task experiments.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00536 ,  558kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00575\\rDate: Sun, 31 Dec 2023 19:25:34 GMT   (7787kb,D)\\r\\rTitle: Neural Networks Against (and For) Self-Training: Classification with\\r  Small Labeled and Large Unlabeled Sets\\rAuthors: Payam Karisani\\rCategories: cs.CL\\rComments: ACL Findings 2023\\r\\\\\\\\\\r  We propose a semi-supervised text classifier based on self-training using one\\rpositive and one negative property of neural networks. One of the weaknesses of\\rself-training is the semantic drift problem, where noisy pseudo-labels\\raccumulate over iterations and consequently the error rate soars. In order to\\rtackle this challenge, we reshape the role of pseudo-labels and create a\\rhierarchical order of information. In addition, a crucial step in self-training\\ris to use the classifier confidence prediction to select the best candidate\\rpseudo-labels. This step cannot be efficiently done by neural networks, because\\rit is known that their output is poorly calibrated. To overcome this challenge,\\rwe propose a hybrid metric to replace the plain confidence measurement. Our\\rmetric takes into account the prediction uncertainty via a subsampling\\rtechnique. We evaluate our model in a set of five standard benchmarks, and show\\rthat it significantly outperforms a set of ten diverse baseline models.\\rFurthermore, we show that the improvement achieved by our model is additive to\\rlanguage model pretraining, which is a widely used technique for using\\runlabeled documents. Our code is available at\\rhttps://github.com/p-karisani/RST.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00575 ,  7787kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00579\\rDate: Sun, 31 Dec 2023 20:02:10 GMT   (8138kb,D)\\r\\rTitle: Exploring the Effectiveness of Instruction Tuning in Biomedical Language\\r  Processing\\rAuthors: Omid Rohanian, Mohammadmahdi Nouriborji, David A. Clifton\\rCategories: cs.CL cs.AI cs.LG\\rMSC-class: 68T50\\rACM-class: I.2.7\\r\\\\\\\\\\r  Large Language Models (LLMs), particularly those similar to ChatGPT, have\\rsignificantly influenced the field of Natural Language Processing (NLP). While\\rthese models excel in general language tasks, their performance in\\rdomain-specific downstream tasks such as biomedical and clinical Named Entity\\rRecognition (NER), Relation Extraction (RE), and Medical Natural Language\\rInference (NLI) is still evolving. In this context, our study investigates the\\rpotential of instruction tuning for biomedical language processing, applying\\rthis technique to two general LLMs of substantial scale. We present a\\rcomprehensive, instruction-based model trained on a dataset that consists of\\rapproximately $200,000$ instruction-focused samples. This dataset represents a\\rcarefully curated compilation of existing data, meticulously adapted and\\rreformatted to align with the specific requirements of our instruction-based\\rtasks. This initiative represents an important step in utilising such models to\\rachieve results on par with specialised encoder-only models like BioBERT and\\rBioClinicalBERT for various classical biomedical NLP tasks. Our work includes\\ran analysis of the dataset's composition and its impact on model performance,\\rproviding insights into the intricacies of instruction tuning. By sharing our\\rcodes, models, and the distinctively assembled instruction-based dataset, we\\rseek to encourage ongoing research and development in this area.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00579 ,  8138kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00582\\rDate: Sun, 31 Dec 2023 20:21:58 GMT   (302kb,D)\\r\\rTitle: An Analysis of Embedding Layers and Similarity Scores using Siamese\\r  Neural Networks\\rAuthors: Yash Bingi and Yiqiao Yin\\rCategories: cs.CL cs.LG\\rComments: 10 pages, 11 figures\\r\\\\\\\\\\r  Large Lanugage Models (LLMs) are gaining increasing popularity in a variety\\rof use cases, from language understanding and writing to assistance in\\rapplication development. One of the most important aspects for optimal\\rfuncionality of LLMs is embedding layers. Word embeddings are distributed\\rrepresentations of words in a continuous vector space. In the context of LLMs,\\rwords or tokens from the input text are transformed into high-dimensional\\rvectors using unique algorithms specific to the model. Our research examines\\rthe embedding algorithms from leading companies in the industry, such as\\rOpenAI, Google's PaLM, and BERT. Using medical data, we have analyzed\\rsimilarity scores of each embedding layer, observing differences in performance\\ramong each algorithm. To enhance each model and provide an additional encoding\\rlayer, we also implemented Siamese Neural Networks. After observing changes in\\rperformance with the addition of the model, we measured the carbon footage per\\repoch of training. The carbon footprint associated with large language models\\r(LLMs) is a significant concern, and should be taken into consideration when\\rselecting algorithms for a variety of use cases. Overall, our research compared\\rthe accuracy different, leading embedding algorithms and their carbon footage,\\rallowing for a holistic review of each embedding algorithm.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00582 ,  302kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00595\\rDate: Sun, 31 Dec 2023 22:21:36 GMT   (7446kb,D)\\r\\rTitle: State of What Art? A Call for Multi-Prompt LLM Evaluation\\rAuthors: Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf,\\r  Gabriel Stanovsky\\rCategories: cs.CL\\r\\\\\\\\\\r  Recent advances in large language models (LLMs) have led to the development\\rof various evaluation benchmarks. These benchmarks typically rely on a single\\rinstruction template for evaluating all LLMs on a specific task. In this paper,\\rwe comprehensively analyze the brittleness of results obtained via\\rsingle-prompt evaluations across 6.5M instances, involving 20 different LLMs\\rand 39 tasks from 3 benchmarks. To improve robustness of the analysis, we\\rpropose to evaluate LLMs with a set of diverse prompts instead. We discuss\\rtailored evaluation metrics for specific use cases (e.g., LLM developers vs.\\rdevelopers interested in a specific downstream task), ensuring a more reliable\\rand meaningful assessment of LLM capabilities. We then implement these criteria\\rand conduct evaluations of multiple models, providing insights into the true\\rstrengths and limitations of current LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00595 ,  7446kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00609\\rDate: Sun, 31 Dec 2023 23:41:41 GMT   (630kb,D)\\r\\rTitle: A Survey of Personality, Persona, and Profile in Conversational Agents\\r  and Chatbots\\rAuthors: Richard Sutcliffe\\rCategories: cs.CL cs.AI\\rComments: 25 pages, 6 tables, 207 references\\r\\\\\\\\\\r  We present a review of personality in neural conversational agents (CAs),\\ralso called chatbots. First, we define Personality, Persona, and Profile. We\\rexplain all personality schemes which have been used in CAs, and list models\\runder the scheme(s) which they use. Second we describe 21 datasets which have\\rbeen developed in recent CA personality research. Third, we define the methods\\rused to embody personality in a CA, and review recent models using them.\\rFourth, we survey some relevant reviews on CAs, personality, and related\\rtopics. Finally, we draw conclusions and identify some research challenges for\\rthis important emerging field.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00609 ,  630kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00642\\rDate: Mon, 1 Jan 2024 03:04:14 GMT   (8006kb,D)\\r\\rTitle: Predicting Anti-microbial Resistance using Large Language Models\\rAuthors: Hyunwoo Yoo, Bahrad Sokhansanj, James R. Brown, Gail Rosen\\rCategories: cs.CL\\r\\\\\\\\\\r  During times of increasing antibiotic resistance and the spread of infectious\\rdiseases like COVID-19, it is important to classify genes related to antibiotic\\rresistance. As natural language processing has advanced with transformer-based\\rlanguage models, many language models that learn characteristics of nucleotide\\rsequences have also emerged. These models show good performance in classifying\\rvarious features of nucleotide sequences. When classifying nucleotide\\rsequences, not only the sequence itself, but also various background knowledge\\ris utilized. In this study, we use not only a nucleotide sequence-based\\rlanguage model but also a text language model based on PubMed articles to\\rreflect more biological background knowledge in the model. We propose a method\\rto fine-tune the nucleotide sequence language model and the text language model\\rbased on various databases of antibiotic resistance genes. We also propose an\\rLLM-based augmentation technique to supplement the data and an ensemble method\\rto effectively combine the two models. We also propose a benchmark for\\revaluating the model. Our method achieved better performance than the\\rnucleotide sequence language model in the drug resistance class prediction.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00642 ,  8006kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00689\\rDate: Mon, 1 Jan 2024 07:35:29 GMT   (3122kb,D)\\r\\rTitle: Large language model for Bible sentiment analysis: Sermon on the Mount\\rAuthors: Mahek Vora, Tom Blau, Vansh Kachhwal, Ashu M. G. Solo, Rohitash\\r  Chandra\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  The revolution of natural language processing via large language models has\\rmotivated its use in multidisciplinary areas that include social sciences and\\rhumanities and more specifically, comparative religion. Sentiment analysis\\rprovides a mechanism to study the emotions expressed in text. Recently,\\rsentiment analysis has been used to study and compare translations of the\\rBhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we\\ruse sentiment analysis for studying selected chapters of the Bible. These\\rchapters are known as the Sermon on the Mount. We utilize a pre-trained\\rlanguage model for sentiment analysis by reviewing five translations of the\\rSermon on the Mount, which include the King James version, the New\\rInternational Version, the New Revised Standard Version, the Lamsa Version, and\\rthe Basic English Version. We provide a chapter-by-chapter and verse-by-verse\\rcomparison using sentiment and semantic analysis and review the major\\rsentiments expressed. Our results highlight the varying sentiments across the\\rchapters and verses. We found that the vocabulary of the respective\\rtranslations is significantly different. We detected different levels of\\rhumour, optimism, and empathy in the respective chapters that were used by\\rJesus to deliver his message.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00689 ,  3122kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00690\\rDate: Mon, 1 Jan 2024 07:35:31 GMT   (4700kb,D)\\r\\rTitle: Benchmarking Large Language Models on Controllable Generation under\\r  Diversified Instructions\\rAuthors: Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, Zhendong Mao\\rCategories: cs.CL\\rComments: Accepted to AAAI 2024\\r\\\\\\\\\\r  While large language models (LLMs) have exhibited impressive\\rinstruction-following capabilities, it is still unclear whether and to what\\rextent they can respond to explicit constraints that might be entailed in\\rvarious instructions. As a significant aspect of LLM alignment, it is thus\\rimportant to formulate such a specialized set of instructions as well as\\rinvestigate the resulting behavior of LLMs. To address this vacancy, we propose\\ra new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs'\\rresponses to instructions with various constraints. We construct a large\\rcollection of constraints-attributed instructions as a test suite focused on\\rboth generalization and coverage. Specifically, we advocate an instruction\\rdiversification process to synthesize diverse forms of constraint expression\\rand also deliberate the candidate task taxonomy with even finer-grained\\rsub-categories. Finally, we automate the entire evaluation process to\\rfacilitate further developments. Different from existing studies on\\rcontrollable text generation, CoDI-Eval extends the scope to the prevalent\\rinstruction-following paradigm for the first time. We provide extensive\\revaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval,\\rrevealing their limitations in following instructions with specific constraints\\rand there is still a significant gap between open-source and commercial\\rclosed-source LLMs. We believe this benchmark will facilitate research into\\rimproving the controllability of LLMs' responses to instructions. Our data and\\rcode are available at https://github.com/Xt-cyh/CoDI-Eval.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00690 ,  4700kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00698\\rDate: Mon, 1 Jan 2024 08:32:50 GMT   (383kb)\\r\\rTitle: Large Language Models aren't all that you need\\rAuthors: Kiran Voderhobli Holla, Chaithanya Kumar, Aryan Singh\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  This paper describes the architecture and systems built towards solving the\\rSemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity\\rRecognition) [1]. We evaluate two approaches (a) a traditional Conditional\\rRandom Fields model and (b) a Large Language Model (LLM) fine-tuned with a\\rcustomized head and compare the two approaches. The novel ideas explored are:\\r1) Decaying auxiliary loss (with residual) - where we train the model on an\\rauxiliary task of Coarse-Grained NER and include this task as a part of the\\rloss function 2) Triplet token blending - where we explore ways of blending the\\rembeddings of neighboring tokens in the final NER layer prior to prediction 3)\\rTask-optimal heads - where we explore a variety of custom heads and learning\\rrates for the final layer of the LLM. We also explore multiple LLMs including\\rGPT-3 and experiment with a variety of dropout and other hyperparameter\\rsettings before arriving at our final model which achieves micro & macro f1 of\\r0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while\\rpre-trained LLMs, by themselves, bring about a large improvement in scores as\\rcompared to traditional models, we also demonstrate that tangible improvements\\rto the Macro-F1 score can be made by augmenting the LLM with additional\\rfeature/loss/model engineering techniques described above.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00698 ,  383kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00741\\rDate: Mon, 1 Jan 2024 12:49:36 GMT   (9910kb,D)\\r\\rTitle: ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of\\r  Large Language Models in Real-world Scenarios\\rAuthors: Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian\\r  Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, Xuanjing Huang\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Existing evaluations of tool learning primarily focus on validating the\\ralignment of selected tools for large language models (LLMs) with expected\\routcomes. However, these approaches rely on a limited set of scenarios where\\ranswers can be pre-determined, diverging from genuine needs. Furthermore, a\\rsole emphasis on outcomes disregards the intricate capabilities essential for\\rLLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a\\rfine-grained system tailored for the evaluation of the LLMs' tool learning\\rcapabilities in authentic scenarios. The system meticulously examines seven\\rreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\\rlearning: format alignment, intent comprehension, behavior planning, tool\\rselection, and answer organization. Additionally, ToolEyes incorporates a tool\\rlibrary boasting approximately 600 tools, serving as an intermediary between\\rLLMs and the physical world. Evaluations involving ten LLMs across three\\rcategories reveal a preference for specific scenarios and limited cognitive\\rabilities in tool learning. Intriguingly, expanding the model size even\\rexacerbates the hindrance to tool learning. These findings offer instructive\\rinsights aimed at advancing the field of tool learning. The data is available\\ratt https://github.com/Junjie-Ye/ToolEyes.git.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00741 ,  9910kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00751\\rDate: Mon, 1 Jan 2024 13:28:46 GMT   (435kb,D)\\r\\rTitle: Machine Translation Testing via Syntactic Tree Pruning\\rAuthors: Quanjun Zhang, Juan Zhai, Chunrong Fang, Jiawei Liu, Weisong Sun,\\r  Haichuan Hu, Qingyu Wang\\rCategories: cs.CL cs.SE\\rComments: Accepted to ACM Transactions on Software Engineering and Methodology\\r  2024 (TOSEM'24)\\r\\\\\\\\\\r  Machine translation systems have been widely adopted in our daily life,\\rmaking life easier and more convenient. Unfortunately, erroneous translations\\rmay result in severe consequences, such as financial losses. This requires to\\rimprove the accuracy and the reliability of machine translation systems.\\rHowever, it is challenging to test machine translation systems because of the\\rcomplexity and intractability of the underlying neural models. To tackle these\\rchallenges, we propose a novel metamorphic testing approach by syntactic tree\\rpruning (STP) to validate machine translation systems. Our key insight is that\\ra pruned sentence should have similar crucial semantics compared with the\\roriginal sentence. Specifically, STP (1) proposes a core semantics-preserving\\rpruning strategy by basic sentence structure and dependency relations on the\\rlevel of syntactic tree representation; (2) generates source sentence pairs\\rbased on the metamorphic relation; (3) reports suspicious issues whose\\rtranslations break the consistency property by a bag-of-words model. We further\\revaluate STP on two state-of-the-art machine translation systems (i.e., Google\\rTranslate and Bing Microsoft Translator) with 1,200 source sentences as inputs.\\rThe results show that STP can accurately find 5,073 unique erroneous\\rtranslations in Google Translate and 5,100 unique erroneous translations in\\rBing Microsoft Translator (400% more than state-of-the-art techniques), with\\r64.5% and 65.4% precision, respectively. The reported erroneous translations\\rvary in types and more than 90% of them cannot be found by state-of-the-art\\rtechniques. There are 9,393 erroneous translations unique to STP, which is\\r711.9% more than state-of-the-art techniques. Moreover, STP is quite effective\\rto detect translation errors for the original sentences with a recall reaching\\r74.0%, improving state-of-the-art techniques by 55.1% on average.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00751 ,  435kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00779\\rDate: Mon, 1 Jan 2024 14:58:53 GMT   (722kb,D)\\r\\rTitle: Temporal Validity Change Prediction\\rAuthors: Georg Wenzel and Adam Jatowt\\rCategories: cs.CL cs.AI\\rComments: 9 pages, 9 figures, 3 tables\\rMSC-class: 68T50\\rACM-class: I.2.7\\r\\\\\\\\\\r  Temporal validity is an important property of text that is useful for many\\rdownstream applications, such as recommender systems, conversational AI, or\\rstory understanding. Existing benchmarking tasks often require models to\\ridentify the temporal validity duration of a single statement. However, in many\\rcases, additional contextual information, such as sentences in a story or posts\\ron a social media profile, can be collected from the available text stream.\\rThis contextual information may greatly alter the duration for which a\\rstatement is expected to be valid. We propose Temporal Validity Change\\rPrediction, a natural language processing task benchmarking the capability of\\rmachine learning models to detect contextual statements that induce such\\rchange. We create a dataset consisting of temporal target statements sourced\\rfrom Twitter and crowdsource sample context statements. We then benchmark a set\\rof transformer-based language models on our dataset. Finally, we experiment\\rwith temporal validity duration prediction as an auxiliary task to improve the\\rperformance of the state-of-the-art model.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00779 ,  722kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00788\\rDate: Mon, 1 Jan 2024 15:30:19 GMT   (2123kb,D)\\r\\rTitle: Astraios: Parameter-Efficient Instruction Tuning Code Large Language\\r  Models\\rAuthors: Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von\\r  Werra, Harm de Vries, Qian Liu, Niklas Muennighoff\\rCategories: cs.CL cs.AI cs.SE\\rComments: 25 pages (12 main), 19 figures, 8 tables\\r\\\\\\\\\\r  The high cost of full-parameter fine-tuning (FFT) of Large Language Models\\r(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\\rHowever, it remains unclear which methods provide the best cost-performance\\rtrade-off at different model scales. We introduce Astraios, a suite of 28\\rinstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\\rto 16 billion parameters. Through investigations across 5 tasks and 8 different\\rdatasets encompassing both code comprehension and code generation tasks, we\\rfind that FFT generally leads to the best downstream performance across all\\rscales, and PEFT methods differ significantly in their efficacy based on the\\rmodel scale. LoRA usually offers the most favorable trade-off between cost and\\rperformance. Further investigation into the effects of these methods on both\\rmodel robustness and code security reveals that larger models tend to\\rdemonstrate reduced robustness and less security. At last, we explore the\\rrelationships among updated parameters, cross-entropy loss, and task\\rperformance. We find that the tuning effectiveness observed in small models\\rgeneralizes well to larger models, and the validation loss in instruction\\rtuning can be a reliable indicator of overall downstream performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00788 ,  2123kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00811\\rDate: Mon, 1 Jan 2024 16:42:56 GMT   (2227kb,D)\\r\\rTitle: PerSHOP -- A Persian dataset for shopping dialogue systems modeling\\rAuthors: Keyvan Mahmoudi, Heshaam Faili\\rCategories: cs.CL cs.HC\\r\\\\\\\\\\r  Nowadays, dialogue systems are used in many fields of industry and research.\\rThere are successful instances of these systems, such as Apple Siri, Google\\rAssistant, and IBM Watson. Task-oriented dialogue system is a category of\\rthese, that are used in specific tasks. They can perform tasks such as booking\\rplane tickets or making restaurant reservations. Shopping is one of the most\\rpopular areas on these systems. The bot replaces the human salesperson and\\rinteracts with the customers by speaking. To train the models behind the scenes\\rof these systems, annotated data is needed. In this paper, we developed a\\rdataset of dialogues in the Persian language through crowd-sourcing. We\\rannotated these dialogues to train a model. This dataset contains nearly 22k\\rutterances in 15 different domains and 1061 dialogues. This is the largest\\rPersian dataset in this field, which is provided freely so that future\\rresearchers can use it. Also, we proposed some baseline models for natural\\rlanguage understanding (NLU) tasks. These models perform two tasks for NLU:\\rintent classification and entity extraction. The F-1 score metric obtained for\\rintent classification is around 91% and for entity extraction is around 93%,\\rwhich can be a baseline for future research.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00811 ,  2227kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00812\\rDate: Mon, 1 Jan 2024 16:51:20 GMT   (18105kb,D)\\r\\rTitle: If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code\\r  Empowers Large Language Models to Serve as Intelligent Agents\\rAuthors: Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan\\r  Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, Chengxiang Zhai\\rCategories: cs.CL\\r\\\\\\\\\\r  The prominent large language models (LLMs) of today differ from past language\\rmodels not only in size, but also in the fact that they are trained on a\\rcombination of natural language and formal language (code). As a medium between\\rhumans and computers, code translates high-level goals into executable steps,\\rfeaturing standard syntax, logical consistency, abstraction, and modularity. In\\rthis survey, we present an overview of the various benefits of integrating code\\rinto LLMs' training data. Specifically, beyond enhancing LLMs in code\\rgeneration, we observe that these unique properties of code help (i) unlock the\\rreasoning ability of LLMs, enabling their applications to a range of more\\rcomplex natural language tasks; (ii) steer LLMs to produce structured and\\rprecise intermediate steps, which can then be connected to external execution\\rends through function calls; and (iii) take advantage of code compilation and\\rexecution environment, which also provides diverse feedback for model\\rimprovement. In addition, we trace how these profound capabilities of LLMs,\\rbrought by code, have led to their emergence as intelligent agents (IAs) in\\rsituations where the ability to understand instructions, decompose goals, plan\\rand execute actions, and refine from feedback are crucial to their success on\\rdownstream tasks. Finally, we present several key challenges and future\\rdirections of empowering LLMs with code.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00812 ,  18105kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00820\\rDate: Mon, 1 Jan 2024 17:32:28 GMT   (8956kb,D)\\r\\rTitle: A Computational Framework for Behavioral Assessment of LLM Therapists\\rAuthors: Yu Ying Chiu, Ashish Sharma, Inna Wanyin Lin, Tim Althoff\\rCategories: cs.CL cs.HC\\r\\\\\\\\\\r  The emergence of ChatGPT and other large language models (LLMs) has greatly\\rincreased interest in utilizing LLMs as therapists to support individuals\\rstruggling with mental health challenges. However, due to the lack of\\rsystematic studies, our understanding of how LLM therapists behave, i.e., ways\\rin which they respond to clients, is significantly limited. Understanding their\\rbehavior across a wide range of clients and situations is crucial to accurately\\rassess their capabilities and limitations in the high-risk setting of mental\\rhealth, where undesirable behaviors can lead to severe consequences. In this\\rpaper, we propose BOLT, a novel computational framework to study the\\rconversational behavior of LLMs when employed as therapists. We develop an\\rin-context learning method to quantitatively measure the behavior of LLMs based\\ron 13 different psychotherapy techniques including reflections, questions,\\rsolutions, normalizing, and psychoeducation. Subsequently, we compare the\\rbehavior of LLM therapists against that of high- and low-quality human therapy,\\rand study how their behavior can be modulated to better reflect behaviors\\robserved in high-quality therapy. Our analysis of GPT and Llama-variants\\rreveals that these LLMs often resemble behaviors more commonly exhibited in\\rlow-quality therapy rather than high-quality therapy, such as offering a higher\\rdegree of problem-solving advice when clients share emotions, which is against\\rtypical recommendations. At the same time, unlike low-quality therapy, LLMs\\rreflect significantly more upon clients' needs and strengths. Our analysis\\rframework suggests that despite the ability of LLMs to generate anecdotal\\rexamples that appear similar to human therapists, LLM therapists are currently\\rnot fully consistent with high-quality care, and thus require additional\\rresearch to ensure quality care.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00820 ,  8956kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00027\\rDate: Fri, 29 Dec 2023 02:59:40 GMT   (40359kb,D)\\r\\rTitle: Efficient Multi-scale Network with Learnable Discrete Wavelet Transform\\r  for Blind Motion Deblurring\\rAuthors: Xin Gao, Tianheng Qiu, Xinyu Zhang, Hanlin Bai, Kang Liu, Xuan Huang,\\r  Hu Wei, Guoying Zhang, Huaping Liu\\rCategories: cs.CV\\r\\\\\\\\\\r  Coarse-to-fine schemes are widely used in traditional single-image motion\\rdeblur; however, in the context of deep learning, existing multi-scale\\ralgorithms not only require the use of complex modules for feature fusion of\\rlow-scale RGB images and deep semantics, but also manually generate\\rlow-resolution pairs of images that do not have sufficient confidence. In this\\rwork, we propose a multi-scale network based on single-input and\\rmultiple-outputs(SIMO) for motion deblurring. This simplifies the complexity of\\ralgorithms based on a coarse-to-fine scheme. To alleviate restoration defects\\rimpacting detail information brought about by using a multi-scale architecture,\\rwe combine the characteristics of real-world blurring trajectories with a\\rlearnable wavelet transform module to focus on the directional continuity and\\rfrequency features of the step-by-step transitions between blurred images to\\rsharp images. In conclusion, we propose a multi-scale network with a learnable\\rdiscrete wavelet transform (MLWNet), which exhibits state-of-the-art\\rperformance on multiple real-world deblurred datasets, in terms of both\\rsubjective and objective quality as well as computational efficiency.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00027 ,  40359kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00028\\rDate: Fri, 29 Dec 2023 03:08:57 GMT   (3069kb,D)\\r\\rTitle: An Empirical Study of Scaling Law for OCR\\rAuthors: Miao Rang, Zhenni Bi, Chuanjian Liu, Yunhe Wang, Kai Han\\rCategories: cs.CV\\r\\\\\\\\\\r  The laws of model size, data volume, computation and model performance have\\rbeen extensively studied in the field of Natural Language Processing (NLP).\\rHowever, the scaling laws in Optical Character Recognition (OCR) have not yet\\rbeen investigated. To address this, we conducted comprehensive studies that\\rinvolved examining the correlation between performance and the scale of models,\\rdata volume and computation in the field of text recognition.Conclusively, the\\rstudy demonstrates smooth power laws between performance and model size, as\\rwell as training data volume, when other influencing factors are held constant.\\rAdditionally, we have constructed a large-scale dataset called REBU-Syn, which\\rcomprises 6 million real samples and 18 million synthetic samples. Based on our\\rscaling law and new dataset, we have successfully trained a scene text\\rrecognition model, achieving a new state-ofthe-art on 6 common test benchmarks\\rwith a top-1 average accuracy of 97.42%.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00028 ,  3069kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00029\\rDate: Fri, 29 Dec 2023 05:28:35 GMT   (15424kb,D)\\r\\rTitle: 6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation\\rAuthors: Li Xu, Haoxuan Qu, Yujun Cai, Jun Liu\\rCategories: cs.CV\\r\\\\\\\\\\r  Estimating the 6D object pose from a single RGB image often involves noise\\rand indeterminacy due to challenges such as occlusions and cluttered\\rbackgrounds. Meanwhile, diffusion models have shown appealing performance in\\rgenerating high-quality images from random noise with high indeterminacy\\rthrough step-by-step denoising. Inspired by their denoising capability, we\\rpropose a novel diffusion-based framework (6D-Diff) to handle the noise and\\rindeterminacy in object pose estimation for better performance. In our\\rframework, to establish accurate 2D-3D correspondence, we formulate 2D\\rkeypoints detection as a reverse diffusion (denoising) process. To facilitate\\rsuch a denoising process, we design a Mixture-of-Cauchy-based forward diffusion\\rprocess and condition the reverse process on the object features. Extensive\\rexperiments on the LM-O and YCB-V datasets demonstrate the effectiveness of our\\rframework.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00029 ,  15424kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00036\\rDate: Fri, 29 Dec 2023 18:35:04 GMT   (25242kb,D)\\r\\rTitle: Discrete Distribution Networks\\rAuthors: Lei Yang\\rCategories: cs.CV cs.LG\\rComments: Project page: https://Discrete-Distribution-Networks.github.io/\\r\\\\\\\\\\r  We introduce a novel generative model, the Discrete Distribution Networks\\r(DDN), that approximates data distribution using hierarchical discrete\\rdistributions. We posit that since the features within a network inherently\\rcontain distributional information, liberating the network from a single output\\rto concurrently generate multiple samples proves to be highly effective.\\rTherefore, DDN fits the target distribution, including continuous ones, by\\rgenerating multiple discrete sample points. To capture finer details of the\\rtarget data, DDN selects the output that is closest to the Ground Truth (GT)\\rfrom the coarse results generated in the first layer. This selected output is\\rthen fed back into the network as a condition for the second layer, thereby\\rgenerating new outputs more similar to the GT. As the number of DDN layers\\rincreases, the representational space of the outputs expands exponentially, and\\rthe generated samples become increasingly similar to the GT. This hierarchical\\routput pattern of discrete distributions endows DDN with two intriguing\\rproperties: highly compressed representation and more general zero-shot\\rconditional generation. We demonstrate the efficacy of DDN and these intriguing\\rproperties through experiments on CIFAR-10 and FFHQ.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00036 ,  25242kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00067\\rDate: Fri, 29 Dec 2023 20:24:20 GMT   (1874kb,D)\\r\\rTitle: Particle-Based Shape Modeling for Arbitrary Regions-of-Interest\\rAuthors: Hong Xu, Alan Morris, Shireen Y. Elhabian\\rCategories: cs.CV\\rJournal-ref: Shape in Medical Imaging (ShapeMI 2023), p47_54, Springer Nature\\r  Switzerland\\r\\\\\\\\\\r  Statistical Shape Modeling (SSM) is a quantitative method for analyzing\\rmorphological variations in anatomical structures. These analyses often\\rnecessitate building models on targeted anatomical regions of interest to focus\\ron specific morphological features. We propose an extension to \\\\particle-based\\rshape modeling (PSM), a widely used SSM framework, to allow shape modeling to\\rarbitrary regions of interest. Existing methods to define regions of interest\\rare computationally expensive and have topological limitations. To address\\rthese shortcomings, we use mesh fields to define free-form constraints, which\\rallow for delimiting arbitrary regions of interest on shape surfaces.\\rFurthermore, we add a quadratic penalty method to the model optimization to\\renable computationally efficient enforcement of any combination of\\rcutting-plane and free-form constraints. We demonstrate the effectiveness of\\rthis method on a challenging synthetic dataset and two medical datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00067 ,  1874kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00080\\rDate: Fri, 29 Dec 2023 21:48:20 GMT   (1946kb,D)\\r\\rTitle: A Large-Scale Re-identification Analysis in Sporting Scenarios: the\\r  Betrayal of Reaching a Critical Point\\rAuthors: David Freire-Obreg\\\\'on, Javier Lorenzo-Navarro, Oliverio J. Santana,\\r  Daniel Hern\\\\'andez-Sosa, Modesto Castrill\\\\'on-Santana\\rCategories: cs.CV\\rComments: Accepted at 7th International Joint Conference on Biometrics (IJCB\\r  2023)\\r\\\\\\\\\\r  Re-identifying participants in ultra-distance running competitions can be\\rdaunting due to the extensive distances and constantly changing terrain. To\\rovercome these challenges, computer vision techniques have been developed to\\ranalyze runners' faces, numbers on their bibs, and clothing. However, our study\\rpresents a novel gait-based approach for runners' re-identification (re-ID) by\\rleveraging various pre-trained human action recognition (HAR) models and loss\\rfunctions. Our results show that this approach provides promising results for\\rre-identifying runners in ultra-distance competitions. Furthermore, we\\rinvestigate the significance of distinct human body movements when athletes are\\rapproaching their endurance limits and their potential impact on re-ID\\raccuracy. Our study examines how the recognition of a runner's gait is affected\\rby a competition's critical point (CP), defined as a moment of severe fatigue\\rand the point where the finish line comes into view, just a few kilometers away\\rfrom this location. We aim to determine how this CP can improve the accuracy of\\rathlete re-ID. Our experimental results demonstrate that gait recognition can\\rbe significantly enhanced (up to a 9% increase in mAP) as athletes approach\\rthis point. This highlights the potential of utilizing gait recognition in\\rreal-world scenarios, such as ultra-distance competitions or long-duration\\rsurveillance tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00080 ,  1946kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00094\\rDate: Fri, 29 Dec 2023 23:04:00 GMT   (6606kb,D)\\r\\rTitle: Generating Enhanced Negatives for Training Language-Based Object\\r  Detectors\\rAuthors: Shiyu Zhao, Long Zhao, Vijay Kumar B.G, Yumin Suh, Dimitris N.\\r  Metaxas, Manmohan Chandraker, Samuel Schulter\\rCategories: cs.CV\\rComments: 21 pages, 17 figures\\r\\\\\\\\\\r  The recent progress in language-based open-vocabulary object detection can be\\rlargely attributed to finding better ways of leveraging large-scale data with\\rfree-form text annotations. Training such models with a discriminative\\robjective function has proven successful, but requires good positive and\\rnegative samples. However, the free-form nature and the open vocabulary of\\robject descriptions make the space of negatives extremely large. Prior works\\rrandomly sample negatives or use rule-based techniques to build them. In\\rcontrast, we propose to leverage the vast knowledge built into modern\\rgenerative models to automatically build negatives that are more relevant to\\rthe original data. Specifically, we use large-language-models to generate\\rnegative text descriptions, and text-to-image diffusion models to also generate\\rcorresponding negative images. Our experimental analysis confirms the relevance\\rof the generated negative data, and its use in language-based detectors\\rimproves performance on two complex benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00094 ,  6606kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00110\\rDate: Sat, 30 Dec 2023 01:24:25 GMT   (15283kb,D)\\r\\rTitle: Diffusion Model with Perceptual Loss\\rAuthors: Shanchuan Lin, Xiao Yang\\rCategories: cs.CV cs.AI cs.LG\\r\\\\\\\\\\r  Diffusion models trained with mean squared error loss tend to generate\\runrealistic samples. Current state-of-the-art models rely on classifier-free\\rguidance to improve sample quality, yet its surprising effectiveness is not\\rfully understood. In this paper, We show that the effectiveness of\\rclassifier-free guidance partly originates from it being a form of implicit\\rperceptual guidance. As a result, we can directly incorporate perceptual loss\\rin diffusion training to improve sample quality. Since the score matching\\robjective used in diffusion training strongly resembles the denoising\\rautoencoder objective used in unsupervised training of perceptual networks, the\\rdiffusion model itself is a perceptual network and can be used to generate\\rmeaningful perceptual loss. We propose a novel self-perceptual objective that\\rresults in diffusion models capable of generating more realistic samples. For\\rconditional generation, our method only improves sample quality without\\rentanglement with the conditional input and therefore does not sacrifice sample\\rdiversity. Our method can also improve sample quality for unconditional\\rgeneration, which was not possible with classifier-free guidance before.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00110 ,  15283kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00127\\rDate: Sat, 30 Dec 2023 03:19:54 GMT   (5262kb,D)\\r\\rTitle: Pushing Boundaries: Exploring Zero Shot Object Classification with Large\\r  Multimodal Models\\rAuthors: Ashhadul Islam, Md. Rafiul Biswas, Wajdi Zaghouani, Samir Brahim\\r  Belhaouari, Zubair Shah\\rCategories: cs.CV cs.SI\\rComments: 5 pages,6 figures, 4 tables, Accepted on The International Symposium\\r  on Foundation and Large Language Models (FLLM2023)\\rJournal-ref: https://fllm-conference.org/2023/\\r\\\\\\\\\\r  $ $The synergy of language and vision models has given rise to Large Language\\rand Vision Assistant models (LLVAs), designed to engage users in rich\\rconversational experiences intertwined with image-based queries. These\\rcomprehensive multimodal models seamlessly integrate vision encoders with Large\\rLanguage Models (LLMs), expanding their applications in general-purpose\\rlanguage and visual comprehension. The advent of Large Multimodal Models (LMMs)\\rheralds a new era in Artificial Intelligence (AI) assistance, extending the\\rhorizons of AI utilization. This paper takes a unique perspective on LMMs,\\rexploring their efficacy in performing image classification tasks using\\rtailored prompts designed for specific datasets. We also investigate the LLVAs\\rzero-shot learning capabilities. Our study includes a benchmarking analysis\\racross four diverse datasets: MNIST, Cats Vs. Dogs, Hymnoptera (Ants Vs. Bees),\\rand an unconventional dataset comprising Pox Vs. Non-Pox skin images. The\\rresults of our experiments demonstrate the model's remarkable performance,\\rachieving classification accuracies of 85\\\\%, 100\\\\%, 77\\\\%, and 79\\\\% for the\\rrespective datasets without any fine-tuning. To bolster our analysis, we assess\\rthe model's performance post fine-tuning for specific tasks. In one instance,\\rfine-tuning is conducted over a dataset comprising images of faces of children\\rwith and without autism. Prior to fine-tuning, the model demonstrated a test\\raccuracy of 55\\\\%, which significantly improved to 83\\\\% post fine-tuning. These\\rresults, coupled with our prior findings, underscore the transformative\\rpotential of LLVAs and their versatile applications in real-world scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00127 ,  5262kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00151\\rDate: Sat, 30 Dec 2023 06:20:36 GMT   (11303kb,D)\\r\\rTitle: CamPro: Camera-based Anti-Facial Recognition\\rAuthors: Wenjun Zhu, Yuan Sun, Jiani Liu, Yushi Cheng, Xiaoyu Ji, Wenyuan Xu\\rCategories: cs.CV cs.CR\\rComments: Accepted by NDSS Symposium 2024\\r\\\\\\\\\\r  The proliferation of images captured from millions of cameras and the\\radvancement of facial recognition (FR) technology have made the abuse of FR a\\rsevere privacy threat. Existing works typically rely on obfuscation, synthesis,\\ror adversarial examples to modify faces in images to achieve anti-facial\\rrecognition (AFR). However, the unmodified images captured by camera modules\\rthat contain sensitive personally identifiable information (PII) could still be\\rleaked. In this paper, we propose a novel approach, CamPro, to capture inborn\\rAFR images. CamPro enables well-packed commodity camera modules to produce\\rimages that contain little PII and yet still contain enough information to\\rsupport other non-sensitive vision applications, such as person detection.\\rSpecifically, CamPro tunes the configuration setup inside the camera image\\rsignal processor (ISP), i.e., color correction matrix and gamma correction, to\\rachieve AFR, and designs an image enhancer to keep the image quality for\\rpossible human viewers. We implemented and validated CamPro on a\\rproof-of-concept camera, and our experiments demonstrate its effectiveness on\\rten state-of-the-art black-box FR models. The results show that CamPro images\\rcan significantly reduce face identification accuracy to 0.3\\\\% while having\\rlittle impact on the targeted non-sensitive vision application. Furthermore, we\\rfind that CamPro is resilient to adaptive attackers who have re-trained their\\rFR models using images generated by CamPro, even with full knowledge of\\rprivacy-preserving ISP parameters.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00151 ,  11303kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00155\\rDate: Sat, 30 Dec 2023 06:55:30 GMT   (1711kb,D)\\r\\rTitle: A comprehensive framework for occluded human pose estimation\\rAuthors: Linhao Xu, Lin Zhao, Xinxin Sun, Guangyu Li, Kedong Yan\\rCategories: cs.CV\\rComments: Accepted to ICASSP 2024\\r\\\\\\\\\\r  Occlusion presents a significant challenge in human pose estimation. The\\rchallenges posed by occlusion can be attributed to the following factors: 1)\\rData: The collection and annotation of occluded human pose samples are\\rrelatively challenging. 2) Feature: Occlusion can cause feature confusion due\\rto the high similarity between the target person and interfering individuals.\\r3) Inference: Robust inference becomes challenging due to the loss of complete\\rbody structural information. The existing methods designed for occluded human\\rpose estimation usually focus on addressing only one of these factors. In this\\rpaper, we propose a comprehensive framework DAG (Data, Attention, Graph) to\\raddress the performance degradation caused by occlusion. Specifically, we\\rintroduce the mask joints with instance paste data augmentation technique to\\rsimulate occlusion scenarios. Additionally, an Adaptive Discriminative\\rAttention Module (ADAM) is proposed to effectively enhance the features of\\rtarget individuals. Furthermore, we present the Feature-Guided Multi-Hop GCN\\r(FGMP-GCN) to fully explore the prior knowledge of body structure and improve\\rpose estimation results. Through extensive experiments conducted on three\\rbenchmark datasets for occluded human pose estimation, we demonstrate that the\\rproposed method outperforms existing methods. Code and data will be publicly\\ravailable.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00155 ,  1711kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00208\\rDate: Sat, 30 Dec 2023 11:26:55 GMT   (26950kb,D)\\r\\rTitle: Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with\\r  Generative Diffusion Models\\rAuthors: Han Jiang, Haosen Sun, Ruoxuan Li, Chi-Keung Tang, Yu-Wing Tai\\rCategories: cs.CV\\r\\\\\\\\\\r  Current Neural Radiance Fields (NeRF) can generate photorealistic novel\\rviews. For editing 3D scenes represented by NeRF, with the advent of generative\\rmodels, this paper proposes Inpaint4DNeRF to capitalize on state-of-the-art\\rstable diffusion models (e.g., ControlNet) for direct generation of the\\runderlying completed background content, regardless of static or dynamic. The\\rkey advantages of this generative approach for NeRF inpainting are twofold.\\rFirst, after rough mask propagation, to complete or fill in previously occluded\\rcontent, we can individually generate a small subset of completed images with\\rplausible content, called seed images, from which simple 3D geometry proxies\\rcan be derived. Second and the remaining problem is thus 3D multiview\\rconsistency among all completed images, now guided by the seed images and their\\r3D proxies. Without other bells and whistles, our generative Inpaint4DNeRF\\rbaseline framework is general which can be readily extended to 4D dynamic\\rNeRFs, where temporal consistency can be naturally handled in a similar way as\\rour multiview consistency.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00208 ,  26950kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00237\\rDate: Sat, 30 Dec 2023 13:58:50 GMT   (862kb,D)\\r\\rTitle: A Novel Approach for Defect Detection of Wind Turbine Blade Using\\r  Virtual Reality and Deep Learning\\rAuthors: Md Fazle Rabbi, Solayman Hossain Emon, Ehtesham Mahmud Nishat,\\r  Tzu-Liang (Bill) Tseng, Atira Ferdoushi, Chun-Che Huang and Md Fashiar Rahman\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  Wind turbines are subjected to continuous rotational stresses and unusual\\rexternal forces such as storms, lightning, strikes by flying objects, etc.,\\rwhich may cause defects in turbine blades. Hence, it requires a periodical\\rinspection to ensure proper functionality and avoid catastrophic failure. The\\rtask of inspection is challenging due to the remote location and inconvenient\\rreachability by human inspection. Researchers used images with cropped defects\\rfrom the wind turbine in the literature. They neglected possible background\\rbiases, which may hinder real-time and autonomous defect detection using aerial\\rvehicles such as drones or others. To overcome such challenges, in this paper,\\rwe experiment with defect detection accuracy by having the defects with the\\rbackground using a two-step deep-learning methodology. In the first step, we\\rdevelop virtual models of wind turbines to synthesize the near-reality images\\rfor four types of common defects - cracks, leading edge erosion, bending, and\\rlight striking damage. The Unity perception package is used to generate wind\\rturbine blade defects images with variations in background, randomness, camera\\rangle, and light effects. In the second step, a customized U-Net architecture\\ris trained to classify and segment the defect in turbine blades. The outcomes\\rof U-Net architecture have been thoroughly tested and compared with 5-fold\\rvalidation datasets. The proposed methodology provides reasonable defect\\rdetection accuracy, making it suitable for autonomous and remote inspection\\rthrough aerial vehicles.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00237 ,  862kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00241\\rDate: Sat, 30 Dec 2023 14:11:08 GMT   (2491kb)\\r\\rTitle: Image Super-resolution Reconstruction Network based on Enhanced Swin\\r  Transformer via Alternating Aggregation of Local-Global Features\\rAuthors: Yuming Huang, Yingpin Chen, Changhui Wu, Hanrong Xie, Binhui Song, Hui\\r  Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  The Swin Transformer image super-resolution reconstruction network only\\rrelies on the long-range relationship of window attention and shifted window\\rattention to explore features. This mechanism has two limitations. On the one\\rhand, it only focuses on global features while ignoring local features. On the\\rother hand, it is only concerned with spatial feature interactions while\\rignoring channel features and channel interactions, thus limiting its\\rnon-linear mapping ability. To address the above limitations, this paper\\rproposes enhanced Swin Transformer modules via alternating aggregation of\\rlocal-global features. In the local feature aggregation stage, this paper\\rintroduces shift convolution to realize the interaction between local spatial\\rinformation and channel information. This paper proposes a block sparse global\\rperception module in the global feature aggregation stage. This module\\rorganizes the spatial information first, then sends the recombination\\rinformation into a spatial gating unit to implement the further interaction of\\rspatial and channel information. Then, a multi-scale self-attention module and\\ra low-parameter residual channel attention module are introduced to realize\\rinformation aggregation at different scales. Finally, the proposed network is\\rvalidated on five publicly available datasets. The experimental results show\\rthat the proposed network outperforms the other state-of-the-art\\rsuper-resolution networks.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00241 ,  2491kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00247\\rDate: Sat, 30 Dec 2023 14:21:30 GMT   (9001kb,D)\\r\\rTitle: Probing the Limits and Capabilities of Diffusion Models for the Anatomic\\r  Editing of Digital Twins\\rAuthors: Karim Kadry, Shreya Gupta, Farhad R. Nezami, Elazer R. Edelman\\rCategories: cs.CV eess.IV\\rComments: 11 pages\\r\\\\\\\\\\r  Numerical simulations can model the physical processes that govern\\rcardiovascular device deployment. When such simulations incorporate digital\\rtwins; computational models of patient-specific anatomy, they can expedite and\\rde-risk the device design process. Nonetheless, the exclusive use of\\rpatient-specific data constrains the anatomic variability which can be\\rprecisely or fully explored. In this study, we investigate the capacity of\\rLatent Diffusion Models (LDMs) to edit digital twins to create anatomic\\rvariants, which we term digital siblings. Digital twins and their corresponding\\rsiblings can serve as the basis for comparative simulations, enabling the study\\rof how subtle anatomic variations impact the simulated deployment of\\rcardiovascular devices, as well as the augmentation of virtual cohorts for\\rdevice assessment. However, while diffusion models have been characterized in\\rtheir ability to edit natural images, their capacity to anatomically edit\\rdigital twins has yet to be studied. Using a case example centered on 3D\\rdigital twins of cardiac anatomy, we implement various methods for generating\\rdigital siblings and characterize them through morphological and topological\\ranalyses. We specifically edit digital twins to introduce anatomic variation at\\rdifferent spatial scales and within localized regions, demonstrating the\\rexistence of bias towards common anatomic features. We further show that such\\ranatomic bias can be leveraged for virtual cohort augmentation through\\rselective editing, partially alleviating issues related to dataset imbalance\\rand lack of diversity. Our experimental framework thus delineates the limits\\rand capabilities of using latent diffusion models in synthesizing anatomic\\rvariation for in silico trials.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00247 ,  9001kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00248\\rDate: Sat, 30 Dec 2023 14:24:33 GMT   (23784kb,D)\\r\\rTitle: Promoting Segment Anything Model towards Highly Accurate Dichotomous\\r  Image Segmentation\\rAuthors: Xianjie Liu, Keren Fu, Qijun Zhao\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Segmenting any object represents a crucial step towards achieving artificial\\rgeneral intelligence, and the Segment Anything Model (SAM) has significantly\\radvanced the development of foundational models in computer vision. We have\\rhigh expectations regarding whether SAM can enhance highly accurate dichotomous\\rimage segmentation. In fact, the evidence presented in this article\\rdemonstrates that by inputting SAM with simple prompt boxes and utilizing the\\rresults output by SAM as input for IS5Net, we can greatly improve the\\reffectiveness of highly accurate dichotomous image segmentation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00248 ,  23784kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00254\\rDate: Sat, 30 Dec 2023 14:53:09 GMT   (1930kb,D)\\r\\rTitle: Masked Image Modeling via Dynamic Token Morphing\\rAuthors: Taekyung Kim, Dongyoon Han, Byeongho Heo\\rCategories: cs.CV\\rComments: 15 pages, 6 figures\\r\\\\\\\\\\r  Masked Image Modeling (MIM) arises as a promising option for Vision\\rTransformers among various self-supervised learning (SSL) methods. The essence\\rof MIM lies in token-wise masked patch predictions, with targets patchified\\rfrom images; or generated by pre-trained tokenizers or models. We argue targets\\rfrom the pre-trained models usually exhibit spatial inconsistency, which makes\\rit excessively challenging for the model to follow to learn more discriminative\\rrepresentations. To mitigate the issue, we introduce a novel self-supervision\\rsignal based on Dynamic Token Morphing (DTM), which dynamically aggregates\\rcontextually related tokens. DTM can be generally applied to various SSL\\rframeworks, yet we propose a simple MIM that employs DTM to effectively improve\\rthe performance barely introducing extra training costs. Our experiments on\\rImageNet-1K and ADE20K evidently demonstrate the superiority of our methods.\\rFurthermore, the comparative evaluation of iNaturalist and Fine-grained Visual\\rClassification datasets further validates the transferability of our method on\\rvarious downstream tasks. Our code will be released publicly.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00254 ,  1930kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00260\\rDate: Sat, 30 Dec 2023 15:24:50 GMT   (552kb,D)\\r\\rTitle: GazeCLIP: Towards Enhancing Gaze Estimation via Text Guidance\\rAuthors: Jun Wang, Hao Ruan, Mingjie Wang, Chuanghui Zhang, Chunhua Li, Jun\\r  Zhou\\rCategories: cs.CV\\rComments: 10 pages, 5 figures\\r\\\\\\\\\\r  Over the past decade, visual gaze estimation has garnered growing attention\\rwithin the research community, thanks to its wide-ranging application\\rscenarios. While existing estimation approaches have achieved remarkable\\rsuccess in enhancing prediction accuracy, they primarily infer gaze directions\\rfrom single-image signals and discard the huge potentials of the currently\\rdominant text guidance. Notably, visual-language collaboration has been\\rextensively explored across a range of visual tasks, such as image synthesis\\rand manipulation, leveraging the remarkable transferability of large-scale\\rContrastive Language-Image Pre-training (CLIP) model. Nevertheless, existing\\rgaze estimation approaches ignore the rich semantic cues conveyed by linguistic\\rsignals and priors in CLIP feature space, thereby yielding performance\\rsetbacks. In pursuit of making up this gap, we delve deeply into the text-eye\\rcollaboration protocol and introduce a novel gaze estimation framework in this\\rpaper, referred to as GazeCLIP. Specifically, we intricately design a\\rlinguistic description generator to produce text signals with coarse\\rdirectional cues. Additionally, a CLIP-based backbone that excels in\\rcharacterizing text-eye pairs for gaze estimation is presented. This is\\rfollowed by the implementation of a fine-grained multi-modal fusion module\\raimed at modeling the interrelationships between heterogeneous inputs.\\rExtensive experiments on three challenging datasets demonstrate the superiority\\rof the proposed GazeCLIP which surpasses the previous approaches and achieves\\rthe state-of-the-art estimation accuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00260 ,  552kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00268\\rDate: Sat, 30 Dec 2023 15:47:36 GMT   (3974kb,D)\\r\\rTitle: COMMA: Co-Articulated Multi-Modal Learning\\rAuthors: Lianyu Hu, Liqing Gao, Zekang Liu, Chi-Man Pun, Wei Feng\\rCategories: cs.CV\\rComments: Accepted to AAAI2024. Code is available at\\r  https://github.com/hulianyuyy/COMMA\\r\\\\\\\\\\r  Pretrained large-scale vision-language models such as CLIP have demonstrated\\rexcellent generalizability over a series of downstream tasks. However, they are\\rsensitive to the variation of input text prompts and need a selection of prompt\\rtemplates to achieve satisfactory performance. Recently, various methods have\\rbeen proposed to dynamically learn the prompts as the textual inputs to avoid\\rthe requirements of laboring hand-crafted prompt engineering in the fine-tuning\\rprocess. We notice that these methods are suboptimal in two aspects. First, the\\rprompts of the vision and language branches in these methods are usually\\rseparated or uni-directionally correlated. Thus, the prompts of both branches\\rare not fully correlated and may not provide enough guidance to align the\\rrepresentations of both branches. Second, it's observed that most previous\\rmethods usually achieve better performance on seen classes but cause\\rperformance degeneration on unseen classes compared to CLIP. This is because\\rthe essential generic knowledge learned in the pretraining stage is partly\\rforgotten in the fine-tuning process. In this paper, we propose Co-Articulated\\rMulti-Modal Learning (COMMA) to handle the above limitations. Especially, our\\rmethod considers prompts from both branches to generate the prompts to enhance\\rthe representation alignment of both branches. Besides, to alleviate forgetting\\rabout the essential knowledge, we minimize the feature discrepancy between the\\rlearned prompts and the embeddings of hand-crafted prompts in the pre-trained\\rCLIP in the late transformer layers. We evaluate our method across three\\rrepresentative tasks of generalization to novel classes, new target datasets\\rand unseen domain shifts. Experimental results demonstrate the superiority of\\rour method by exhibiting a favorable performance boost upon all tasks with high\\refficiency.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00268 ,  3974kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00271\\rDate: Sat, 30 Dec 2023 16:12:13 GMT   (779kb,D)\\r\\rTitle: HybridGait: A Benchmark for Spatial-Temporal Cloth-Changing Gait\\r  Recognition with Hybrid Explorations\\rAuthors: Yilan Dong, Chunlin Yu, Ruiyang Ha, Ye Shi, Yuexin Ma, Lan Xu, Yanwei\\r  Fu, Jingya Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  Existing gait recognition benchmarks mostly include minor clothing variations\\rin the laboratory environments, but lack persistent changes in appearance over\\rtime and space. In this paper, we propose the first in-the-wild benchmark\\rCCGait for cloth-changing gait recognition, which incorporates diverse clothing\\rchanges, indoor and outdoor scenes, and multi-modal statistics over 92 days. To\\rfurther address the coupling effect of clothing and viewpoint variations, we\\rpropose a hybrid approach HybridGait that exploits both temporal dynamics and\\rthe projected 2D information of 3D human meshes. Specifically, we introduce a\\rCanonical Alignment Spatial-Temporal Transformer (CA-STT) module to encode\\rhuman joint position-aware features, and fully exploit 3D dense priors via a\\rSilhouette-guided Deformation with 3D-2D Appearance Projection (SilD) strategy.\\rOur contributions are twofold: we provide a challenging benchmark CCGait that\\rcaptures realistic appearance changes across an expanded and space, and we\\rpropose a hybrid framework HybridGait that outperforms prior works on CCGait\\rand Gait3D benchmarks. Our project page is available at\\rhttps://github.com/HCVLab/HybridGait.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00271 ,  779kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00285\\rDate: Sat, 30 Dec 2023 17:32:44 GMT   (5632kb,D)\\r\\rTitle: BusReF: Infrared-Visible images registration and fusion focus on\\r  reconstructible area using one set of features\\rAuthors: Zeyang Zhang, Hui Li, Tianyang Xu, Xiaojun Wu, Josef Kittler\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  In a scenario where multi-modal cameras are operating together, the problem\\rof working with non-aligned images cannot be avoided. Yet, existing image\\rfusion algorithms rely heavily on strictly registered input image pairs to\\rproduce more precise fusion results, as a way to improve the performance of\\rdownstream high-level vision tasks. In order to relax this assumption, one can\\rattempt to register images first. However, the existing methods for registering\\rmultiple modalities have limitations, such as complex structures and reliance\\ron significant semantic information. This paper aims to address the problem of\\rimage registration and fusion in a single framework, called BusRef. We focus on\\rInfrared-Visible image registration and fusion task (IVRF). In this framework,\\rthe input unaligned image pairs will pass through three stages: Coarse\\rregistration, Fine registration and Fusion. It will be shown that the unified\\rapproach enables more robust IVRF. We also propose a novel training and\\revaluation strategy, involving the use of masks to reduce the influence of\\rnon-reconstructible regions on the loss functions, which greatly improves the\\raccuracy and robustness of the fusion task. Last but not least, a\\rgradient-aware fusion network is designed to preserve the complementary\\rinformation. The advanced performance of this algorithm is demonstrated by\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00285 ,  5632kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00320\\rDate: Sat, 30 Dec 2023 20:52:20 GMT   (41640kb,D)\\r\\rTitle: DXAI: Explaining Classification by Image Decomposition\\rAuthors: Elnatan Kadar, Guy Gilboa\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  We propose a new way to explain and to visualize neural network\\rclassification through a decomposition-based explainable AI (DXAI). Instead of\\rproviding an explanation heatmap, our method yields a decomposition of the\\rimage into class-agnostic and class-distinct parts, with respect to the data\\rand chosen classifier. Following a fundamental signal processing paradigm of\\ranalysis and synthesis, the original image is the sum of the decomposed parts.\\rWe thus obtain a radically different way of explaining classification. The\\rclass-agnostic part ideally is composed of all image features which do not\\rposses class information, where the class-distinct part is its complementary.\\rThis new visualization can be more helpful and informative in certain\\rscenarios, especially when the attributes are dense, global and additive in\\rnature, for instance, when colors or textures are essential for class\\rdistinction. Code is available at https://github.com/dxai2024/dxai.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00320 ,  41640kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00334\\rDate: Sat, 30 Dec 2023 21:48:20 GMT   (3227kb,D)\\r\\rTitle: Explainability-Driven Leaf Disease Classification using Adversarial\\r  Training and Knowledge Distillation\\rAuthors: Sebastian-Vasile Echim, Iulian-Marius T\\\\u{a}iatu, Dumitru-Clementin\\r  Cercel, Florin Pop\\rCategories: cs.CV cs.LG\\rComments: 10 pages, 8 figures, Accepted by ICAART 2024\\r\\\\\\\\\\r  This work focuses on plant leaf disease classification and explores three\\rcrucial aspects: adversarial training, model explainability, and model\\rcompression. The models' robustness against adversarial attacks is enhanced\\rthrough adversarial training, ensuring accurate classification even in the\\rpresence of threats. Leveraging explainability techniques, we gain insights\\rinto the model's decision-making process, improving trust and transparency.\\rAdditionally, we explore model compression techniques to optimize computational\\refficiency while maintaining classification performance. Through our\\rexperiments, we determine that on a benchmark dataset, the robustness can be\\rthe price of the classification accuracy with performance reductions of 3%-20%\\rfor regular tests and gains of 50%-70% for adversarial attack tests. We also\\rdemonstrate that a student model can be 15-25 times more computationally\\refficient for a slight performance reduction, distilling the knowledge of more\\rcomplex models.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00334 ,  3227kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00343\\rDate: Sat, 30 Dec 2023 23:01:31 GMT   (19179kb,D)\\r\\rTitle: SHARE: Single-view Human Adversarial REconstruction\\rAuthors: Shreelekha Revankar, Shijia Liao, Yu Shen, Junbang Liang, Huaishu\\r  Peng, Ming Lin\\rCategories: cs.CV\\r\\\\\\\\\\r  The accuracy of 3D Human Pose and Shape reconstruction (HPS) from an image is\\rprogressively improving. Yet, no known method is robust across all image\\rdistortion. To address issues due to variations of camera poses, we introduce\\rSHARE, a novel fine-tuning method that utilizes adversarial data augmentation\\rto enhance the robustness of existing HPS techniques. We perform a\\rcomprehensive analysis on the impact of camera poses on HPS reconstruction\\routcomes. We first generated large-scale image datasets captured systematically\\rfrom diverse camera perspectives. We then established a mapping between camera\\rposes and reconstruction errors as a continuous function that characterizes the\\rrelationship between camera poses and HPS quality. Leveraging this\\rrepresentation, we introduce RoME (Regions of Maximal Error), a novel sampling\\rtechnique for our adversarial fine-tuning method.\\r  The SHARE framework is generalizable across various single-view HPS methods\\rand we demonstrate its performance on HMR, SPIN, PARE, CLIFF and ExPose. Our\\rresults illustrate a reduction in mean joint errors across single-view HPS\\rtechniques, for images captured from multiple camera positions without\\rcompromising their baseline performance. In many challenging cases, our method\\rsurpasses the performance of existing models, highlighting its practical\\rsignificance for diverse real-world applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00343 ,  19179kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00370\\rDate: Sun, 31 Dec 2023 02:16:29 GMT   (9060kb,D)\\r\\rTitle: UGPNet: Universal Generative Prior for Image Restoration\\rAuthors: Hwayoon Lee, Kyoungkook Kang, Hyeongmin Lee, Seung-Hwan Baek, Sunghyun\\r  Cho\\rCategories: cs.CV eess.IV\\rComments: Accepted to WACV 2024\\r\\\\\\\\\\r  Recent image restoration methods can be broadly categorized into two classes:\\r(1) regression methods that recover the rough structure of the original image\\rwithout synthesizing high-frequency details and (2) generative methods that\\rsynthesize perceptually-realistic high-frequency details even though the\\rresulting image deviates from the original structure of the input. While both\\rdirections have been extensively studied in isolation, merging their benefits\\rwith a single framework has been rarely studied. In this paper, we propose\\rUGPNet, a universal image restoration framework that can effectively achieve\\rthe benefits of both approaches by simply adopting a pair of an existing\\rregression model and a generative model. UGPNet first restores the image\\rstructure of a degraded input using a regression model and synthesizes a\\rperceptually-realistic image with a generative model on top of the regressed\\routput. UGPNet then combines the regressed output and the synthesized output,\\rresulting in a final result that faithfully reconstructs the structure of the\\roriginal image in addition to perceptually-realistic textures. Our extensive\\rexperiments on deblurring, denoising, and super-resolution demonstrate that\\rUGPNet can successfully exploit both regression and generative methods for\\rhigh-fidelity image restoration.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00370 ,  9060kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00371\\rDate: Sun, 31 Dec 2023 02:17:14 GMT   (718kb)\\r\\rTitle: Multi-Granularity Representation Learning for Sketch-based Dynamic Face\\r  Image Retrieval\\rAuthors: Liang Wang, Dawei Dai, Shiyu Fu, Guoyin Wang\\rCategories: cs.CV\\rComments: 5 pages,5 figures\\r\\\\\\\\\\r  In specific scenarios, face sketch can be used to identify a person. However,\\rdrawing a face sketch often requires exceptional skill and is time-consuming,\\rlimiting its widespread applications in actual scenarios. The new framework of\\rsketch less face image retrieval (SLFIR)[1] attempts to overcome the barriers\\rby providing a means for humans and machines to interact during the drawing\\rprocess. Considering SLFIR problem, there is a large gap between a partial\\rsketch with few strokes and any whole face photo, resulting in poor performance\\rat the early stages. In this study, we propose a multigranularity (MG)\\rrepresentation learning (MGRL) method to address the SLFIR problem, in which we\\rlearn the representation of different granularity regions for a partial sketch,\\rand then, by combining all MG regions of the sketches and images, the final\\rdistance was determined. In the experiments, our method outperformed\\rstate-of-the-art baselines in terms of early retrieval on two accessible\\rdatasets. Codes are available at https://github.com/ddw2AIGROUP2CQUPT/MGRL.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00371 ,  718kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00374\\rDate: Sun, 31 Dec 2023 02:25:41 GMT   (4339kb,D)\\r\\rTitle: EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Masked\\r  Audio Gesture Modeling\\rAuthors: Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su,\\r  You Zhou, Xuefei Zhe, Naoya Iwamoto, Bo Zheng, Michael J. Black\\rCategories: cs.CV\\rComments: Project Page: https://pantomatrix.github.io/EMAGE/\\r\\\\\\\\\\r  We propose EMAGE, a framework to generate full-body human gestures from audio\\rand masked gestures, encompassing facial, local body, hands, and global\\rmovements. To achieve this, we first introduce BEATX (BEAT-SMPLX-FLAME), a new\\rmesh-level holistic co-speech dataset. BEATX combines MoShed SMPLX body with\\rFLAME head parameters and further refines the modeling of head, neck, and\\rfinger movements, offering a community-standardized, high-quality 3D motion\\rcaptured dataset. EMAGE leverages masked body gesture priors during training to\\rboost inference performance. It involves a Masked Audio Gesture Transformer,\\rfacilitating joint training on audio-to-gesture generation and masked gesture\\rreconstruction to effectively encode audio and body gesture hints. Encoded body\\rhints from masked gestures are then separately employed to generate facial and\\rbody movements. Moreover, EMAGE adaptively merges speech features from the\\raudio's rhythm and content and utilizes four compositional VQ-VAEs to enhance\\rthe results' fidelity and diversity. Experiments demonstrate that EMAGE\\rgenerates holistic gestures with state-of-the-art performance and is flexible\\rin accepting predefined spatial-temporal gesture inputs, generating complete,\\raudio-synchronized results. Our code and dataset are available at\\rhttps://pantomatrix.github.io/EMAGE/\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00374 ,  4339kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00390\\rDate: Sun, 31 Dec 2023 04:09:55 GMT   (3659kb,D)\\r\\rTitle: Horizontal Federated Computer Vision\\rAuthors: Paul K. Mandal, Cole Leo, Connor Hurley\\rCategories: cs.CV cs.AI cs.DC cs.LG\\rComments: 11 pages, 7 figures\\rACM-class: C.2.4; I.2.8; I.4; I.4.8\\r\\\\\\\\\\r  In the modern world, the amount of visual data recorded has been rapidly\\rincreasing. In many cases, data is stored in geographically distinct locations\\rand thus requires a large amount of time and space to consolidate. Sometimes,\\rthere are also regulations for privacy protection which prevent data\\rconsolidation. In this work, we present federated implementations for object\\rdetection and recognition using a federated Faster R-CNN (FRCNN) and image\\rsegmentation using a federated Fully Convolutional Network (FCN). Our FRCNN was\\rtrained on 5000 examples of the COCO2017 dataset while our FCN was trained on\\rthe entire train set of the CamVid dataset. The proposed federated models\\raddress the challenges posed by the increasing volume and decentralized nature\\rof visual data, offering efficient solutions in compliance with privacy\\rregulations.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00390 ,  3659kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00393\\rDate: Sun, 31 Dec 2023 04:34:58 GMT   (975kb)\\r\\rTitle: Generative Model-Driven Synthetic Training Image Generation: An Approach\\r  to Cognition in Rail Defect Detection\\rAuthors: Rahatara Ferdousi, Chunsheng Yang, M. Anwar Hossain, Fedwa Laamarti,\\r  M. Shamim Hossain, Abdulmotaleb El Saddik\\rCategories: cs.CV cs.AI cs.LG cs.MM eess.IV\\rComments: 26 pages, 13 figures, Springer Journal\\rMSC-class: 68T05, 94A08, 90B25\\rACM-class: I.2.6; I.2.10; I.5.4; I.4.10\\r\\\\\\\\\\r  Recent advancements in cognitive computing, with the integration of deep\\rlearning techniques, have facilitated the development of intelligent cognitive\\rsystems (ICS). This is particularly beneficial in the context of rail defect\\rdetection, where the ICS would emulate human-like analysis of image data for\\rdefect patterns. Despite the success of Convolutional Neural Networks (CNN) in\\rvisual defect classification, the scarcity of large datasets for rail defect\\rdetection remains a challenge due to infrequent accident events that would\\rresult in defective parts and images. Contemporary researchers have addressed\\rthis data scarcity challenge by exploring rule-based and generative data\\raugmentation models. Among these, Variational Autoencoder (VAE) models can\\rgenerate realistic data without extensive baseline datasets for noise modeling.\\rThis study proposes a VAE-based synthetic image generation technique for rail\\rdefects, incorporating weight decay regularization and image reconstruction\\rloss to prevent overfitting. The proposed method is applied to create a\\rsynthetic dataset for the Canadian Pacific Railway (CPR) with just 50 real\\rsamples across five classes. Remarkably, 500 synthetic samples are generated\\rwith a minimal reconstruction loss of 0.021. A Visual Transformer (ViT) model\\runderwent fine-tuning using this synthetic CPR dataset, achieving high accuracy\\rrates (98%-99%) in classifying the five defect classes. This research offers a\\rpromising solution to the data scarcity challenge in rail defect detection,\\rshowcasing the potential for robust ICS development in this domain.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00393 ,  975kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00405\\rDate: Sun, 31 Dec 2023 05:39:38 GMT   (34139kb,D)\\r\\rTitle: Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen\\r  Objects\\rAuthors: Qirui Wu, Daniel Ritchie, Manolis Savva, Angel X. Chang\\rCategories: cs.CV\\r\\\\\\\\\\r  Single-view 3D shape retrieval is a challenging task that is increasingly\\rimportant with the growth of available 3D data. Prior work that has studied\\rthis task has not focused on evaluating how realistic occlusions impact\\rperformance, and how shape retrieval methods generalize to scenarios where\\reither the target 3D shape database contains unseen shapes, or the input image\\rcontains unseen objects. In this paper, we systematically evaluate single-view\\r3D shape retrieval along three different axes: the presence of object\\rocclusions and truncations, generalization to unseen 3D shape data, and\\rgeneralization to unseen objects in the input images. We standardize two\\rexisting datasets of real images and propose a dataset generation pipeline to\\rproduce a synthetic dataset of scenes with multiple objects exhibiting\\rrealistic occlusions. Our experiments show that training on occlusion-free data\\ras was commonly done in prior work leads to significant performance degradation\\rfor inputs with occlusion. We find that that by first pretraining on our\\rsynthetic dataset with occlusions and then finetuning on real data, we can\\rsignificantly outperform models from prior work and demonstrate robustness to\\rboth unseen 3D shapes and unseen objects.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00405 ,  34139kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00406\\rDate: Sun, 31 Dec 2023 05:45:22 GMT   (587kb)\\r\\rTitle: Low-cost Geometry-based Eye Gaze Detection using Facial Landmarks\\r  Generated through Deep Learning\\rAuthors: Esther Enhui Ye, John Enzhou Ye, Joseph Ye, Jacob Ye, Runzhou Ye\\rCategories: cs.CV\\r\\\\\\\\\\r  Introduction: In the realm of human-computer interaction and behavioral\\rresearch, accurate real-time gaze estimation is critical. Traditional methods\\roften rely on expensive equipment or large datasets, which are impractical in\\rmany scenarios. This paper introduces a novel, geometry-based approach to\\raddress these challenges, utilizing consumer-grade hardware for broader\\rapplicability. Methods: We leverage novel face landmark detection neural\\rnetworks capable of fast inference on consumer-grade chips to generate accurate\\rand stable 3D landmarks of the face and iris. From these, we derive a small set\\rof geometry-based descriptors, forming an 8-dimensional manifold representing\\rthe eye and head movements. These descriptors are then used to formulate linear\\requations for predicting eye-gaze direction. Results: Our approach demonstrates\\rthe ability to predict gaze with an angular error of less than 1.9 degrees,\\rrivaling state-of-the-art systems while operating in real-time and requiring\\rnegligible computational resources. Conclusion: The developed method marks a\\rsignificant step forward in gaze estimation technology, offering a highly\\raccurate, efficient, and accessible alternative to traditional systems. It\\ropens up new possibilities for real-time applications in diverse fields, from\\rgaming to psychological research.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00406 ,  587kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00409\\rDate: Sun, 31 Dec 2023 06:46:46 GMT   (4578kb,D)\\r\\rTitle: A Two-stream Hybrid CNN-Transformer Network for Skeleton-based Human\\r  Interaction Recognition\\rAuthors: Ruoqi Yin, Jianqin Yin\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Human Interaction Recognition is the process of identifying interactive\\ractions between multiple participants in a specific situation. The aim is to\\rrecognise the action interactions between multiple entities and their meaning.\\rMany single Convolutional Neural Network has issues, such as the inability to\\rcapture global instance interaction features or difficulty in training, leading\\rto ambiguity in action semantics. In addition, the computational complexity of\\rthe Transformer cannot be ignored, and its ability to capture local information\\rand motion features in the image is poor. In this work, we propose a Two-stream\\rHybrid CNN-Transformer Network (THCT-Net), which exploits the local specificity\\rof CNN and models global dependencies through the Transformer. CNN and\\rTransformer simultaneously model the entity, time and space relationships\\rbetween interactive entities respectively. Specifically, Transformer-based\\rstream integrates 3D convolutions with multi-head self-attention to learn\\rinter-token correlations; We propose a new multi-branch CNN framework for\\rCNN-based streams that automatically learns joint spatio-temporal features from\\rskeleton sequences. The convolutional layer independently learns the local\\rfeatures of each joint neighborhood and aggregates the features of all joints.\\rAnd the raw skeleton coordinates as well as their temporal difference are\\rintegrated with a dual-branch paradigm to fuse the motion features of the\\rskeleton. Besides, a residual structure is added to speed up training\\rconvergence. Finally, the recognition results of the two branches are fused\\rusing parallel splicing. Experimental results on diverse and challenging\\rdatasets, demonstrate that the proposed method can better comprehend and infer\\rthe meaning and context of various actions, outperforming state-of-the-art\\rmethods.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00409 ,  4578kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00414\\rDate: Sun, 31 Dec 2023 07:16:10 GMT   (4914kb,D)\\r\\rTitle: Is It Possible to Backdoor Face Forgery Detection with Natural Triggers?\\rAuthors: Xiaoxuan Han, Songlin Yang, Wei Wang, Ziwen He, Jing Dong\\rCategories: cs.CV\\r\\\\\\\\\\r  Deep neural networks have significantly improved the performance of face\\rforgery detection models in discriminating Artificial Intelligent Generated\\rContent (AIGC). However, their security is significantly threatened by the\\rinjection of triggers during model training (i.e., backdoor attacks). Although\\rexisting backdoor defenses and manual data selection can mitigate those using\\rhuman-eye-sensitive triggers, such as patches or adversarial noises, the more\\rchallenging natural backdoor triggers remain insufficiently researched. To\\rfurther investigate natural triggers, we propose a novel analysis-by-synthesis\\rbackdoor attack against face forgery detection models, which embeds natural\\rtriggers in the latent space. We thoroughly study such backdoor vulnerability\\rfrom two perspectives: (1) Model Discrimination (Optimization-Based Trigger):\\rwe adopt a substitute detection model and find the trigger by minimizing the\\rcross-entropy loss; (2) Data Distribution (Custom Trigger): we manipulate the\\runcommon facial attributes in the long-tailed distribution to generate poisoned\\rsamples without the supervision from detection models. Furthermore, to\\rcompletely evaluate the detection models towards the latest AIGC, we utilize\\rboth state-of-the-art StyleGAN and Stable Diffusion for trigger generation.\\rFinally, these backdoor triggers introduce specific semantic features to the\\rgenerated poisoned samples (e.g., skin textures and smile), which are more\\rnatural and robust. Extensive experiments show that our method is superior from\\rthree levels: (1) Attack Success Rate: ours achieves a high attack success rate\\r(over 99%) and incurs a small model accuracy drop (below 0.2%) with a low\\rpoisoning rate (less than 3%); (2) Backdoor Defense: ours shows better robust\\rperformance when faced with existing backdoor defense methods; (3) Human\\rInspection: ours is less human-eye-sensitive from a comprehensive user study.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00414 ,  4914kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00416\\rDate: Sun, 31 Dec 2023 07:44:05 GMT   (1844kb,D)\\r\\rTitle: SVFAP: Self-supervised Video Facial Affect Perceiver\\rAuthors: Licai Sun, Zheng Lian, Kexin Wang, Yu He, Mingyu Xu, Haiyang Sun, Bin\\r  Liu, and Jianhua Tao\\rCategories: cs.CV cs.HC cs.MM\\rComments: Submitted to IEEE Trans. on Affective Computing (February 8, 2023)\\r\\\\\\\\\\r  Video-based facial affect analysis has recently attracted increasing\\rattention owing to its critical role in human-computer interaction. Previous\\rstudies mainly focus on developing various deep learning architectures and\\rtraining them in a fully supervised manner. Although significant progress has\\rbeen achieved by these supervised methods, the longstanding lack of large-scale\\rhigh-quality labeled data severely hinders their further improvements.\\rMotivated by the recent success of self-supervised learning in computer vision,\\rthis paper introduces a self-supervised approach, termed Self-supervised Video\\rFacial Affect Perceiver (SVFAP), to address the dilemma faced by supervised\\rmethods. Specifically, SVFAP leverages masked facial video autoencoding to\\rperform self-supervised pre-training on massive unlabeled facial videos.\\rConsidering that large spatiotemporal redundancy exists in facial videos, we\\rpropose a novel temporal pyramid and spatial bottleneck Transformer as the\\rencoder of SVFAP, which not only enjoys low computational cost but also\\rachieves excellent performance. To verify the effectiveness of our method, we\\rconduct experiments on nine datasets spanning three downstream tasks, including\\rdynamic facial expression recognition, dimensional emotion recognition, and\\rpersonality recognition. Comprehensive results demonstrate that SVFAP can learn\\rpowerful affect-related representations via large-scale self-supervised\\rpre-training and it significantly outperforms previous state-of-the-art methods\\ron all datasets. Codes will be available at https://github.com/sunlicai/SVFAP.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00416 ,  1844kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00420\\rDate: Sun, 31 Dec 2023 08:06:53 GMT   (34888kb,D)\\r\\rTitle: SynCDR : Training Cross Domain Retrieval Models with Synthetic Data\\rAuthors: Samarth Mishra, Kate Saenko, Venkatesh Saligrama\\rCategories: cs.CV cs.AI\\rComments: Pre-print\\r\\\\\\\\\\r  In cross-domain retrieval, a model is required to identify images from the\\rsame semantic category across two visual domains. For instance, given a sketch\\rof an object, a model needs to retrieve a real image of it from an online\\rstore's catalog. A standard approach for such a problem is learning a feature\\rspace of images where Euclidean distances reflect similarity. Even without\\rhuman annotations, which may be expensive to acquire, prior methods function\\rreasonably well using unlabeled images for training. Our problem constraint\\rtakes this further to scenarios where the two domains do not necessarily share\\rany common categories in training data. This can occur when the two domains in\\rquestion come from different versions of some biometric sensor recording\\ridentities of different people. We posit a simple solution, which is to\\rgenerate synthetic data to fill in these missing category examples across\\rdomains. This, we do via category preserving translation of images from one\\rvisual domain to another. We compare approaches specifically trained for this\\rtranslation for a pair of domains, as well as those that can use large-scale\\rpre-trained text-to-image diffusion models via prompts, and find that the\\rlatter can generate better replacement synthetic data, leading to more accurate\\rcross-domain retrieval models. Code for our work is available at\\rhttps://github.com/samarth4149/SynCDR .\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00420 ,  34888kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00421\\rDate: Sun, 31 Dec 2023 08:13:47 GMT   (10784kb,D)\\r\\rTitle: From Text to Pixels: A Context-Aware Semantic Synergy Solution for\\r  Infrared and Visible Image Fusion\\rAuthors: Xingyuan Li, Yang Zou, Jinyuan Liu, Zhiying Jiang, Long Ma, Xin Fan,\\r  Risheng Liu\\rCategories: cs.CV\\rComments: 10 pages, 12 figures, 3 tables, conference\\rMSC-class: 68T45\\rACM-class: I.4.3\\r\\\\\\\\\\r  With the rapid progression of deep learning technologies, multi-modality\\rimage fusion has become increasingly prevalent in object detection tasks.\\rDespite its popularity, the inherent disparities in how different sources\\rdepict scene content make fusion a challenging problem. Current fusion\\rmethodologies identify shared characteristics between the two modalities and\\rintegrate them within this shared domain using either iterative optimization or\\rdeep learning architectures, which often neglect the intricate semantic\\rrelationships between modalities, resulting in a superficial understanding of\\rinter-modal connections and, consequently, suboptimal fusion outcomes. To\\raddress this, we introduce a text-guided multi-modality image fusion method\\rthat leverages the high-level semantics from textual descriptions to integrate\\rsemantics from infrared and visible images. This method capitalizes on the\\rcomplementary characteristics of diverse modalities, bolstering both the\\raccuracy and robustness of object detection. The codebook is utilized to\\renhance a streamlined and concise depiction of the fused intra- and\\rinter-domain dynamics, fine-tuned for optimal performance in detection tasks.\\rWe present a bilevel optimization strategy that establishes a nexus between the\\rjoint problem of fusion and detection, optimizing both processes concurrently.\\rFurthermore, we introduce the first dataset of paired infrared and visible\\rimages accompanied by text prompts, paving the way for future research.\\rExtensive experiments on several datasets demonstrate that our method not only\\rproduces visually superior fusion results but also achieves a higher detection\\rmAP over existing methods, achieving state-of-the-art results.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00421 ,  10784kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00431\\rDate: Sun, 31 Dec 2023 09:01:34 GMT   (8459kb,D)\\r\\rTitle: Wild2Avatar: Rendering Humans Behind Occlusions\\rAuthors: Tiange Xiang, Adam Sun, Scott Delp, Kazuki Kozuka, Li Fei-Fei, Ehsan\\r  Adeli\\rCategories: cs.CV\\r\\\\\\\\\\r  Rendering the visual appearance of moving humans from occluded monocular\\rvideos is a challenging task. Most existing research renders 3D humans under\\rideal conditions, requiring a clear and unobstructed scene. Those methods\\rcannot be used to render humans in real-world scenes where obstacles may block\\rthe camera's view and lead to partial occlusions. In this work, we present\\rWild2Avatar, a neural rendering approach catered for occluded in-the-wild\\rmonocular videos. We propose occlusion-aware scene parameterization for\\rdecoupling the scene into three parts - occlusion, human, and background.\\rAdditionally, extensive objective functions are designed to help enforce the\\rdecoupling of the human from both the occlusion and the background and to\\rensure the completeness of the human model. We verify the effectiveness of our\\rapproach with experiments on in-the-wild videos.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00431 ,  8459kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00435\\rDate: Sun, 31 Dec 2023 09:24:21 GMT   (3017kb,D)\\r\\rTitle: Bidirectional Trained Tree-Structured Decoder for Handwritten\\r  Mathematical Expression Recognition\\rAuthors: Hanbo Cheng, Chenyu Liu, Pengfei Hu, Zhenrong Zhang, Jiefeng Ma, Jun\\r  Du\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  The Handwritten Mathematical Expression Recognition (HMER) task is a critical\\rbranch in the field of OCR. Recent studies have demonstrated that incorporating\\rbidirectional context information significantly improves the performance of\\rHMER models. However, existing methods fail to effectively utilize\\rbidirectional context information during the inference stage. Furthermore,\\rcurrent bidirectional training methods are primarily designed for string\\rdecoders and cannot adequately generalize to tree decoders, which offer\\rsuperior generalization capabilities and structural analysis capacity. In order\\rto overcome these limitations, we propose the Mirror-Flipped Symbol Layout Tree\\r(MF-SLT) and Bidirectional Asynchronous Training (BAT) structure. Our method\\rextends the bidirectional training strategy to the tree decoder, allowing for\\rmore effective training by leveraging bidirectional information. Additionally,\\rwe analyze the impact of the visual and linguistic perception of the HMER model\\rseparately and introduce the Shared Language Modeling (SLM) mechanism. Through\\rthe SLM, we enhance the model's robustness and generalization when dealing with\\rvisual ambiguity, particularly in scenarios with abundant training data. Our\\rapproach has been validated through extensive experiments, demonstrating its\\rability to achieve new state-of-the-art results on the CROHME 2014, 2016, and\\r2019 datasets, as well as the HME100K dataset. The code used in our experiments\\rwill be publicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00435 ,  3017kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00436\\rDate: Sun, 31 Dec 2023 09:24:28 GMT   (7845kb,D)\\r\\rTitle: Diff-PCR: Diffusion-Based Correspondence Searching in Doubly Stochastic\\r  Matrix Space for Point Cloud Registration\\rAuthors: Qianliang Wu, Haobo Jiang, Yaqing Ding, Lei Luo, Jin Xie, Jian Yang\\rCategories: cs.CV\\r\\\\\\\\\\r  Efficiently finding optimal correspondences between point clouds is crucial\\rfor solving both rigid and non-rigid point cloud registration problems.\\rExisting methods often rely on geometric or semantic feature embedding to\\restablish correspondences and estimate transformations or flow fields.\\rRecently, state-of-the-art methods have employed RAFT-like iterative updates to\\rrefine the solution. However, these methods have certain limitations. Firstly,\\rtheir iterative refinement design lacks transparency, and their iterative\\rupdates follow a fixed path during the refinement process, which can lead to\\rsuboptimal results. Secondly, these methods overlook the importance of refining\\ror optimizing correspondences (or matching matrices) as a precursor to solving\\rtransformations or flow fields. They typically compute candidate\\rcorrespondences based on distances in the point feature space. However, they\\ronly project the candidate matching matrix into some matrix space once with\\rSinkhorn or dual softmax operations to obtain final correspondences. This\\rone-shot projected matching matrix may be far from the globally optimal one,\\rand these approaches do not consider the distribution of the target matching\\rmatrix. In this paper, we propose a novel approach that exploits the Denoising\\rDiffusion Model to predict a searching gradient for the optimal matching matrix\\rwithin the Doubly Stochastic Matrix Space. During the reverse denoising\\rprocess, our method iteratively searches for better solutions along this\\rdenoising gradient, which points towards the maximum likelihood direction of\\rthe target matching matrix. Our method offers flexibility by allowing the\\rsearch to start from any initial matching matrix provided by the online\\rbackbone or white noise. Experimental evaluations on the 3DMatch/3DLoMatch and\\r4DMatch/4DLoMatch datasets demonstrate the effectiveness of our newly designed\\rframework.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00436 ,  7845kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00438\\rDate: Sun, 31 Dec 2023 09:36:55 GMT   (451kb,D)\\r\\rTitle: SFGANS Self-supervised Future Generator for human ActioN Segmentation\\rAuthors: Or Berman and Adam Goldbraikh and Shlomi Laufer\\rCategories: cs.CV\\r\\\\\\\\\\r  The ability to locate and classify action segments in long untrimmed video is\\rof particular interest to many applications such as autonomous cars, robotics\\rand healthcare applications. Today, the most popular pipeline for action\\rsegmentation is composed of encoding the frames into feature vectors, which are\\rthen processed by a temporal model for segmentation. In this paper we present a\\rself-supervised method that comes in the middle of the standard pipeline and\\rgenerated refined representations of the original feature vectors. Experiments\\rshow that this method improves the performance of existing models on different\\rsub-tasks of action segmentation, even without additional hyper parameter\\rtuning.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00438 ,  451kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00440\\rDate: Sun, 31 Dec 2023 09:38:53 GMT   (5716kb,D)\\r\\rTitle: TSGAN: An Optical-to-SAR Dual Conditional GAN for Optical based SAR\\r  Temporal Shifting\\rAuthors: Moien Rangzan, Sara Attarchi, Richard Gloaguen, Seyed Kazem Alavipanah\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  In contrast to the well-investigated field of SAR-to-Optical translation,\\rthis study explores the lesser-investigated domain of Optical-to-SAR\\rtranslation, a challenging field due to the ill-posed nature of this\\rtranslation. The complexity arises as a single optical data can have multiple\\rSAR representations based on the SAR viewing geometry. We propose a novel\\rapproach, termed SAR Temporal Shifting, which inputs an optical data from the\\rdesired timestamp along with a SAR data from a different temporal point but\\rwith a consistent viewing geometry as the expected SAR data, both complemented\\rwith a change map of optical data during the intervening period. This model\\rmodifies the SAR data based on the changes observed in optical data to generate\\rthe SAR data for the desired timestamp. Our model, a dual conditional\\rGenerative Adversarial Network (GAN), named Temporal Shifting GAN (TSGAN),\\rincorporates a siamese encoder in both the Generator and the Discriminator. To\\rprevent the model from overfitting on the input SAR data, we employed a change\\rweighted loss function. Our approach surpasses traditional translation methods\\rby eliminating the GAN's fiction phenomenon, particularly in unchanged regions,\\rresulting in higher SSIM and PSNR in these areas. Additionally, modifications\\rto the Pix2Pix architecture and the inclusion of attention mechanisms have\\renhanced the model's performance on all regions of the data. This research\\rpaves the way for leveraging legacy optical datasets, the most abundant and\\rlongstanding source of Earth datary data, extending their use to SAR domains\\rand temporal analyses. To foster further research, we provide the code,\\rdatasets used in our study, and a framework for generating paired SAR-Optical\\rdatasets for new regions of interest. These resources are available on\\rgithub.com/moienr/TemporalGAN\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00440 ,  5716kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00442\\rDate: Sun, 31 Dec 2023 09:49:37 GMT   (21kb)\\r\\rTitle: A Comprehensive Overview of Fish-Eye Camera Distortion Correction\\r  Methods\\rAuthors: Jian Xu, De-Wei Han, Kang Li, Jun-Jie Li, Zhao-Yuan Ma\\rCategories: cs.CV cs.GL\\r\\\\\\\\\\r  The fisheye camera, with its unique wide field of view and other\\rcharacteristics, has found extensive applications in various fields. However,\\rthe fisheye camera suffers from significant distortion compared to pinhole\\rcameras, resulting in distorted images of captured objects. Fish-eye camera\\rdistortion is a common issue in digital image processing, requiring effective\\rcorrection techniques to enhance image quality. This review provides a\\rcomprehensive overview of various methods used for fish-eye camera distortion\\rcorrection. The article explores the polynomial distortion model, which\\rutilizes polynomial functions to model and correct radial distortions.\\rAdditionally, alternative approaches such as panorama mapping, grid mapping,\\rdirect methods, and deep learning-based methods are discussed. The review\\rhighlights the advantages, limitations, and recent advancements of each method,\\renabling readers to make informed decisions based on their specific needs.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00442 ,  21kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00456\\rDate: Sun, 31 Dec 2023 11:16:12 GMT   (3222kb,D)\\r\\rTitle: Double-well Net for Image Segmentation\\rAuthors: Hao Liu, Jun Liu, Raymond Chan, Xue-Cheng Tai\\rCategories: cs.CV\\rMSC-class: 68U10, 94A08\\r\\\\\\\\\\r  In this study, our goal is to integrate classical mathematical models with\\rdeep neural networks by introducing two novel deep neural network models for\\rimage segmentation known as Double-well Nets. Drawing inspiration from the\\rPotts model, our models leverage neural networks to represent a region force\\rfunctional. We extend the well-know MBO (Merriman-Bence-Osher) scheme to solve\\rthe Potts model. The widely recognized Potts model is approximated using a\\rdouble-well potential and then solved by an operator-splitting method, which\\rturns out to be an extension of the well-known MBO scheme. Subsequently, we\\rreplace the region force functional in the Potts model with a UNet-type\\rnetwork, which is data-driven, and also introduce control variables to enhance\\reffectiveness. The resulting algorithm is a neural network activated by a\\rfunction that minimizes the double-well potential. What sets our proposed\\rDouble-well Nets apart from many existing deep learning methods for image\\rsegmentation is their strong mathematical foundation. They are derived from the\\rnetwork approximation theory and employ the MBO scheme to approximately solve\\rthe Potts model. By incorporating mathematical principles, Double-well Nets\\rbridge the MBO scheme and neural networks, and offer an alternative perspective\\rfor designing networks with mathematical backgrounds. Through comprehensive\\rexperiments, we demonstrate the performance of Double-well Nets, showcasing\\rtheir superior accuracy and robustness compared to state-of-the-art neural\\rnetworks. Overall, our work represents a valuable contribution to the field of\\rimage segmentation by combining the strengths of classical variational models\\rand deep neural networks. The Double-well Nets introduce an innovative approach\\rthat leverages mathematical foundations to enhance segmentation performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00456 ,  3222kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00460\\rDate: Sun, 31 Dec 2023 11:30:42 GMT   (36515kb,D)\\r\\rTitle: RainSD: Rain Style Diversification Module for Image Synthesis\\r  Enhancement using Feature-Level Style Distribution\\rAuthors: Hyeonjae Jeon, Junghyun Seo, Taesoo Kim, Sungho Son, Jungki Lee,\\r  Gyeungho Choi, Yongseob Lim\\rCategories: cs.CV\\rComments: Under Review\\rMSC-class: 14J60 (Autonomous Vehicles) 14F05, 14J26 (Adverse Weather Condition)\\r\\\\\\\\\\r  Autonomous driving technology nowadays targets to level 4 or beyond, but the\\rresearchers are faced with some limitations for developing reliable driving\\ralgorithms in diverse challenges. To promote the autonomous vehicles to spread\\rwidely, it is important to address safety issues on this technology. Among\\rvarious safety concerns, the sensor blockage problem by severe weather\\rconditions can be one of the most frequent threats for multi-task learning\\rbased perception algorithms during autonomous driving. To handle this problem,\\rthe importance of the generation of proper datasets is becoming more\\rsignificant. In this paper, a synthetic road dataset with sensor blockage\\rgenerated from real road dataset BDD100K is suggested in the format of BDD100K\\rannotation. Rain streaks for each frame were made by an experimentally\\restablished equation and translated utilizing the image-to-image translation\\rnetwork based on style transfer. Using this dataset, the degradation of the\\rdiverse multi-task networks for autonomous driving, such as lane detection,\\rdriving area segmentation, and traffic object detection, has been thoroughly\\revaluated and analyzed. The tendency of the performance degradation of deep\\rneural network-based perception systems for autonomous vehicle has been\\ranalyzed in depth. Finally, we discuss the limitation and the future directions\\rof the deep neural network-based perception algorithms and autonomous driving\\rdataset generation based on image-to-image translation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00460 ,  36515kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00463\\rDate: Sun, 31 Dec 2023 11:38:50 GMT   (4114kb,D)\\r\\rTitle: Analyzing Local Representations of Self-supervised Vision Transformers\\rAuthors: Ani Vanyan, Alvard Barseghyan, Hakob Tamazyan, Vahan Huroyan, Hrant\\r  Khachatrian, Martin Danelljan\\rCategories: cs.CV\\r\\\\\\\\\\r  In this paper, we present a comparative analysis of various self-supervised\\rVision Transformers (ViTs), focusing on their local representative power.\\rInspired by large language models, we examine the abilities of ViTs to perform\\rvarious computer vision tasks with little to no fine-tuning. We design an\\revaluation framework to analyze the quality of local, i.e. patch-level,\\rrepresentations in the context of few-shot semantic segmentation, instance\\ridentification, object retrieval, and tracking. We discover that contrastive\\rlearning based methods like DINO produce more universal patch representations\\rthat can be immediately applied for downstream tasks with no parameter tuning,\\rcompared to masked image modeling. The embeddings learned using the latter\\rapproach, e.g. in masked autoencoders, have high variance features that harm\\rdistance-based algorithms, such as k-NN, and do not contain useful information\\rfor most downstream tasks. Furthermore, we demonstrate that removing these\\rhigh-variance features enhances k-NN by providing an analysis of the benchmarks\\rfor this work and for Scale-MAE, a recent extension of masked autoencoders.\\rFinally, we find an object instance retrieval setting where DINOv2, a model\\rpretrained on two orders of magnitude more data, performs worse than its less\\rcompute-intensive counterpart DINO.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00463 ,  4114kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00496\\rDate: Sun, 31 Dec 2023 13:32:18 GMT   (7543kb,D)\\r\\rTitle: SAR-RARP50: Segmentation of surgical instrumentation and Action\\r  Recognition on Robot-Assisted Radical Prostatectomy Challenge\\rAuthors: Dimitrios Psychogyios, Emanuele Colleoni, Beatrice Van Amsterdam,\\r  Chih-Yang Li, Shu-Yu Huang, Yuchong Li, Fucang Jia, Baosheng Zou, Guotai\\r  Wang, Yang Liu, Maxence Boels, Jiayu Huo, Rachel Sparks, Prokar Dasgupta,\\r  Alejandro Granados, Sebastien Ourselin, Mengya Xu, An Wang, Yanan Wu, Long\\r  Bai, Hongliang Ren, Atsushi Yamada, Yuriko Harai, Yuto Ishikawa, Kazuyuki\\r  Hayashi, Jente Simoens, Pieter DeBacker, Francesco Cisternino, Gabriele\\r  Furnari, Alex Mottrie, Federica Ferraguti, Satoshi Kondo, Satoshi Kasai,\\r  Kousuke Hirasawa, Soohee Kim, Seung Hyun Lee, Kyu Eun Lee, Hyoun-Joong Kong,\\r  Kui Fu, Chao Li, Shan An, Stefanie Krell, Sebastian Bodenstedt, Nicolas\\r  Ayobi, Alejandra Perez, Santiago Rodriguez, Juanita Puentes, Pablo Arbelaez,\\r  Omid Mohareri, Danail Stoyanov\\rCategories: cs.CV cs.AI cs.LG\\r\\\\\\\\\\r  Surgical tool segmentation and action recognition are fundamental building\\rblocks in many computer-assisted intervention applications, ranging from\\rsurgical skills assessment to decision support systems. Nowadays,\\rlearning-based action recognition and segmentation approaches outperform\\rclassical methods, relying, however, on large, annotated datasets. Furthermore,\\raction recognition and tool segmentation algorithms are often trained and make\\rpredictions in isolation from each other, without exploiting potential\\rcross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, we\\rrelease the first multimodal, publicly available, in-vivo, dataset for surgical\\raction recognition and semantic instrumentation segmentation, containing 50\\rsuturing video segments of Robotic Assisted Radical Prostatectomy (RARP). The\\raim of the challenge is twofold. First, to enable researchers to leverage the\\rscale of the provided dataset and develop robust and highly accurate\\rsingle-task action recognition and tool segmentation approaches in the surgical\\rdomain. Second, to further explore the potential of multitask-based learning\\rapproaches and determine their comparative advantage against their single-task\\rcounterparts. A total of 12 teams participated in the challenge, contributing 7\\raction recognition methods, 9 instrument segmentation techniques, and 4\\rmultitask approaches that integrated both action recognition and instrument\\rsegmentation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00496 ,  7543kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00551\\rDate: Sun, 31 Dec 2023 17:41:48 GMT   (30976kb,D)\\r\\rTitle: A Generalist FaceX via Learning Unified Facial Representation\\rAuthors: Yue Han, Jiangning Zhang, Junwei Zhu, Xiangtai Li, Yanhao Ge, Wei Li,\\r  Chengjie Wang, Yong Liu, Xiaoming Liu, Ying Tai\\rCategories: cs.CV\\rComments: Project page: https://diffusion-facex.github.io/\\r\\\\\\\\\\r  This work presents FaceX framework, a novel facial generalist model capable\\rof handling diverse facial tasks simultaneously. To achieve this goal, we\\rinitially formulate a unified facial representation for a broad spectrum of\\rfacial editing tasks, which macroscopically decomposes a face into fundamental\\ridentity, intra-personal variation, and environmental factors. Based on this,\\rwe introduce Facial Omni-Representation Decomposing (FORD) for seamless\\rmanipulation of various facial components, microscopically decomposing the core\\raspects of most facial editing tasks. Furthermore, by leveraging the prior of a\\rpretrained StableDiffusion (SD) to enhance generation quality and accelerate\\rtraining, we design Facial Omni-Representation Steering (FORS) to first\\rassemble unified facial representations and then effectively steer the SD-aware\\rgeneration process by the efficient Facial Representation Controller (FRC).\\r%Without any additional features, Our versatile FaceX achieves competitive\\rperformance compared to elaborate task-specific models on popular facial\\rediting tasks. Full codes and models will be available at\\rhttps://github.com/diffusion-facex/FaceX.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00551 ,  30976kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00604\\rDate: Sun, 31 Dec 2023 23:04:25 GMT   (32542kb,D)\\r\\rTitle: SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via\\r  Stein Identity\\rAuthors: Peihao Wang, Zhiwen Fan, Dejia Xu, Dilin Wang, Sreyas Mohan, Forrest\\r  Iandola, Rakesh Ranjan, Yilei Li, Qiang Liu, Zhangyang Wang, Vikas Chandra\\rCategories: cs.CV\\rComments: Project page: https://vita-group.github.io/SteinDreamer/\\r\\\\\\\\\\r  Score distillation has emerged as one of the most prevalent approaches for\\rtext-to-3D asset synthesis. Essentially, score distillation updates 3D\\rparameters by lifting and back-propagating scores averaged over different\\rviews. In this paper, we reveal that the gradient estimation in score\\rdistillation is inherent to high variance. Through the lens of variance\\rreduction, the effectiveness of SDS and VSD can be interpreted as applications\\rof various control variates to the Monte Carlo estimator of the distilled\\rscore. Motivated by this rethinking and based on Stein's identity, we propose a\\rmore general solution to reduce variance for score distillation, termed Stein\\rScore Distillation (SSD). SSD incorporates control variates constructed by\\rStein identity, allowing for arbitrary baseline functions. This enables us to\\rinclude flexible guidance priors and network architectures to explicitly\\roptimize for variance reduction. In our experiments, the overall pipeline,\\rdubbed SteinDreamer, is implemented by instantiating the control variate with a\\rmonocular depth estimator. The results suggest that SSD can effectively reduce\\rthe distillation variance and consistently improve visual quality for both\\robject- and scene-level generation. Moreover, we demonstrate that SteinDreamer\\rachieves faster convergence than existing methods due to more stable gradient\\rupdates.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00604 ,  32542kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00608\\rDate: Sun, 31 Dec 2023 23:32:03 GMT   (418kb,D)\\r\\rTitle: Bringing Back the Context: Camera Trap Species Identification as Link\\r  Prediction on Multimodal Knowledge Graphs\\rAuthors: Vardaan Pahuja, Weidi Luo, Yu Gu, Cheng-Hao Tu, Hong-You Chen, Tanya\\r  Berger-Wolf, Charles Stewart, Song Gao, Wei-Lun Chao, Yu Su\\rCategories: cs.CV cs.AI\\rComments: 13 pages, 4 figures\\r\\\\\\\\\\r  Camera traps are valuable tools in animal ecology for biodiversity monitoring\\rand conservation. However, challenges like poor generalization to deployment at\\rnew unseen locations limit their practical application. Images are naturally\\rassociated with heterogeneous forms of context possibly in different\\rmodalities. In this work, we leverage the structured context associated with\\rthe camera trap images to improve out-of-distribution generalization for the\\rtask of species identification in camera traps. For example, a photo of a wild\\ranimal may be associated with information about where and when it was taken, as\\rwell as structured biology knowledge about the animal species. While typically\\roverlooked by existing work, bringing back such context offers several\\rpotential benefits for better image understanding, such as addressing data\\rscarcity and enhancing generalization. However, effectively integrating such\\rheterogeneous context into the visual domain is a challenging problem. To\\raddress this, we propose a novel framework that reformulates species\\rclassification as link prediction in a multimodal knowledge graph (KG). This\\rframework seamlessly integrates various forms of multimodal context for visual\\rrecognition. We apply this framework for out-of-distribution species\\rclassification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets\\rand achieve competitive performance with state-of-the-art approaches.\\rFurthermore, our framework successfully incorporates biological taxonomy for\\rimproved generalization and enhances sample efficiency for recognizing\\runder-represented species.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00608 ,  418kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00616\\rDate: Mon, 1 Jan 2024 00:08:39 GMT   (32944kb,D)\\r\\rTitle: GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for\\r  One-shot Generalizable Neural Radiance Fields\\rAuthors: Xiao Pan, Zongxin Yang, Shuai Bai, Yi Yang\\rCategories: cs.CV\\rComments: Submitted to Journal\\r\\\\\\\\\\r  In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task\\rwhich targets synthesizing photo-realistic novel views given only one reference\\rimage per scene. Previous One-shot Generalizable Neural Radiance Fields\\r(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,\\ryet suffer the blurry issue due to the encoder-only architecture that highly\\rrelies on the limited reference image. On the other hand, recent\\rdiffusion-based image-to-3d methods show vivid plausible results via distilling\\rpre-trained 2D diffusion models into a 3D representation, yet require tedious\\rper-scene optimization. Targeting these issues, we propose the GD^2-NeRF, a\\rGenerative Detail compensation framework via GAN and Diffusion that is both\\rinference-time finetuning-free and with vivid plausible details. In detail,\\rfollowing a coarse-to-fine strategy, GD^2-NeRF is mainly composed of a\\rOne-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer\\r(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model\\rinto the existing OG-NeRF pipeline for primarily relieving the blurry issue\\rwith in-distribution priors captured from the training dataset, achieving a\\rgood balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at\\rthe fine stage, Diff3DE further leverages the pre-trained image diffusion\\rmodels to complement rich out-distribution details while maintaining decent 3D\\rconsistency. Extensive experiments on both the synthetic and real-world\\rdatasets show that GD$^2$-NeRF noticeably improves the details while without\\rper-scene finetuning.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00616 ,  32944kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00617\\rDate: Mon, 1 Jan 2024 00:10:58 GMT   (1379kb,D)\\r\\rTitle: Towards Improved Proxy-based Deep Metric Learning via Data-Augmented\\r  Domain Adaptation\\rAuthors: Li Ren, Chen Chen, Liqiang Wang, Kien Hua\\rCategories: cs.CV\\rComments: Accepted by AAAI 2024\\r\\\\\\\\\\r  Deep Metric Learning (DML) plays an important role in modern computer vision\\rresearch, where we learn a distance metric for a set of image representations.\\rRecent DML techniques utilize the proxy to interact with the corresponding\\rimage samples in the embedding space. However, existing proxy-based DML methods\\rfocus on learning individual proxy-to-sample distance while the overall\\rdistribution of samples and proxies lacks attention. In this paper, we present\\ra novel proxy-based DML framework that focuses on aligning the sample and proxy\\rdistributions to improve the efficiency of proxy-based DML losses.\\rSpecifically, we propose the Data-Augmented Domain Adaptation (DADA) method to\\radapt the domain gap between the group of samples and proxies. To the best of\\rour knowledge, we are the first to leverage domain adaptation to boost the\\rperformance of proxy-based DML. We show that our method can be easily plugged\\rinto existing proxy-based DML losses. Our experiments on benchmarks, including\\rthe popular CUB-200-2011, CARS196, Stanford Online Products, and In-Shop\\rClothes Retrieval, show that our learning algorithm significantly improves the\\rexisting proxy losses and achieves superior results compared to the existing\\rmethods.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00617 ,  1379kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00639\\rDate: Mon, 1 Jan 2024 02:35:13 GMT   (45369kb,D)\\r\\rTitle: Geometry Depth Consistency in RGBD Relative Pose Estimation\\rAuthors: Sourav Kumar, Chiang-Heng Chien, Benjamin Kimia\\rCategories: cs.CV\\r\\\\\\\\\\r  Relative pose estimation for RGBD cameras is crucial in a number of\\rapplications. Previous approaches either rely on the RGB aspect of the images\\rto estimate pose thus not fully making use of depth in the estimation process\\ror estimate pose from the 3D cloud of points that each image produces, thus not\\rmaking full use of RGB information. This paper shows that if one pair of\\rcorrespondences is hypothesized from the RGB-based ranked-ordered\\rcorrespondence list, then the space of remaining correspondences is restricted\\rto corresponding pairs of curves nested around the hypothesized correspondence,\\rimplicitly capturing depth consistency. This simple Geometric Depth Constraint\\r(GDC) significantly reduces potential matches. In effect this becomes a filter\\ron possible correspondences that helps reduce the number of outliers and thus\\rexpedites RANSAC significantly. As such, the same budget of time allows for\\rmore RANSAC iterations and therefore additional robustness and a significant\\rspeedup. In addition, the paper proposed a Nested RANSAC approach that also\\rspeeds up the process, as shown through experiments on TUM, ICL-NUIM, and RGBD\\rScenes v2 datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00639 ,  45369kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00652\\rDate: Mon, 1 Jan 2024 03:40:07 GMT   (18997kb,D)\\r\\rTitle: From Covert Hiding to Visual Editing: Robust Generative Video\\r  Steganography\\rAuthors: Xueying Mao, Xiaoxiao Hu, Wanli Peng, Zhenliang Gan, Qichao Ying,\\r  Zhenxing Qian, Sheng Li and Xinpeng Zhang\\rCategories: cs.CV\\rComments: Under Review\\r\\\\\\\\\\r  Traditional video steganography methods are based on modifying the covert\\rspace for embedding, whereas we propose an innovative approach that embeds\\rsecret message within semantic feature for steganography during the video\\rediting process. Although existing traditional video steganography methods\\rdisplay a certain level of security and embedding capacity, they lack adequate\\rrobustness against common distortions in online social networks (OSNs). In this\\rpaper, we introduce an end-to-end robust generative video steganography network\\r(RoGVS), which achieves visual editing by modifying semantic feature of videos\\rto embed secret message. We employ face-swapping scenario to showcase the\\rvisual editing effects. We first design a secret message embedding module to\\radaptively hide secret message into the semantic feature of videos. Extensive\\rexperiments display that the proposed RoGVS method applied to facial video\\rdatasets demonstrate its superiority over existing video and image\\rsteganography techniques in terms of both robustness and capacity.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00652 ,  18997kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00653\\rDate: Mon, 1 Jan 2024 03:45:07 GMT   (1161kb,D)\\r\\rTitle: PROMPT-IML: Image Manipulation Localization with Pre-trained Foundation\\r  Models Through Prompt Tuning\\rAuthors: Xuntao Liu, Yuzhou Yang, Qichao Ying, Zhenxing Qian, Xinpeng Zhang and\\r  Sheng Li\\rCategories: cs.CV\\rComments: Under Review\\r\\\\\\\\\\r  Deceptive images can be shared in seconds with social networking services,\\rposing substantial risks. Tampering traces, such as boundary artifacts and\\rhigh-frequency information, have been significantly emphasized by massive\\rnetworks in the Image Manipulation Localization (IML) field. However, they are\\rprone to image post-processing operations, which limit the generalization and\\rrobustness of existing methods. We present a novel Prompt-IML framework. We\\robserve that humans tend to discern the authenticity of an image based on both\\rsemantic and high-frequency information, inspired by which, the proposed\\rframework leverages rich semantic knowledge from pre-trained visual foundation\\rmodels to assist IML. We are the first to design a framework that utilizes\\rvisual foundation models specially for the IML task. Moreover, we design a\\rFeature Alignment and Fusion module to align and fuse features of semantic\\rfeatures with high-frequency features, which aims at locating tampered regions\\rfrom multiple perspectives. Experimental results demonstrate that our model can\\rachieve better performance on eight typical fake image datasets and outstanding\\rrobustness.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00653 ,  1161kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00663\\rDate: Mon, 1 Jan 2024 04:24:48 GMT   (1247kb,D)\\r\\rTitle: 1st Place Solution for 5th LSVOS Challenge: Referring Video Object\\r  Segmentation\\rAuthors: Zhuoyan Luo, Yicheng Xiao, Yong Liu, Yitong Wang, Yansong Tang, Xiu\\r  Li, Yujiu Yang\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  The recent transformer-based models have dominated the Referring Video Object\\rSegmentation (RVOS) task due to the superior performance. Most prior works\\radopt unified DETR framework to generate segmentation masks in\\rquery-to-instance manner. In this work, we integrate strengths of that leading\\rRVOS models to build up an effective paradigm. We first obtain binary mask\\rsequences from the RVOS models. To improve the consistency and quality of\\rmasks, we propose Two-Stage Multi-Model Fusion strategy. Each stage rationally\\rensembles RVOS models based on framework design as well as training strategy,\\rand leverages different video object segmentation (VOS) models to enhance mask\\rcoherence by object propagation mechanism. Our method achieves 75.7% J&F on\\rRef-Youtube-VOS validation set and 70% J&F on test set, which ranks 1st place\\ron 5th Large-scale Video Object Segmentation Challenge (ICCV 2023) track 3.\\rCode is available at https://github.com/RobertLuo1/iccv2023_RVOS_Challenge.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00663 ,  1247kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00695\\rDate: Mon, 1 Jan 2024 08:19:21 GMT   (765kb,D)\\r\\rTitle: Credible Teacher for Semi-Supervised Object Detection in Open Scene\\rAuthors: Jingyu Zhuang, Kuo Wang, Liang Lin, Guanbin Li\\rCategories: cs.CV\\rComments: Accpet by ICASSP 2024\\r\\\\\\\\\\r  Semi-Supervised Object Detection (SSOD) has achieved resounding success by\\rleveraging unlabeled data to improve detection performance. However, in Open\\rScene Semi-Supervised Object Detection (O-SSOD), unlabeled data may contains\\runknown objects not observed in the labeled data, which will increase\\runcertainty in the model's predictions for known objects. It is detrimental to\\rthe current methods that mainly rely on self-training, as more uncertainty\\rleads to the lower localization and classification precision of pseudo labels.\\rTo this end, we propose Credible Teacher, an end-to-end framework. Credible\\rTeacher adopts an interactive teaching mechanism using flexible labels to\\rprevent uncertain pseudo labels from misleading the model and gradually reduces\\rits uncertainty through the guidance of other credible pseudo labels. Empirical\\rresults have demonstrated our method effectively restrains the adverse effect\\rcaused by O-SSOD and significantly outperforms existing counterparts.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00695 ,  765kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00701\\rDate: Mon, 1 Jan 2024 08:54:18 GMT   (13961kb,D)\\r\\rTitle: Towards Efficient and Effective Text-to-Video Retrieval with\\r  Coarse-to-Fine Visual Representation Learning\\rAuthors: Kaibin Tian and Yanhua Cheng and Yi Liu and Xinglin Hou and Quan Chen\\r  and Han Li\\rCategories: cs.CV\\r\\\\\\\\\\r  In recent years, text-to-video retrieval methods based on CLIP have\\rexperienced rapid development. The primary direction of evolution is to exploit\\rthe much wider gamut of visual and textual cues to achieve alignment.\\rConcretely, those methods with impressive performance often design a heavy\\rfusion block for sentence (words)-video (frames) interaction, regardless of the\\rprohibitive computation complexity. Nevertheless, these approaches are not\\roptimal in terms of feature utilization and retrieval efficiency. To address\\rthis issue, we adopt multi-granularity visual feature learning, ensuring the\\rmodel's comprehensiveness in capturing visual content features spanning from\\rabstract to detailed levels during the training phase. To better leverage the\\rmulti-granularity features, we devise a two-stage retrieval architecture in the\\rretrieval phase. This solution ingeniously balances the coarse and fine\\rgranularity of retrieval content. Moreover, it also strikes a harmonious\\requilibrium between retrieval effectiveness and efficiency. Specifically, in\\rtraining phase, we design a parameter-free text-gated interaction block (TIB)\\rfor fine-grained video representation learning and embed an extra Pearson\\rConstraint to optimize cross-modal representation learning. In retrieval phase,\\rwe use coarse-grained video representations for fast recall of top-k\\rcandidates, which are then reranked by fine-grained video representations.\\rExtensive experiments on four benchmarks demonstrate the efficiency and\\reffectiveness. Notably, our method achieves comparable performance with the\\rcurrent state-of-the-art methods while being nearly 50 times faster.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00701 ,  13961kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00708\\rDate: Mon, 1 Jan 2024 09:25:03 GMT   (47456kb,D)\\r\\rTitle: Revisiting Nonlocal Self-Similarity from Continuous Representation\\rAuthors: Yisi Luo, Xile Zhao, Deyu Meng\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  Nonlocal self-similarity (NSS) is an important prior that has been\\rsuccessfully applied in multi-dimensional data processing tasks, e.g., image\\rand video recovery. However, existing NSS-based methods are solely suitable for\\rmeshgrid data such as images and videos, but are not suitable for emerging\\roff-meshgrid data, e.g., point cloud and climate data. In this work, we revisit\\rthe NSS from the continuous representation perspective and propose a novel\\rContinuous Representation-based NonLocal method (termed as CRNL), which has two\\rinnovative features as compared with classical nonlocal methods. First, based\\ron the continuous representation, our CRNL unifies the measure of\\rself-similarity for on-meshgrid and off-meshgrid data and thus is naturally\\rsuitable for both of them. Second, the nonlocal continuous groups can be more\\rcompactly and efficiently represented by the coupled low-rank function\\rfactorization, which simultaneously exploits the similarity within each group\\rand across different groups, while classical nonlocal methods neglect the\\rsimilarity across groups. This elaborately designed coupled mechanism allows\\rour method to enjoy favorable performance over conventional NSS methods in\\rterms of both effectiveness and efficiency. Extensive multi-dimensional data\\rprocessing experiments on-meshgrid (e.g., image inpainting and image denoising)\\rand off-meshgrid (e.g., climate data prediction and point cloud recovery)\\rvalidate the versatility, effectiveness, and efficiency of our CRNL as compared\\rwith state-of-the-art methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00708 ,  47456kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00711\\rDate: Mon, 1 Jan 2024 09:39:57 GMT   (18533kb,D)\\r\\rTitle: Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven\\r  Body Controllable Attribute\\rAuthors: Chaoqun Gong, Yuqin Dai, Ronghui Li, Achun Bao, Jun Li, Jian Yang,\\r  Yachao Zhang, Xiu Li\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Generating 3D human models directly from text helps reduce the cost and time\\rof character modeling. However, achieving multi-attribute controllable and\\rrealistic 3D human avatar generation is still challenging due to feature\\rcoupling and the scarcity of realistic 3D human avatar datasets. To address\\rthese issues, we propose Text2Avatar, which can generate realistic-style 3D\\ravatars based on the coupled text prompts. Text2Avatar leverages a discrete\\rcodebook as an intermediate feature to establish a connection between text and\\ravatars, enabling the disentanglement of features. Furthermore, to alleviate\\rthe scarcity of realistic style 3D human avatar data, we utilize a pre-trained\\runconditional 3D human avatar generation model to obtain a large amount of 3D\\ravatar pseudo data, which allows Text2Avatar to achieve realistic style\\rgeneration. Experimental results demonstrate that our method can generate\\rrealistic 3D avatars from coupled textual data, which is challenging for other\\rexisting methods in this field.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00711 ,  18533kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00719\\rDate: Mon, 1 Jan 2024 10:46:42 GMT   (1662kb,D)\\r\\rTitle: Depth Map Denoising Network and Lightweight Fusion Network for Enhanced\\r  3D Face Recognition\\rAuthors: Ruizhuo Xu, Ke Wang, Chao Deng, Mei Wang, Xi Chen, Wenhui Huang,\\r  Junlan Feng, Weihong Deng\\rCategories: cs.CV cs.AI\\rComments: Accepted by Pattern Recognition\\r\\\\\\\\\\r  With the increasing availability of consumer depth sensors, 3D face\\rrecognition (FR) has attracted more and more attention. However, the data\\racquired by these sensors are often coarse and noisy, making them impractical\\rto use directly. In this paper, we introduce an innovative Depth map denoising\\rnetwork (DMDNet) based on the Denoising Implicit Image Function (DIIF) to\\rreduce noise and enhance the quality of facial depth images for low-quality 3D\\rFR. After generating clean depth faces using DMDNet, we further design a\\rpowerful recognition network called Lightweight Depth and Normal Fusion network\\r(LDNFNet), which incorporates a multi-branch fusion block to learn unique and\\rcomplementary features between different modalities such as depth and normal\\rimages. Comprehensive experiments conducted on four distinct low-quality\\rdatabases demonstrate the effectiveness and robustness of our proposed methods.\\rFurthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art\\rresults on the Lock3DFace database.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00719 ,  1662kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00722\\rDate: Mon, 1 Jan 2024 10:49:09 GMT   (2068kb,D)\\r\\rTitle: BRAU-Net++: U-Shaped Hybrid CNN-Transformer Network for Medical Image\\r  Segmentation\\rAuthors: Libin Lan, Pengzhou Cai, Lu Jiang, Xiaojuan Liu, Yongmei Li, and\\r  Yudong Zhang\\rCategories: cs.CV\\rComments: 12 pages, 6 figures, 9 tables code:\\r  https://github.com/Caipengzhou/BRAU-Netplusplus\\r\\\\\\\\\\r  Accurate medical image segmentation is essential for clinical quantification,\\rdisease diagnosis, treatment planning and many other applications. Both\\rconvolution-based and transformer-based u-shaped architectures have made\\rsignificant success in various medical image segmentation tasks. The former can\\refficiently learn local information of images while requiring much more\\rimage-specific inductive biases inherent to convolution operation. The latter\\rcan effectively capture long-range dependency at different feature scales using\\rself-attention, whereas it typically encounters the challenges of quadratic\\rcompute and memory requirements with sequence length increasing. To address\\rthis problem, through integrating the merits of these two paradigms in a\\rwell-designed u-shaped architecture, we propose a hybrid yet effective\\rCNN-Transformer network, named BRAU-Net++, for an accurate medical image\\rsegmentation task. Specifically, BRAU-Net++ uses bi-level routing attention as\\rthe core building block to design our u-shaped encoder-decoder structure, in\\rwhich both encoder and decoder are hierarchically constructed, so as to learn\\rglobal semantic information while reducing computational complexity.\\rFurthermore, this network restructures skip connection by incorporating\\rchannel-spatial attention which adopts convolution operations, aiming to\\rminimize local spatial information loss and amplify global\\rdimension-interaction of multi-scale features. Extensive experiments on three\\rpublic benchmark datasets demonstrate that our proposed approach surpasses\\rother state-of-the-art methods including its baseline: BRAU-Net under almost\\rall evaluation metrics. We achieve the average Dice-Similarity Coefficient\\r(DSC) of 82.47, 90.10, and 92.94 on Synapse multi-organ segmentation, ISIC-2018\\rChallenge, and CVC-ClinicDB, as well as the mIoU of 84.01 and 88.17 on\\rISIC-2018 Challenge and CVC-ClinicDB, respectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00722 ,  2068kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00729\\rDate: Mon, 1 Jan 2024 11:54:51 GMT   (8847kb,D)\\r\\rTitle: NightRain: Nighttime Video Deraining via Adaptive-Rain-Removal and\\r  Adaptive-Correction\\rAuthors: Beibei Lin, Yeying Jin, Wending Yan, Wei Ye, Yuan Yuan, Shunli Zhang,\\r  Robby Tan\\rCategories: cs.CV\\rComments: Accepted by AAAI24\\r\\\\\\\\\\r  Existing deep-learning-based methods for nighttime video deraining rely on\\rsynthetic data due to the absence of real-world paired data. However, the\\rintricacies of the real world, particularly with the presence of light effects\\rand low-light regions affected by noise, create significant domain gaps,\\rhampering synthetic-trained models in removing rain streaks properly and\\rleading to over-saturation and color shifts. Motivated by this, we introduce\\rNightRain, a novel nighttime video deraining method with adaptive-rain-removal\\rand adaptive-correction. Our adaptive-rain-removal uses unlabeled rain videos\\rto enable our model to derain real-world rain videos, particularly in regions\\raffected by complex light effects. The idea is to allow our model to obtain\\rrain-free regions based on the confidence scores. Once rain-free regions and\\rthe corresponding regions from our input are obtained, we can have region-based\\rpaired real data. These paired data are used to train our model using a\\rteacher-student framework, allowing the model to iteratively learn from less\\rchallenging regions to more challenging regions. Our adaptive-correction aims\\rto rectify errors in our model's predictions, such as over-saturation and color\\rshifts. The idea is to learn from clear night input training videos based on\\rthe differences or distance between those input videos and their corresponding\\rpredictions. Our model learns from these differences, compelling our model to\\rcorrect the errors. From extensive experiments, our method demonstrates\\rstate-of-the-art performance. It achieves a PSNR of 26.73dB, surpassing\\rexisting nighttime video deraining methods by a substantial margin of 13.7%.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00729 ,  8847kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00736\\rDate: Mon, 1 Jan 2024 12:25:57 GMT   (13502kb,D)\\r\\rTitle: Diffusion Models, Image Super-Resolution And Everything: A Survey\\rAuthors: Brian B. Moser, Arundhati S. Shanbhag, Federico Raue, Stanislav\\r  Frolov, Sebastian Palacio and Andreas Dengel\\rCategories: cs.CV cs.AI cs.GL cs.LG cs.MM\\r\\\\\\\\\\r  Diffusion Models (DMs) represent a significant advancement in image\\rSuper-Resolution (SR), aligning technical image quality more closely with human\\rpreferences and expanding SR applications. DMs address critical limitations of\\rprevious methods, enhancing overall realism and details in SR images. However,\\rDMs suffer from color-shifting issues, and their high computational costs call\\rfor efficient sampling alternatives, underscoring the challenge of balancing\\rcomputational efficiency and image quality. This survey gives an overview of\\rDMs applied to image SR and offers a detailed analysis that underscores the\\runique characteristics and methodologies within this domain, distinct from\\rbroader existing reviews in the field. It presents a unified view of DM\\rfundamentals and explores research directions, including alternative input\\rdomains, conditioning strategies, guidance, corruption spaces, and zero-shot\\rmethods. This survey provides insights into the evolution of image SR with DMs,\\raddressing current trends, challenges, and future directions in this rapidly\\revolving field.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00736 ,  13502kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00739\\rDate: Mon, 1 Jan 2024 12:42:32 GMT   (39532kb,D)\\r\\rTitle: DiffMorph: Text-less Image Morphing with Diffusion Models\\rAuthors: Shounak Chatterjee\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Text-conditioned image generation models are a prevalent use of AI image\\rsynthesis, yet intuitively controlling output guided by an artist remains\\rchallenging. Current methods require multiple images and textual prompts for\\reach object to specify them as concepts to generate a single customized image.\\r  On the other hand, our work, \\\\verb|DiffMorph|, introduces a novel approach\\rthat synthesizes images that mix concepts without the use of textual prompts.\\rOur work integrates a sketch-to-image module to incorporate user sketches as\\rinput. \\\\verb|DiffMorph| takes an initial image with conditioning artist-drawn\\rsketches to generate a morphed image.\\r  We employ a pre-trained text-to-image diffusion model and fine-tune it to\\rreconstruct each image faithfully. We seamlessly merge images and concepts from\\rsketches into a cohesive composition. The image generation capability of our\\rwork is demonstrated through our results and a comparison of these with\\rprompt-based image generation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00739 ,  39532kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00766\\rDate: Mon, 1 Jan 2024 14:14:35 GMT   (45163kb,D)\\r\\rTitle: Bracketing is All You Need: Unifying Image Restoration and Enhancement\\r  Tasks with Multi-Exposure Images\\rAuthors: Zhilu Zhang, Shuohao Zhang, Renlong Wu, Zifei Yan, Wangmeng Zuo\\rCategories: cs.CV eess.IV\\rComments: 17 pages\\r\\\\\\\\\\r  It is challenging but highly desired to acquire high-quality photos with\\rclear content in low-light environments. Although multi-image processing\\rmethods (using burst, dual-exposure, or multi-exposure images) have made\\rsignificant progress in addressing this issue, they typically focus exclusively\\ron specific restoration or enhancement tasks, being insufficient in exploiting\\rmulti-image. Motivated by that multi-exposure images are complementary in\\rdenoising, deblurring, high dynamic range imaging, and super-resolution, we\\rpropose to utilize bracketing photography to unify restoration and enhancement\\rtasks in this work. Due to the difficulty in collecting real-world pairs, we\\rsuggest a solution that first pre-trains the model with synthetic paired data\\rand then adapts it to real-world unlabeled images. In particular, a temporally\\rmodulated recurrent network (TMRNet) and self-supervised adaptation method are\\rproposed. Moreover, we construct a data simulation pipeline to synthesize pairs\\rand collect real-world images from 200 nighttime scenarios. Experiments on both\\rdatasets show that our method performs favorably against the state-of-the-art\\rmulti-image processing ones. The dataset, code, and pre-trained models are\\ravailable at https://github.com/cszhilu1998/BracketIRE.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00766 ,  45163kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00789\\rDate: Mon, 1 Jan 2024 15:31:06 GMT   (4986kb,D)\\r\\rTitle: Retrieval-Augmented Egocentric Video Captioning\\rAuthors: Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng,\\r  Weidi Xie\\rCategories: cs.CV\\r\\\\\\\\\\r  Understanding human actions from videos of first-person view poses\\rsignificant challenges. Most prior approaches explore representation learning\\ron egocentric videos only, while overlooking the potential benefit of\\rexploiting existing large-scale third-person videos. In this paper, (1) we\\rdevelop EgoInstructor, a retrieval-augmented multimodal captioning model that\\rautomatically retrieves semantically relevant third-person instructional videos\\rto enhance the video captioning of egocentric videos. (2) For training the\\rcross-view retrieval module, we devise an automatic pipeline to discover\\rego-exo video pairs from distinct large-scale egocentric and exocentric\\rdatasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE\\rloss that pulls egocentric and exocentric video features closer by aligning\\rthem to shared text features that describe similar actions. (4) Through\\rextensive experiments, our cross-view retrieval module demonstrates superior\\rperformance across seven benchmarks. Regarding egocentric video captioning,\\rEgoInstructor exhibits significant improvements by leveraging third-person\\rvideos as references.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00789 ,  4986kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00816\\rDate: Mon, 1 Jan 2024 17:15:42 GMT   (2137kb,D)\\r\\rTitle: GLIMPSE: Generalized Local Imaging with MLPs\\rAuthors: AmirEhsan Khorashadizadeh, Valentin Debarnot, Tianlin Liu, and Ivan\\r  Dokmani\\\\'c\\rCategories: cs.CV cs.LG eess.IV\\rComments: 12 pages, 10 figures\\r\\\\\\\\\\r  Deep learning is the current de facto state of the art in tomographic\\rimaging. A common approach is to feed the result of a simple inversion, for\\rexample the backprojection, to a convolutional neural network (CNN) which then\\rcomputes the reconstruction. Despite strong results on 'in-distribution' test\\rdata similar to the training data, backprojection from sparse-view data\\rdelocalizes singularities, so these approaches require a large receptive field\\rto perform well. As a consequence, they overfit to certain global structures\\rwhich leads to poor generalization on out-of-distribution (OOD) samples.\\rMoreover, their memory complexity and training time scale unfavorably with\\rimage resolution, making them impractical for application at realistic clinical\\rresolutions, especially in 3D: a standard U-Net requires a substantial 140GB of\\rmemory and 2600 seconds per epoch on a research-grade GPU when training on\\r1024x1024 images. In this paper, we introduce GLIMPSE, a local processing\\rneural network for computed tomography which reconstructs a pixel value by\\rfeeding only the measurements associated with the neighborhood of the pixel to\\ra simple MLP. While achieving comparable or better performance with successful\\rCNNs like the U-Net on in-distribution test data, GLIMPSE significantly\\routperforms them on OOD samples while maintaining a memory footprint almost\\rindependent of image resolution; 5GB memory suffices to train on 1024x1024\\rimages. Further, we built GLIMPSE to be fully differentiable, which enables\\rfeats such as recovery of accurate projection angles if they are out of\\rcalibration.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00816 ,  2137kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00825\\rDate: Mon, 1 Jan 2024 17:48:38 GMT   (2562kb,D)\\r\\rTitle: Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using\\r  Sharpness Prior\\rAuthors: Byeonghyeon Lee, Howoong Lee, Usman Ali, Eunbyung Park\\rCategories: cs.CV cs.GR eess.IV\\rComments: Accepted to WACV 2024\\r\\\\\\\\\\r  Neural Radiance Fields (NeRF) have shown remarkable performance in neural\\rrendering-based novel view synthesis. However, NeRF suffers from severe visual\\rquality degradation when the input images have been captured under imperfect\\rconditions, such as poor illumination, defocus blurring, and lens aberrations.\\rEspecially, defocus blur is quite common in the images when they are normally\\rcaptured using cameras. Although few recent studies have proposed to render\\rsharp images of considerably high-quality, yet they still face many key\\rchallenges. In particular, those methods have employed a Multi-Layer Perceptron\\r(MLP) based NeRF, which requires tremendous computational time. To overcome\\rthese shortcomings, this paper proposes a novel technique Sharp-NeRF -- a\\rgrid-based NeRF that renders clean and sharp images from the input blurry\\rimages within half an hour of training. To do so, we used several grid-based\\rkernels to accurately model the sharpness/blurriness of the scene. The\\rsharpness level of the pixels is computed to learn the spatially varying blur\\rkernels. We have conducted experiments on the benchmarks consisting of blurry\\rimages and have evaluated full-reference and non-reference metrics. The\\rqualitative and quantitative results have revealed that our approach renders\\rthe sharp novel views with vivid colors and fine details, and it has\\rconsiderably faster training time than the previous works. Our project page is\\ravailable at https://benhenryl.github.io/SharpNeRF/\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00825 ,  2562kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00833\\rDate: Mon, 1 Jan 2024 18:23:39 GMT   (1985kb,D)\\r\\rTitle: Rethinking RAFT for Efficient Optical Flow\\rAuthors: Navid Eslami, Farnoosh Arefi, Amir M. Mansourian, Shohreh Kasaei\\rCategories: cs.CV\\rComments: 7 pages, 5 figures, 4 tables\\rMSC-class: ACM-class: F.2.2, I.2.7\\r\\\\\\\\\\r  Despite significant progress in deep learning-based optical flow methods,\\raccurately estimating large displacements and repetitive patterns remains a\\rchallenge. The limitations of local features and similarity search patterns\\rused in these algorithms contribute to this issue. Additionally, some existing\\rmethods suffer from slow runtime and excessive graphic memory consumption. To\\raddress these problems, this paper proposes a novel approach based on the RAFT\\rframework. The proposed Attention-based Feature Localization (AFL) approach\\rincorporates the attention mechanism to handle global feature extraction and\\raddress repetitive patterns. It introduces an operator for matching pixels with\\rcorresponding counterparts in the second frame and assigning accurate flow\\rvalues. Furthermore, an Amorphous Lookup Operator (ALO) is proposed to enhance\\rconvergence speed and improve RAFTs ability to handle large displacements by\\rreducing data redundancy in its search operator and expanding the search space\\rfor similarity extraction. The proposed method, Efficient RAFT\\r(Ef-RAFT),achieves significant improvements of 10% on the Sintel dataset and 5%\\ron the KITTI dataset over RAFT. Remarkably, these enhancements are attained\\rwith a modest 33% reduction in speed and a mere 13% increase in memory usage.\\rThe code is available at: https://github.com/n3slami/Ef-RAFT\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00833 ,  1985kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00834\\rDate: Mon, 1 Jan 2024 18:23:51 GMT   (14582kb,D)\\r\\rTitle: Deblurring 3D Gaussian Splatting\\rAuthors: Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park\\rCategories: cs.CV\\rComments: 19 pages, 8 figures\\r\\\\\\\\\\r  Recent studies in Radiance Fields have paved the robust way for novel view\\rsynthesis with their photorealistic rendering quality. Nevertheless, they\\rusually employ neural networks and volumetric rendering, which are costly to\\rtrain and impede their broad use in various real-time applications due to the\\rlengthy rendering time. Lately 3D Gaussians splatting-based approach has been\\rproposed to model the 3D scene, and it achieves remarkable visual quality while\\rrendering the images in real-time. However, it suffers from severe degradation\\rin the rendering quality if the training images are blurry. Blurriness commonly\\roccurs due to the lens defocusing, object motion, and camera shake, and it\\rinevitably intervenes in clean image acquisition. Several previous studies have\\rattempted to render clean and sharp images from blurry input images using\\rneural fields. The majority of those works, however, are designed only for\\rvolumetric rendering-based neural radiance fields and are not straightforwardly\\rapplicable to rasterization-based 3D Gaussian splatting methods. Thus, we\\rpropose a novel real-time deblurring framework, deblurring 3D Gaussian\\rSplatting, using a small Multi-Layer Perceptron (MLP) that manipulates the\\rcovariance of each 3D Gaussian to model the scene blurriness. While deblurring\\r3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct\\rfine and sharp details from blurry images. A variety of experiments have been\\rconducted on the benchmark, and the results have revealed the effectiveness of\\rour approach for deblurring. Qualitative results are available at\\rhttps://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00834 ,  14582kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00847\\rDate: Mon, 1 Jan 2024 18:56:54 GMT   (10362kb,D)\\r\\rTitle: Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches\\r  and a Head-Mounted Camera\\rAuthors: Jiye Lee, Hanbyul Joo\\rCategories: cs.CV cs.GR\\rComments: Project page: https://jiyewise.github.io/projects/MocapEvery/\\r\\\\\\\\\\r  We present a lightweight and affordable motion capture method based on two\\rsmartwatches and a head-mounted camera. In contrast to the existing approaches\\rthat use six or more expert-level IMU devices, our approach is much more\\rcost-effective and convenient. Our method can make wearable motion capture\\raccessible to everyone everywhere, enabling 3D full-body motion capture in\\rdiverse environments. As a key idea to overcome the extreme sparsity and\\rambiguities of sensor inputs, we integrate 6D head poses obtained from the\\rhead-mounted cameras for motion estimation. To enable capture in expansive\\rindoor and outdoor scenes, we propose an algorithm to track and update floor\\rlevel changes to define head poses, coupled with a multi-stage\\rTransformer-based regression module. We also introduce novel strategies\\rleveraging visual cues of egocentric images to further enhance the motion\\rcapture quality while reducing ambiguities. We demonstrate the performance of\\rour method on various challenging scenarios, including complex outdoor\\renvironments and everyday motions including object interactions and social\\rinteractions among multiple individuals.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00847 ,  10362kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00849\\rDate: Mon, 1 Jan 2024 18:58:42 GMT   (7277kb,D)\\r\\rTitle: COSMO: COntrastive Streamlined MultimOdal Model with Interleaved\\r  Pre-Training\\rAuthors: Alex Jinpeng Wang, Linjie Li, Kevin Qinghong Lin, Jianfeng Wang, Kevin\\r  Lin, Zhengyuan Yang, Lijuan Wang, Mike Zheng Shou\\rCategories: cs.CV\\rComments: 16 pages; Website: http://fingerrec.github.io/cosmo\\r\\\\\\\\\\r  In the evolution of Vision-Language Pre-training, shifting from short-text\\rcomprehension to encompassing extended textual contexts is pivotal. Recent\\rautoregressive vision-language models like \\\\cite{flamingo, palme}, leveraging\\rthe long-context capability of Large Language Models, have excelled in few-shot\\rtext generation tasks but face challenges in alignment tasks. Addressing this\\rgap, we introduce the contrastive loss into text generation models, presenting\\rthe COntrastive-Streamlined MultimOdal framework (\\\\ModelName), strategically\\rpartitioning the language model into dedicated unimodal text processing and\\radept multimodal data handling components. \\\\ModelName, our unified framework,\\rmerges unimodal and multimodal elements, enhancing model performance for tasks\\rinvolving textual and visual data while notably reducing learnable parameters.\\rHowever, these models demand extensive long-text datasets, yet the availability\\rof high-quality long-text video datasets remains limited. To bridge this gap,\\rthis work introduces \\\\VideoDatasetName, an inaugural interleaved video-text\\rdataset featuring comprehensive captions, marking a significant step forward.\\rDemonstrating its impact, we illustrate how \\\\VideoDatasetName{} enhances model\\rperformance in image-text tasks. With 34% learnable parameters and utilizing\\r72\\\\% of the available data, our model demonstrates significant superiority over\\rOpenFlamingo~\\\\cite{openflamingo}. For instance, in the 4-shot flickr captioning\\rtask, performance notably improves from 57.2% to 65.\\\\%. The contributions of\\r\\\\ModelName{} and \\\\VideoDatasetName{} are underscored by notable performance\\rgains across 14 diverse downstream datasets encompassing both image-text and\\rvideo-text tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00849 ,  7277kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00850\\rDate: Mon, 1 Jan 2024 18:59:33 GMT   (40650kb,D)\\r\\rTitle: Refining Pre-Trained Motion Models\\rAuthors: Xinglong Sun, Adam W. Harley, and Leonidas J. Guibas\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Given the difficulty of manually annotating motion in video, the current best\\rmotion estimation methods are trained with synthetic data, and therefore\\rstruggle somewhat due to a train/test gap. Self-supervised methods hold the\\rpromise of training directly on real video, but typically perform worse. These\\rinclude methods trained with warp error (i.e., color constancy) combined with\\rsmoothness terms, and methods that encourage cycle-consistency in the estimates\\r(i.e., tracking backwards should yield the opposite trajectory as tracking\\rforwards). In this work, we take on the challenge of improving state-of-the-art\\rsupervised models with self-supervised training. We find that when the\\rinitialization is supervised weights, most existing self-supervision techniques\\ractually make performance worse instead of better, which suggests that the\\rbenefit of seeing the new data is overshadowed by the noise in the training\\rsignal. Focusing on obtaining a ``clean'' training signal from real-world\\runlabelled video, we propose to separate label-making and training into two\\rdistinct stages. In the first stage, we use the pre-trained model to estimate\\rmotion in a video, and then select the subset of motion estimates which we can\\rverify with cycle-consistency. This produces a sparse but accurate\\rpseudo-labelling of the video. In the second stage, we fine-tune the model to\\rreproduce these outputs, while also applying augmentations on the input. We\\rcomplement this boot-strapping method with simple techniques that densify and\\rre-balance the pseudo-labels, ensuring that we do not merely train on ``easy''\\rtracks. We show that our method yields reliable gains over fully-supervised\\rmethods in real videos, for both short-term (flow-based) and long-range\\r(multi-frame) pixel tracking.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00850 ,  40650kb)\\r%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00052 (*cross-listing*)\\rDate: Fri, 29 Dec 2023 19:11:55 GMT   (1662kb,D)\\r\\rTitle: ChatEd: A Chatbot Leveraging ChatGPT for an Enhanced Learning Experience\\r  in Higher Education\\rAuthors: Kevin Wang, Jason Ramos, Ramon Lawrence\\rCategories: cs.CY cs.CL\\rComments: To appear at INTED2024 - 18th annual International Technology,\\r  Education and Development Conference\\r\\\\\\\\\\r  With the rapid evolution of Natural Language Processing (NLP), Large Language\\rModels (LLMs) like ChatGPT have emerged as powerful tools capable of\\rtransforming various sectors. Their vast knowledge base and dynamic interaction\\rcapabilities represent significant potential in improving education by\\roperating as a personalized assistant. However, the possibility of generating\\rincorrect, biased, or unhelpful answers are a key challenge to resolve when\\rdeploying LLMs in an education context. This work introduces an innovative\\rarchitecture that combines the strengths of ChatGPT with a traditional\\rinformation retrieval based chatbot framework to offer enhanced student support\\rin higher education. Our empirical evaluations underscore the high promise of\\rthis approach.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00052 ,  1662kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00139 (*cross-listing*)\\rDate: Sat, 30 Dec 2023 04:51:46 GMT   (12224kb,D)\\r\\rTitle: Is Knowledge All Large Language Models Needed for Causal Reasoning?\\rAuthors: Hengrui Cai, Shengjie Liu, Rui Song\\rCategories: cs.AI cs.CL cs.LG stat.ME\\rComments: A Python implementation of our proposed method is available at\\r  https://github.com/ncsulsj/Causal_LLM\\r\\\\\\\\\\r  This paper explores the causal reasoning of large language models (LLMs) to\\renhance their interpretability and reliability in advancing artificial\\rintelligence. Despite the proficiency of LLMs in a range of tasks, their\\rpotential for understanding causality requires further exploration. We propose\\ra novel causal attribution model that utilizes do-operators for constructing\\rcounterfactual scenarios, allowing us to systematically quantify the influence\\rof input numerical data and LLMs' pre-existing knowledge on their causal\\rreasoning processes. Our newly developed experimental setup assesses LLMs'\\rreliance on contextual information and inherent knowledge across various\\rdomains. Our evaluation reveals that LLMs' causal reasoning ability depends on\\rthe context and domain-specific knowledge provided, and supports the argument\\rthat knowledge is, indeed, what LLMs principally require for sound causal\\rreasoning. On the contrary, in the absence of knowledge, LLMs still maintain a\\rdegree of causal reasoning using the available numerical data, albeit with\\rlimitations in the calculations.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00139 ,  12224kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00273 (*cross-listing*)\\rDate: Sat, 30 Dec 2023 16:15:02 GMT   (87kb)\\r\\rTitle: Investigating Zero-Shot Generalizability on Mandarin-English\\r  Code-Switched ASR and Speech-to-text Translation of Recent Foundation Models\\r  with Self-Supervision and Weak Supervision\\rAuthors: Chih-Kai Yang, Kuan-Po Huang, Ke-Han Lu, Chun-Yi Kuan, Chi-Yuan Hsiao,\\r  Hung-yi Lee\\rCategories: eess.AS cs.CL\\rComments: Submitted to ICASSP 2024 Self-supervision in Audio, Speech and Beyond\\r  workshop\\r\\\\\\\\\\r  This work evaluated several cutting-edge large-scale foundation models based\\ron self-supervision or weak supervision, including SeamlessM4T, SeamlessM4T v2,\\rand Whisper-large-v3, on three code-switched corpora. We found that\\rself-supervised models can achieve performances close to the supervised model,\\rindicating the effectiveness of multilingual self-supervised pre-training. We\\ralso observed that these models still have room for improvement as they kept\\rmaking similar mistakes and had unsatisfactory performances on modeling\\rintra-sentential code-switching. In addition, the validity of several variants\\rof Whisper was explored, and we concluded that they remained effective in a\\rcode-switching scenario, and similar techniques for self-supervised models are\\rworth studying to boost the performance of code-switched tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00273 ,  87kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00448 (*cross-listing*)\\rDate: Sun, 31 Dec 2023 10:53:58 GMT   (2015kb,D)\\r\\rTitle: Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\\r  Scaling Laws\\rAuthors: Nikhil Sardana and Jonathan Frankle\\rCategories: cs.LG cs.CL\\rComments: 8 pages, 2 figures, To appear in the 3rd NeurIPS Workshop on\\r  Efficient Natural Language and Speech Processing (ENLSP), 2023\\r\\\\\\\\\\r  Large language model (LLM) scaling laws are empirical formulas that estimate\\rchanges in model quality as a result of increasing parameter count and training\\rdata. However, these formulas, including the popular DeepMind Chinchilla\\rscaling laws, neglect to include the cost of inference. We modify the\\rChinchilla scaling laws to calculate the optimal LLM parameter count and\\rpre-training data size to train and deploy a model of a given quality and\\rinference demand. We conduct our analysis both in terms of a compute budget and\\rreal-world costs and find that LLM researchers expecting reasonably large\\rinference demand (~1B requests) should train models smaller and longer than\\rChinchilla-optimal.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00448 ,  2015kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00676 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 06:04:52 GMT   (1342kb,D)\\r\\rTitle: Digger: Detecting Copyright Content Mis-usage in Large Language Model\\r  Training\\rAuthors: Haodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei\\r  Zhang, Yang Liu, Guoai Xu, Guosheng Xu, Haoyu Wang\\rCategories: cs.CR cs.CL cs.LG\\r\\\\\\\\\\r  Pre-training, which utilizes extensive and varied datasets, is a critical\\rfactor in the success of Large Language Models (LLMs) across numerous\\rapplications. However, the detailed makeup of these datasets is often not\\rdisclosed, leading to concerns about data security and potential misuse. This\\ris particularly relevant when copyrighted material, still under legal\\rprotection, is used inappropriately, either intentionally or unintentionally,\\rinfringing on the rights of the authors.\\r  In this paper, we introduce a detailed framework designed to detect and\\rassess the presence of content from potentially copyrighted books within the\\rtraining datasets of LLMs. This framework also provides a confidence estimation\\rfor the likelihood of each content sample's inclusion. To validate our\\rapproach, we conduct a series of simulated experiments, the results of which\\raffirm the framework's effectiveness in identifying and addressing instances of\\rcontent misuse in LLM training processes. Furthermore, we investigate the\\rpresence of recognizable quotes from famous literary works within these\\rdatasets. The outcomes of our study have significant implications for ensuring\\rthe ethical use of copyrighted materials in the development of LLMs,\\rhighlighting the need for more transparent and responsible data management\\rpractices in this field.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00676 ,  1342kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00757 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 13:53:53 GMT   (2328kb,D)\\r\\rTitle: A & B == B & A: Triggering Logical Reasoning Failures in Large Language\\r  Models\\rAuthors: Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang,\\r  Pinjia He, Wenxiang Jiao, Michael R. Lyu\\rCategories: cs.SE cs.AI cs.CL cs.LO\\r\\\\\\\\\\r  Recent advancements in large language models (LLMs) have propelled Artificial\\rIntelligence (AI) to new heights, enabling breakthroughs in various tasks such\\ras writing assistance, code generation, and machine translation. A significant\\rdistinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to\\rreason. However, evaluating the reasoning ability of LLMs remains a challenge\\ras most existing evaluations focus on their accuracy on the downstream tasks\\rrather than directly assessing their reasoning processes. Efforts have been\\rmade to develop benchmarks and metrics to assess reasoning in LLMs, but they\\rsuffer from data leakage or limited scope. In this paper, we introduce\\rLogicAsker, an automatic approach that comprehensively evaluates and improves\\rthe logical reasoning abilities of LLMs under a set of atomic reasoning skills\\rbased on propositional and predicate logic. The results provide insights into\\rLLMs' reasoning abilities and reveal the logical rules the LLMs did not learn\\rwell. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,\\rChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases\\rfrom LogicAsker can find logical reasoning failures in different LLMs with a\\rrate of 25\\\\% - 94\\\\%. In addition, the test cases of LogicAsker can be further\\rused to design demonstration examples for in-context learning, which\\reffectively improves the logical reasoning ability of LLMs, e.g., 10\\\\% for\\rGPT-4. As far as we know, our work is the first to create prompts based on\\rtesting results to improve LLMs' formal reasoning ability effectively. All the\\rcode, data, and results will be released for reproduction and future research.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00757 ,  2328kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00761 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 14:02:27 GMT   (4165kb,D)\\r\\rTitle: The Earth is Flat? Unveiling Factual Errors in Large Language Models\\rAuthors: Wenxuan Wang, Juluan Shi, Zhaopeng Tu, Youliang Yuan, Jen-tse Huang,\\r  Wenxiang Jiao, Michael R. Lyu\\rCategories: cs.SE cs.AI cs.CL\\r\\\\\\\\\\r  Large Language Models (LLMs) like ChatGPT are foundational in various\\rapplications due to their extensive knowledge from pre-training and\\rfine-tuning. Despite this, they are prone to generating factual and commonsense\\rerrors, raising concerns in critical areas like healthcare, journalism, and\\reducation to mislead users. Current methods for evaluating LLMs' veracity are\\rlimited by test data leakage or the need for extensive human labor, hindering\\refficient and accurate error detection. To tackle this problem, we introduce a\\rnovel, automatic testing framework, FactChecker, aimed at uncovering factual\\rinaccuracies in LLMs. This framework involves three main steps: First, it\\rconstructs a factual knowledge graph by retrieving fact triplets from a\\rlarge-scale knowledge database. Then, leveraging the knowledge graph,\\rFactChecker employs a rule-based approach to generates three types of questions\\r(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and\\rmulti-hop relations, along with correct answers. Lastly, it assesses the LLMs'\\rresponses for accuracy using tailored matching strategies for each question\\rtype. Our extensive tests on six prominent LLMs, including text-davinci-002,\\rtext-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal\\rthat FactChecker can trigger factual errors in up to 45\\\\% of questions in these\\rmodels. Moreover, we demonstrate that FactChecker's test cases can improve\\rLLMs' factual accuracy through in-context learning and fine-tuning (e.g.,\\rllama-2-13b-chat's accuracy increase from 35.3\\\\% to 68.5\\\\%). We are making all\\rcode, data, and results available for future research endeavors.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00761 ,  4165kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00763 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 14:06:55 GMT   (3406kb,D)\\r\\rTitle: New Job, New Gender? Measuring the Social Bias in Image Generation\\r  Models\\rAuthors: Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan,\\r  Haoyi Qiu, Nanyun Peng, Michael R. Lyu\\rCategories: cs.SE cs.AI cs.CL cs.CV cs.MM\\r\\\\\\\\\\r  Image generation models can generate or edit images from a given text. Recent\\radvancements in image generation technology, exemplified by DALL-E and\\rMidjourney, have been groundbreaking. These advanced models, despite their\\rimpressive capabilities, are often trained on massive Internet datasets, making\\rthem susceptible to generating content that perpetuates social stereotypes and\\rbiases, which can lead to severe consequences. Prior research on assessing bias\\rwithin image generation models suffers from several shortcomings, including\\rlimited accuracy, reliance on extensive human labor, and lack of comprehensive\\ranalysis. In this paper, we propose BiasPainter, a novel metamorphic testing\\rframework that can accurately, automatically and comprehensively trigger social\\rbias in image generation models. BiasPainter uses a diverse range of seed\\rimages of individuals and prompts the image generation models to edit these\\rimages using gender, race, and age-neutral queries. These queries span 62\\rprofessions, 39 activities, 57 types of objects, and 70 personality traits. The\\rframework then compares the edited images to the original seed images, focusing\\ron any changes related to gender, race, and age. BiasPainter adopts a testing\\roracle that these characteristics should not be modified when subjected to\\rneutral prompts. Built upon this design, BiasPainter can trigger the social\\rbias and evaluate the fairness of image generation models. To evaluate the\\reffectiveness of BiasPainter, we use BiasPainter to test five widely-used\\rcommercial image generation software and models, such as stable diffusion and\\rMidjourney. Experimental results show that 100\\\\% of the generated test cases\\rcan successfully trigger social bias in image generation models.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00763 ,  3406kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00793 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 15:40:35 GMT   (252kb,D)\\r\\rTitle: SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\\r  Large Language Models\\rAuthors: Jinglong Luo, Yehong Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu,\\r  Zenglin Xu\\rCategories: cs.LG cs.CL cs.CR\\rComments: 12pages, 15figures\\r\\\\\\\\\\r  With the growing use of large language models hosted on cloud platforms to\\roffer inference services, privacy concerns are escalating, especially\\rconcerning sensitive data like investment plans and bank account details.\\rSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\\rthe privacy of inference data and model parameters. However, the application of\\rSMPC in Privacy-Preserving Inference (PPI) for large language models,\\rparticularly those based on the Transformer architecture, often leads to\\rconsiderable slowdowns or declines in performance. This is largely due to the\\rmultitude of nonlinear operations in the Transformer architecture, which are\\rnot well-suited to SMPC and are difficult to circumvent or optimize\\reffectively. To address this concern, we introduce an advanced optimization\\rframework called SecFormer, designed to strike an optimal balance between\\rperformance and efficiency in PPI for Transformer models. By implementing\\rknowledge distillation techniques, we successfully eliminate the high-cost\\rexponential and maximum operations in PPI without sacrificing model\\rperformance. Additionally, we have developed a suite of efficient SMPC\\rprotocols that utilize segmented polynomials and Goldschmidt's method to handle\\rother complex nonlinear functions within PPI, such as GeLU, LayerNorm, and\\rSoftmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer\\rin performance, showing improvements of $5.6\\\\%$ and $24.2\\\\%$ for\\rBERT$_{\\\\text{BASE}}$ and BERT$_{\\\\text{LARGE}}$, respectively. In terms of\\refficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its\\reffectiveness and speed.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00793 ,  252kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00824 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 17:48:25 GMT   (5443kb,D)\\r\\rTitle: Graph-Convolutional Autoencoder Ensembles for the Humanities,\\r  Illustrated with a Study of the American Slave Trade\\rAuthors: Tom Lippincott\\rCategories: cs.LG cs.CL\\rComments: More in-depth technical companion to A general neural ensemble\\r  technique to support traditional scholarship, Digital Humanities 2020\\r\\\\\\\\\\r  We introduce a graph-aware autoencoder ensemble framework, with associated\\rformalisms and tooling, designed to facilitate deep learning for scholarship in\\rthe humanities. By composing sub-architectures to produce a model isomorphic to\\ra humanistic domain we maintain interpretability while providing function\\rsignatures for each sub-architectural choice, allowing both traditional and\\rcomputational researchers to collaborate without disrupting established\\rpractices. We illustrate a practical application of our approach to a\\rhistorical study of the American post-Atlantic slave trade, and make several\\rspecific technical contributions: a novel hybrid graph-convolutional\\rautoencoder mechanism, batching policies for common graph topologies, and\\rmasking techniques for particular use-cases. The effectiveness of the framework\\rfor broadening participation of diverse domains is demonstrated by a growing\\rsuite of two dozen studies, both collaborations with humanists and established\\rtasks from machine learning literature, spanning a variety of fields and data\\rmodalities. We make performance comparisons of several different architectural\\rchoices and conclude with an ambitious list of imminent next steps for this\\rresearch.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00824 ,  5443kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00014 (*cross-listing*)\\rDate: Fri, 22 Dec 2023 16:33:03 GMT   (195kb,D)\\r\\rTitle: Resource-Limited Automated Ki67 Index Estimation in Breast Cancer\\rAuthors: J. Gliozzo, G. Marin\\\\`o, A. Bonometti, M. Frasca and D. Malchiodi\\rCategories: q-bio.QM cs.CV cs.LG eess.IV\\r\\\\\\\\\\r  The prediction of tumor progression and chemotherapy response has been\\rrecently tackled exploiting Tumor Infiltrating Lymphocytes (TILs) and the\\rnuclear protein Ki67 as prognostic factors. Recently, deep neural networks\\r(DNNs) have been shown to achieve top results in estimating Ki67 expression and\\rsimultaneous determination of intratumoral TILs score in breast cancer cells.\\rHowever, in the last ten years the extraordinary progress induced by deep\\rmodels proliferated at least as much as their resource demand. The exorbitant\\rcomputational costs required to query (and in some cases also to store) a deep\\rmodel represent a strong limitation in resource-limited contexts, like that of\\rIoT-based applications to support healthcare personnel. To this end, we propose\\ra resource consumption-aware DNN for the effective estimate of the percentage\\rof Ki67-positive cells in breast cancer screenings. Our approach reduced up to\\r75% and 89% the usage of memory and disk space respectively, up to 1.5x the\\renergy consumption, and preserved or improved the overall accuracy of a\\rbenchmark state-of-the-art solution. Encouraged by such positive results, we\\rdeveloped and structured the adopted framework so as to allow its general\\rpurpose usage, along with a public software repository to support its usage.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00014 ,  195kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00025 (*cross-listing*)\\rDate: Thu, 28 Dec 2023 23:34:43 GMT   (1628kb,D)\\r\\rTitle: Any-point Trajectory Modeling for Policy Learning\\rAuthors: Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, Pieter\\r  Abbeel\\rCategories: cs.RO cs.CV\\rComments: 16 pages, 10 figures\\r\\\\\\\\\\r  Learning from demonstration is a powerful method for teaching robots new\\rskills, and more demonstration data often improves policy learning. However,\\rthe high cost of collecting demonstration data is a significant bottleneck.\\rVideos, as a rich data source, contain knowledge of behaviors, physics, and\\rsemantics, but extracting control-specific information from them is challenging\\rdue to the lack of action labels. In this work, we introduce a novel framework,\\rAny-point Trajectory Modeling (ATM), that utilizes video demonstrations by\\rpre-training a trajectory model to predict future trajectories of arbitrary\\rpoints within a video frame. Once trained, these trajectories provide detailed\\rcontrol guidance, enabling the learning of robust visuomotor policies with\\rminimal action-labeled data. Our method's effectiveness is demonstrated across\\r130 simulation tasks, focusing on language-conditioned manipulation tasks.\\rVisualizations and code are available at:\\r\\\\url{https://xingyu-lin.github.io/atm}.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00025 ,  1628kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00057 (*cross-listing*)\\rDate: Fri, 29 Dec 2023 19:25:34 GMT   (1744kb,D)\\r\\rTitle: Generalization properties of contrastive world models\\rAuthors: Kandan Ramakrishnan, R. James Cotton, Xaq Pitkow, Andreas S. Tolias\\rCategories: cs.LG cs.CV\\rComments: Accepted at the NeurIPS 2023 Workshop: Self-Supervised Learning -\\r  Theory and Practice\\r\\\\\\\\\\r  Recent work on object-centric world models aim to factorize representations\\rin terms of objects in a completely unsupervised or self-supervised manner.\\rSuch world models are hypothesized to be a key component to address the\\rgeneralization problem. While self-supervision has shown improved performance\\rhowever, OOD generalization has not been systematically and explicitly tested.\\rIn this paper, we conduct an extensive study on the generalization properties\\rof contrastive world model. We systematically test the model under a number of\\rdifferent OOD generalization scenarios such as extrapolation to new object\\rattributes, introducing new conjunctions or new attributes. Our experiments\\rshow that the contrastive world model fails to generalize under the different\\rOOD tests and the drop in performance depends on the extent to which the\\rsamples are OOD. When visualizing the transition updates and convolutional\\rfeature maps, we observe that any changes in object attributes (such as\\rpreviously unseen colors, shapes, or conjunctions of color and shape) breaks\\rdown the factorization of object representations. Overall, our work highlights\\rthe importance of object-centric representations for generalization and current\\rmodels are limited in their capacity to learn such representations required for\\rhuman-level generalization.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00057 ,  1744kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00065 (*cross-listing*)\\rDate: Fri, 29 Dec 2023 19:46:18 GMT   (8278kb)\\r\\rTitle: Accelerating Process Development for 3D Printing of New Metal Alloys\\rAuthors: David Guirguis, Conrad Tucker, Jack Beuth\\rCategories: cond-mat.mtrl-sci cs.CV cs.LG\\rACM-class: J.2\\r\\\\\\\\\\r  Addressing the uncertainty and variability in the quality of 3D printed\\rmetals can further the wide spread use of this technology. Process mapping for\\rnew alloys is crucial for determining optimal process parameters that\\rconsistently produce acceptable printing quality. Process mapping is typically\\rperformed by conventional methods and is used for the design of experiments and\\rex situ characterization of printed parts. On the other hand, in situ\\rapproaches are limited because their observable features are limited and they\\rrequire complex high-cost setups to obtain temperature measurements to boost\\raccuracy. Our method relaxes these limitations by incorporating the temporal\\rfeatures of molten metal dynamics during laser-metal interactions using video\\rvision transformers and high-speed imaging. Our approach can be used in\\rexisting commercial machines and can provide in situ process maps for efficient\\rdefect and variability quantification. The generalizability of the approach is\\rdemonstrated by performing cross-dataset evaluations on alloys with different\\rcompositions and intrinsic thermofluid properties.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00065 ,  8278kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00125 (*cross-listing*)\\rDate: Sat, 30 Dec 2023 02:53:45 GMT   (6482kb,D)\\r\\rTitle: LLM-Assist: Enhancing Closed-Loop Planning with Language-Based Reasoning\\rAuthors: S P Sharan, Francesco Pittaluga, Vijay Kumar B G, Manmohan Chandraker\\rCategories: cs.AI cs.CV\\rComments: 15 pages, 8 figures, 7 tables\\r\\\\\\\\\\r  Although planning is a crucial component of the autonomous driving stack,\\rresearchers have yet to develop robust planning algorithms that are capable of\\rsafely handling the diverse range of possible driving scenarios. Learning-based\\rplanners suffer from overfitting and poor long-tail performance. On the other\\rhand, rule-based planners generalize well, but might fail to handle scenarios\\rthat require complex driving maneuvers. To address these limitations, we\\rinvestigate the possibility of leveraging the common-sense reasoning\\rcapabilities of Large Language Models (LLMs) such as GPT4 and Llama2 to\\rgenerate plans for self-driving vehicles. In particular, we develop a novel\\rhybrid planner that leverages a conventional rule-based planner in conjunction\\rwith an LLM-based planner. Guided by commonsense reasoning abilities of LLMs,\\rour approach navigates complex scenarios which existing planners struggle with,\\rproduces well-reasoned outputs while also remaining grounded through working\\ralongside the rule-based approach. Through extensive evaluation on the nuPlan\\rbenchmark, we achieve state-of-the-art performance, outperforming all existing\\rpure learning- and rule-based methods across most metrics. Our code will be\\ravailable at https://llmassist.github.io.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00125 ,  6482kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00128 (*cross-listing*)\\rDate: Sat, 30 Dec 2023 03:28:51 GMT   (1545kb)\\r\\rTitle: Quantifying intra-tumoral genetic heterogeneity of glioblastoma toward\\r  precision medicine using MRI and a data-inclusive machine learning algorithm\\rAuthors: Lujia Wang, Hairong Wang, Fulvio D'Angelo, Lee Curtin, Christopher P.\\r  Sereduk, Gustavo De Leon, Kyle W. Singleton, Javier Urcuyo, Andrea\\r  Hawkins-Daarud, Pamela R. Jackson, Chandan Krishna, Richard S. Zimmerman,\\r  Devi P. Patra, Bernard R. Bendok, Kris A. Smith, Peter Nakaji, Kliment Donev,\\r  Leslie C. Baxter, Maciej M. Mruga{\\\\l}a, Michele Ceccarelli, Antonio Iavarone,\\r  Kristin R. Swanson, Nhan L. Tran, Leland S. Hu, Jing Li\\rCategories: cs.LG cs.CV math.OC\\rComments: 36 pages, 8 figures, 3 tables\\r\\\\\\\\\\r  Glioblastoma (GBM) is one of the most aggressive and lethal human cancers.\\rIntra-tumoral genetic heterogeneity poses a significant challenge for\\rtreatment. Biopsy is invasive, which motivates the development of non-invasive,\\rMRI-based machine learning (ML) models to quantify intra-tumoral genetic\\rheterogeneity for each patient. This capability holds great promise for\\renabling better therapeutic selection to improve patient outcomes. We proposed\\ra novel Weakly Supervised Ordinal Support Vector Machine (WSO-SVM) to predict\\rregional genetic alteration status within each GBM tumor using MRI. WSO-SVM was\\rapplied to a unique dataset of 318 image-localized biopsies with spatially\\rmatched multiparametric MRI from 74 GBM patients. The model was trained to\\rpredict the regional genetic alteration of three GBM driver genes (EGFR,\\rPDGFRA, and PTEN) based on features extracted from the corresponding region of\\rfive MRI contrast images. For comparison, a variety of existing ML algorithms\\rwere also applied. The classification accuracy of each gene was compared\\rbetween the different algorithms. The SHapley Additive exPlanations (SHAP)\\rmethod was further applied to compute contribution scores of different contrast\\rimages. Finally, the trained WSO-SVM was used to generate prediction maps\\rwithin the tumoral area of each patient to help visualize the intra-tumoral\\rgenetic heterogeneity. This study demonstrated the feasibility of using MRI and\\rWSO-SVM to enable non-invasive prediction of intra-tumoral regional genetic\\ralteration for each GBM patient, which can inform future adaptive therapies for\\rindividualized oncology.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00128 ,  1545kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00135 (*cross-listing*)\\rDate: Sat, 30 Dec 2023 04:11:08 GMT   (1853kb)\\r\\rTitle: Deep Radon Prior: A Fully Unsupervised Framework for Sparse-View CT\\r  Reconstruction\\rAuthors: Shuo Xu, Yucheng Zhang, Gang Chen, Xincheng Xiang, Peng Cong, and\\r  Yuewen Sun\\rCategories: eess.IV cs.CV\\rComments: 11 pages, 12 figures, Journal paper\\r\\\\\\\\\\r  Although sparse-view computed tomography (CT) has significantly reduced\\rradiation dose, it also introduces severe artifacts which degrade the image\\rquality. In recent years, deep learning-based methods for inverse problems have\\rmade remarkable progress and have become increasingly popular in CT\\rreconstruction. However, most of these methods suffer several limitations:\\rdependence on high-quality training data, weak interpretability, etc. In this\\rstudy, we propose a fully unsupervised framework called Deep Radon Prior (DRP),\\rinspired by Deep Image Prior (DIP), to address the aforementioned limitations.\\rDRP introduces a neural network as an implicit prior into the iterative method,\\rthereby realizing cross-domain gradient feedback. During the reconstruction\\rprocess, the neural network is progressively optimized in multiple stages to\\rnarrow the solution space in radon domain for the under-constrained imaging\\rprotocol, and the convergence of the proposed method has been discussed in this\\rwork. Compared with the popular pre-trained method, the proposed framework\\rrequires no dataset and exhibits superior interpretability and generalization\\rability. The experimental results demonstrate that the proposed method can\\rgenerate detailed images while effectively suppressing image\\rartifacts.Meanwhile, DRP achieves comparable or better performance than the\\rsupervised methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00135 ,  1853kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00137 (*cross-listing*)\\rDate: Sat, 30 Dec 2023 04:21:12 GMT   (336kb,D)\\r\\rTitle: SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for\\r  Object Detection\\rAuthors: Qiannan Wang, Changchun Yin, Liming Fang, Lu Zhou, Zhe Liu, Run Wang,\\r  Chenhao Lin\\rCategories: cs.CR cs.CV\\r\\\\\\\\\\r  The extensive adoption of Self-supervised learning (SSL) has led to an\\rincreased security threat from backdoor attacks. While existing research has\\rmainly focused on backdoor attacks in image classification, there has been\\rlimited exploration into their implications for object detection. In this work,\\rwe propose the first backdoor attack designed for object detection tasks in SSL\\rscenarios, termed Object Transform Attack (SSL-OTA). SSL-OTA employs a trigger\\rcapable of altering predictions of the target object to the desired category,\\rencompassing two attacks: Data Poisoning Attack (NA) and Dual-Source Blending\\rAttack (DSBA). NA conducts data poisoning during downstream fine-tuning of the\\robject detector, while DSBA additionally injects backdoors into the pre-trained\\rencoder. We establish appropriate metrics and conduct extensive experiments on\\rbenchmark datasets, demonstrating the effectiveness and utility of our proposed\\rattack. Notably, both NA and DSBA achieve high attack success rates (ASR) at\\rextremely low poisoning rates (0.5%). The results underscore the importance of\\rconsidering backdoor threats in SSL-based object detection and contribute a\\rnovel perspective to the field.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00137 ,  336kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00148 (*cross-listing*)\\rDate: Sat, 30 Dec 2023 06:06:01 GMT   (3588kb,D)\\r\\rTitle: TPatch: A Triggered Physical Adversarial Patch\\rAuthors: Wenjun Zhu, Xiaoyu Ji, Yushi Cheng, Shibo Zhang, Wenyuan Xu\\rCategories: cs.CR cs.CV\\rComments: Appeared in 32nd USENIX Security Symposium (USENIX Security 23)\\r\\\\\\\\\\r  Autonomous vehicles increasingly utilize the vision-based perception module\\rto acquire information about driving environments and detect obstacles. Correct\\rdetection and classification are important to ensure safe driving decisions.\\rExisting works have demonstrated the feasibility of fooling the perception\\rmodels such as object detectors and image classifiers with printed adversarial\\rpatches. However, most of them are indiscriminately offensive to every passing\\rautonomous vehicle. In this paper, we propose TPatch, a physical adversarial\\rpatch triggered by acoustic signals. Unlike other adversarial patches, TPatch\\rremains benign under normal circumstances but can be triggered to launch a\\rhiding, creating or altering attack by a designed distortion introduced by\\rsignal injection attacks towards cameras. To avoid the suspicion of human\\rdrivers and make the attack practical and robust in the real world, we propose\\ra content-based camouflage method and an attack robustness enhancement method\\rto strengthen it. Evaluations with three object detectors, YOLO V3/V5 and\\rFaster R-CNN, and eight image classifiers demonstrate the effectiveness of\\rTPatch in both the simulation and the real world. We also discuss possible\\rdefenses at the sensor, algorithm, and system levels.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00148 ,  3588kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00159 (*cross-listing*)\\rDate: Sat, 30 Dec 2023 07:28:56 GMT   (4448kb,D)\\r\\rTitle: Automatic hip osteoarthritis grading with uncertainty estimation from\\r  computed tomography using digitally-reconstructed radiographs\\rAuthors: Masachika Masuda, Mazen Soufi, Yoshito Otake, Keisuke Uemura, Sotaro\\r  Kono, Kazuma Takashima, Hidetoshi Hamada, Yi Gu, Masaki Takao, Seiji Okada,\\r  Nobuhiko Sugano, Yoshinobu Sato\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Progression of hip osteoarthritis (hip OA) leads to pain and disability,\\rlikely leading to surgical treatment such as hip arthroplasty at the terminal\\rstage. The severity of hip OA is often classified using the Crowe and\\rKellgren-Lawrence (KL) classifications. However, as the classification is\\rsubjective, we aimed to develop an automated approach to classify the disease\\rseverity based on the two grades using digitally-reconstructed radiographs\\r(DRRs) from CT images. Automatic grading of the hip OA severity was performed\\rusing deep learning-based models. The models were trained to predict the\\rdisease grade using two grading schemes, i.e., predicting the Crowe and KL\\rgrades separately, and predicting a new ordinal label combining both grades and\\rrepresenting the disease progression of hip OA. The models were trained in\\rclassification and regression settings. In addition, the model uncertainty was\\restimated and validated as a predictor of classification accuracy. The models\\rwere trained and validated on a database of 197 hip OA patients, and externally\\rvalidated on 52 patients. The model accuracy was evaluated using exact class\\raccuracy (ECA), one-neighbor class accuracy (ONCA), and balanced accuracy.The\\rdeep learning models produced a comparable accuracy of approximately 0.65 (ECA)\\rand 0.95 (ONCA) in the classification and regression settings. The model\\runcertainty was significantly larger in cases with large classification errors\\r(P<6e-3). In this study, an automatic approach for grading hip OA severity from\\rCT images was developed. The models have shown comparable performance with high\\rONCA, which facilitates automated grading in large-scale CT databases and\\rindicates the potential for further disease progression analysis.\\rClassification accuracy was correlated with the model uncertainty, which would\\rallow for the prediction of classification errors.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00159 ,  4448kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00275 (*cross-listing*)\\rDate: Sat, 30 Dec 2023 16:27:43 GMT   (5751kb,D)\\r\\rTitle: An $\\\\ell^1$-Plug-and-Play Approach for Magnetic Particle Imaging Using a\\r  Zero Shot Denoiser with Validation on the 3D Open MPI Dataset\\rAuthors: Vladyslav Gapyak and Corinna Rentschler and Thomas M\\\\arz and Andreas\\r  Weinmann\\rCategories: eess.IV cs.CV cs.LG cs.NA math.NA\\rComments: 73 pages, 4 figures, additional supplementary material\\r\\\\\\\\\\r  Magnetic particle imaging (MPI) is an emerging medical imaging modality which\\rhas gained increasing interest in recent years. Among the benefits of MPI are\\rits high temporal resolution, and that the technique does not expose the\\rspecimen to any kind of ionizing radiation. It is based on the non-linear\\rresponse of magnetic nanoparticles to an applied magnetic field. From the\\relectric signal measured in receive coils, the particle concentration has to be\\rreconstructed. Due to the ill-posedness of the reconstruction problem, various\\rregularization methods have been proposed for reconstruction ranging from early\\rstopping methods, via classical Tikhonov regularization and iterative methods\\rto modern machine learning approaches. In this work, we contribute to the\\rlatter class: we propose a plug-and-play approach based on a generic zero-shot\\rdenoiser with an $\\\\ell^1$-prior. Moreover, we develop parameter selection\\rstrategies. Finally, we quantitatively and qualitatively evaluate the proposed\\ralgorithmic scheme on the 3D Open MPI data set with different levels of\\rpreprocessing.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00275 ,  5751kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00314 (*cross-listing*)\\rDate: Sat, 30 Dec 2023 20:16:45 GMT   (7236kb,D)\\r\\rTitle: GAN-GA: A Generative Model based on Genetic Algorithm for Medical Image\\r  Generation\\rAuthors: M. AbdulRazek, G. Khoriba and M. Belal\\rCategories: eess.IV cs.CV cs.LG cs.NE\\rComments: 10 pages, 2 figures. Abstract published in Frontiers in Medical\\r  Technology, presented at the 27th Conference on Medical Image Understanding\\r  and Analysis 2023. DOI: 10.3389/978-2-8325-1231-9. URL:\\r  https://doi.org/10.3389/978-2-8325-1231-9\\rMSC-class: 68T05, 68T07, 68T45, 68U10 (Primary), 92C55 (Secondary)\\rACM-class: I.2.10; I.4.9; J.3\\rJournal-ref: 27th Conference on Medical Image Understanding and Analysis 2023,\\r  Frontiers, 2023, pp. 30-39\\rDOI: 10.3389/978-2-8325-1231-9\\r\\\\\\\\\\r  Medical imaging is an essential tool for diagnosing and treating diseases.\\rHowever, lacking medical images can lead to inaccurate diagnoses and\\rineffective treatments. Generative models offer a promising solution for\\raddressing medical image shortage problems due to their ability to generate new\\rdata from existing datasets and detect anomalies in this data. Data\\raugmentation with position augmentation methods like scaling, cropping,\\rflipping, padding, rotation, and translation could lead to more overfitting in\\rdomains with little data, such as medical image data. This paper proposes the\\rGAN-GA, a generative model optimized by embedding a genetic algorithm. The\\rproposed model enhances image fidelity and diversity while preserving\\rdistinctive features. The proposed medical image synthesis approach improves\\rthe quality and fidelity of medical images, an essential aspect of image\\rinterpretation. To evaluate synthesized images: Frechet Inception Distance\\r(FID) is used. The proposed GAN-GA model is tested by generating Acute\\rlymphoblastic leukemia (ALL) medical images, an image dataset, and is the first\\rtime to be used in generative models. Our results were compared to those of\\rInfoGAN as a baseline model. The experimental results show that the proposed\\roptimized GAN-GA enhances FID scores by about 6.8\\\\%, especially in earlier\\rtraining epochs. The source code and dataset will be available at:\\rhttps://github.com/Mustafa-AbdulRazek/InfoGAN-GA.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00314 ,  7236kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00365 (*cross-listing*)\\rDate: Sun, 31 Dec 2023 01:39:38 GMT   (42053kb,D)\\r\\rTitle: HQ-VAE: Hierarchical Discrete Representation Learning with Variational\\r  Bayes\\rAuthors: Yuhta Takida, Yukara Ikemiya, Takashi Shibuya, Kazuki Shimada, Woosung\\r  Choi, Chieh-Hsin Lai, Naoki Murata, Toshimitsu Uesaka, Kengo Uchida,\\r  Wei-Hsiang Liao, Yuki Mitsufuji\\rCategories: cs.LG cs.AI cs.CV\\rComments: 31 pages with 16 figures\\r\\\\\\\\\\r  Vector quantization (VQ) is a technique to deterministically learn features\\rwith discrete codebook representations. It is commonly performed with a\\rvariational autoencoding model, VQ-VAE, which can be further extended to\\rhierarchical structures for making high-fidelity reconstructions. However, such\\rhierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse\\rissue, where the codebook is not efficiently used to express the data, and\\rhence degrades reconstruction accuracy. To mitigate this problem, we propose a\\rnovel unified framework to stochastically learn hierarchical discrete\\rrepresentation on the basis of the variational Bayes framework, called\\rhierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally\\rgeneralizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and\\rresidual-quantized VAE (RQ-VAE), and provides them with a Bayesian training\\rscheme. Our comprehensive experiments on image datasets show that HQ-VAE\\renhances codebook usage and improves reconstruction performance. We also\\rvalidated HQ-VAE in terms of its applicability to a different modality with an\\raudio dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00365 ,  42053kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00391 (*cross-listing*)\\rDate: Sun, 31 Dec 2023 04:14:43 GMT   (5444kb,D)\\r\\rTitle: Controllable Safety-Critical Closed-loop Traffic Simulation via Guided\\r  Diffusion\\rAuthors: Wei-Jer Chang, Francesco Pittaluga, Masayoshi Tomizuka, Wei Zhan,\\r  Manmohan Chandraker\\rCategories: cs.RO cs.AI cs.CV cs.LG\\rComments: Submitted to CVPR 2024\\rACM-class: I.2.9; I.2.6\\r\\\\\\\\\\r  Evaluating the performance of autonomous vehicle planning algorithms\\rnecessitates simulating long-tail traffic scenarios. Traditional methods for\\rgenerating safety-critical scenarios often fall short in realism and\\rcontrollability. Furthermore, these techniques generally neglect the dynamics\\rof agent interactions. To mitigate these limitations, we introduce a novel\\rclosed-loop simulation framework rooted in guided diffusion models. Our\\rapproach yields two distinct advantages: 1) the generation of realistic\\rlong-tail scenarios that closely emulate real-world conditions, and 2) enhanced\\rcontrollability, enabling more comprehensive and interactive evaluations. We\\rachieve this through novel guidance objectives that enhance road progress while\\rlowering collision and off-road rates. We develop a novel approach to simulate\\rsafety-critical scenarios through an adversarial term in the denoising process,\\rwhich allows the adversarial agent to challenge a planner with plausible\\rmaneuvers, while all agents in the scene exhibit reactive and realistic\\rbehaviors. We validate our framework empirically using the NuScenes dataset,\\rdemonstrating improvements in both realism and controllability. These findings\\raffirm that guided diffusion models provide a robust and versatile foundation\\rfor safety-critical, interactive traffic simulation, extending their utility\\racross the broader landscape of autonomous driving. For additional resources\\rand demonstrations, visit our project page at https://safe-sim.github.io.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00391 ,  5444kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00403 (*cross-listing*)\\rDate: Sun, 31 Dec 2023 05:37:27 GMT   (748kb,D)\\r\\rTitle: Client-wise Modality Selection for Balanced Multi-modal Federated\\r  Learning\\rAuthors: Yunfeng Fan, Wenchao Xu, Haozhao Wang, Penghui Ruan and Song Guo\\rCategories: cs.LG cs.CV cs.MM\\rComments: 10 pages,6 figures,2 tables\\r\\\\\\\\\\r  Selecting proper clients to participate in the iterative federated learning\\r(FL) rounds is critical to effectively harness a broad range of distributed\\rdatasets. Existing client selection methods simply consider the variability\\ramong FL clients with uni-modal data, however, have yet to consider clients\\rwith multi-modalities. We reveal that traditional client selection scheme in\\rMFL may suffer from a severe modality-level bias, which impedes the\\rcollaborative exploitation of multi-modal data, leading to insufficient local\\rdata exploration and global aggregation. To tackle this challenge, we propose a\\rClient-wise Modality Selection scheme for MFL (CMSFed) that can comprehensively\\rutilize information from each modality via avoiding such client selection bias\\rcaused by modality imbalance. Specifically, in each MFL round, the local data\\rfrom different modalities are selectively employed to participate in local\\rtraining and aggregation to mitigate potential modality imbalance of the global\\rmodel. To approximate the fully aggregated model update in a balanced way, we\\rintroduce a novel local training loss function to enhance the weak modality and\\ralign the divergent feature spaces caused by inconsistent modality adoption\\rstrategies for different clients simultaneously. Then, a modality-level\\rgradient decoupling method is designed to derive respective submodular\\rfunctions to maintain the gradient diversity during the selection progress and\\rbalance MFL according to local modality imbalance in each iteration. Our\\rextensive experiments showcase the superiority of CMSFed over baselines and its\\reffectiveness in multi-modal data exploitation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00403 ,  748kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00523 (*cross-listing*)\\rDate: Sun, 31 Dec 2023 15:38:50 GMT   (2788kb,D)\\r\\rTitle: Compressing Deep Image Super-resolution Models\\rAuthors: Yuxuan Jiang, Jakub Nawala, Fan Zhang, and David Bull\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Deep learning techniques have been applied in the context of image\\rsuper-resolution (SR), achieving remarkable advances in terms of reconstruction\\rperformance. Existing techniques typically employ highly complex model\\rstructures which result in large model sizes and slow inference speeds. This\\roften leads to high energy consumption and restricts their adoption for\\rpractical applications. To address this issue, this work employs a three-stage\\rworkflow for compressing deep SR models which significantly reduces their\\rmemory requirement. Restoration performance has been maintained through\\rteacher-student knowledge distillation using a newly designed distillation\\rloss. We have applied this approach to two popular image super-resolution\\rnetworks, SwinIR and EDSR, to demonstrate its effectiveness. The resulting\\rcompact models, SwinIRmini and EDSRmini, attain an 89% and 96% reduction in\\rboth model size and floating-point operations (FLOPs) respectively, compared to\\rtheir original versions. They also retain competitive super-resolution\\rperformance compared to their original models and other commonly used SR\\rapproaches. The source code and pre-trained models for these two lightweight SR\\rapproaches are released at https://pikapi22.github.io/CDISM/.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00523 ,  2788kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00657 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 04:01:40 GMT   (8317kb,D)\\r\\rTitle: Optimizing ADMM and Over-Relaxed ADMM Parameters for Linear Quadratic\\r  Problems\\rAuthors: Jintao Song, Wenqi Lu, Yunwen Lei, Yuchao Tang, Zhenkuan Pan, Jinming\\r  Duan\\rCategories: math.OC cs.CV math.SP\\rComments: Accepted to AAAI 2024\\r\\\\\\\\\\r  The Alternating Direction Method of Multipliers (ADMM) has gained significant\\rattention across a broad spectrum of machine learning applications.\\rIncorporating the over-relaxation technique shows potential for enhancing the\\rconvergence rate of ADMM. However, determining optimal algorithmic parameters,\\rincluding both the associated penalty and relaxation parameters, often relies\\ron empirical approaches tailored to specific problem domains and contextual\\rscenarios. Incorrect parameter selection can significantly hinder ADMM's\\rconvergence rate. To address this challenge, in this paper we first propose a\\rgeneral approach to optimize the value of penalty parameter, followed by a\\rnovel closed-form formula to compute the optimal relaxation parameter in the\\rcontext of linear quadratic problems (LQPs). We then experimentally validate\\rour parameter selection methods through random instantiations and diverse\\rimaging applications, encompassing diffeomorphic image registration, image\\rdeblurring, and MRI reconstruction.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00657 ,  8317kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00692 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 08:11:38 GMT   (8000kb,D)\\r\\rTitle: Self-supervised learning for skin cancer diagnosis with limited training\\r  data\\rAuthors: Hamish Haggerty and Rohitash Chandra\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  Cancer diagnosis is a well-studied problem in machine learning since early\\rdetection of cancer is often the determining factor in prognosis. Supervised\\rdeep learning achieves excellent results in cancer image classification,\\rusually through transfer learning. However, these models require large amounts\\rof labelled data and for several types of cancer, large labelled datasets do\\rnot exist. In this paper, we demonstrate that a model pre-trained using a\\rself-supervised learning algorithm known as Barlow Twins can outperform the\\rconventional supervised transfer learning pipeline. We juxtapose two base\\rmodels: i) pretrained in a supervised fashion on ImageNet; ii) pretrained in a\\rself-supervised fashion on ImageNet. Both are subsequently fine tuned on a\\rsmall labelled skin lesion dataset and evaluated on a large test set. We\\rachieve a mean test accuracy of 70\\\\% for self-supervised transfer in comparison\\rto 66\\\\% for supervised transfer. Interestingly, boosting performance further is\\rpossible by self-supervised pretraining a second time (on unlabelled skin\\rlesion images) before subsequent fine tuning. This hints at an alternative path\\rto collecting more labelled data in settings where this is challenging - namely\\rjust collecting more unlabelled images. Our framework is applicable to cancer\\rimage classification models in the low-labelled data regime.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00692 ,  8000kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00700 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 08:46:29 GMT   (675kb)\\r\\rTitle: An attempt to generate new bridge types from latent space of generative\\r  adversarial network\\rAuthors: Hongjun Zhang\\rCategories: cs.LG cs.AI cs.CV\\rComments: 8 pages, 4 figures\\r\\\\\\\\\\r  Try to generate new bridge types using generative artificial intelligence\\rtechnology. Symmetric structured image dataset of three-span beam bridge, arch\\rbridge, cable-stayed bridge and suspension bridge are used . Based on Python\\rprogramming language, TensorFlow and Keras deep learning platform framework ,\\ras well as Wasserstein loss function and Lipschitz constraints, generative\\radversarial network is constructed and trained. From the obtained low\\rdimensional bridge-type latent space sampling, new bridge types with asymmetric\\rstructures can be generated. Generative adversarial network can create new\\rbridge types by organically combining different structural components on the\\rbasis of human original bridge types. It has a certain degree of human original\\rability. Generative artificial intelligence technology can open up imagination\\rspace and inspire humanity.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00700 ,  675kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00728 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 11:50:01 GMT   (1181kb,D)\\r\\rTitle: MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for\\r  Chest X-Ray Image Classification\\rAuthors: Saurabh Agarwal, K. V. Arya, Yogesh Kumar Meena\\rCategories: eess.IV cs.CV cs.LG\\rComments: 19 pages\\r\\\\\\\\\\r  Chest X-ray imaging is a critical diagnostic tool for identifying pulmonary\\rdiseases. However, manual interpretation of these images is time-consuming and\\rerror-prone. Automated systems utilizing convolutional neural networks (CNNs)\\rhave shown promise in improving the accuracy and efficiency of chest X-ray\\rimage classification. While previous work has mainly focused on using feature\\rmaps from the final convolution layer, there is a need to explore the benefits\\rof leveraging additional layers for improved disease classification. Extracting\\rrobust features from limited medical image datasets remains a critical\\rchallenge. In this paper, we propose a novel deep learning-based multilayer\\rmultimodal fusion model that emphasizes extracting features from different\\rlayers and fusing them. Our disease detection model considers the\\rdiscriminatory information captured by each layer. Furthermore, we propose the\\rfusion of different-sized feature maps (FDSFM) module to effectively merge\\rfeature maps from diverse layers. The proposed model achieves a significantly\\rhigher accuracy of 97.21% and 99.60% for both three-class and two-class\\rclassifications, respectively. The proposed multilayer multimodal fusion model,\\ralong with the FDSFM module, holds promise for accurate disease classification\\rand can also be extended to other disease classifications in chest X-ray\\rimages.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00728 ,  1181kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.00740 (*cross-listing*)\\rDate: Mon, 1 Jan 2024 12:48:23 GMT   (43360kb,D)\\r\\rTitle: Beyond Subspace Isolation: Many-to-Many Transformer for Light Field\\r  Image Super-resolution\\rAuthors: Zeke Zexi Hu, Xiaoming Chen, Vera Yuk Ying Chung, Yiran Shen\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  The effective extraction of spatial-angular features plays a crucial role in\\rlight field image super-resolution (LFSR) tasks, and the introduction of\\rconvolution and Transformers leads to significant improvement in this area.\\rNevertheless, due to the large 4D data volume of light field images, many\\rexisting methods opted to decompose the data into a number of lower-dimensional\\rsubspaces and perform Transformers in each sub-space individually. As a side\\reffect, these methods inadvertently restrict the self-attention mechanisms to a\\rOne-to-One scheme accessing only a limited subset of LF data, explicitly\\rpreventing comprehensive optimization on all spatial and angular cues. In this\\rpaper, we identify this limitation as subspace isolation and introduce a novel\\rMany-to-Many Transformer (M2MT) to address it. M2MT aggregates angular\\rinformation in the spatial subspace before performing the self-attention\\rmechanism. It enables complete access to all information across all\\rsub-aperture images (SAIs) in a light field image. Consequently, M2MT is\\renabled to comprehensively capture long-range correlation dependencies. With\\rM2MT as the pivotal component, we develop a simple yet effective M2MT network\\rfor LFSR. Our experimental results demonstrate that M2MT achieves\\rstate-of-the-art performance across various public datasets. We further conduct\\rin-depth analysis using local attribution maps (LAM) to obtain visual\\rinterpretability, and the results validate that M2MT is empowered with a truly\\rnon-local context in both spatial and angular subspaces to mitigate subspace\\risolation and acquire effective spatial-angular representation.\\r\\\\\\\\ ( https://arxiv.org/abs/2401.00740 ,  43360kb)\\r%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.09709\\rreplaced with revised version Mon, 1 Jan 2024 01:35:08 GMT   (903kb,D)\\r\\rTitle: BSpell: A CNN-Blended BERT Based Bangla Spell Checker\\rAuthors: Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad\\r  Rafsan, Mohammed Eunus Ali\\rCategories: cs.CL\\rJournal-ref: Association for Computational Linguistics, 2023\\rDOI: 10.18653/v1/2023.banglalp-1.2\\r\\\\\\\\ ( https://arxiv.org/abs/2208.09709 ,  903kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.02478\\rreplaced with revised version Mon, 1 Jan 2024 02:10:43 GMT   (1741kb)\\r\\rTitle: Exploring AI-Generated Text in Student Writing: How Does AI Help?\\rAuthors: David James Woo (1), Hengky Susanto (2), Chi Ho Yeung (2), Kai Guo\\r  (3), and (4) April Ka Yeng Fung ((1) Precious Blood Secondary School, Hong\\r  Kong, (2) Department of Science and Environmental Studies, The Education\\r  University of Hong Kong, Hong Kong, (3) Faculty of Education, The University\\r  of Hong Kong, Hong Kong, and (4) Hoi Ping Chamber of Commerce Secondary\\r  School, Hong Kong)\\rCategories: cs.CL cs.AI cs.CY\\rComments: 45 pages, 11 figures, 3 tables\\rACM-class: J.5; K.3.1\\r\\\\\\\\ ( https://arxiv.org/abs/2304.02478 ,  1741kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.05928\\rreplaced with revised version Fri, 29 Dec 2023 21:24:40 GMT   (30kb)\\r\\rTitle: WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in\\r  Wikipedia\\rAuthors: Kenichiro Ando, Satoshi Sekine, Mamoru Komachi\\rCategories: cs.CL\\rComments: AAAI 2024 Main Track Accepted\\r\\\\\\\\ ( https://arxiv.org/abs/2305.05928 ,  30kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.08298\\rreplaced with revised version Sat, 30 Dec 2023 21:23:17 GMT   (465kb,D)\\r\\rTitle: Symbol tuning improves in-context learning in language models\\rAuthors: Jerry Wei and Le Hou and Andrew Lampinen and Xiangning Chen and Da\\r  Huang and Yi Tay and Xinyun Chen and Yifeng Lu and Denny Zhou and Tengyu Ma\\r  and Quoc V. Le\\rCategories: cs.CL\\rComments: EMNLP 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2305.08298 ,  465kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.05836\\rreplaced with revised version Sun, 31 Dec 2023 15:22:18 GMT   (444kb,D)\\r\\rTitle: Can Large Language Models Infer Causation from Correlation?\\rAuthors: Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan,\\r  Rada Mihalcea, Mona Diab, Bernhard Sch\\\\olkopf\\rCategories: cs.CL cs.AI cs.LG\\rComments: v2.0: added 5 fine-tuned model performance; de-duplicated data; and\\r  provided more fine-grained error analysis\\r\\\\\\\\ ( https://arxiv.org/abs/2306.05836 ,  444kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.11380\\rreplaced with revised version Sat, 30 Dec 2023 13:17:52 GMT   (1402kb,D)\\r\\rTitle: Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect\\r  ChatGPT-Generated Text\\rAuthors: Lingyi Yang, Feng Jiang, Haizhou Li\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2307.11380 ,  1402kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.05450\\rreplaced with revised version Sun, 31 Dec 2023 04:20:00 GMT   (779kb,D)\\r\\rTitle: Empower Nested Boolean Logic via Self-Supervised Curriculum Learning\\rAuthors: Hongqiu Wu, Linfeng Liu, Hai Zhao, Min Zhang\\rCategories: cs.CL\\rComments: Accepted by EMNLP2023\\r\\\\\\\\ ( https://arxiv.org/abs/2310.05450 ,  779kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.11097\\rreplaced with revised version Sun, 31 Dec 2023 12:09:52 GMT   (1510kb,D)\\r\\rTitle: Experimenting AI Technologies for Disinformation Combat: the IDMO\\r  Project\\rAuthors: Lorenzo Canale, Alberto Messina\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2310.11097 ,  1510kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.15218\\rreplaced with revised version Mon, 1 Jan 2024 13:26:38 GMT   (218kb,D)\\r\\rTitle: Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and\\r  Qualitative Analysis\\rAuthors: Sai Akash Bathini, Dagli Cihan\\rCategories: cs.CL cs.CE cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2311.15218 ,  218kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.08299\\rreplaced with revised version Sat, 30 Dec 2023 16:45:13 GMT   (2002kb,D)\\r\\rTitle: Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted\\r  Outcomes to Analyze Longitudinal Social Media Data\\rAuthors: Van Minh Nguyen, Nasheen Nur, William Stern, Thomas Mercer, Chiradeep\\r  Sen, Siddhartha Bhattacharyya, Victor Tumbiolo, Seng Jhing Goh\\rCategories: cs.CL cs.CY cs.SI\\rComments: Presented at ICMLA 2023, Special Session: Machine Learning in Health,\\r  8 pages, 6 figures, 7 tables\\rDOI: 10.1109/ICMLA58977.2023.00316\\r\\\\\\\\ ( https://arxiv.org/abs/2312.08299 ,  2002kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12999\\rreplaced with revised version Sat, 30 Dec 2023 17:39:56 GMT   (4886kb,D)\\r\\rTitle: Machine Mindset: An MBTI Exploration of Large Language Models\\rAuthors: Jiaxi Cui, Liuzhenghao Lv, Jing Wen, Rongsheng Wang, Jing Tang,\\r  YongHong Tian, Li Yuan\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12999 ,  4886kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13401\\rreplaced with revised version Sat, 30 Dec 2023 22:11:07 GMT   (8942kb,D)\\r\\rTitle: Time is Encoded in the Weights of Finetuned Language Models\\rAuthors: Kai Nylund, Suchin Gururangan, Noah A. Smith\\rCategories: cs.CL\\rComments: Added references to Jaidka et al. (2018) and Loureiro et al. (2022)\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13401 ,  8942kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.14335\\rreplaced with revised version Sun, 31 Dec 2023 22:31:48 GMT   (45kb)\\r\\rTitle: Context-aware Decoding Reduces Hallucination in Query-focused\\r  Summarization\\rAuthors: Zhichao Xu\\rCategories: cs.CL cs.IR\\rComments: technical report\\r\\\\\\\\ ( https://arxiv.org/abs/2312.14335 ,  45kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.14557\\rreplaced with revised version Mon, 1 Jan 2024 09:24:47 GMT   (4518kb,D)\\r\\rTitle: Aurora:Activating Chinese chat capability for Mixtral-8x7B sparse\\r  Mixture-of-Experts through Instruction-Tuning\\rAuthors: Rongsheng Wang, Haoming Chen, Ruizhe Zhou, Yaofei Duan, Kunyan Cai,\\r  Han Ma, Jiaxi Cui, Jian Li, Patrick Cheong-Iao Pang, Yapeng Wang, Tao Tan\\rCategories: cs.CL\\rComments: 10 pages, 2 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2312.14557 ,  4518kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.17122\\rreplaced with revised version Fri, 29 Dec 2023 21:54:00 GMT   (5557kb,D)\\r\\rTitle: Large Language Model for Causal Decision Making\\rAuthors: Haitao Jiang, Lin Ge, Yuhe Gao, Jianian Wang, Rui Song\\rCategories: cs.CL cs.AI stat.ML\\r\\\\\\\\ ( https://arxiv.org/abs/2312.17122 ,  5557kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.17660\\rreplaced with revised version Mon, 1 Jan 2024 08:59:40 GMT   (34kb,D)\\r\\rTitle: Normalization of Lithuanian Text Using Regular Expressions\\rAuthors: Pijus Kasparaitis\\rCategories: cs.CL\\rComments: 21 pages\\rACM-class: I.2.7\\r\\\\\\\\ ( https://arxiv.org/abs/2312.17660 ,  34kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2102.13272\\rreplaced with revised version Sun, 31 Dec 2023 02:55:49 GMT   (576kb)\\r\\rTitle: Accelerating Large Kernel Convolutions with Nested Winograd\\r  Transformation.pdf\\rAuthors: Jingbo Jiang, Xizi Chen, Chi-Ying Tsui\\rCategories: cs.CV cs.AI\\rComments: published ref to https://ieeexplore.ieee.org/document/10321932\\rMSC-class: 68T01, 68W35, 94C30\\rDOI: 10.1109/VLSI-SoC57769.2023.10321932\\r\\\\\\\\ ( https://arxiv.org/abs/2102.13272 ,  576kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2109.14174\\rreplaced with revised version Sat, 30 Dec 2023 06:24:49 GMT   (22398kb,D)\\r\\rTitle: Cross-Camera Human Motion Transfer by Time Series Analysis\\rAuthors: Yaping Zhao, Guanghan Li, Edmund Y. Lam\\rCategories: cs.CV\\rComments: 5 pages, 9 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2109.14174 ,  22398kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2203.06424\\rreplaced with revised version Mon, 1 Jan 2024 08:50:45 GMT   (0kb,I)\\r\\rTitle: VariabilityTrack:Multi-Object Tracking with Variable Speed Object\\r  Movement\\rAuthors: Run Luo, JinLin Wei, and Qiao Lin\\rCategories: cs.CV\\rComments: find some mistake in this paper\\r\\\\\\\\ ( https://arxiv.org/abs/2203.06424 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2203.07436\\rreplaced with revised version Sun, 31 Dec 2023 01:17:27 GMT   (29598kb,D)\\r\\rTitle: SuperAnimal pretrained pose estimation models for behavioral analysis\\rAuthors: Shaokai Ye and Anastasiia Filippova and Jessy Lauer and Steffen\\r  Schneider and Maxime Vidal and Tian Qiu and Alexander Mathis and Mackenzie\\r  Weygandt Mathis\\rCategories: cs.CV cs.AI q-bio.QM\\rComments: Models and demos available at http://modelzoo.deeplabcut.org\\r\\\\\\\\ ( https://arxiv.org/abs/2203.07436 ,  29598kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.00946\\rreplaced with revised version Sun, 31 Dec 2023 07:43:10 GMT   (2787kb,D)\\r\\rTitle: Motion-aware Memory Network for Fast Video Salient Object Detection\\rAuthors: Xing Zhao, Haoran Liang, Peipei Li, Guodao Sun, Dongdong Zhao, Ronghua\\r  Liang and Xiaofei He\\rCategories: cs.CV\\rComments: 13 pages, 10 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2208.00946 ,  2787kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.10867\\rreplaced with revised version Sun, 31 Dec 2023 12:25:45 GMT   (2517kb,D)\\r\\rTitle: Rethinking the Paradigm of Content Constraints in GAN-based Unpaired\\r  Image-to-Image Translation\\rAuthors: Xiuding Cai, Yaoyao Zhu, Dong Miao, Linjie Fu, Yu Yao\\rCategories: cs.CV\\rComments: Accepted by AAAI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2211.10867 ,  2517kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.10772\\rreplaced with revised version Mon, 1 Jan 2024 05:40:17 GMT   (48512kb,D)\\r\\rTitle: Low-Light Image and Video Enhancement: A Comprehensive Survey and Beyond\\rAuthors: Shen Zheng, Yiling Ma, Jinqian Pan, Changjie Lu, Gaurav Gupta\\rCategories: cs.CV\\rComments: 21 pages, 10 tables, and 17 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2212.10772 ,  48512kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.14418\\rreplaced with revised version Sat, 30 Dec 2023 03:27:01 GMT   (15929kb,D)\\r\\rTitle: PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry\\rAuthors: Yu Zhang, Junle Yu, Xiaolin Huang, Wenhui Zhou, Ji Hou\\rCategories: cs.CV\\rComments: accepted to ECCV2022; code at https://github.com/Gardlin/PCR-CG\\r\\\\\\\\ ( https://arxiv.org/abs/2302.14418 ,  15929kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.05807\\rreplaced with revised version Sat, 30 Dec 2023 02:42:12 GMT   (20098kb,D)\\r\\rTitle: Aleth-NeRF: Low-light Condition View Synthesis with Concealing Fields\\rAuthors: Ziteng Cui, Lin Gu, Xiao Sun, Xianzheng Ma, Yu Qiao, Tatsuya Harada\\rCategories: cs.CV\\rComments: website page: https://cuiziteng.github.io/Aleth_NeRF_web/, refer to\\r  new version: arXiv:2312.09093\\r\\\\\\\\ ( https://arxiv.org/abs/2303.05807 ,  20098kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.09541\\rreplaced with revised version Sun, 31 Dec 2023 00:17:33 GMT   (10480kb,D)\\r\\rTitle: Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in\\r  Challenging Domains\\rAuthors: Zhenzhen Weng, Laura Bravo-S\\\\'anchez, Serena Yeung-Levy\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2303.09541 ,  10480kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05673\\rreplaced with revised version Sun, 31 Dec 2023 16:09:53 GMT   (2324kb,D)\\r\\rTitle: Precise localization of corneal reflections in eye images using deep\\r  learning trained on synthetic data\\rAuthors: Sean Anthony Byrne, Marcus Nystr\\\\om, Virmarie Maquiling, Enkelejda\\r  Kasneci, Diederick C. Niehorster\\rCategories: cs.CV cs.AI\\rComments: Published in Behavioural Research Methods\\rDOI: 10.3758/s13428-023-02297-w\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05673 ,  2324kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.07597\\rreplaced with revised version Sun, 31 Dec 2023 02:14:40 GMT   (8958kb,D)\\r\\rTitle: An Instance Segmentation Dataset of Yeast Cells in Microstructures\\rAuthors: Christoph Reich, Tim Prangemeier, Andr\\\\'e O. Fran\\\\c{c}ani, Heinz\\r  Koeppl\\rCategories: cs.CV\\rComments: IEEE EMBC 2023, Christoph Reich and Tim Prangemeier - both authors\\r  contributed equally\\rDOI: 10.1109/EMBC40787.2023.10340268\\r\\\\\\\\ ( https://arxiv.org/abs/2304.07597 ,  8958kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.08842\\rreplaced with revised version Mon, 1 Jan 2024 13:56:49 GMT   (33037kb,D)\\r\\rTitle: UDTIRI: An Online Open-Source Intelligent Road Inspection Benchmark\\r  Suite\\rAuthors: Sicen Guo, Jiahang Li, Yi Feng, Dacheng Zhou, Denghuang Zhang, Chen\\r  Chen, Shuai Su, Xingyi Zhu, Qijun Chen, Rui Fan\\rCategories: cs.CV cs.AI cs.LG cs.RO\\rComments: Database webpage: https://www.udtiri.com/, Kaggle webpage:\\r  https://www.kaggle.com/datasets/jiahangli617/udtiri\\r\\\\\\\\ ( https://arxiv.org/abs/2304.08842 ,  33037kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.11946\\rreplaced with revised version Fri, 29 Dec 2023 20:16:33 GMT   (7572kb,D)\\r\\rTitle: Image2SSM: Reimagining Statistical Shape Models from Images with Radial\\r  Basis Functions\\rAuthors: Hong Xu and Shireen Y. Elhabian\\rCategories: cs.CV\\rJournal-ref: Medical Image Computing and Computer Assisted Intervention. MICCAI\\r  2023 Conference, pp. 508_517, Springer Nature Switzerland\\r\\\\\\\\ ( https://arxiv.org/abs/2305.11946 ,  7572kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14669\\rreplaced with revised version Mon, 1 Jan 2024 14:40:33 GMT   (3620kb,D)\\r\\rTitle: NegVSR: Augmenting Negatives for Generalized Noise Modeling in\\r  Real-World Video Super-Resolution\\rAuthors: Yexing Song, Meilin Wang, Zhijing Yang, Xiaoyu Xian, Yukai Shi\\rCategories: cs.CV\\rComments: Accepted by AAAI2024, a effective data augmentation framework for\\r  real-world video super-resolution, see our demo at: https://negvsr.github.io/\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14669 ,  3620kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.16283\\rreplaced with revised version Sat, 30 Dec 2023 21:49:22 GMT   (19427kb,D)\\r\\rTitle: CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graph\\r  Diffusion\\rAuthors: Guangyao Zhai, Evin P{\\\\i}nar \\\\Ornek, Shun-Cheng Wu, Yan Di, Federico\\r  Tombari, Nassir Navab, Benjamin Busam\\rCategories: cs.CV\\rComments: NeurIPS 2023 camera-ready\\r\\\\\\\\ ( https://arxiv.org/abs/2305.16283 ,  19427kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.17939\\rreplaced with revised version Sat, 30 Dec 2023 12:31:23 GMT   (2200kb,D)\\r\\rTitle: Fourier Analysis on Robustness of Graph Convolutional Neural Networks\\r  for Skeleton-based Action Recognition\\rAuthors: Nariki Tanaka, Hiroshi Kera, Kazuhiko Kawamoto\\rCategories: cs.CV\\rComments: 18 pages, 13 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2305.17939 ,  2200kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.18601\\rreplaced with revised version Sun, 31 Dec 2023 04:01:38 GMT   (28298kb,D)\\r\\rTitle: BRICS: Bi-level feature Representation of Image CollectionS\\rAuthors: Dingdong Yang, Yizhi Wang, Ali Mahdavi-Amiri, Hao Zhang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2305.18601 ,  28298kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.00354\\rreplaced with revised version Sat, 30 Dec 2023 13:50:02 GMT   (25549kb,D)\\r\\rTitle: Addressing Negative Transfer in Diffusion Models\\rAuthors: Hyojun Go, JinYoung Kim, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh,\\r  Hyeongdon Moon, Seungtaek Choi\\rCategories: cs.CV cs.AI cs.LG\\rComments: Neurips 2023. Project page:\\r  https://gohyojun15.github.io/ANT_diffusion/\\r\\\\\\\\ ( https://arxiv.org/abs/2306.00354 ,  25549kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.07520\\rreplaced with revised version Sun, 31 Dec 2023 16:54:05 GMT   (5893kb,D)\\r\\rTitle: Instruct-ReID: A Multi-purpose Person Re-identification Task with\\r  Instructions\\rAuthors: Weizhen He and Yiheng Deng and Shixiang Tang and Qihao Chen and\\r  Qingsong Xie and Yizhou Wang and Lei Bai and Feng Zhu and Rui Zhao and Wanli\\r  Ouyang and Donglian Qi and Yunfeng Yan\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.07520 ,  5893kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.16846\\rreplaced with revised version Mon, 1 Jan 2024 06:25:08 GMT   (19289kb,D)\\r\\rTitle: Lightweight texture transfer based on texture feature preset\\rAuthors: ShiQi Jiang\\rCategories: cs.CV eess.IV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.16846 ,  19289kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.03407\\rreplaced with revised version Sat, 30 Dec 2023 21:44:15 GMT   (34855kb,D)\\r\\rTitle: Spatially Varying Nanophotonic Neural Networks\\rAuthors: Kaixuan Wei, Xiao Li, Johannes Froech, Praneeth Chakravarthula, James\\r  Whitehead, Ethan Tseng, Arka Majumdar, Felix Heide\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2308.03407 ,  34855kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.06412\\rreplaced with revised version Sat, 30 Dec 2023 00:10:57 GMT   (11693kb,D)\\r\\rTitle: Taming Self-Training for Open-Vocabulary Object Detection\\rAuthors: Shiyu Zhao, Samuel Schulter, Long Zhao, Zhixing Zhang, Vijay Kumar\\r  B.G, Yumin Suh, Manmohan Chandraker, Dimitris N. Metaxas\\rCategories: cs.CV\\rComments: 19 pages, 8 figures. Code: https://github.com/xiaofeng94/SAS-Det\\r\\\\\\\\ ( https://arxiv.org/abs/2308.06412 ,  11693kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.07017\\rreplaced with revised version Sat, 30 Dec 2023 01:31:51 GMT   (0kb,I)\\r\\rTitle: Contrastive Bi-Projector for Unsupervised Domain Adaption\\rAuthors: Lin-Chieh Huang, Hung-Hsu Tsai\\rCategories: cs.CV\\rComments: Professor asks to withdraw this paper on arxiv. This paper will\\r  upload again if the paper is published in the journal\\r\\\\\\\\ ( https://arxiv.org/abs/2308.07017 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.07931\\rreplaced with revised version Sat, 30 Dec 2023 01:10:41 GMT   (17785kb,D)\\r\\rTitle: Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation\\rAuthors: William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling,\\r  Phillip Isola\\rCategories: cs.CV cs.AI cs.CL cs.LG cs.RO\\rComments: Project website at https://f3rm.csail.mit.edu, Accepted at the 7th\\r  Annual Conference on Robot Learning (CoRL), 2023 in Atlanta, US\\r\\\\\\\\ ( https://arxiv.org/abs/2308.07931 ,  17785kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.08316\\rreplaced with revised version Sat, 30 Dec 2023 04:21:34 GMT   (8930kb,D)\\r\\rTitle: Dual-Stream Diffusion Net for Text-to-Video Generation\\rAuthors: Binhui Liu, Xin Liu, Anbo Dai, Zhiyong Zeng, Dan Wang, Zhen Cui, Jian\\r  Yang\\rCategories: cs.CV\\rComments: 8pages, 7 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2308.08316 ,  8930kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.09091\\rreplaced with revised version Sat, 30 Dec 2023 04:09:45 GMT   (16853kb,D)\\r\\rTitle: Edit Temporal-Consistent Videos with Image Diffusion Model\\rAuthors: Yuanzhi Wang, Yong Li, Xiaoya Zhang, Xin Liu, Anbo Dai, Antoni B.\\r  Chan, Zhen Cui\\rCategories: cs.CV\\rComments: 10 pages, 7 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2308.09091 ,  16853kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.10273\\rreplaced with revised version Sun, 31 Dec 2023 14:33:40 GMT   (12617kb,D)\\r\\rTitle: Turning Waste into Wealth: Leveraging Low-Quality Samples for Enhancing\\r  Continuous Conditional Generative Adversarial Networks\\rAuthors: Xin Ding and Yongwei Wang and Zuheng Xu\\rCategories: cs.CV cs.LG\\rJournal-ref: AAAI2024\\r\\\\\\\\ ( https://arxiv.org/abs/2308.10273 ,  12617kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.14181\\rreplaced with revised version Mon, 1 Jan 2024 14:48:48 GMT   (5177kb,D)\\r\\rTitle: Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level\\r  Vision\\rAuthors: Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao,\\r  Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin\\rCategories: cs.CV cs.AI cs.MM\\rComments: 27 pages, 11 tables, with updated results\\r\\\\\\\\ ( https://arxiv.org/abs/2309.14181 ,  5177kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.15031\\rreplaced with revised version Sun, 31 Dec 2023 12:29:58 GMT   (3901kb)\\r\\rTitle: Nuclear Morphometry using a Deep Learning-based Algorithm has Prognostic\\r  Relevance for Canine Cutaneous Mast Cell Tumors\\rAuthors: Andreas Haghofer, Eda Parlak, Alexander Bartel, Taryn A. Donovan,\\r  Charles-Antoine Assenmacher, Pompei Bolfa, Michael J. Dark, Andrea\\r  Fuchs-Baumgartinger, Andrea Klang, Kathrin J\\\\ager, Robert Klopfleisch,\\r  Sophie Merz, Barbara Richter, F. Yvonne Schulman, Jonathan Ganz, Josef\\r  Scharinger, Marc Aubreville, Stephan M. Winkler, Matti Kiupel, Christof A.\\r  Bertram\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.15031 ,  3901kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.03940\\rreplaced with revised version Sun, 31 Dec 2023 05:46:45 GMT   (13126kb,D)\\r\\rTitle: Hard View Selection for Self-Supervised Learning\\rAuthors: Fabio Ferreira, Ivo Rapant, Frank Hutter\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2310.03940 ,  13126kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.06594\\rreplaced with revised version Sat, 30 Dec 2023 02:19:22 GMT   (6316kb,D)\\r\\rTitle: On the Evaluation and Refinement of Vision-Language Instruction Tuning\\r  Datasets\\rAuthors: Ning Liao, Shaofeng Zhang, Renqiu Xia, Min Cao, Yu Qiao, Junchi Yan\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2310.06594 ,  6316kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.16542\\rreplaced with revised version Mon, 1 Jan 2024 18:26:05 GMT   (15515kb,D)\\r\\rTitle: ParisLuco3D: A high-quality target dataset for domain generalization of\\r  LiDAR perception\\rAuthors: Jules Sanchez, Louis Soum-Fontez, Jean-Emmanuel Deschaud, Francois\\r  Goulette\\rCategories: cs.CV cs.RO\\r\\\\\\\\ ( https://arxiv.org/abs/2310.16542 ,  15515kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.01004\\rreplaced with revised version Sat, 30 Dec 2023 17:17:58 GMT   (2514kb,D)\\r\\rTitle: Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning\\r  for Medical Image Captioning\\rAuthors: Zhenyu Zhang, Benlu Wang, Weijie Liang, Yizhi Li, Xuechen Guo,\\r  Guanhong Wang, Shiyan Li, Gaoang Wang\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2311.01004 ,  2514kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.03198\\rreplaced with revised version Sat, 30 Dec 2023 06:39:46 GMT   (1162kb,D)\\r\\rTitle: LCPR: A Multi-Scale Attention-Based LiDAR-Camera Fusion Network for\\r  Place Recognition\\rAuthors: Zijie Zhou, Jingyi Xu, Guangming Xiong, Junyi Ma\\rCategories: cs.CV cs.RO\\rComments: Accepted by IEEE Robotics and Automation Letters (RAL) 2023\\rDOI: 10.1109/LRA.2023.3346753\\r\\\\\\\\ ( https://arxiv.org/abs/2311.03198 ,  1162kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.03356\\rreplaced with revised version Fri, 29 Dec 2023 01:53:21 GMT   (7955kb,D)\\r\\rTitle: GLaMM: Pixel Grounding Large Multimodal Model\\rAuthors: Hanoona Rasheed, Muhammad Maaz, Sahal Shaji Mullappilly, Abdelrahman\\r  Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Erix Xing, Ming-Hsuan\\r  Yang, Fahad S. Khan\\rCategories: cs.CV cs.AI\\rComments: Technical Report of GLaMM\\r\\\\\\\\ ( https://arxiv.org/abs/2311.03356 ,  7955kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.06542\\rreplaced with revised version Sun, 31 Dec 2023 15:21:23 GMT   (214kb)\\r\\rTitle: Generation Of Colors using Bidirectional Long Short Term Memory Networks\\rAuthors: A. Sinha\\rCategories: cs.CV\\rComments: 8 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2311.06542 ,  214kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.10116\\rreplaced with revised version Sun, 31 Dec 2023 09:40:10 GMT   (7280kb,D)\\r\\rTitle: Wildfire Smoke Detection with Cross Contrast Patch Embedding\\rAuthors: Chong Wang, Cheng Xu, Adeel Akram, Zhilin Shan, Qixing Zhang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2311.10116 ,  7280kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.16754\\rreplaced with revised version Mon, 1 Jan 2024 12:27:23 GMT   (40831kb,D)\\r\\rTitle: Towards Full-scene Domain Generalization in Multi-agent Collaborative\\r  Bird's Eye View Segmentation for Connected and Autonomous Driving\\rAuthors: Senkang Hu, Zhengru Fang, Xianhao Chen, Yuguang Fang, Sam Kwong\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2311.16754 ,  40831kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.01324\\rreplaced with revised version Mon, 1 Jan 2024 13:27:15 GMT   (1740kb,D)\\r\\rTitle: MABViT -- Modified Attention Block Enhances Vision Transformers\\rAuthors: Mahesh Ramesh and Aswinkumar Ramkumar\\rCategories: cs.CV cs.LG\\rComments: Accepted at Deployable AI Workshop, AAAI Conference\\r\\\\\\\\ ( https://arxiv.org/abs/2312.01324 ,  1740kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.05634\\rreplaced with revised version Sun, 31 Dec 2023 17:50:47 GMT   (7389kb,D)\\r\\rTitle: PGS: Pose-Guided Supervision for Mitigating Clothes-Changing in Person\\r  Re-Identification\\rAuthors: Quoc-Huy Trinh and Nhat-Tan Bui and Dinh-Hieu Hoang and Phuoc-Thao Vo\\r  Thi and Hai-Dang Nguyen and Debesh Jha and Ulas Bagci and Ngan Le and\\r  Minh-Triet Tran\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2312.05634 ,  7389kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.07586\\rreplaced with revised version Mon, 1 Jan 2024 04:25:04 GMT   (17299kb,D)\\r\\rTitle: Characteristic Guidance: Non-linear Correction for Diffusion Model at\\r  Large Guidance Scale\\rAuthors: Candi Zheng, Yuan Lan\\rCategories: cs.CV cs.AI cs.LG physics.data-an\\rComments: 8 pages, 7 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2312.07586 ,  17299kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.09168\\rreplaced with revised version Mon, 1 Jan 2024 10:15:46 GMT   (46164kb,D)\\r\\rTitle: DiffusionLight: Light Probes for Free by Painting a Chrome Ball\\rAuthors: Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet,\\r  Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn\\rCategories: cs.CV cs.GR cs.LG\\rComments: For more info and code, please visit our website\\r  https://diffusionlight.github.io/\\rACM-class: I.3.3; I.4.8\\r\\\\\\\\ ( https://arxiv.org/abs/2312.09168 ,  46164kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11451\\rreplaced with revised version Sun, 31 Dec 2023 07:38:13 GMT   (5154kb,D)\\r\\rTitle: Language-Assisted 3D Scene Understanding\\rAuthors: Yanmin Wu, Qiankun Gao, Renrui Zhang, and Jian Zhang\\rCategories: cs.CV cs.RO\\rComments: Technical report, unpublished, 16 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11451 ,  5154kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13108\\rreplaced with revised version Mon, 1 Jan 2024 14:26:39 GMT   (3554kb,D)\\r\\rTitle: ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation\\rAuthors: Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao,\\r  Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, Hengxu Wang, Luowei Zhou,\\r  Mike Zheng Shou\\rCategories: cs.CV\\rComments: Project Page: https://showlab.github.io/assistgui/\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13108 ,  3554kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.16256\\rreplaced with revised version Fri, 29 Dec 2023 08:49:49 GMT   (35084kb,D)\\r\\rTitle: DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision\\rAuthors: Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan,\\r  Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, Xuanmao Li, Xingpeng Sun, Rohan\\r  Ashok, Aniruddha Mukherjee, Hao Kang, Xiangrui Kong, Gang Hua, Tianyi Zhang,\\r  Bedrich Benes, Aniket Bera\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2312.16256 ,  35084kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.16477\\rreplaced with revised version Sat, 30 Dec 2023 08:21:36 GMT   (14789kb,D)\\r\\rTitle: Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding\\rAuthors: Lixiang Xu, Qingzhe Cui, Richang Hong, Wei Xu, Enhong Chen, Xin Yuan,\\r  Chenglong Li, Yuanyan Tang\\rCategories: cs.CV cs.AI\\rComments: 13pages, 8 figuers\\rMSC-class: 68\\rACM-class: I.2.10\\r\\\\\\\\ ( https://arxiv.org/abs/2312.16477 ,  14789kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.16580\\rreplaced with revised version Sun, 31 Dec 2023 03:51:32 GMT   (5100kb,D)\\r\\rTitle: VLCounter: Text-aware Visual Representation for Zero-Shot Object\\r  Counting\\rAuthors: Seunggu Kang, WonJun Moon, Euiyeon Kim, Jae-Pil Heo\\rCategories: cs.CV\\rComments: Accepted to AAAI 2024. Code is available at\\r  https://github.com/Seunggu0305/VLCounter\\r\\\\\\\\ ( https://arxiv.org/abs/2312.16580 ,  5100kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.16886\\rreplaced with revised version Sat, 30 Dec 2023 04:59:21 GMT   (1907kb,D)\\r\\rTitle: MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile\\r  Devices\\rAuthors: Xiangxiang Chu and Limeng Qiao and Xinyang Lin and Shuang Xu and Yang\\r  Yang and Yiming Hu and Fei Wei and Xinyu Zhang and Bo Zhang and Xiaolin Wei\\r  and Chunhua Shen\\rCategories: cs.CV\\rComments: Tech Report\\r\\\\\\\\ ( https://arxiv.org/abs/2312.16886 ,  1907kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.17163\\rreplaced with revised version Mon, 1 Jan 2024 14:40:50 GMT   (6510kb,D)\\r\\rTitle: FENet: Focusing Enhanced Network for Lane Detection\\rAuthors: Liman Wang, Hanyang Zhong\\rCategories: cs.CV cs.AI\\rComments: 12 pages including appendix. The website will be released soon\\r\\\\\\\\ ( https://arxiv.org/abs/2312.17163 ,  6510kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.17205\\rreplaced with revised version Sat, 30 Dec 2023 08:45:03 GMT   (22050kb,D)\\r\\rTitle: EFHQ: Multi-purpose ExtremePose-Face-HQ dataset\\rAuthors: Trung Tuan Dao, Duc Hong Vu, Cuong Pham, Anh Tran\\rCategories: cs.CV\\rComments: Project Page: https://bomcon123456.github.io/efhq/\\r\\\\\\\\ ( https://arxiv.org/abs/2312.17205 ,  22050kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.17046\\rreplaced with revised version Mon, 1 Jan 2024 17:56:39 GMT   (16269kb,D)\\r\\rTitle: Representing and Modeling Inconsistent, Impossible, and Incoherent\\r  Shapes and Scenes with 2D Non-Conservative Vector Fields mapped on\\r  2-Complexes\\rAuthors: Ergun Akleman and Youyou Wang and Ozgur Gonen\\rCategories: cs.GR\\rComments: 21 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2312.17046 ,  16269kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.00392 (*cross-listing*)\\rreplaced with revised version Sun, 31 Dec 2023 19:32:39 GMT   (60kb)\\r\\rTitle: Physical Computing: A Category Theoretic Perspective on Physical\\r  Computation and System Compositionality\\rAuthors: Nima Dehghani, Gianluca Caterina\\rCategories: quant-ph cs.AI cs.CL physics.comp-ph q-bio.OT\\r\\\\\\\\ ( https://arxiv.org/abs/2210.00392 ,  60kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.08793\\rreplaced with revised version Sun, 31 Dec 2023 05:30:30 GMT   (2457kb,D)\\r\\rTitle: Forbidden Facts: An Investigation of Competing Objectives in Llama-2\\rAuthors: Tony T. Wang, Miles Wang, Kaivalya Hariharan, Nir Shavit\\rCategories: cs.LG cs.AI cs.CL cs.CR\\rComments: Accepted to the ATTRIB and SoLaR workshops at NeurIPS 2023; (v3:\\r  clarified experimental details)\\r\\\\\\\\ ( https://arxiv.org/abs/2312.08793 ,  2457kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.14416 (*cross-listing*)\\rreplaced with revised version Fri, 29 Dec 2023 21:24:49 GMT   (6514kb,D)\\r\\rTitle: Residual Back Projection With Untrained Neural Networks\\rAuthors: Ziyu Shu and Alireza Entezari\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2210.14416 ,  6514kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.13586 (*cross-listing*)\\rreplaced with revised version Sat, 30 Dec 2023 03:40:28 GMT   (8306kb,D)\\r\\rTitle: Energy-Based Sliced Wasserstein Distance\\rAuthors: Khai Nguyen and Nhat Ho\\rCategories: stat.ML cs.CV cs.GR cs.LG\\rComments: Accepted to NeurIPS 2023, 30 pages, 8 figures, 6 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2304.13586 ,  8306kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.11766\\rreplaced with revised version Sun, 31 Dec 2023 06:42:28 GMT   (2244kb,D)\\r\\rTitle: Dictionary Attack on IMU-based Gait Authentication\\rAuthors: Rajesh Kumar and Can Isik and Chilukuri K. Mohan\\rCategories: cs.CR cs.CV cs.LG eess.SP\\rComments: 12 pages, 9 figures, accepted at AISec23 colocated with ACM CCS,\\r  November 30, 2023, Copenhagen, Denmark\\rACM-class: K.6.5\\r\\\\\\\\ ( https://arxiv.org/abs/2309.11766 ,  2244kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.10835 (*cross-listing*)\\rreplaced with revised version Fri, 29 Dec 2023 23:01:15 GMT   (26783kb,D)\\r\\rTitle: Provable Probabilistic Imaging using Score-Based Generative Priors\\rAuthors: Yu Sun, Zihui Wu, Yifan Chen, Berthy T. Feng, Katherine L. Bouman\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2310.10835 ,  26783kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.03380\\rreplaced with revised version Mon, 1 Jan 2024 09:26:56 GMT   (1391kb)\\r\\rTitle: An attempt to generate new bridge types from latent space of variational\\r  autoencoder\\rAuthors: Hongjun Zhang\\rCategories: cs.LG cs.AI cs.CV\\rComments: 9 pages, 8 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2311.03380 ,  1391kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.13091\\rreplaced with revised version Sun, 31 Dec 2023 20:04:25 GMT   (15509kb,D)\\r\\rTitle: Stable Unlearnable Example: Enhancing the Robustness of Unlearnable\\r  Examples via Stable Error-Minimizing Noise\\rAuthors: Yixin Liu, Kaidi Xu, Xun Chen, and Lichao Sun\\rCategories: cs.LG cs.CR cs.CV\\rComments: Accepted to AAAI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.13091 ,  15509kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.10324\\rreplaced with revised version Sat, 30 Dec 2023 01:59:27 GMT   (1333kb,D)\\r\\rTitle: Federated Learning with Instance-Dependent Noisy Labels\\rAuthors: Lei Wang, Jieming Bian, Jie Xu\\rCategories: cs.LG cs.CV\\rComments: Accepted by ICASSP 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.10324 ,  1333kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.16471 (*cross-listing*)\\rreplaced with revised version Sat, 30 Dec 2023 06:05:06 GMT   (81kb)\\r\\rTitle: A Survey on Super Resolution for video Enhancement Using GAN\\rAuthors: Ankush Maity, Roshan Pious, Sourabh Kumar Lenka, Vishal Choudhary and\\r  Prof. Sharayu Lokhande\\rCategories: eess.IV cs.CV cs.LG cs.MM\\rComments: 7 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2312.16471 ,  81kb)\\r%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%--- ------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\rSend any comments regarding submissions directly to submitter.\\r------------------------------------------------------------------------------\\rArchives at http://arxiv.org/\\rTo unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel\\r------------------------------------------------------------------------------\\r Submissions to:\\rComputation and Language\\rComputer Vision and Pattern Recognition\\r received from  Mon 11 Mar 24 18:00:00 GMT  to  Tue 12 Mar 24 18:00:00 GMT\\r------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07087\\rDate: Mon, 11 Mar 2024 18:25:01 GMT   (814kb)\\r\\rTitle: LSTM-Based Text Generation: A Study on Historical Datasets\\rAuthors: Mustafa Abbas Hussein Hussein, Serkan Sava\\\\c{s}\\rCategories: cs.CL cs.AI\\rReport-no: ISBN: 978-625-6879-50-8\\rJournal-ref: 16th International Istanbul Scientific Research Congress on Life,\\r  Engineering, Architecture, and Mathematical Sciences Proceedings Book, Pages:\\r  42-49, 2024\\rDOI: 10.5281/zenodo.10776102\\r\\\\\\\\\\r  This paper presents an exploration of Long Short-Term Memory (LSTM) networks\\rin the realm of text generation, focusing on the utilization of historical\\rdatasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in\\rhandling sequential data, are applied here to model complex language patterns\\rand structures inherent in historical texts. The study demonstrates that\\rLSTM-based models, when trained on historical datasets, can not only generate\\rtext that is linguistically rich and contextually relevant but also provide\\rinsights into the evolution of language patterns over time. The finding\\rpresents models that are highly accurate and efficient in predicting text from\\rworks of Nietzsche, with low loss values and a training time of 100 iterations.\\rThe accuracy of the model is 0.9521, indicating high accuracy. The loss of the\\rmodel is 0.2518, indicating its effectiveness. The accuracy of the model in\\rpredicting text from the work of Shakespeare is 0.9125, indicating a low error\\rrate. The training time of the model is 100, mirroring the efficiency of the\\rNietzsche dataset. This efficiency demonstrates the effectiveness of the model\\rdesign and training methodology, especially when handling complex literary\\rtexts. This research contributes to the field of natural language processing by\\rshowcasing the versatility of LSTM networks in text generation and offering a\\rpathway for future explorations in historical linguistics and beyond.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07087 ,  814kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07088\\rDate: Mon, 11 Mar 2024 18:26:02 GMT   (1663kb,D)\\r\\rTitle: SPA: Towards A Computational Friendly Cloud-Base and On-Devices\\r  Collaboration Seq2seq Personalized Generation\\rAuthors: Yanming Liu, Xinyue Peng, Jiannan Cao, Le Dai, Xingzu Liu, Weihao Liu,\\r  Mingbang Wang\\rCategories: cs.CL\\rComments: 11 pages, first version of SPA(Side Plugin Adaption)\\r\\\\\\\\\\r  Large language models(LLMs) have shown its outperforming ability on various\\rtasks and question answering. However, LLMs require high computation cost and\\rlarge memory cost. At the same time, LLMs may cause privacy leakage when\\rtraining or prediction procedure contains sensitive information. In this paper,\\rwe propose SPA(Side Plugin Adaption), a lightweight architecture for fast\\ron-devices inference and privacy retaining on the constraints of strict\\ron-devices computation and memory constraints. Compared with other on-devices\\rseq2seq generation, SPA could make a fast and stable inference on low-resource\\rconstraints, allowing it to obtain cost effiency. Our method establish an\\rinteraction between a pretrained LLMs on-cloud and additive parameters\\ron-devices, which could provide the knowledge on both pretrained LLMs and\\rprivate personal feature.Further more, SPA provides a framework to keep\\rfeature-base parameters on private guaranteed but low computational devices\\rwhile leave the parameters containing general information on the high\\rcomputational devices.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07088 ,  1663kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07118\\rDate: Mon, 11 Mar 2024 19:19:59 GMT   (1331kb,D)\\r\\rTitle: Narrating Causal Graphs with Large Language Models\\rAuthors: Atharva Phatak, Vijay K. Mago, Ameeta Agrawal, Aravind Inbasekaran,\\r  Philippe J. Giabbanelli\\rCategories: cs.CL\\rComments: HICSS '24\\r\\\\\\\\\\r  The use of generative AI to create text descriptions from graphs has mostly\\rfocused on knowledge graphs, which connect concepts using facts. In this work\\rwe explore the capability of large pretrained language models to generate text\\rfrom causal graphs, where salient concepts are represented as nodes and\\rcausality is represented via directed, typed edges. The causal reasoning\\rencoded in these graphs can support applications as diverse as healthcare or\\rmarketing. Using two publicly available causal graph datasets, we empirically\\rinvestigate the performance of four GPT-3 models under various settings. Our\\rresults indicate that while causal text descriptions improve with training\\rdata, compared to fact-based graphs, they are harder to generate under\\rzero-shot settings. Results further suggest that users of generative AI can\\rdeploy future applications faster since similar performances are obtained when\\rtraining a model with only a few examples as compared to fine-tuning via a\\rlarge curated dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07118 ,  1331kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07144\\rDate: Mon, 11 Mar 2024 20:28:27 GMT   (1928kb,D)\\r\\rTitle: Thought Graph: Generating Thought Process for Biological Reasoning\\rAuthors: Chi-Yang Hsu, Kyle Cox, Jiawei Xu, Zhen Tan, Tianhua Zhai, Mengzhou\\r  Hu, Dexter Pratt, Tianlong Chen, Ziniu Hu, Ying Ding\\rCategories: cs.CL\\rComments: 4 pages. Accepted by Web Conf 2024\\rDOI: 10.1145/3589335.3651572\\r\\\\\\\\\\r  We present the Thought Graph as a novel framework to support complex\\rreasoning and use gene set analysis as an example to uncover semantic\\rrelationships between biological processes. Our framework stands out for its\\rability to provide a deeper understanding of gene sets, significantly\\rsurpassing GSEA by 40.28% and LLM baselines by 5.38% based on cosine similarity\\rto human annotations. Our analysis further provides insights into future\\rdirections of biological processes naming, and implications for bioinformatics\\rand precision medicine.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07144 ,  1928kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07175\\rDate: Mon, 11 Mar 2024 21:33:05 GMT   (571kb,D)\\r\\rTitle: Rebuilding ROME : Resolving Model Collapse during Sequential Model\\r  Editing\\rAuthors: Akshat Gupta, Gopala Anumanchipalli\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Recent work on model editing using Rank-One Model Editing (ROME), a popular\\rmodel editing method, has shown that there are certain facts that the algorithm\\ris unable to edit without breaking the model. Such edits have previously been\\rcalled disabling edits. These disabling edits cause immediate model collapse\\rand limits the use of ROME for sequential editing. In this paper, we make two\\rmain contributions. Firstly, we show that model collapse with ROME only happens\\rwhen making edits using the CounterFact dataset and does not happen when using\\rthe zsRE dataset. Secondly, we find that disabling edits are an artifact of the\\roriginal implementation of ROME. With this paper, we provide a more stable\\rimplementation ROME, which we call r-ROME and show that we no longer observe\\rmodel collapse when making large scale sequential edits with ROME.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07175 ,  571kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07183\\rDate: Mon, 11 Mar 2024 21:51:39 GMT   (570kb,D)\\r\\rTitle: Monitoring AI-Modified Content at Scale: A Case Study on the Impact of\\r  ChatGPT on AI Conference Peer Reviews\\rAuthors: Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao,\\r  Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A.\\r  McFarland, James Y. Zou\\rCategories: cs.CL cs.AI cs.LG cs.SI\\rComments: 42 pages, 30 figures\\rACM-class: I.2.7\\r\\\\\\\\\\r  We present an approach for estimating the fraction of text in a large corpus\\rwhich is likely to be substantially modified or produced by a large language\\rmodel (LLM). Our maximum likelihood model leverages expert-written and\\rAI-generated reference texts to accurately and efficiently examine real-world\\rLLM-use at the corpus level. We apply this approach to a case study of\\rscientific peer review in AI conferences that took place after the release of\\rChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest\\rthat between 6.5% and 16.9% of text submitted as peer reviews to these\\rconferences could have been substantially modified by LLMs, i.e. beyond\\rspell-checking or minor writing updates. The circumstances in which generated\\rtext occurs offer insight into user behavior: the estimated fraction of\\rLLM-generated text is higher in reviews which report lower confidence, were\\rsubmitted close to the deadline, and from reviewers who are less likely to\\rrespond to author rebuttals. We also observe corpus-level trends in generated\\rtext which may be too subtle to detect at the individual level, and discuss the\\rimplications of such trends on peer review. We call for future\\rinterdisciplinary work to examine how LLM use is changing our information and\\rknowledge practices.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07183 ,  570kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07193\\rDate: Mon, 11 Mar 2024 22:27:16 GMT   (873kb)\\r\\rTitle: CuentosIE: can a chatbot about tales with a message help to teach\\r  emotional intelligence?\\rAuthors: Antonio Ferr\\\\'andez, Roc\\\\'io Lavigne-Cerv\\\\'an, Jes\\\\'us Peral, Ignasi\\r  Navarro-Soria, \\\\'Angel Lloret, David Gil, Carmen Rocamora\\rCategories: cs.CL cs.AI\\rComments: 26 pages\\rACM-class: I.2.0\\rJournal-ref: PeerJ Computer Science, Volume 10, February 2024, ID e1866\\rDOI: 10.7717/peerj-cs.1866\\r\\\\\\\\\\r  In this article, we present CuentosIE (TalesEI: chatbot of tales with a\\rmessage to develop Emotional Intelligence), an educational chatbot on emotions\\rthat also provides teachers and psychologists with a tool to monitor their\\rstudents/patients through indicators and data compiled by CuentosIE. The use of\\rtales with a message is justified by their simplicity and easy understanding,\\rthanks to their moral or associated metaphors. The main contributions of\\rCuentosIE are the selection, collection, and classification of a set of highly\\rspecialized tales, as well as the provision of tools (searching, reading\\rcomprehension, chatting, recommending, and classifying) that are useful for\\rboth educating users about emotions and monitoring their emotional development.\\rThe preliminary evaluation of the tool has obtained encouraging results, which\\rprovides an affirmative answer to the question posed in the title of the\\rarticle.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07193 ,  873kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07202\\rDate: Mon, 11 Mar 2024 22:58:58 GMT   (444kb,D)\\r\\rTitle: SPAWNing Structural Priming Predictions from a Cognitively Motivated\\r  Parser\\rAuthors: Grusha Prasad and Tal Linzen\\rCategories: cs.CL\\r\\\\\\\\\\r  Structural priming is a widely used psycholinguistic paradigm to study human\\rsentence representations. In this work we propose a framework for using\\rempirical priming patterns to build a theory characterizing the structural\\rrepresentations humans construct when processing sentences. This framework uses\\ra new cognitively motivated parser, SPAWN, to generate quantitative priming\\rpredictions from theoretical syntax and evaluate these predictions with\\rempirical human behavior. As a case study, we apply this framework to study\\rreduced relative clause representations in English. We use SPAWN to generate\\rpriming predictions from two theoretical accounts which make different\\rassumptions about the structure of relative clauses. We find that the\\rpredictions from only one of these theories (Participial-Phase) align with\\rempirical priming patterns, thus highlighting which assumptions about relative\\rclause better capture human sentence representations.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07202 ,  444kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07230\\rDate: Tue, 12 Mar 2024 00:58:19 GMT   (821kb,D)\\r\\rTitle: Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked\\r  Preferences\\rAuthors: Pulkit Pattnaik and Rishabh Maheshwary and Kelechi Ogueji and Vikas\\r  Yadav and Sathwik Tejaswi Madhusudhan\\rCategories: cs.CL cs.AI cs.LG\\rComments: Work in progress\\r\\\\\\\\\\r  Direct Preference Optimization (DPO) is an effective technique that leverages\\rpairwise preference data (usually one chosen and rejected response pair per\\ruser prompt) to align LLMs to human preferences. In practice, multiple\\rresponses can exist for a given prompt with varying quality relative to each\\rother. With availability of such quality ratings for multiple responses, we\\rpropose utilizing these responses to create multiple preference pairs for a\\rgiven prompt. Our work focuses on systematically using the constructed multiple\\rpreference pair in DPO training via curriculum learning methodology. In\\rparticular, we order these multiple pairs of preference data from easy to hard\\r(emulating curriculum training) according to various criteria. We show detailed\\rcomparisons of our proposed approach to the standard single-pair DPO setting.\\rOur method, which we call Curry-DPO consistently shows increased performance\\rgains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,\\rhighlighting its effectiveness. More specifically, Curry-DPO achieves a score\\rof 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs\\rwith similar parameter size. Curry-DPO also achieves the highest adjusted win\\rrates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and\\r87.9% respectively) in our experiments, with notable gains of upto 7.5% when\\rcompared to standard DPO technique.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07230 ,  821kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07260\\rDate: Tue, 12 Mar 2024 02:37:11 GMT   (784kb,D)\\r\\rTitle: CKERC : Joint Large Language Models with Commonsense Knowledge for\\r  Emotion Recognition in Conversation\\rAuthors: Yumeng Fu\\rCategories: cs.CL\\r\\\\\\\\\\r  Emotion recognition in conversation (ERC) is a task which predicts the\\remotion of an utterance in the context of a conversation. It tightly depends on\\rdialogue context, speaker identity information, multiparty dialogue scenario\\rand so on. However, the state-of-the-art method (instructERC) solely\\ridentifying speaker, and ignores commonsense knowledge(i.e., reaction of the\\rlisteners and intention of the speaker, etc.) behind speakers during a\\rconversation, which can deeply mine speaker information. To this end, we\\rpropose a novel joint large language models with commonsense knowledge\\rframework for emotion recognition in conversation, namely CKERC.We design\\rprompts to generate interlocutors' commonsense based on historical utterances\\rwith large language model. And we use the interlocutor commonsense\\ridentification task for LLM pre-training to fine-tune speaker implicit clues\\rinformation.By solving above challenge, our method achieve state-of-the-art.We\\rextensive experiment on three widely-used datasets, i.e., IEMOCAP, MELD,\\rEmoryNLP, demonstrate our method superiority. Also, we conduct in-depth\\ranalysis and further demonstrate the effectiveness of commonsense knowledge in\\rERC task in large language model.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07260 ,  784kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07279\\rDate: Tue, 12 Mar 2024 03:17:59 GMT   (3097kb,D)\\r\\rTitle: A Survey of Explainable Knowledge Tracing\\rAuthors: Yanhong Bai, Jiabao Zhao, Tingjiang Wei, Qing Cai, Liang He\\rCategories: cs.CL\\r\\\\\\\\\\r  With the long term accumulation of high quality educational data, artificial\\rintelligence has shown excellent performance in knowledge tracing. However, due\\rto the lack of interpretability and transparency of some algorithms, this\\rapproach will result in reduced stakeholder trust and a decreased acceptance of\\rintelligent decisions. Therefore, algorithms need to achieve high accuracy, and\\rusers need to understand the internal operating mechanism and provide reliable\\rexplanations for decisions. This paper thoroughly analyzes the interpretability\\rof KT algorithms. First, the concepts and common methods of explainable\\rartificial intelligence and knowledge tracing are introduced. Next, explainable\\rknowledge tracing models are classified into two categories: transparent models\\rand black box models. Then, the interpretable methods used are reviewed from\\rthree stages: ante hoc interpretable methods, post hoc interpretable methods,\\rand other dimensions. It is worth noting that current evaluation methods for\\rexplainable knowledge tracing are lacking. Hence, contrast and deletion\\rexperiments are conducted to explain the prediction results of the deep\\rknowledge tracing model on the ASSISTment2009 by using three XAI methods.\\rMoreover, this paper offers some insights into evaluation methods from the\\rperspective of educational stakeholders. This paper provides a detailed and\\rcomprehensive review of the research on explainable knowledge tracing, aiming\\rto offer some basis and inspiration for researchers interested in the\\rinterpretability of knowledge tracing.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07279 ,  3097kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07311\\rDate: Tue, 12 Mar 2024 04:47:29 GMT   (897kb,D)\\r\\rTitle: Knowledge Graph Large Language Model (KG-LLM) for Link Prediction\\rAuthors: Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Mengnan Du, Yongfeng\\r  Zhang\\rCategories: cs.CL cs.LG\\rComments: 24 pages, 3 figures, submit to ECML PKDD 2024\\r\\\\\\\\\\r  The task of predicting multiple links within knowledge graphs (KGs) stands as\\ra challenge in the field of knowledge graph analysis, a challenge increasingly\\rresolvable due to advancements in natural language processing (NLP) and KG\\rembedding techniques. This paper introduces a novel methodology, the Knowledge\\rGraph Large Language Model Framework (KG-LLM), which leverages pivotal NLP\\rparadigms, including chain-of-thought (CoT) prompting and in-context learning\\r(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a\\rCoT prompt, our framework is designed to discern and learn the latent\\rrepresentations of entities and their interrelations. To show the efficacy of\\rthe KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)\\rwithin this framework, employing both non-ICL and ICL tasks for a comprehensive\\revaluation. Further, we explore the framework's potential to provide LLMs with\\rzero-shot capabilities for handling previously unseen prompts. Our experimental\\rfindings discover that integrating ICL and CoT not only augments the\\rperformance of our approach but also significantly boosts the models'\\rgeneralization capacity, thereby ensuring more precise predictions in\\runfamiliar scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07311 ,  897kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07321\\rDate: Tue, 12 Mar 2024 05:15:21 GMT   (363kb,D)\\r\\rTitle: GPT-generated Text Detection: Benchmark Dataset and Tensor-based\\r  Detection Method\\rAuthors: Zubair Qazi, William Shiao, and Evangelos E. Papalexakis\\rCategories: cs.CL\\rComments: 4 pages, 2 figures, published in the WWW 2024 Short Papers Track\\r\\\\\\\\\\r  As natural language models like ChatGPT become increasingly prevalent in\\rapplications and services, the need for robust and accurate methods to detect\\rtheir output is of paramount importance. In this paper, we present GPT Reddit\\rDataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text\\rdetection dataset designed to assess the performance of detection models in\\ridentifying generated responses from ChatGPT. The dataset consists of a diverse\\rcollection of context-prompt pairs based on Reddit, with human-generated and\\rChatGPT-generated responses. We provide an analysis of the dataset's\\rcharacteristics, including linguistic diversity, context complexity, and\\rresponse quality. To showcase the dataset's utility, we benchmark several\\rdetection methods on it, demonstrating their efficacy in distinguishing between\\rhuman and ChatGPT-generated responses. This dataset serves as a resource for\\revaluating and advancing detection techniques in the context of ChatGPT and\\rcontributes to the ongoing efforts to ensure responsible and trustworthy\\rAI-driven communication on the internet. Finally, we propose GpTen, a novel\\rtensor-based GPT text detection method that is semi-supervised in nature since\\rit only has access to human-generated text and performs on par with\\rfully-supervised baselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07321 ,  363kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07342\\rDate: Tue, 12 Mar 2024 06:01:04 GMT   (2887kb,D)\\r\\rTitle: Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive\\r  Learning\\rAuthors: Qiao Sun, Liujia Yang, Minghao Ma, Nanyang Ye, Qinying Gu\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of\\rfine-grained sentiment analysis, aiming to extract structured sentiment\\rtriplets from unstructured textual data. Existing approaches to ASTE often\\rcomplicate the task with additional structures or external data. In this\\rresearch, we propose a novel tagging scheme and employ a contrastive learning\\rapproach to mitigate these challenges. The proposed approach demonstrates\\rcomparable or superior performance in comparison to state-of-the-art\\rtechniques, while featuring a more compact design and reduced computational\\roverhead. Notably, even in the era of Large Language Models (LLMs), our method\\rexhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning\\rscenarios. This study also provides valuable insights for the advancement of\\rASTE techniques within the paradigm of large language models.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07342 ,  2887kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07350\\rDate: Tue, 12 Mar 2024 06:16:33 GMT   (254kb)\\r\\rTitle: KEBench: A Benchmark on Knowledge Editing for Large Vision-Language\\r  Models\\rAuthors: Han Huang, Haitian Zhong, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan\\rCategories: cs.CL cs.AI cs.CV\\rComments: 13 pages\\r\\\\\\\\\\r  Currently, little research has been done on knowledge editing for Large\\rVision-Language Models (LVLMs). Editing LVLMs faces the challenge of\\reffectively integrating diverse modalities (image and text) while ensuring\\rcoherent and contextually relevant modifications. An existing benchmark has\\rthree metrics (Reliability, Locality and Generality) to measure knowledge\\rediting for LVLMs. However, the benchmark falls short in the quality of\\rgenerated images used in evaluation and cannot assess whether models\\reffectively utilize edited knowledge in relation to the associated content. We\\radopt different data collection methods to construct a new benchmark,\\r$\\\\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive\\revaluation. Leveraging a multimodal knowledge graph, our image data exhibits\\rclear directionality towards entities. This directional aspect can be further\\rutilized to extract entity-related knowledge and form editing data. We\\rconducted experiments of different editing methods on five LVLMs, and\\rthoroughly analyze how these methods impact the models. The results reveal\\rstrengths and deficiencies of these methods and, hopefully, provide insights\\rinto potential avenues for future research.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07350 ,  254kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07378\\rDate: Tue, 12 Mar 2024 07:31:18 GMT   (318kb,D)\\r\\rTitle: SVD-LLM: Truncation-aware Singular Value Decomposition for Large\\r  Language Model Compression\\rAuthors: Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang\\rCategories: cs.CL cs.LG\\rComments: Under Review\\r\\\\\\\\\\r  The advancements in Large Language Models (LLMs) have been hindered by their\\rsubstantial sizes, which necessitate LLM compression methods for practical\\rdeployment. Singular Value Decomposition (SVD) offers a promising solution for\\rLLM compression. However, state-of-the-art SVD-based LLM compression methods\\rhave two key limitations: truncating smaller singular values may lead to higher\\rcompression loss, and the lack of update on the remaining model parameters\\rafter SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM\\rcompression method that addresses the limitations of existing methods. SVD-LLM\\rincorporates a truncation-aware data whitening strategy to ensure a direct\\rmapping between singular values and compression loss. Moreover, SVD-LLM adopts\\ra layer-wise closed-form model parameter update strategy to compensate for\\raccuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total\\rof 11 datasets and seven models from three different LLM families at four\\rdifferent scales. Our results demonstrate the superiority of SVD-LLM over\\rstate-of-the-arts, especially at high model compression ratios. The source code\\ris available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07378 ,  318kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07384\\rDate: Tue, 12 Mar 2024 07:45:33 GMT   (195kb,D)\\r\\rTitle: SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large\\r  Language Models by Summarizing Training Trajectories of Small Models\\rAuthors: Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, Baharan Mirzasoleiman\\rCategories: cs.CL cs.AI cs.LG\\r\\\\\\\\\\r  Despite the effectiveness of data selection for large language models (LLMs)\\rduring pretraining and instruction fine-tuning phases, improving data\\refficiency in supervised fine-tuning (SFT) for specialized domains poses\\rsignificant challenges due to the complexity of fine-tuning data. To bridge\\rthis gap, we introduce an effective and scalable data selection method for SFT,\\rSmallToLarge (S2L), which leverages training trajectories from small models to\\rguide the data selection for larger models. We demonstrate through extensive\\rexperiments that S2L significantly improves data efficiency in SFT for\\rmathematical problem-solving, reducing the training data to just 11% of the\\roriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\\rperformance while outperforming state-of-the-art data selection algorithms by\\ran average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\\rselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\\rchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\\ral., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\\r(Johnson et al., 2016), S2L again outperforms training on the full dataset\\rusing only 50% of the data. Notably, S2L can perform data selection using a\\rreference model 40x smaller than the target model, proportionally reducing the\\rcost of data selection.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07384 ,  195kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07398\\rDate: Tue, 12 Mar 2024 08:13:52 GMT   (1010kb,D)\\r\\rTitle: Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs\\rAuthors: Tianqing Fang, Zeming Chen, Yangqiu Song, Antoine Bosselut\\rCategories: cs.CL cs.AI\\rComments: 19 pages\\r\\\\\\\\\\r  Event commonsense reasoning requires the ability to reason about the\\rrelationship between events, as well as infer implicit context underlying that\\rrelationship. However, data scarcity makes it challenging for language models\\rto learn to generate commonsense inferences for contexts and questions\\rinvolving interactions between complex events. To address this demand, we\\rpresent COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop\\rlogical queries (e.g., the joint effect or cause of both event A and B, or the\\reffect of the effect of event C) from an existing commonsense knowledge graph\\r(CSKG), and verbalizing them using handcrafted rules and large language models\\rinto multiple-choice and text generation questions. Our experiments show that\\rlanguage models trained on COM2 exhibit significant improvements in complex\\rreasoning ability, resulting in enhanced zero-shot performance in both\\rin-domain and out-of-domain tasks for question answering and generative\\rcommonsense reasoning, without expensive human annotations.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07398 ,  1010kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07440\\rDate: Tue, 12 Mar 2024 09:32:25 GMT   (1122kb,D)\\r\\rTitle: Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A\\r  Brain-Inspired Method for Parameter-Efficient Fine-Tuning\\rAuthors: Yao Liang, Yuwei Wang, Yi Zeng\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have\\rbeen proven to significantly enhance model performance on a variety of\\rdownstream tasks and effectively control the output behaviors of LPLMs. Recent\\rstudies have proposed numerous methods for fine-tuning a small number of\\rparameters based on open-source LPLMs, reducing the demand for computational\\rand storage resources. Among these, reparameterization fine-tuning methods\\rrepresented by LoRA (Low-Rank Adaptation) have gained popularity. We find that\\ralthough these methods perform well in many aspects, there is still\\rconsiderable room for improvement in terms of complex task adaptability,\\rperformance, stability, and algorithm complexity. In response to this, inspired\\rby the idea that the functions of the brain are shaped by its geometric\\rstructure, this paper integrates this idea into LoRA technology and proposes a\\rnew matrix transformation-based reparameterization method for efficient\\rfine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).\\rMTLoRA aims to dynamically alter its spatial geometric structure by applying a\\rtransformation-matrix T to perform linear transformations, such as rotation,\\rscaling, and translation, on the task-specific parameter matrix, generating new\\rmatrix feature patterns (eigenvectors) to mimic the fundamental influence of\\rcomplex geometric structure feature patterns in the brain on functions, thereby\\renhancing the model's performance in downstream tasks. In Natural Language\\rUnderstanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and\\rthe results reveal that MTLoRA achieves an overall performance increase of\\rabout 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,\\rMTLoRA improves performance by an average of 0.95% and 0.31% in the DART and\\rWebNLG tasks, respectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07440 ,  1122kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07544\\rDate: Tue, 12 Mar 2024 11:32:30 GMT   (9135kb,D)\\r\\rTitle: MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki\\rAuthors: Timothee Mickus, Stig-Arne Gr\\\\onroos, Joseph Attieh, Michele Boggia,\\r  Ona De Gibert, Shaoxiong Ji, Niki Andreas Lopi, Alessandro Raganato, Ra\\\\'ul\\r  V\\\\'azquez, J\\\\org Tiedemann\\rCategories: cs.CL\\rComments: Presented as a demo at EACL 2024\\r\\\\\\\\\\r  NLP in the age of monolithic large language models is approaching its limits\\rin terms of size and information that can be handled. The trend goes to\\rmodularization, a necessary step into the direction of designing smaller\\rsub-networks and components with specialized functionality. In this paper, we\\rpresent the MAMMOTH toolkit: a framework designed for training massively\\rmultilingual modular machine translation systems at scale, initially derived\\rfrom OpenNMT-py and then adapted to ensure efficient training across\\rcomputation clusters. We showcase its efficiency across clusters of A100 and\\rV100 NVIDIA GPUs, and discuss our design philosophy and plans for future\\rinformation. The toolkit is publicly available online.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07544 ,  9135kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07556\\rDate: Tue, 12 Mar 2024 11:40:44 GMT   (3847kb,D)\\r\\rTitle: Truth-Aware Context Selection: Mitigating the Hallucinations of Large\\r  Language Models Being Misled by Untruthful Contexts\\rAuthors: Tian Yu, Shaolei Zhang and Yang Feng\\rCategories: cs.CL\\rComments: Code is available at: https://github.com/ictnlp/TACS\\r\\\\\\\\\\r  Although large language models (LLMs) have demonstrated impressive text\\rgeneration capabilities, they are easily misled by the untruthful context\\rprovided by users or knowledge argumentation tools, thereby producing\\rhallucinations. To alleviate the LLMs from being misled by untruthful\\rinformation and take advantage of knowledge argumentation, we propose\\rTruth-Aware Context Selection (TACS), a lightweight method to shield untruthful\\rcontext from the inputs. TACS begins by performing truth detection on the input\\rcontext, leveraging the parameterized knowledge within the LLM. Subsequently,\\rit constructs a corresponding attention mask based on the truthfulness of each\\rposition, selecting the truthful context and discarding the untruthful context.\\rAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\\rto further study the LLMs' ability to accept truthful information and resist\\runtruthful information. Experimental results show that TACS can effectively\\rfilter information in context and significantly improve the overall quality of\\rLLMs' responses when presented with misleading information.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07556 ,  3847kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07557\\rDate: Tue, 12 Mar 2024 11:41:51 GMT   (6967kb,D)\\r\\rTitle: SIFiD: Reassess Summary Factual Inconsistency Detection with LLM\\rAuthors: Jiuding Yang, Hui Liu, Weidong Guo, Zhuwei Rao, Yu Xu, Di Niu\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  Ensuring factual consistency between the summary and the original document is\\rparamount in summarization tasks. Consequently, considerable effort has been\\rdedicated to detecting inconsistencies. With the advent of Large Language\\rModels (LLMs), recent studies have begun to leverage their advanced language\\runderstanding capabilities for inconsistency detection. However, early attempts\\rhave shown that LLMs underperform traditional models due to their limited\\rability to follow instructions and the absence of an effective detection\\rmethodology. In this study, we reassess summary inconsistency detection with\\rLLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in\\rLLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency\\rDetection with Filtered Document) that identify key sentences within documents\\rby either employing natural language inference or measuring semantic similarity\\rbetween summaries and documents.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07557 ,  6967kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07567\\rDate: Tue, 12 Mar 2024 11:53:27 GMT   (9520kb,D)\\r\\rTitle: Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource\\r  Agglutinative Data-to-Text Generation\\rAuthors: Francois Meyer and Jan Buys\\rCategories: cs.CL\\r\\\\\\\\\\r  Most data-to-text datasets are for English, so the difficulties of modelling\\rdata-to-text for low-resource languages are largely unexplored. In this paper\\rwe tackle data-to-text for isiXhosa, which is low-resource and agglutinative.\\rWe introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of\\rWebNLG, which presents a new linguistic context that shifts modelling demands\\rto subword-driven techniques. We also develop an evaluation framework for T2X\\rthat measures how accurately generated text describes the data. This enables\\rfuture users of T2X to go beyond surface-level metrics in evaluation. On the\\rmodelling side we explore two classes of methods - dedicated data-to-text\\rmodels trained from scratch and pretrained language models (PLMs). We propose a\\rnew dedicated architecture aimed at agglutinative data-to-text, the Subword\\rSegmental Pointer Generator (SSPG). It jointly learns to segment words and copy\\rentities, and outperforms existing dedicated models for 2 agglutinative\\rlanguages (isiXhosa and Finnish). We investigate pretrained solutions for T2X,\\rwhich reveals that standard PLMs come up short. Fine-tuning machine translation\\rmodels emerges as the best method overall. These findings underscore the\\rdistinct challenge presented by T2X: neither well-established data-to-text\\rarchitectures nor customary pretrained methodologies prove optimal. We conclude\\rwith a qualitative analysis of generation errors and an ablation study.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07567 ,  9520kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07581\\rDate: Tue, 12 Mar 2024 12:10:18 GMT   (653kb,D)\\r\\rTitle: LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced\\r  Personality Detection Model\\rAuthors: Linmei Hu, Hongyu He, Duokang Wang, Ziwang Zhao, Yingxia Shao, Liqiang\\r  Nie\\rCategories: cs.CL\\r\\\\\\\\\\r  Personality detection aims to detect one's personality traits underlying in\\rsocial media posts. One challenge of this task is the scarcity of ground-truth\\rpersonality traits which are collected from self-report questionnaires. Most\\rexisting methods learn post features directly by fine-tuning the pre-trained\\rlanguage models under the supervision of limited personality labels. This leads\\rto inferior quality of post features and consequently affects the performance.\\rIn addition, they treat personality traits as one-hot classification labels,\\roverlooking the semantic information within them. In this paper, we propose a\\rlarge language model (LLM) based text augmentation enhanced personality\\rdetection model, which distills the LLM's knowledge to enhance the small model\\rfor personality detection, even when the LLM fails in this task. Specifically,\\rwe enable LLM to generate post analyses (augmentations) from the aspects of\\rsemantic, sentiment, and linguistic, which are critical for personality\\rdetection. By using contrastive learning to pull them together in the embedding\\rspace, the post encoder can better capture the psycho-linguistic information\\rwithin the post representations, thus improving personality detection.\\rFurthermore, we utilize the LLM to enrich the information of personality labels\\rfor enhancing the detection performance. Experimental results on the benchmark\\rdatasets demonstrate that our model outperforms the state-of-the-art methods on\\rpersonality detection.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07581 ,  653kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07678\\rDate: Tue, 12 Mar 2024 14:12:59 GMT   (983kb,D)\\r\\rTitle: MoralBERT: Detecting Moral Values in Social Discourse\\rAuthors: Vjosa Preniqi, Iacopo Ghinassi, Kyriaki Kalimeri, Charalampos Saitis\\rCategories: cs.CL cs.CY\\r\\\\\\\\\\r  Morality plays a fundamental role in how we perceive information while\\rgreatly influencing our decisions and judgements. Controversial topics,\\rincluding vaccination, abortion, racism, and sexuality, often elicit opinions\\rand attitudes that are not solely based on evidence but rather reflect moral\\rworldviews. Recent advances in natural language processing have demonstrated\\rthat moral values can be gauged in human-generated textual content. Here, we\\rdesign a range of language representation models fine-tuned to capture exactly\\rthe moral nuances in text, called MoralBERT. We leverage annotated moral data\\rfrom three distinct sources: Twitter, Reddit, and Facebook user-generated\\rcontent covering various socially relevant topics. This approach broadens\\rlinguistic diversity and potentially enhances the models' ability to comprehend\\rmorality in various contexts. We also explore a domain adaptation technique and\\rcompare it to the standard fine-tuned BERT model, using two different\\rframeworks for moral prediction: single-label and multi-label. We compare\\rin-domain approaches with conventional models relying on lexicon-based\\rtechniques, as well as a Machine Learning classifier with Word2Vec\\rrepresentation. Our results showed that in-domain prediction models\\rsignificantly outperformed traditional models. While the single-label setting\\rreaches a higher accuracy than previously achieved for the task when using BERT\\rpretrained models. Experiments in an out-of-domain setting, instead, suggest\\rthat further work is needed for existing domain adaptation techniques to\\rgeneralise between different social media platforms, especially for the\\rmulti-label task. The investigations and outcomes from this study pave the way\\rfor further exploration, enabling a more profound comprehension of moral\\rnarratives about controversial social issues.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07678 ,  983kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07691\\rDate: Tue, 12 Mar 2024 14:34:08 GMT   (8428kb,D)\\r\\rTitle: Reference-free Monolithic Preference Optimization with Odds Ratio\\rAuthors: Jiwoo Hong, Noah Lee, James Thorne\\rCategories: cs.CL cs.AI\\rComments: Preprint\\r\\\\\\\\\\r  While recent preference alignment algorithms for language models have\\rdemonstrated promising results, supervised fine-tuning (SFT) remains imperative\\rfor achieving successful convergence. In this paper, we study the crucial role\\rof SFT within the context of preference alignment, emphasizing that a minor\\rpenalty for the disfavored generation style is sufficient for\\rpreference-aligned SFT. Building on this foundation, we introduce a\\rstraightforward and innovative reference model-free monolithic odds ratio\\rpreference optimization algorithm, ORPO, eliminating the necessity for an\\radditional preference alignment phase. We demonstrate, both empirically and\\rtheoretically, that the odds ratio is a sensible choice for contrasting favored\\rand disfavored styles during SFT across the diverse sizes from 125M to 7B.\\rSpecifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with\\rORPO on the UltraFeedback alone surpasses the performance of state-of-the-art\\rlanguage models with more than 7B and 13B parameters: achieving up to 12.20% on\\r$\\\\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12.\\rWe release code and model checkpoints for Mistral-ORPO-$\\\\alpha$ (7B) and\\rMistral-ORPO-$\\\\beta$ (7B).\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07691 ,  8428kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07693\\rDate: Tue, 12 Mar 2024 14:37:03 GMT   (1507kb,D)\\r\\rTitle: Large, Small or Both: A Novel Data Augmentation Framework Based on\\r  Language Models for Debiasing Opinion Summarization\\rAuthors: Yanyue Zhang, Pengfei Li, Yilong Lai and Deyu Zhou\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  As more than 70$\\\\%$ of reviews in the existing opinion summary data set are\\rpositive, current opinion summarization approaches are reluctant to generate\\rnegative summaries given the input of negative texts. To address such sentiment\\rbias, a direct approach without the over-reliance on a specific framework is to\\rgenerate additional data based on large language models to balance the\\remotional distribution of the dataset. However, data augmentation based on\\rlarge language models faces two disadvantages: 1) the potential issues or\\rtoxicity in the augmented data; 2) the expensive costs. Therefore, in this\\rpaper, we propose a novel data augmentation framework based on both large and\\rsmall language models for debiasing opinion summarization. In specific, a small\\rsize of synthesized negative reviews is obtained by rewriting the positive text\\rvia a large language model. Then, a disentangle reconstruction model is trained\\rbased on the generated data. After training, a large amount of synthetic data\\rcan be obtained by decoding the new representation obtained from the\\rcombination of different sample representations and filtering based on\\rconfusion degree and sentiment classification. Experiments have proved that our\\rframework can effectively alleviate emotional bias same as using only large\\rmodels, but more economically.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07693 ,  1507kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07708\\rDate: Tue, 12 Mar 2024 14:51:57 GMT   (339kb,D)\\r\\rTitle: Improving Reinforcement Learning from Human Feedback Using Contrastive\\r  Rewards\\rAuthors: Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, Yang\\r  Liu\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm\\rused to align large language models (LLMs) with human preferences. Yet existing\\rRLHF heavily relies on accurate and informative reward models, which are\\rvulnerable and sensitive to noise from various sources, e.g. human labeling\\rerrors, making the pipeline fragile. In this work, we improve the effectiveness\\rof the reward model by introducing a penalty term on the reward, named as\\r\\\\textit{contrastive rewards}. %Contrastive rewards Our approach involves two\\rsteps: (1) an offline sampling step to obtain responses to prompts that serve\\ras baseline calculation and (2) a contrastive reward calculated using the\\rbaseline responses and used in the Proximal Policy Optimization (PPO) step. We\\rshow that contrastive rewards enable the LLM to penalize reward uncertainty,\\rimprove robustness, encourage improvement over baselines, calibrate according\\rto task difficulty, and reduce variance in PPO. We show empirically contrastive\\rrewards can improve RLHF substantially, evaluated by both GPTs and humans, and\\rour method consistently outperforms strong baselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07708 ,  339kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07714\\rDate: Tue, 12 Mar 2024 14:57:40 GMT   (765kb,D)\\r\\rTitle: StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\\r  Learning of Large Language Models\\rAuthors: Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li,\\r  Zhiyuan Liu, Maosong Sun, Yang Liu\\rCategories: cs.CL\\r\\\\\\\\\\r  Large Language Models (LLMs) have witnessed remarkable advancements in recent\\ryears, prompting the exploration of tool learning, which integrates LLMs with\\rexternal tools to address diverse real-world challenges. Assessing the\\rcapability of LLMs to utilise tools necessitates large-scale and stable\\rbenchmarks. However, previous works relied on either hand-crafted online tools\\rwith limited scale, or large-scale real online APIs suffering from instability\\rof API status. To address this problem, we introduce StableToolBench, a\\rbenchmark evolving from ToolBench, proposing a virtual API server and stable\\revaluation system. The virtual API server contains a caching system and API\\rsimulators which are complementary to alleviate the change in API status.\\rMeanwhile, the stable evaluation system designs solvable pass and win rates\\rusing GPT-4 as the automatic evaluator to eliminate the randomness during\\revaluation. Experimental results demonstrate the stability of StableToolBench,\\rand further discuss the effectiveness of API simulators, the caching system,\\rand the evaluator system.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07714 ,  765kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07726\\rDate: Tue, 12 Mar 2024 15:06:22 GMT   (8830kb,D)\\r\\rTitle: SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and\\r  Related Observable Overgeneration Mistakes\\rAuthors: Timothee Mickus, Elaine Zosa, Ra\\\\'ul V\\\\'azquez, Teemu Vahtola, J\\\\org\\r  Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki\\rCategories: cs.CL\\rComments: SemEval 2024 shared task. Pre-review version\\r\\\\\\\\\\r  This paper presents the results of the SHROOM, a shared task focused on\\rdetecting hallucinations: outputs from natural language generation (NLG)\\rsystems that are fluent, yet inaccurate. Such cases of overgeneration put in\\rjeopardy many NLG applications, where correctness is often mission-critical.\\rThe shared task was conducted with a newly constructed dataset of 4000 model\\routputs labeled by 5 annotators each, spanning 3 NLP tasks: machine\\rtranslation, paraphrase generation and definition modeling.\\r  The shared task was tackled by a total of 58 different users grouped in 42\\rteams, out of which 27 elected to write a system description paper;\\rcollectively, they submitted over 300 prediction sets on both tracks of the\\rshared task. We observe a number of key trends in how this approach was tackled\\r-- many participants rely on a handful of model, and often rely either on\\rsynthetic data for fine-tuning or zero-shot prompting strategies. While a\\rmajority of the teams did outperform our proposed baseline system, the\\rperformances of top-scoring systems are still consistent with a random handling\\rof the more challenging items.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07726 ,  8830kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07747\\rDate: Tue, 12 Mar 2024 15:32:39 GMT   (1022kb,D)\\r\\rTitle: FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese\\r  Large Language Models\\rAuthors: Yan Liu, Renren Jin, Lin Shi, Zheng Yao, Deyi Xiong\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  To thoroughly assess the mathematical reasoning abilities of Large Language\\rModels (LLMs), we need to carefully curate evaluation datasets covering diverse\\rmathematical concepts and mathematical problems at different difficulty levels.\\rIn pursuit of this objective, we propose FineMath in this paper, a fine-grained\\rmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath\\ris created to cover the major key mathematical concepts taught in elementary\\rschool math, which are further divided into 17 categories of math word\\rproblems, enabling in-depth analysis of mathematical reasoning abilities of\\rLLMs. All the 17 categories of math word problems are manually annotated with\\rtheir difficulty levels according to the number of reasoning steps required to\\rsolve these problems. We conduct extensive experiments on a wide range of LLMs\\ron FineMath and find that there is still considerable room for improvements in\\rterms of mathematical reasoning capability of Chinese LLMs. We also carry out\\ran in-depth analysis on the evaluation process and methods that have been\\roverlooked previously. These two factors significantly influence the model\\rresults and our understanding of their mathematical reasoning capabilities. The\\rdataset will be publicly available soon.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07747 ,  1022kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07794\\rDate: Tue, 12 Mar 2024 16:33:30 GMT   (1526kb,D)\\r\\rTitle: Fine-tuning Large Language Models with Sequential Instructions\\rAuthors: Hanxu Hu, Pinzhen Chen, Edoardo M. Ponti\\rCategories: cs.CL\\rComments: 11pages, 3 figures\\r\\\\\\\\\\r  Large language models (LLMs) struggle to follow a sequence of instructions in\\ra single query as they may ignore or misinterpret part of it. This impairs\\rtheir performance in complex problems whose solution requires multiple\\rintermediate steps, such as multilingual (translate then answer) and multimodal\\r(caption then answer) tasks. We empirically verify this with open-source LLMs\\ras large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential\\rinstructions in present-day data, we propose sequential instruction tuning, a\\rsimple yet effective strategy to automatically augment instruction tuning data\\rand equip LLMs with the ability to execute multiple sequential instructions.\\rAfter exploring interleaving instructions in existing datasets, such as Alpaca,\\rwith a wide range of intermediate tasks, we find that sequential\\rinstruction-tuned models consistently outperform the conventional\\rinstruction-tuned baselines in downstream tasks involving reasoning,\\rmultilingual, and multimodal abilities. To shed further light on our technique,\\rwe analyse how adversarial intermediate texts, unseen tasks, prompt\\rverbalization, number of tasks, and prompt length affect SIT. We hope that this\\rmethod will open new research avenues on instruction tuning for complex tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07794 ,  1526kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07805\\rDate: Tue, 12 Mar 2024 16:42:44 GMT   (8762kb,D)\\r\\rTitle: Beyond Memorization: The Challenge of Random Memory Access in Language\\r  Models\\rAuthors: Tongyao Zhu, Qian Liu, Liang Pang, Zhengbao Jiang, Min-Yen Kan, Min\\r  Lin\\rCategories: cs.CL cs.AI\\rComments: 8 pages, 4 figures\\r\\\\\\\\\\r  Recent developments in Language Models (LMs) have shown their effectiveness\\rin NLP tasks, particularly in knowledge-intensive tasks. However, the\\rmechanisms underlying knowledge storage and memory access within their\\rparameters remain elusive. In this paper, we investigate whether a generative\\rLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through\\rcarefully-designed synthetic tasks, covering the scenarios of full recitation,\\rselective recitation and grounded question answering, we reveal that LMs manage\\rto sequentially access their memory while encountering challenges in randomly\\raccessing memorized content. We find that techniques including recitation and\\rpermutation improve the random memory access capability of LMs. Furthermore, by\\rapplying this intervention to realistic scenarios of open-domain question\\ranswering, we validate that enhancing random access by recitation leads to\\rnotable improvements in question answering. The code to reproduce our\\rexperiments can be found at https://github.\\rcom/sail-sg/lm-random-memory-access.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07805 ,  8762kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07816\\rDate: Tue, 12 Mar 2024 16:54:58 GMT   (946kb,D)\\r\\rTitle: Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM\\rAuthors: Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria\\r  Lin, Baptiste Rozi\\\\`ere, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston,\\r  Xian Li\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  We investigate efficient methods for training Large Language Models (LLMs) to\\rpossess capabilities in multiple specialized domains, such as coding, math\\rreasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts\\rfrom a seed model, which is branched to train experts in embarrassingly\\rparallel fashion with high throughput and reduced communication cost. After\\rindividual experts are asynchronously trained, BTX brings together their\\rfeedforward parameters as experts in Mixture-of-Expert (MoE) layers and\\raverages the remaining parameters, followed by an MoE-finetuning stage to learn\\rtoken-level routing. BTX generalizes two special cases, the Branch-Train-Merge\\rmethod, which does not have the MoE finetuning stage to learn routing, and\\rsparse upcycling, which omits the stage of training experts asynchronously.\\rCompared to alternative approaches, BTX achieves the best accuracy-efficiency\\rtradeoff.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07816 ,  946kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07825\\rDate: Tue, 12 Mar 2024 17:04:28 GMT   (11626kb,D)\\r\\rTitle: The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage\\r  Brought By Model Editing\\rAuthors: Jianchen Wang, Zhouhong Gu, Zhuozhi Xiong, Hongwei Feng, Yanghua Xiao\\rCategories: cs.CL\\r\\\\\\\\\\r  Large Language Models have revolutionized numerous tasks with their\\rremarkable efficacy.However, the editing of these models, crucial for\\rrectifying outdated or erroneous information, often leads to a complex issue\\rknown as the ripple effect in the hidden space. This effect, while difficult to\\rdetect, can significantly impede the efficacy of model editing tasks and\\rdeteriorate model performance.This paper addresses this scientific challenge by\\rproposing a novel evaluation methodology, Graphical Outlier Relation based\\rAssessment(GORA), which quantitatively evaluates the adaptations of the model\\rand the subsequent impact of editing. Furthermore, we introduce the Selective\\rOutlier Re-Editing Approach(SORA), a model editing method designed to mitigate\\rthis ripple effect. Our comprehensive evaluations reveal that the ripple effect\\rin the hidden space is a significant issue in all current model editing\\rmethods. However, our proposed methods, GORA and SORA, effectively identify and\\ralleviate this issue, respectively, contributing to the advancement of LLM\\rediting techniques.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07825 ,  11626kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07865\\rDate: Tue, 12 Mar 2024 17:55:38 GMT   (8284kb,D)\\r\\rTitle: Exploring Safety Generalization Challenges of Large Language Models via\\r  Code\\rAuthors: Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam,\\r  Lizhuang Ma\\rCategories: cs.CL cs.AI cs.CR cs.LG cs.SE\\r\\\\\\\\\\r  The rapid advancement of Large Language Models (LLMs) has brought about\\rremarkable capabilities in natural language processing but also raised concerns\\rabout their potential misuse. While strategies like supervised fine-tuning and\\rreinforcement learning from human feedback have enhanced their safety, these\\rmethods primarily focus on natural languages, which may not generalize to other\\rdomains. This paper introduces CodeAttack, a framework that transforms natural\\rlanguage inputs into code inputs, presenting a novel environment for testing\\rthe safety generalization of LLMs. Our comprehensive studies on\\rstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\\rcommon safety vulnerability of these models against code input: CodeAttack\\rconsistently bypasses the safety guardrails of all models more than 80\\\\% of the\\rtime. Furthermore, we find that a larger distribution gap between CodeAttack\\rand natural language leads to weaker safety generalization, such as encoding\\rnatural language input with data structures or using less popular programming\\rlanguages. These findings highlight new safety risks in the code domain and the\\rneed for more robust safety alignment algorithms to match the code capabilities\\rof LLMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07865 ,  8284kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07872\\rDate: Tue, 12 Mar 2024 17:59:48 GMT   (11812kb,D)\\r\\rTitle: Rethinking Generative Large Language Model Evaluation for Semantic\\r  Comprehension\\rAuthors: Fangyun Wei, Xi Chen, Lin Luo\\rCategories: cs.CL\\r\\\\\\\\\\r  Despite their sophisticated capabilities, large language models (LLMs)\\rencounter a major hurdle in effective assessment. This paper first revisits the\\rprevalent evaluation method-multiple choice question answering (MCQA), which\\rallows for straightforward accuracy measurement. Through a comprehensive\\revaluation of 24 models across 11 benchmarks, we highlight several potential\\rdrawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation\\rand the generation of open-ended responses in practical scenarios. In response,\\rwe introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5,\\rGoogle-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with\\rGPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This\\rsystem is designed to mirror real-world usage, and for this purpose, we have\\rcompiled a new benchmark called ``Real-world questions'' (RWQ), comprising\\r20,772 authentic user inquiries. Additionally, we thoroughly analyze the\\rcharacteristics of our system and compare it with prior leaderboards like\\rAlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo\\rsystem, the feasibility of registering new models, and its potential to reshape\\rLLM leaderboards.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07872 ,  11812kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07032\\rDate: Mon, 11 Mar 2024 04:56:10 GMT   (2608kb,D)\\r\\rTitle: STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning\\r  for Real-world Scene Flow\\rAuthors: Zhiyang Lu and Qinghan Chen and Ming Cheng\\rCategories: cs.CV cs.AI\\rComments: 10 pages, 8 figures, CVPR template\\r\\\\\\\\\\r  Scene flow prediction is a crucial underlying task in understanding dynamic\\rscenes as it offers fundamental motion information. However, contemporary scene\\rflow methods encounter three major challenges. Firstly, flow estimation solely\\rbased on local receptive fields lacks long-dependency matching of point pairs.\\rTo address this issue, we propose global attentive flow embedding to match\\rall-to-all point pairs in both feature space and Euclidean space, providing\\rglobal initialization before local refinement. Secondly, there are deformations\\rexisting in non-rigid objects after warping, which leads to variations in the\\rspatiotemporal relation between the consecutive frames. For a more precise\\restimation of residual flow, a spatial temporal feature re-embedding module is\\rdevised to acquire the sequence features after deformation. Furthermore,\\rprevious methods perform poor generalization due to the significant domain gap\\rbetween the synthesized and LiDAR-scanned datasets. We leverage novel domain\\radaptive losses to effectively bridge the gap of motion inference from\\rsynthetic to real-world. Experiments demonstrate that our approach achieves\\rstate-of-the-art performance across various datasets, with particularly\\routstanding results on real-world LiDAR-scanned datasets. Our code is available\\rat https://github.com/O-VIGIA/StarFlow.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07032 ,  2608kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07071\\rDate: Mon, 11 Mar 2024 18:02:52 GMT   (22599kb,D)\\r\\rTitle: LISO: Lidar-only Self-Supervised 3D Object Detection\\rAuthors: Stefan Baur, Frank Moosmann, Andreas Geiger\\rCategories: cs.CV\\r\\\\\\\\\\r  3D object detection is one of the most important components in any\\rSelf-Driving stack, but current state-of-the-art (SOTA) lidar object detectors\\rrequire costly & slow manual annotation of 3D bounding boxes to perform well.\\rRecently, several methods emerged to generate pseudo ground truth without human\\rsupervision, however, all of these methods have various drawbacks: Some methods\\rrequire sensor rigs with full camera coverage and accurate calibration, partly\\rsupplemented by an auxiliary optical flow engine. Others require expensive\\rhigh-precision localization to find objects that disappeared over multiple\\rdrives. We introduce a novel self-supervised method to train SOTA lidar object\\rdetection networks which works on unlabeled sequences of lidar point clouds\\ronly, which we call trajectory-regularized self-training. It utilizes a SOTA\\rself-supervised lidar scene flow network under the hood to generate, track, and\\riteratively refine pseudo ground truth. We demonstrate the effectiveness of our\\rapproach for multiple SOTA object detection networks across multiple real-world\\rdatasets. Code will be released.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07071 ,  22599kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07113\\rDate: Mon, 11 Mar 2024 19:06:04 GMT   (4290kb,D)\\r\\rTitle: Class Imbalance in Object Detection: An Experimental Diagnosis and Study\\r  of Mitigation Strategies\\rAuthors: Nieves Crasto\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Object detection, a pivotal task in computer vision, is frequently hindered\\rby dataset imbalances, particularly the under-explored issue of\\rforeground-foreground class imbalance. This lack of attention to\\rforeground-foreground class imbalance becomes even more pronounced in the\\rcontext of single-stage detectors. This study introduces a benchmarking\\rframework utilizing the YOLOv5 single-stage detector to address the problem of\\rforeground-foreground class imbalance. We crafted a novel 10-class long-tailed\\rdataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common\\rreal-world detection scenarios with a limited number of object classes. Against\\rthis backdrop, we scrutinized three established techniques: sampling, loss\\rweighing, and data augmentation. Our comparative analysis reveals that sampling\\rand loss reweighing methods, while shown to be beneficial in two-stage detector\\rsettings, do not translate as effectively in improving YOLOv5's performance on\\rthe COCO-ZIPF dataset. On the other hand, data augmentation methods,\\rspecifically mosaic and mixup, significantly enhance the model's mean Average\\rPrecision (mAP), by introducing more variability and complexity into the\\rtraining data. (Code available:\\rhttps://github.com/craston/object_detection_cib)\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07113 ,  4290kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07142\\rDate: Mon, 11 Mar 2024 20:23:59 GMT   (7580kb,D)\\r\\rTitle: One Category One Prompt: Dataset Distillation using Diffusion Models\\rAuthors: Ali Abbasi, Ashkan Shahbazi, Hamed Pirsiavash, Soheil Kolouri\\rCategories: cs.CV cs.CL cs.LG\\r\\\\\\\\\\r  The extensive amounts of data required for training deep neural networks pose\\rsignificant challenges on storage and transmission fronts. Dataset distillation\\rhas emerged as a promising technique to condense the information of massive\\rdatasets into a much smaller yet representative set of synthetic samples.\\rHowever, traditional dataset distillation approaches often struggle to scale\\reffectively with high-resolution images and more complex architectures due to\\rthe limitations in bi-level optimization. Recently, several works have proposed\\rexploiting knowledge distillation with decoupled optimization schemes to scale\\rup dataset distillation. Although these methods effectively address the\\rscalability issue, they rely on extensive image augmentations requiring the\\rstorage of soft labels for augmented images. In this paper, we introduce\\rDataset Distillation using Diffusion Models (D3M) as a novel paradigm for\\rdataset distillation, leveraging recent advancements in generative\\rtext-to-image foundation models. Our approach utilizes textual inversion, a\\rtechnique for fine-tuning text-to-image generative models, to create concise\\rand informative representations for large datasets. By employing these learned\\rtext prompts, we can efficiently store and infer new samples for introducing\\rdata variability within a fixed memory budget. We show the effectiveness of our\\rmethod through extensive experiments across various computer vision benchmark\\rdatasets with different memory budgets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07142 ,  7580kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07153\\rDate: Mon, 11 Mar 2024 20:51:18 GMT   (2401kb,D)\\r\\rTitle: 2023 Low-Power Computer Vision Challenge (LPCVC) Summary\\rAuthors: Leo Chen, Benjamin Boardley, Ping Hu, Yiru Wang, Yifan Pu, Xin Jin,\\r  Yongqiang Yao, Ruihao Gong, Bo Li, Gao Huang, Xianglong Liu, Zifu Wan,\\r  Xinwang Chen, Ning Liu, Ziyi Zhang, Dongping Liu, Ruijie Shan, Zhengping Che,\\r  Fachao Zhang, Xiaofeng Mou, Jian Tang, Maxim Chuprov, Ivan Malofeev,\\r  Alexander Goncharenko, Andrey Shcherbin, Arseny Yanchenko, Sergey Alyamkin,\\r  Xiao Hu, George K. Thiruvathukal, Yung Hsiang Lu\\rCategories: cs.CV\\rComments: LPCVC 2023, website: https://lpcv.ai/\\r\\\\\\\\\\r  This article describes the 2023 IEEE Low-Power Computer Vision Challenge\\r(LPCVC). Since 2015, LPCVC has been an international competition devoted to\\rtackling the challenge of computer vision (CV) on edge devices. Most CV\\rresearchers focus on improving accuracy, at the expense of ever-growing sizes\\rof machine models. LPCVC balances accuracy with resource requirements. Winners\\rmust achieve high accuracy with short execution time when their CV solutions\\rrun on an embedded device, such as Raspberry PI or Nvidia Jetson Nano. The\\rvision problem for 2023 LPCVC is segmentation of images acquired by Unmanned\\rAerial Vehicles (UAVs, also called drones) after disasters. The 2023 LPCVC\\rattracted 60 international teams that submitted 676 solutions during the\\rsubmission window of one month. This article explains the setup of the\\rcompetition and highlights the winners' methods that improve accuracy and\\rshorten execution time.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07153 ,  2401kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07198\\rDate: Mon, 11 Mar 2024 22:46:46 GMT   (24697kb,D)\\r\\rTitle: Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions\\rAuthors: Lan Wang, Vishnu Boddeti, and Sernam Lim\\rCategories: cs.CV\\r\\\\\\\\\\r  We introduce a novel text-to-pose video editing method, ReimaginedAct. While\\rexisting video editing tasks are limited to changes in attributes, backgrounds,\\rand styles, our method aims to predict open-ended human action changes in\\rvideo. Moreover, our method can accept not only direct instructional text\\rprompts but also `what if' questions to predict possible action changes.\\rReimaginedAct comprises video understanding, reasoning, and editing modules.\\rFirst, an LLM is utilized initially to obtain a plausible answer for the\\rinstruction or question, which is then used for (1) prompting Grounded-SAM to\\rproduce bounding boxes of relevant individuals and (2) retrieving a set of pose\\rvideos that we have collected for editing human actions. The retrieved pose\\rvideos and the detected individuals are then utilized to alter the poses\\rextracted from the original video. We also employ a timestep blending module to\\rensure the edited video retains its original content except where necessary\\rmodifications are needed. To facilitate research in text-to-pose video editing,\\rwe introduce a new evaluation dataset, WhatifVideo-1.0. This dataset includes\\rvideos of different scenarios spanning a range of difficulty levels, along with\\rquestions and text prompts. Experimental results demonstrate that existing\\rvideo editing methods struggle with human action editing, while our approach\\rcan achieve effective action editing and even imaginary editing from\\rcounterfactual questions.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07198 ,  24697kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07203\\rDate: Mon, 11 Mar 2024 23:08:29 GMT   (14629kb,D)\\r\\rTitle: How to Handle Sketch-Abstraction in Sketch-Based Image Retrieval?\\rAuthors: Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath\\r  Chowdhury, Tao Xiang, Yi-Zhe Song\\rCategories: cs.CV\\rComments: Accepted in CVPR 2024. Project page available at\\r  https://subhadeepkoley.github.io/AbstractAway\\r\\\\\\\\\\r  In this paper, we propose a novel abstraction-aware sketch-based image\\rretrieval framework capable of handling sketch abstraction at varied levels.\\rPrior works had mainly focused on tackling sub-factors such as drawing style\\rand order, we instead attempt to model abstraction as a whole, and propose\\rfeature-level and retrieval granularity-level designs so that the system builds\\rinto its DNA the necessary means to interpret abstraction. On learning\\rabstraction-aware features, we for the first-time harness the rich semantic\\rembedding of pre-trained StyleGAN model, together with a novel\\rabstraction-level mapper that deciphers the level of abstraction and\\rdynamically selects appropriate dimensions in the feature matrix\\rcorrespondingly, to construct a feature matrix embedding that can be freely\\rtraversed to accommodate different levels of abstraction. For granularity-level\\rabstraction understanding, we dictate that the retrieval model should not treat\\rall abstraction-levels equally and introduce a differentiable surrogate Acc.@q\\rloss to inject that understanding into the system. Different to the\\rgold-standard triplet loss, our Acc.@q loss uniquely allows a sketch to\\rnarrow/broaden its focus in terms of how stringent the evaluation should be -\\rthe more abstract a sketch, the less stringent (higher $q$). Extensive\\rexperiments depict our method to outperform existing state-of-the-arts in\\rstandard SBIR tasks along with challenging scenarios like early retrieval,\\rforensic sketch-photo matching, and style-invariant retrieval.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07203 ,  14629kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07214\\rDate: Tue, 12 Mar 2024 00:02:03 GMT   (2261kb,D)\\r\\rTitle: Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers\\rAuthors: Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath\\r  Chowdhury, Tao Xiang, Yi-Zhe Song\\rCategories: cs.CV\\rComments: Accepted in CVPR 2024. Project page available at\\r  https://subhadeepkoley.github.io/DiffusionZSSBIR/\\r\\\\\\\\\\r  This paper, for the first time, explores text-to-image diffusion models for\\rZero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal\\rdiscovery: the capacity of text-to-image diffusion models to seamlessly bridge\\rthe gap between sketches and photos. This proficiency is underpinned by their\\rrobust cross-modal capabilities and shape bias, findings that are substantiated\\rthrough our pilot studies. In order to harness pre-trained diffusion models\\reffectively, we introduce a straightforward yet powerful strategy focused on\\rtwo key aspects: selecting optimal feature layers and utilising visual and\\rtextual prompts. For the former, we identify which layers are most enriched\\rwith information and are best suited for the specific retrieval requirements\\r(category-level or fine-grained). Then we employ visual and textual prompts to\\rguide the model's feature extraction process, enabling it to generate more\\rdiscriminative and contextually relevant cross-modal representations. Extensive\\rexperiments on several benchmark datasets validate significant performance\\rimprovements.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07214 ,  2261kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07219\\rDate: Tue, 12 Mar 2024 00:26:08 GMT   (14955kb,D)\\r\\rTitle: Monocular Microscope to CT Registration using Pose Estimation of the\\r  Incus for Augmented Reality Cochlear Implant Surgery\\rAuthors: Yike Zhang, Eduardo Davalos, Dingjie Su, Ange Lou, Jack H. Noble\\rCategories: cs.CV\\r\\\\\\\\\\r  For those experiencing severe-to-profound sensorineural hearing loss, the\\rcochlear implant (CI) is the preferred treatment. Augmented reality (AR) aided\\rsurgery can potentially improve CI procedures and hearing outcomes. Typically,\\rAR solutions for image-guided surgery rely on optical tracking systems to\\rregister pre-operative planning information to the display so that hidden\\ranatomy or other important information can be overlayed and co-registered with\\rthe view of the surgical scene. In this paper, our goal is to develop a method\\rthat permits direct 2D-to-3D registration of the microscope video to the\\rpre-operative Computed Tomography (CT) scan without the need for external\\rtracking equipment. Our proposed solution involves using surface mapping of a\\rportion of the incus in surgical recordings and determining the pose of this\\rstructure relative to the surgical microscope by performing pose estimation via\\rthe perspective-n-point (PnP) algorithm. This registration can then be applied\\rto pre-operative segmentations of other anatomy-of-interest, as well as the\\rplanned electrode insertion trajectory to co-register this information for the\\rAR display. Our results demonstrate the accuracy with an average rotation error\\rof less than 25 degrees and a translation error of less than 2 mm, 3 mm, and\\r0.55% for the x, y, and z axes, respectively. Our proposed method has the\\rpotential to be applicable and generalized to other surgical procedures while\\ronly needing a monocular microscope during intra-operation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07219 ,  14955kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07222\\rDate: Tue, 12 Mar 2024 00:27:18 GMT   (7446kb,D)\\r\\rTitle: You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image\\r  Retrieval\\rAuthors: Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath\\r  Chowdhury, Tao Xiang, Yi-Zhe Song\\rCategories: cs.CV\\rComments: Accepted in CVPR 2023. Project page available at\\r  https://subhadeepkoley.github.io/SBCIR/\\r\\\\\\\\\\r  Two primary input modalities prevail in image retrieval: sketch and text.\\rWhile text is widely used for inter-category retrieval tasks, sketches have\\rbeen established as the sole preferred modality for fine-grained image\\rretrieval due to their ability to capture intricate visual details. In this\\rpaper, we question the reliance on sketches alone for fine-grained image\\rretrieval by simultaneously exploring the fine-grained representation\\rcapabilities of both sketch and text, orchestrating a duet between the two. The\\rend result enables precise retrievals previously unattainable, allowing users\\rto pose ever-finer queries and incorporate attributes like colour and\\rcontextual cues from text. For this purpose, we introduce a novel\\rcompositionality framework, effectively combining sketches and text using\\rpre-trained CLIP models, while eliminating the need for extensive fine-grained\\rtextual descriptions. Last but not least, our system extends to novel\\rapplications in composite image retrieval, domain attribute transfer, and\\rfine-grained generation, providing solutions for various real-world scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07222 ,  7446kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07231\\rDate: Tue, 12 Mar 2024 00:58:19 GMT   (28478kb,D)\\r\\rTitle: Learn and Search: An Elegant Technique for Object Lookup using\\r  Contrastive Learning\\rAuthors: Chandan Kumar, Jansel Herrera-Gerena, John Just, Matthew Darr, Ali\\r  Jannesari\\rCategories: cs.CV\\rComments: 9 pages, 4 figures\\r\\\\\\\\\\r  The rapid proliferation of digital content and the ever-growing need for\\rprecise object recognition and segmentation have driven the advancement of\\rcutting-edge techniques in the field of object classification and segmentation.\\rThis paper introduces Learn and Search, a novel approach for object lookup\\rthat leverages the power of contrastive learning to enhance the efficiency and\\reffectiveness of retrieval systems.\\r  In this study, we present an elegant and innovative methodology that\\rintegrates deep learning principles and contrastive learning to tackle the\\rchallenges of object search. Our extensive experimentation reveals compelling\\rresults, with Learn and Search achieving superior Similarity Grid Accuracy,\\rshowcasing its efficacy in discerning regions of utmost similarity within an\\rimage relative to a cropped image.\\r  The seamless fusion of deep learning and contrastive learning to address the\\rintricacies of object identification not only promises transformative\\rapplications in image recognition, recommendation systems, and content tagging\\rbut also revolutionizes content-based search and retrieval. The amalgamation of\\rthese techniques, as exemplified by Learn and Search, represents a\\rsignificant stride in the ongoing evolution of methodologies in the dynamic\\rrealm of object classification and segmentation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07231 ,  28478kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07234\\rDate: Tue, 12 Mar 2024 01:05:25 GMT   (24992kb,D)\\r\\rTitle: It's All About Your Sketch: Democratising Sketch Control in Diffusion\\r  Models\\rAuthors: Subhadeep Koley, Ayan Kumar Bhunia, Deeptanshu Sekhri, Aneeshan Sain,\\r  Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song\\rCategories: cs.CV\\rComments: Accepted in CVPR 2024. Project page available at\\r  https://github.com/subhadeepkoley/DemoSketch2RGB\\r\\\\\\\\\\r  This paper unravels the potential of sketches for diffusion models,\\raddressing the deceptive promise of direct sketch control in generative AI. We\\rimportantly democratise the process, enabling amateur sketches to generate\\rprecise images, living up to the commitment of what you sketch is what you\\rget. A pilot study underscores the necessity, revealing that deformities in\\rexisting models stem from spatial-conditioning. To rectify this, we propose an\\rabstraction-aware framework, utilising a sketch adapter, adaptive time-step\\rsampling, and discriminative guidance from a pre-trained fine-grained\\rsketch-based image retrieval model, working synergistically to reinforce\\rfine-grained sketch-photo association. Our approach operates seamlessly during\\rinference without the need for textual prompts; a simple, rough sketch akin to\\rwhat you and I can create suffices! We welcome everyone to examine results\\rpresented in the paper and its supplementary. Contributions include\\rdemocratising sketch control, introducing an abstraction-aware framework, and\\rleveraging discriminative guidance, validated through extensive experiments.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07234 ,  24992kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07240\\rDate: Tue, 12 Mar 2024 01:28:00 GMT   (3216kb,D)\\r\\rTitle: Frequency-Aware Deepfake Detection: Improving Generalizability through\\r  Frequency Space Learning\\rAuthors: Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, Yunchao\\r  Wei\\rCategories: cs.CV\\rComments: 9 pages, 4 figures, AAAI24\\r\\\\\\\\\\r  This research addresses the challenge of developing a universal deepfake\\rdetector that can effectively identify unseen deepfake images despite limited\\rtraining data. Existing frequency-based paradigms have relied on\\rfrequency-level artifacts introduced during the up-sampling in GAN pipelines to\\rdetect forgeries. However, the rapid advancements in synthesis technology have\\rled to specific artifacts for each generation model. Consequently, these\\rdetectors have exhibited a lack of proficiency in learning the frequency domain\\rand tend to overfit to the artifacts present in the training data, leading to\\rsuboptimal performance on unseen sources. To address this issue, we introduce a\\rnovel frequency-aware approach called FreqNet, centered around frequency domain\\rlearning, specifically designed to enhance the generalizability of deepfake\\rdetectors. Our method forces the detector to continuously focus on\\rhigh-frequency information, exploiting high-frequency representation of\\rfeatures across spatial and channel dimensions. Additionally, we incorporate a\\rstraightforward frequency domain learning module to learn source-agnostic\\rfeatures. It involves convolutional layers applied to both the phase spectrum\\rand amplitude spectrum between the Fast Fourier Transform (FFT) and Inverse\\rFast Fourier Transform (iFFT). Extensive experimentation involving 17 GANs\\rdemonstrates the effectiveness of our proposed method, showcasing\\rstate-of-the-art performance (+9.8\\\\%) while requiring fewer parameters. The\\rcode is available at {\\\\cred\\r\\\\url{https://github.com/chuangchuangtan/FreqNet-DeepfakeDetection}}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07240 ,  3216kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07241\\rDate: Tue, 12 Mar 2024 01:47:17 GMT   (6949kb,D)\\r\\rTitle: Calibrating Multi-modal Representations: A Pursuit of Group Robustness\\r  without Annotations\\rAuthors: Chenyu You, Yifei Min, Weicheng Dai, Jasjeet S. Sekhon, Lawrence\\r  Staib, James S. Duncan\\rCategories: cs.CV cs.LG\\rComments: Accepted by CVPR 2024\\r\\\\\\\\\\r  Fine-tuning pre-trained vision-language models, like CLIP, has yielded\\rsuccess on diverse downstream tasks. However, several pain points persist for\\rthis paradigm: (i) directly tuning entire pre-trained models becomes both\\rtime-intensive and computationally costly. Additionally, these tuned models\\rtend to become highly specialized, limiting their practicality for real-world\\rdeployment; (ii) recent studies indicate that pre-trained vision-language\\rclassifiers may overly depend on spurious features -- patterns that correlate\\rwith the target in training data, but are not related to the true labeling\\rfunction; and (iii) existing studies on mitigating the reliance on spurious\\rfeatures, largely based on the assumption that we can identify such features,\\rdoes not provide definitive assurance for real-world applications. As a\\rpiloting study, this work focuses on exploring mitigating the reliance on\\rspurious features for CLIP without using any group annotation. To this end, we\\rsystematically study the existence of spurious correlation on CLIP and\\rCILP+ERM. We first, following recent work on Deep Feature Reweighting (DFR),\\rverify that last-layer retraining can greatly improve group robustness on\\rpretrained CLIP. In view of them, we advocate a lightweight representation\\rcalibration method for fine-tuning CLIP, by first generating a calibration set\\rusing the pretrained CLIP, and then calibrating representations of samples\\rwithin this set through contrastive learning, all without the need for group\\rlabels. Extensive experiments and in-depth visualizations on several benchmarks\\rvalidate the effectiveness of our proposals, largely reducing reliance and\\rsignificantly boosting the model generalization.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07241 ,  6949kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07244\\rDate: Tue, 12 Mar 2024 02:04:17 GMT   (16849kb,D)\\r\\rTitle: Time-Efficient Light-Field Acquisition Using Coded Aperture and Events\\rAuthors: Shuji Habuchi, Keita Takahashi, Chihiro Tsutake, Toshiaki Fujii,\\r  Hajime Nagahara\\rCategories: cs.CV eess.IV\\rComments: Accepted to IEEE/CVF Computer Vision and Pattern Recognition\\r  Conference (CVPR) 2024\\r\\\\\\\\\\r  We propose a computational imaging method for time-efficient light-field\\racquisition that combines a coded aperture with an event-based camera.\\rDifferent from the conventional coded-aperture imaging method, our method\\rapplies a sequence of coding patterns during a single exposure for an image\\rframe. The parallax information, which is related to the differences in coding\\rpatterns, is recorded as events. The image frame and events, all of which are\\rmeasured in a single exposure, are jointly used to computationally reconstruct\\ra light field. We also designed an algorithm pipeline for our method that is\\rend-to-end trainable on the basis of deep optics and compatible with real\\rcamera hardware. We experimentally showed that our method can achieve more\\raccurate reconstruction than several other imaging methods with a single\\rexposure. We also developed a hardware prototype with the potential to complete\\rthe measurement on the camera within 22 msec and demonstrated that light fields\\rfrom real 3-D scenes can be obtained with convincing visual quality. Our\\rsoftware and supplementary video are available from our project website.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07244 ,  16849kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07246\\rDate: Tue, 12 Mar 2024 02:07:23 GMT   (26309kb,D)\\r\\rTitle: Towards Zero-shot Human-Object Interaction Detection via Vision-Language\\r  Integration\\rAuthors: Weiying Xue, Qi Liu, Qiwei Xiong, Yuxiao Wang, Zhenao Wei, Xiaofen\\r  Xing, Xiangmin Xu\\rCategories: cs.CV\\r\\\\\\\\\\r  Human-object interaction (HOI) detection aims to locate human-object pairs\\rand identify their interaction categories in images. Most existing methods\\rprimarily focus on supervised learning, which relies on extensive manual HOI\\rannotations. In this paper, we propose a novel framework, termed Knowledge\\rIntegration to HOI (KI2HOI), that effectively integrates the knowledge of\\rvisual-language model to improve zero-shot HOI detection. Specifically, the\\rverb feature learning module is designed based on visual semantics, by\\remploying the verb extraction decoder to convert corresponding verb queries\\rinto interaction-specific category representations. We develop an effective\\radditive self-attention mechanism to generate more comprehensive visual\\rrepresentations. Moreover, the innovative interaction representation decoder\\reffectively extracts informative regions by integrating spatial and visual\\rfeature information through a cross-attention mechanism. To deal with zero-shot\\rlearning in low-data, we leverage a priori knowledge from the CLIP text encoder\\rto initialize the linear classifier for enhanced interaction understanding.\\rExtensive experiments conducted on the mainstream HICO-DET and V-COCO datasets\\rdemonstrate that our model outperforms the previous methods in various\\rzero-shot and full-supervised settings.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07246 ,  26309kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07263\\rDate: Tue, 12 Mar 2024 02:45:24 GMT   (20020kb,D)\\r\\rTitle: Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction\\rAuthors: Alexander Timans, Christoph-Nikolas Straehle, Kaspar Sakmann, Eric\\r  Nalisnick\\rCategories: cs.CV cs.LG stat.ML\\rComments: 38 pages, 14 figures, 6 tables (incl. appendix)\\r\\\\\\\\\\r  Quantifying a model's predictive uncertainty is essential for safety-critical\\rapplications such as autonomous driving. We consider quantifying such\\runcertainty for multi-object detection. In particular, we leverage conformal\\rprediction to obtain uncertainty intervals with guaranteed coverage for object\\rbounding boxes. One challenge in doing so is that bounding box predictions are\\rconditioned on the object's class label. Thus, we develop a novel two-step\\rconformal approach that propagates uncertainty in predicted class labels into\\rthe uncertainty intervals for the bounding boxes. This broadens the validity of\\rour conformal coverage guarantees to include incorrectly classified objects,\\rensuring their usefulness when maximal safety assurances are required.\\rMoreover, we investigate novel ensemble and quantile regression formulations to\\rensure the bounding box intervals are adaptive to object size, leading to a\\rmore balanced coverage across sizes. Validating our two-step approach on\\rreal-world datasets for 2D bounding box localization, we find that desired\\rcoverage levels are satisfied with actionably tight predictive uncertainty\\rintervals.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07263 ,  20020kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07277\\rDate: Tue, 12 Mar 2024 03:15:08 GMT   (39023kb,D)\\r\\rTitle: A Bayesian Approach to OOD Robustness in Image Classification\\rAuthors: Prakhar Kaushik and Adam Kortylewski and Alan Yuille\\rCategories: cs.CV cs.AI\\rComments: CVPR 2024\\r\\\\\\\\\\r  An important and unsolved problem in computer vision is to ensure that the\\ralgorithms are robust to changes in image domains. We address this problem in\\rthe scenario where we have access to images from the target domains but no\\rannotations. Motivated by the challenges of the OOD-CV benchmark where we\\rencounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce\\ra novel Bayesian approach to OOD robustness for object classification. Our work\\rextends Compositional Neural Networks (CompNets), which have been shown to be\\rrobust to occlusion but degrade badly when tested on OOD data. We exploit the\\rfact that CompNets contain a generative head defined over feature vectors\\rrepresented by von Mises-Fisher (vMF) kernels, which correspond roughly to\\robject parts, and can be learned without supervision. We obverse that some vMF\\rkernels are similar between different domains, while others are not. This\\renables us to learn a transitional dictionary of vMF kernels that are\\rintermediate between the source and target domains and train the generative\\rmodel on this dictionary using the annotations on the source domain, followed\\rby iterative refinement. This approach, termed Unsupervised Generative\\rTransition (UGT), performs very well in OOD scenarios even when occlusion is\\rpresent. UGT is evaluated on different OOD benchmarks including the OOD-CV\\rdataset, several popular datasets (e.g., ImageNet-C [9]), artificial image\\rcorruptions (including adding occluders), and synthetic-to-real domain\\rtransfer, and does well in all scenarios outperforming SOTA alternatives (e.g.\\rup to 10% top-1 accuracy on Occluded OOD-CV dataset).\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07277 ,  39023kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07284\\rDate: Tue, 12 Mar 2024 03:34:03 GMT   (4637kb,D)\\r\\rTitle: SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object\\r  Detection\\rAuthors: Hongcheng Zhang, Liu Liang, Pengxin Zeng, Xiao Song, Zhe Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  Sparse 3D detectors have received significant attention since the query-based\\rparadigm embraces low latency without explicit dense BEV feature construction.\\rHowever, these detectors achieve worse performance than their dense\\rcounterparts. In this paper, we find the key to bridging the performance gap is\\rto enhance the awareness of rich representations in two modalities. Here, we\\rpresent a high-performance fully sparse detector for end-to-end multi-modality\\r3D object detection. The detector, termed SparseLIF, contains three key\\rdesigns, which are (1) Perspective-Aware Query Generation (PAQG) to generate\\rhigh-quality 3D queries with perspective priors, (2) RoI-Aware Sampling (RIAS)\\rto further refine prior queries by sampling RoI features from each modality,\\r(3) Uncertainty-Aware Fusion (UAF) to precisely quantify the uncertainty of\\reach sensor modality and adaptively conduct final multi-modality fusion, thus\\rachieving great robustness against sensor noises. By the time of submission\\r(2024/03/08), SparseLIF achieves state-of-the-art performance on the nuScenes\\rdataset, ranking 1st on both validation set and test benchmark, outperforming\\rall state-of-the-art 3D object detectors by a notable margin. The source code\\rwill be released upon acceptance.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07284 ,  4637kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07286\\rDate: Tue, 12 Mar 2024 03:35:17 GMT   (11719kb,D)\\r\\rTitle: MENTOR: Multilingual tExt detectioN TOward leaRning by analogy\\rAuthors: Hsin-Ju Lin, Tsu-Chun Chung, Ching-Chun Hsiao, Pin-Yu Chen, Wei-Chen\\r  Chiu, and Ching-Chun Huang\\rCategories: cs.CV\\rComments: 8 pages, 4 figures, published to IROS 2023\\rJournal-ref: 2023 IEEE/RSJ International Conference on Intelligent Robots and\\r  Systems (IROS), Detroit, MI, USA, 2023, pp. 3248-3255\\rDOI: 10.1109/IROS55552.2023.10342419\\r\\\\\\\\\\r  Text detection is frequently used in vision-based mobile robots when they\\rneed to interpret texts in their surroundings to perform a given task. For\\rinstance, delivery robots in multilingual cities need to be capable of doing\\rmultilingual text detection so that the robots can read traffic signs and road\\rmarkings. Moreover, the target languages change from region to region, implying\\rthe need of efficiently re-training the models to recognize the novel/new\\rlanguages. However, collecting and labeling training data for novel languages\\rare cumbersome, and the efforts to re-train an existing/trained text detector\\rare considerable. Even worse, such a routine would repeat whenever a novel\\rlanguage appears. This motivates us to propose a new problem setting for\\rtackling the aforementioned challenges in a more efficient way: We ask for a\\rgeneralizable multilingual text detection framework to detect and identify both\\rseen and unseen language regions inside scene images without the requirement of\\rcollecting supervised training data for unseen languages as well as model\\rre-training. To this end, we propose MENTOR, the first work to realize a\\rlearning strategy between zero-shot learning and few-shot learning for\\rmultilingual scene text detection.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07286 ,  11719kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07289\\rDate: Tue, 12 Mar 2024 03:44:40 GMT   (615kb,D)\\r\\rTitle: Rediscovering BCE Loss for Uniform Classification\\rAuthors: Qiufu Li, Xi Jia, Jiancan Zhou, Linlin Shen, Jinming Duan\\rCategories: cs.CV\\r\\\\\\\\\\r  This paper introduces the concept of uniform classification, which employs a\\runified threshold to classify all samples rather than adaptive threshold\\rclassifying each individual sample. We also propose the uniform classification\\raccuracy as a metric to measure the model's performance in uniform\\rclassification. Furthermore, begin with a naive loss, we mathematically derive\\ra loss function suitable for the uniform classification, which is the BCE\\rfunction integrated with a unified bias. We demonstrate the unified threshold\\rcould be learned via the bias. The extensive experiments on six classification\\rdatasets and three feature extraction models show that, compared to the SoftMax\\rloss, the models trained with the BCE loss not only exhibit higher uniform\\rclassification accuracy but also higher sample-wise classification accuracy. In\\raddition, the learned bias from BCE loss is very close to the unified threshold\\rused in the uniform classification. The features extracted by the models\\rtrained with BCE loss not only possess uniformity but also demonstrate better\\rintra-class compactness and inter-class distinctiveness, yielding superior\\rperformance on open-set tasks such as face recognition.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07289 ,  615kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07290\\rDate: Tue, 12 Mar 2024 03:44:46 GMT   (3616kb,D)\\r\\rTitle: Learning Hierarchical Color Guidance for Depth Map Super-Resolution\\rAuthors: Runmin Cong, Ronghui Sheng, Hao Wu, Yulan Guo, Yunchao Wei, Wangmeng\\r  Zuo, Yao Zhao, and Sam Kwong\\rCategories: cs.CV\\r\\\\\\\\\\r  Color information is the most commonly used prior knowledge for depth map\\rsuper-resolution (DSR), which can provide high-frequency boundary guidance for\\rdetail restoration. However, its role and functionality in DSR have not been\\rfully developed. In this paper, we rethink the utilization of color information\\rand propose a hierarchical color guidance network to achieve DSR. On the one\\rhand, the low-level detail embedding module is designed to supplement\\rhigh-frequency color information of depth features in a residual mask manner at\\rthe low-level stages. On the other hand, the high-level abstract guidance\\rmodule is proposed to maintain semantic consistency in the reconstruction\\rprocess by using a semantic mask that encodes the global guidance information.\\rThe color information of these two dimensions plays a role in the front and\\rback ends of the attention-based feature projection (AFP) module in a more\\rcomprehensive form. Simultaneously, the AFP module integrates the multi-scale\\rcontent enhancement block and adaptive attention projection block to make full\\ruse of multi-scale information and adaptively project critical restoration\\rinformation in an attention manner for DSR. Compared with the state-of-the-art\\rmethods on four benchmark datasets, our method achieves more competitive\\rperformance both qualitatively and quantitatively.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07290 ,  3616kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07292\\rDate: Tue, 12 Mar 2024 03:50:57 GMT   (27114kb,D)\\r\\rTitle: Continual All-in-One Adverse Weather Removal with Knowledge Replay on a\\r  Unified Network Structure\\rAuthors: De Cheng, Yanling Ji, Dong Gong, Yan Li, Nannan Wang, Junwei Han,\\r  Dingwen Zhang\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  In real-world applications, image degeneration caused by adverse weather is\\ralways complex and changes with different weather conditions from days and\\rseasons. Systems in real-world environments constantly encounter adverse\\rweather conditions that are not previously observed. Therefore, it practically\\rrequires adverse weather removal models to continually learn from incrementally\\rcollected data reflecting various degeneration types. Existing adverse weather\\rremoval approaches, for either single or multiple adverse weathers, are mainly\\rdesigned for a static learning paradigm, which assumes that the data of all\\rtypes of degenerations to handle can be finely collected at one time before a\\rsingle-phase learning process. They thus cannot directly handle the incremental\\rlearning requirements. To address this issue, we made the earliest effort to\\rinvestigate the continual all-in-one adverse weather removal task, in a setting\\rcloser to real-world applications. Specifically, we develop a novel continual\\rlearning framework with effective knowledge replay (KR) on a unified network\\rstructure. Equipped with a principal component projection and an effective\\rknowledge distillation mechanism, the proposed KR techniques are tailored for\\rthe all-in-one weather removal task. It considers the characteristics of the\\rimage restoration task with multiple degenerations in continual learning, and\\rthe knowledge for different degenerations can be shared and accumulated in the\\runified network structure. Extensive experimental results demonstrate the\\reffectiveness of the proposed method to deal with this challenging task, which\\rperforms competitively to existing dedicated or joint training image\\rrestoration methods. Our code is available at\\rhttps://github.com/xiaojihh/CL_all-in-one.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07292 ,  27114kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07301\\rDate: Tue, 12 Mar 2024 04:07:00 GMT   (4327kb,D)\\r\\rTitle: Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal\\r  Storyteller\\rAuthors: Chuanqi Zang, Jiji Tang, Rongsheng Zhang, Zeng Zhao, Tangjie Lv,\\r  Mingtao Pei, Wei Liang\\rCategories: cs.CV\\r\\\\\\\\\\r  Storytelling aims to generate reasonable and vivid narratives based on an\\rordered image stream. The fidelity to the image story theme and the divergence\\rof story plots attract readers to keep reading. Previous works iteratively\\rimproved the alignment of multiple modalities but ultimately resulted in the\\rgeneration of simplistic storylines for image streams. In this work, we propose\\ra new pipeline, termed LLaMS, to generate multimodal human-level stories that\\rare embodied in expressiveness and consistency. Specifically, by fully\\rexploiting the commonsense knowledge within the LLM, we first employ a sequence\\rdata auto-enhancement strategy to enhance factual content expression and\\rleverage a textual reasoning architecture for expressive story generation and\\rprediction. Secondly, we propose SQ-Adatpter module for story illustration\\rgeneration which can maintain sequence consistency. Numerical results are\\rconducted through human evaluation to verify the superiority of proposed LLaMS.\\rEvaluations show that LLaMS achieves state-of-the-art storytelling performance\\rand 86% correlation and 100% consistency win rate as compared with previous\\rSOTA methods. Furthermore, ablation experiments are conducted to verify the\\reffectiveness of proposed sequence data enhancement and SQ-Adapter.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07301 ,  4327kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07304\\rDate: Tue, 12 Mar 2024 04:13:45 GMT   (3383kb,D)\\r\\rTitle: Lumen: Unleashing Versatile Vision-Centric Capabilities of Large\\r  Multimodal Models\\rAuthors: Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, Yu-Gang\\r  Jiang\\rCategories: cs.CV\\rComments: Technical Report\\r\\\\\\\\\\r  Large Multimodal Model (LMM) is a hot research topic in the computer vision\\rarea and has also demonstrated remarkable potential across multiple\\rdisciplinary fields. A recent trend is to further extend and enhance the\\rperception capabilities of LMMs. The current methods follow the paradigm of\\radapting the visual task outputs to the format of the language model, which is\\rthe main component of a LMM. This adaptation leads to convenient development of\\rsuch LMMs with minimal modifications, however, it overlooks the intrinsic\\rcharacteristics of diverse visual tasks and hinders the learning of perception\\rcapabilities. To address this issue, we propose a novel LMM architecture named\\rLumen, a Large multimodal model with versatile vision-centric capability\\renhancement. We decouple the LMM's learning of perception capabilities into\\rtask-agnostic and task-specific stages. Lumen first promotes fine-grained\\rvision-language concept alignment, which is the fundamental capability for\\rvarious visual tasks. Thus the output of the task-agnostic stage is a shared\\rrepresentation for all the tasks we address in this paper. Then the\\rtask-specific decoding is carried out by flexibly routing the shared\\rrepresentation to lightweight task decoders with negligible training efforts.\\rBenefiting from such a decoupled design, our Lumen surpasses existing LMM-based\\rapproaches on the COCO detection benchmark with a clear margin and exhibits\\rseamless scalability to additional visual tasks. Furthermore, we also conduct\\rcomprehensive ablation studies and generalization evaluations for deeper\\rinsights. The code will be released at https://github.com/SxJyJay/Lumen.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07304 ,  3383kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07319\\rDate: Tue, 12 Mar 2024 05:06:07 GMT   (19028kb,D)\\r\\rTitle: Efficient Diffusion Model for Image Restoration by Residual Shifting\\rAuthors: Zongsheng Yue, Jianyi Wang, and Chen Change Loy\\rCategories: cs.CV\\rComments: Extended version of NeurIPS paper. Code:\\r  https://github.com/zsyOAOA/ResShift\\rMSC-class: I.4.4\\r\\\\\\\\\\r  While diffusion-based image restoration (IR) methods have achieved remarkable\\rsuccess, they are still limited by the low inference speed attributed to the\\rnecessity of executing hundreds or even thousands of sampling steps. Existing\\racceleration sampling techniques, though seeking to expedite the process,\\rinevitably sacrifice performance to some extent, resulting in over-blurry\\rrestored outcomes. To address this issue, this study proposes a novel and\\refficient diffusion model for IR that significantly reduces the required number\\rof diffusion steps. Our method avoids the need for post-acceleration during\\rinference, thereby avoiding the associated performance deterioration.\\rSpecifically, our proposed method establishes a Markov chain that facilitates\\rthe transitions between the high-quality and low-quality images by shifting\\rtheir residuals, substantially improving the transition efficiency. A carefully\\rformulated noise schedule is devised to flexibly control the shifting speed and\\rthe noise strength during the diffusion process. Extensive experimental\\revaluations demonstrate that the proposed method achieves superior or\\rcomparable performance to current state-of-the-art methods on three classical\\rIR tasks, namely image super-resolution, image inpainting, and blind face\\rrestoration, \\\\textit{\\\\textbf{even only with four sampling steps}}. Our code and\\rmodel are publicly available at \\\\url{https://github.com/zsyOAOA/ResShift}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07319 ,  19028kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07326\\rDate: Tue, 12 Mar 2024 05:20:44 GMT   (16047kb,D)\\r\\rTitle: SGE: Structured Light System Based on Gray Code with an Event Camera\\rAuthors: Xingyu Lu, Lei Sun, Diyang Gu, Zhijie Xu, Kaiwei Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  Fast and accurate depth sensing has long been a significant research\\rchallenge. Event camera, as a device that quickly responds to intensity\\rchanges, provides a new solution for structured light (SL) systems. In this\\rpaper, we introduce Gray code into event-based SL systems for the first time.\\rOur setup includes an event camera and Digital Light Processing (DLP)\\rprojector, enabling depth estimation through high-speed projection and decoding\\rof Gray code patterns. By employing spatio-temporal encoding for point\\rmatching, our method is immune to timestamp noise, realizing high-speed depth\\restimation without loss of accuracy. The binary nature of events and Gray code\\rminimizes data redundancy, enabling us to fully utilize sensor bandwidth at\\r100%. Experimental results show that our approach achieves accuracy comparable\\rto state-of-the-art scanning methods while surpassing them in data acquisition\\rspeed (up to 41 times improvement) without sacrificing accuracy. Our proposed\\rapproach offers a highly promising solution for ultra-fast, real-time, and\\rhigh-precision dense depth estimation. Code and dataset will be publicly\\ravailable.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07326 ,  16047kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07332\\rDate: Tue, 12 Mar 2024 05:34:51 GMT   (2121kb,D)\\r\\rTitle: Large Window-based Mamba UNet for Medical Image Segmentation: Beyond\\r  Convolution and Self-attention\\rAuthors: Jinhong Wang, Jintai Chen, Danny Chen and Jian Wu\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  In clinical practice, medical image segmentation provides useful information\\ron the contours and dimensions of target organs or tissues, facilitating\\rimproved diagnosis, analysis, and treatment. In the past few years,\\rconvolutional neural networks (CNNs) and Transformers have dominated this area,\\rbut they still suffer from either limited receptive fields or costly long-range\\rmodeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a\\rpromising paradigm for long-range dependency modeling with linear complexity.\\rIn this paper, we introduce a Large Window-based Mamba U}-shape Network, or\\rLMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of\\rour LMa-UNet is its utilization of large windows, excelling in locally spatial\\rmodeling compared to small kernel-based CNNs and small window-based\\rTransformers, while maintaining superior efficiency in global modeling compared\\rto self-attention with quadratic complexity. Additionally, we design a novel\\rhierarchical and bidirectional Mamba block to further enhance the global and\\rneighborhood spatial modeling capability of Mamba. Comprehensive experiments\\rdemonstrate the effectiveness and efficiency of our method and the feasibility\\rof using large window size to achieve large receptive fields. Codes are\\ravailable at https://github.com/wjh892521292/LMa-UNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07332 ,  2121kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07346\\rDate: Tue, 12 Mar 2024 06:04:50 GMT   (24139kb,D)\\r\\rTitle: Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction\\rAuthors: Jianping Jiang, Xinyu Zhou, Bingxuan Wang, Xiaoming Deng, Chao Xu,\\r  Boxin Shi\\rCategories: cs.CV\\r\\\\\\\\\\r  Reliable hand mesh reconstruction (HMR) from commonly-used color and depth\\rsensors is challenging especially under scenarios with varied illuminations and\\rfast motions. Event camera is a highly promising alternative for its high\\rdynamic range and dense temporal resolution properties, but it lacks key\\rtexture appearance for hand mesh reconstruction. In this paper, we propose\\rEvRGBHand -- the first approach for 3D hand mesh reconstruction with an event\\rcamera and an RGB camera compensating for each other. By fusing two modalities\\rof data across time, space, and information dimensions,EvRGBHand can tackle\\roverexposure and motion blur issues in RGB-based HMR and foreground scarcity\\rand background overflow issues in event-based HMR. We further propose\\rEvRGBDegrader, which allows our model to generalize effectively in challenging\\rscenes, even when trained solely on standard scenes, thus reducing data\\racquisition costs. Experiments on real-world data demonstrate that EvRGBHand\\rcan effectively solve the challenging issues when using either type of camera\\ralone via retaining the merits of both, and shows the potential of\\rgeneralization to outdoor scenes and another type of event camera.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07346 ,  24139kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07347\\rDate: Tue, 12 Mar 2024 06:07:29 GMT   (2026kb,D)\\r\\rTitle: Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic\\r  Architecture\\rAuthors: Fei Wang, Dan Guo, Kun Li, Zhun Zhong, Meng Wang\\rCategories: cs.CV\\rComments: Accepted by CVPR2024\\r\\\\\\\\\\r  Video Motion Magnification (VMM) aims to reveal subtle and imperceptible\\rmotion information of objects in the macroscopic world. Prior methods directly\\rmodel the motion field from the Eulerian perspective by Representation Learning\\rthat separates shape and texture or Multi-domain Learning from phase\\rfluctuations. Inspired by the frequency spectrum, we observe that the\\rlow-frequency components with stable energy always possess spatial structure\\rand less noise, making them suitable for modeling the subtle motion field. To\\rthis end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion\\rMagnification with a Multi-level Isomorphic Architecture to capture multi-level\\rhigh-frequency details and a stable low-frequency structure (motion field) in\\rvideo space. Since high-frequency details and subtle motions are susceptible to\\rinformation degradation due to their inherent subtlety and unavoidable external\\rinterference from noise, we carefully design Sparse High/Low-pass Filters to\\renhance the integrity of details and motion structures, and a Sparse Frequency\\rMixer to promote seamless recoupling. Besides, we innovatively design a\\rcontrastive regularization for this task to strengthen the model's ability to\\rdiscriminate irrelevant features, reducing undesired motion magnification.\\rExtensive experiments on both Real-world and Synthetic Datasets show that our\\rFD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\\\\times$\\rand boosts inference speed by 1.68$\\\\times$ than the latest method. Our code is\\ravailable at https://github.com/Jiafei127/FD4MM.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07347 ,  2026kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07354\\rDate: Tue, 12 Mar 2024 06:23:45 GMT   (1279kb,D)\\r\\rTitle: BID: Boundary-Interior Decoding for Unsupervised Temporal Action\\r  Localization Pre-Trainin\\rAuthors: Qihang Fang and Chengcheng Tang and Shugao Ma and Yanchao Yang\\rCategories: cs.CV\\rComments: 18 pages, 8 figures\\rMSC-class: 68T45\\rACM-class: I.4.8\\r\\\\\\\\\\r  Skeleton-based motion representations are robust for action localization and\\runderstanding for their invariance to perspective, lighting, and occlusion,\\rcompared with images. Yet, they are often ambiguous and incomplete when taken\\rout of context, even for human annotators. As infants discern gestures before\\rassociating them with words, actions can be conceptualized before being\\rgrounded with labels. Therefore, we propose the first unsupervised pre-training\\rframework, Boundary-Interior Decoding (BID), that partitions a skeleton-based\\rmotion sequence into discovered semantically meaningful pre-action segments. By\\rfine-tuning our pre-training network with a small number of annotated data, we\\rshow results out-performing SOTA methods by a large margin.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07354 ,  1279kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07356\\rDate: Tue, 12 Mar 2024 06:29:54 GMT   (1128kb,D)\\r\\rTitle: Premonition: Using Generative Models to Preempt Future Data Changes in\\r  Continual Learning\\rAuthors: Mark D. McDonnell, Dong Gong, Ehsan Abbasnejad and Anton van den\\r  Hengel\\rCategories: cs.CV cs.LG\\rComments: 31 pages total (14 main paper, 5 references, 12 appendices)\\r\\\\\\\\\\r  Continual learning requires a model to adapt to ongoing changes in the data\\rdistribution, and often to the set of tasks to be performed. It is rare,\\rhowever, that the data and task changes are completely unpredictable. Given a\\rdescription of an overarching goal or data theme, which we call a realm, humans\\rcan often guess what concepts are associated with it. We show here that the\\rcombination of a large language model and an image generation model can\\rsimilarly provide useful premonitions as to how a continual learning challenge\\rmight develop over time. We use the large language model to generate text\\rdescriptions of semantically related classes that might potentially appear in\\rthe data stream in future. These descriptions are then rendered using Stable\\rDiffusion to generate new labelled image samples. The resulting synthetic\\rdataset is employed for supervised pre-training, but is discarded prior to\\rcommencing continual learning, along with the pre-training classification head.\\rWe find that the backbone of our pre-trained networks can learn representations\\ruseful for the downstream continual learning problem, thus becoming a valuable\\rinput to any existing continual learning method. Although there are\\rcomplexities arising from the domain gap between real and synthetic images, we\\rshow that pre-training models in this manner improves multiple Class Incremenal\\rLearning (CIL) methods on fine-grained image classification benchmarks.\\rSupporting code can be found at https://github.com/cl-premonition/premonition.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07356 ,  1128kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07359\\rDate: Tue, 12 Mar 2024 06:45:34 GMT   (3011kb,D)\\r\\rTitle: FSC: Few-point Shape Completion\\rAuthors: Xianzu Wu, Xianfeng Wu, Tianyu Luan, Yajing Bai, Zhongyuan Lai,\\r  Junsong Yuan\\rCategories: cs.CV\\rComments: Accepted by CVPR 2024\\r\\\\\\\\\\r  While previous studies have demonstrated successful 3D object shape\\rcompletion with a sufficient number of points, they often fail in scenarios\\rwhen a few points, e.g. tens of points, are observed. Surprisingly, via entropy\\ranalysis, we find that even a few points, e.g. 64 points, could retain\\rsubstantial information to help recover the 3D shape of the object. To address\\rthe challenge of shape completion with very sparse point clouds, we then\\rpropose Few-point Shape Completion (FSC) model, which contains a novel\\rdual-branch feature extractor for handling extremely sparse inputs, coupled\\rwith an extensive branch for maximal point utilization with a saliency branch\\rfor dynamic importance assignment. This model is further bolstered by a\\rtwo-stage revision network that refines both the extracted features and the\\rdecoder output, enhancing the detail and authenticity of the completed point\\rcloud. Our experiments demonstrate the feasibility of recovering 3D shapes from\\ra few points. The proposed FSC (FSC) model outperforms previous methods on both\\rfew-point inputs and many-point inputs, and shows good generalizability to\\rdifferent object categories.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07359 ,  3011kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07366\\rDate: Tue, 12 Mar 2024 07:01:57 GMT   (5833kb,D)\\r\\rTitle: Entropy is not Enough for Test-Time Adaptation: From the Perspective of\\r  Disentangled Factors\\rAuthors: Jonghyun Lee, Dahuin Jung, Saehyung Lee, Junsung Park, Juhyeon Shin,\\r  Uiwon Hwang, Sungroh Yoon\\rCategories: cs.CV cs.LG\\rComments: ICLR 2024 Spotlight; 26 pages, 9 figures, 20 tables;\\r\\\\\\\\\\r  Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for\\runseen test data. The primary challenge of TTA is limited access to the entire\\rtest dataset during online updates, causing error accumulation. To mitigate it,\\rTTA methods have utilized the model output's entropy as a confidence metric\\rthat aims to determine which samples have a lower likelihood of causing error.\\rThrough experimental studies, however, we observed the unreliability of entropy\\ras a confidence metric for TTA under biased scenarios and theoretically\\rrevealed that it stems from the neglect of the influence of latent disentangled\\rfactors of data on predictions. Building upon these findings, we introduce a\\rnovel TTA method named Destroy Your Object (DeYO), which leverages a newly\\rproposed confidence metric named Pseudo-Label Probability Difference (PLPD).\\rPLPD quantifies the influence of the shape of an object on prediction by\\rmeasuring the difference between predictions before and after applying an\\robject-destructive transformation. DeYO consists of sample selection and sample\\rweighting, which employ entropy and PLPD concurrently. For robust adaptation,\\rDeYO prioritizes samples that dominantly incorporate shape information when\\rmaking predictions. Our extensive experiments demonstrate the consistent\\rsuperiority of DeYO over baseline methods across various scenarios, including\\rbiased and wild. Project page is publicly available at\\rhttps://whitesnowdrop.github.io/DeYO/.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07366 ,  5833kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07369\\rDate: Tue, 12 Mar 2024 07:06:50 GMT   (4417kb,D)\\r\\rTitle: Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized\\r  Visual Class Discovery\\rAuthors: Haiyang Zheng, Nan Pu, Wenjing Li, Nicu Sebe, Zhun Zhong\\rCategories: cs.CV\\r\\\\\\\\\\r  In this paper, we study the problem of Generalized Category Discovery (GCD),\\rwhich aims to cluster unlabeled data from both known and unknown categories\\rusing the knowledge of labeled data from known categories. Current GCD methods\\rrely on only visual cues, which however neglect the multi-modality perceptive\\rnature of human cognitive processes in discovering novel visual categories. To\\raddress this, we propose a two-phase TextGCD framework to accomplish\\rmulti-modality GCD by exploiting powerful Visual-Language Models. TextGCD\\rmainly includes a retrieval-based text generation (RTG) phase and a\\rcross-modality co-teaching (CCT) phase. First, RTG constructs a visual lexicon\\rusing category tags from diverse datasets and attributes from Large Language\\rModels, generating descriptive texts for images in a retrieval manner. Second,\\rCCT leverages disparities between textual and visual modalities to foster\\rmutual learning, thereby enhancing visual GCD. In addition, we design an\\radaptive class aligning strategy to ensure the alignment of category\\rperceptions between modalities as well as a soft-voting mechanism to integrate\\rmulti-modality cues. Experiments on eight datasets show the large superiority\\rof our approach over state-of-the-art methods. Notably, our approach\\routperforms the best competitor, by 7.7% and 10.8% in All accuracy on\\rImageNet-1k and CUB, respectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07369 ,  4417kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07371\\rDate: Tue, 12 Mar 2024 07:15:29 GMT   (22392kb,D)\\r\\rTitle: Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of\\r  Altered Diffusion Models\\rAuthors: Phuong Dam, Jihoon Jeong, Anh Tran, Daeyoung Kim\\rCategories: cs.CV\\r\\\\\\\\\\r  This study discusses the critical issues of Virtual Try-On in contemporary\\re-commerce and the prospective metaverse, emphasizing the challenges of\\rpreserving intricate texture details and distinctive features of the target\\rperson and the clothes in various scenarios, such as clothing texture and\\ridentity characteristics like tattoos or accessories. In addition to the\\rfidelity of the synthesized images, the efficiency of the synthesis process\\rpresents a significant hurdle. Various existing approaches are explored,\\rhighlighting the limitations and unresolved aspects, e.g., identity information\\romission, uncontrollable artifacts, and low synthesis speed. It then proposes a\\rnovel diffusion-based solution that addresses garment texture preservation and\\ruser identity retention during virtual try-on. The proposed network comprises\\rtwo primary modules - a warping module aligning clothing with individual\\rfeatures and a try-on module refining the attire and generating missing parts\\rintegrated with a mask-aware post-processing technique ensuring the integrity\\rof the individual's identity. It demonstrates impressive results, surpassing\\rthe state-of-the-art in speed by nearly 20 times during inference, with\\rsuperior fidelity in qualitative assessments. Quantitative evaluations confirm\\rcomparable performance with the recent SOTA method on the VITON-HD and\\rDresscode datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07371 ,  22392kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07372\\rDate: Tue, 12 Mar 2024 07:16:20 GMT   (3057kb,D)\\r\\rTitle: Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D\\r  Object Detection\\rAuthors: Jiahui Fu, Chen Gao, Zitian Wang, Lirong Yang, Xiaofei Wang, Beipeng\\r  Mu, Si Liu\\rCategories: cs.CV\\rComments: Accepted by ICRA 2024\\r\\\\\\\\\\r  Recent 3D object detectors typically utilize multi-sensor data and unify\\rmulti-modal features in the shared bird's-eye view (BEV) representation space.\\rHowever, our empirical findings indicate that previous methods have limitations\\rin generating fusion BEV features free from cross-modal conflicts. These\\rconflicts encompass extrinsic conflicts caused by BEV feature construction and\\rinherent conflicts stemming from heterogeneous sensor signals. Therefore, we\\rpropose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly\\reliminate the extrinsic/inherent conflicts in BEV space and produce improved\\rmulti-modal BEV features. Specifically, we devise a Semantic-guided Flow-based\\rAlignment (SFA) module to resolve extrinsic conflicts via unifying spatial\\rdistribution in BEV space before fusion. Moreover, we design a Dissolved Query\\rRecovering (DQR) mechanism to remedy inherent conflicts by preserving\\robjectness clues that are lost in the fusion BEV feature. In general, our\\rmethod maximizes the effective information utilization of each modality and\\rleverages inter-modal complementarity. Our method achieves state-of-the-art\\rperformance in the highly competitive nuScenes 3D object detection dataset. The\\rcode is released at https://github.com/fjhzhixi/ECFusion.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07372 ,  3057kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07376\\rDate: Tue, 12 Mar 2024 07:27:02 GMT   (15541kb,D)\\r\\rTitle: NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning\\r  Disentangled Reasoning\\rAuthors: Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma,\\r  Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang\\rCategories: cs.CV cs.AI cs.CL cs.RO\\r\\\\\\\\\\r  Vision-and-Language Navigation (VLN), as a crucial research problem of\\rEmbodied AI, requires an embodied agent to navigate through complex 3D\\renvironments following natural language instructions. Recent research has\\rhighlighted the promising capacity of large language models (LLMs) in VLN by\\rimproving navigational reasoning accuracy and interpretability. However, their\\rpredominant use in an offline manner usually suffers from substantial domain\\rgap between the VLN task and the LLM training corpus. This paper introduces a\\rnovel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill\\rparameter-efficient in-domain training to enable self-guided navigational\\rdecision, leading to a significant mitigation of the domain gap in a\\rcost-effective manner. Specifically, at each timestep, the LLM is prompted to\\rforecast the navigational chain-of-thought by: 1) acting as a world model to\\rimagine the next observation according to the instruction, 2) selecting the\\rcandidate observation that best aligns with the imagination, and 3) determining\\rthe action based on the reasoning from the prior steps. Through constructing\\rformalized labels for training, the LLM can learn to generate desired and\\rreasonable chain-of-thought outputs for improving the action decision.\\rExperimental results across various training settings and popular VLN\\rbenchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room\\r(R4R)) show the significant superiority of NavCoT over the direct action\\rprediction variants. Through simple parameter-efficient finetuning, our NavCoT\\routperforms a recent GPT4-based approach with ~7% relative improvement on the\\rR2R dataset. We believe that NavCoT will help unlock more task-adaptive and\\rscalable LLM-based embodied agents, which are helpful for developing real-world\\rrobotics applications. Code is available at\\rhttps://github.com/expectorlin/NavCoT.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07376 ,  15541kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07380\\rDate: Tue, 12 Mar 2024 07:41:51 GMT   (466kb,D)\\r\\rTitle: Gabor-guided transformer for single image deraining\\rAuthors: Sijin He, Guangfeng Lin\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Image deraining have have gained a great deal of attention in order to\\raddress the challenges posed by the effects of harsh weather conditions on\\rvisual tasks. While convolutional neural networks (CNNs) are popular, their\\rlimitations in capturing global information may result in ineffective rain\\rremoval. Transformer-based methods with self-attention mechanisms have\\rimproved, but they tend to distort high-frequency details that are crucial for\\rimage fidelity. To solve this problem, we propose the Gabor-guided tranformer\\r(Gabformer) for single image deraining. The focus on local texture features is\\renhanced by incorporating the information processed by the Gabor filter into\\rthe query vector, which also improves the robustness of the model to noise due\\rto the properties of the filter. Extensive experiments on the benchmarks\\rdemonstrate that our method outperforms state-of-the-art approaches.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07380 ,  466kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07389\\rDate: Tue, 12 Mar 2024 07:57:33 GMT   (6634kb,D)\\r\\rTitle: Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from\\r  Duplex to Monoplex IHC Images\\rAuthors: Nicolas Brieu, Nicolas Triltsch, Philipp Wortmann, Dominik Winter,\\r  Shashank Saran, Marlon Rebelatto, G\\\\unter Schmidt\\rCategories: cs.CV cs.AI eess.IV\\rComments: 4 pages, 2 figures\\rMSC-class: I.2.10, J.3, I.4.6\\r\\\\\\\\\\r  Generative models enable the translation from a source image domain where\\rreadily trained models are available to a target domain unseen during training.\\rWhile Cycle Generative Adversarial Networks (GANs) are well established, the\\rassociated cycle consistency constrain relies on that an invertible mapping\\rexists between the two domains. This is, however, not the case for the\\rtranslation between images stained with chromogenic monoplex and duplex\\rimmunohistochemistry (IHC) assays. Focusing on the translation from the latter\\rto the first, we propose - through the introduction of a novel training design,\\ran alternative constrain leveraging a set of immunofluorescence (IF) images as\\ran auxiliary unpaired image domain. Quantitative and qualitative results on a\\rdownstream segmentation task show the benefit of the proposed method in\\rcomparison to baseline approaches.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07389 ,  6634kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07392\\rDate: Tue, 12 Mar 2024 07:59:41 GMT   (1329kb,D)\\r\\rTitle: ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature\\r  Interaction for Dense Predictions\\rAuthors: Chunlong Xia, Xinliang Wang, Feng Lv, Xin Hao, Yifeng Shi\\rCategories: cs.CV\\rComments: CVPR2024\\r\\\\\\\\\\r  Although Vision Transformer (ViT) has achieved significant success in\\rcomputer vision, it does not perform well in dense prediction tasks due to the\\rlack of inner-patch information interaction and the limited diversity of\\rfeature scale. Most existing studies are devoted to designing vision-specific\\rtransformers to solve the above problems, which introduce additional\\rpre-training costs. Therefore, we present a plain, pre-training-free, and\\rfeature-enhanced ViT backbone with Convolutional Multi-scale feature\\rinteraction, named ViT-CoMer, which facilitates bidirectional interaction\\rbetween CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has\\rthe following advantages: (1) We inject spatial pyramid multi-receptive field\\rconvolutional features into the ViT architecture, which effectively alleviates\\rthe problems of limited local information interaction and single-feature\\rrepresentation in ViT. (2) We propose a simple and efficient CNN-Transformer\\rbidirectional fusion interaction module that performs multi-scale fusion across\\rhierarchical features, which is beneficial for handling dense prediction tasks.\\r(3) We evaluate the performance of ViT-CoMer across various dense prediction\\rtasks, different frameworks, and multiple advanced pre-training. Notably, our\\rViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and\\r62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art\\rmethods. We hope ViT-CoMer can serve as a new backbone for dense prediction\\rtasks to facilitate future research. The code will be released at\\rhttps://github.com/Traffic-X/ViT-CoMer.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07392 ,  1329kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07403\\rDate: Tue, 12 Mar 2024 08:32:23 GMT   (18285kb,D)\\r\\rTitle: From Canteen Food to Daily Meals: Generalizing Food Recognition to More\\r  Practical Scenarios\\rAuthors: Guoshan Liu, Yang Jiao, Jingjing Chen, Bin Zhu, Yu-Gang Jiang\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  The precise recognition of food categories plays a pivotal role for\\rintelligent health management, attracting significant research attention in\\rrecent years. Prominent benchmarks, such as Food-101 and VIREO Food-172,\\rprovide abundant food image resources that catalyze the prosperity of research\\rin this field. Nevertheless, these datasets are well-curated from canteen\\rscenarios and thus deviate from food appearances in daily life. This\\rdiscrepancy poses great challenges in effectively transferring classifiers\\rtrained on these canteen datasets to broader daily-life scenarios encountered\\rby humans. Toward this end, we present two new benchmarks, namely DailyFood-172\\rand DailyFood-16, specifically designed to curate food images from everyday\\rmeals. These two datasets are used to evaluate the transferability of\\rapproaches from the well-curated food image domain to the everyday-life food\\rimage domain. In addition, we also propose a simple yet effective baseline\\rmethod named Multi-Cluster Reference Learning (MCRL) to tackle the\\raforementioned domain gap. MCRL is motivated by the observation that food\\rimages in daily-life scenarios exhibit greater intra-class appearance variance\\rcompared with those in well-curated benchmarks. Notably, MCRL can be seamlessly\\rcoupled with existing approaches, yielding non-trivial performance\\renhancements. We hope our new benchmarks can inspire the community to explore\\rthe transferability of food recognition models trained on well-curated datasets\\rtoward practical real-life applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07403 ,  18285kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07406\\rDate: Tue, 12 Mar 2024 08:34:05 GMT   (38kb)\\r\\rTitle: FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental\\r  Learning with Hill-Climbing\\rAuthors: Eduard Hogea, Adrian Popescu, Darian Onchis, Gr\\\\'egoire Petit\\rCategories: cs.CV\\rComments: arXiv admin note: text overlap with arXiv:2211.13131\\r\\\\\\\\\\r  Exemplar-free class-incremental learning (EFCIL) poses significant\\rchallenges, primarily due to catastrophic forgetting, necessitating a delicate\\rbalance between stability and plasticity to accurately recognize both new and\\rprevious classes. Traditional EFCIL approaches typically skew towards either\\rmodel plasticity through successive fine-tuning or stability by employing a\\rfixed feature extractor beyond the initial incremental state. Building upon the\\rfoundational FeTrIL framework, our research extends into novel experimental\\rdomains to examine the efficacy of various oversampling techniques and dynamic\\roptimization strategies across multiple challenging datasets and incremental\\rsettings. We specifically explore how oversampling impacts accuracy relative to\\rfeature availability and how different optimization methodologies, including\\rdynamic recalibration and feature pool diversification, influence incremental\\rlearning outcomes. The results from these comprehensive experiments, conducted\\ron CIFAR100, Tiny-ImageNet, and an ImageNet-Subset, under-score the superior\\rperformance of FeTrIL in balancing accuracy for both new and past classes\\ragainst ten contemporary methods. Notably, our extensions reveal the nuanced\\rimpacts of oversampling and optimization on EFCIL, contributing to a more\\rrefined understanding of feature-space manipulation for class incremental\\rlearning. FeTrIL and its extended analysis in this paper FeTrIL++ pave the way\\rfor more adaptable and efficient EFCIL methodologies, promising significant\\rimprovements in handling catastrophic forgetting without the need for\\rexemplars.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07406 ,  38kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07407\\rDate: Tue, 12 Mar 2024 08:34:34 GMT   (4772kb,D)\\r\\rTitle: In-context learning enables multimodal large language models to classify\\r  cancer pathology images\\rAuthors: Dyke Ferber, Georg W\\\\olflein, Isabella C. Wiest, Marta Ligero,\\r  Srividhya Sainath, Narmin Ghaffari Laleh, Omar S.M. El Nahhas, Gustav\\r  M\\\\uller-Franzes, Dirk J\\\\ager, Daniel Truhn, Jakob Nikolas Kather\\rCategories: cs.CV\\rComments: 40 pages, 5 figures\\r\\\\\\\\\\r  Medical image classification requires labeled, task-specific datasets which\\rare used to train deep learning networks de novo, or to fine-tune foundation\\rmodels. However, this process is computationally and technically demanding. In\\rlanguage processing, in-context learning provides an alternative, where models\\rlearn from within prompts, bypassing the need for parameter updates. Yet,\\rin-context learning remains underexplored in medical image analysis. Here, we\\rsystematically evaluate the model Generative Pretrained Transformer 4 with\\rVision capabilities (GPT-4V) on cancer image processing with in-context\\rlearning on three cancer histopathology tasks of high importance:\\rClassification of tissue subtypes in colorectal cancer, colon polyp subtyping\\rand breast tumor detection in lymph node sections. Our results show that\\rin-context learning is sufficient to match or even outperform specialized\\rneural networks trained for particular tasks, while only requiring a minimal\\rnumber of samples. In summary, this study demonstrates that large vision\\rlanguage models trained on non-domain specific data can be applied out-of-the\\rbox to solve medical image-processing tasks in histopathology. This\\rdemocratizes access of generalist AI models to medical experts without\\rtechnical background especially for areas where annotated data is scarce.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07407 ,  4772kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07408\\rDate: Tue, 12 Mar 2024 08:35:42 GMT   (48121kb,D)\\r\\rTitle: NightHaze: Nighttime Image Dehazing via Self-Prior Learning\\rAuthors: Beibei Lin, Yeying Jin, Wending Yan, Wei Ye, Yuan Yuan and Robby T.\\r  Tan\\rCategories: cs.CV\\r\\\\\\\\\\r  Masked autoencoder (MAE) shows that severe augmentation during training\\rproduces robust representations for high-level tasks. This paper brings the\\rMAE-like framework to nighttime image enhancement, demonstrating that severe\\raugmentation during training produces strong network priors that are resilient\\rto real-world night haze degradations. We propose a novel nighttime image\\rdehazing method with self-prior learning. Our main novelty lies in the design\\rof severe augmentation, which allows our model to learn robust priors. Unlike\\rMAE that uses masking, we leverage two key challenging factors of nighttime\\rimages as augmentation: light effects and noise. During training, we\\rintentionally degrade clear images by blending them with light effects as well\\ras by adding noise, and subsequently restore the clear images. This enables our\\rmodel to learn clear background priors. By increasing the noise values to\\rapproach as high as the pixel intensity values of the glow and light effect\\rblended images, our augmentation becomes severe, resulting in stronger priors.\\rWhile our self-prior learning is considerably effective in suppressing glow and\\rrevealing details of background scenes, in some cases, there are still some\\rundesired artifacts that remain, particularly in the forms of over-suppression.\\rTo address these artifacts, we propose a self-refinement module based on the\\rsemi-supervised teacher-student framework. Our NightHaze, especially our\\rMAE-like self-prior learning, shows that models trained with severe\\raugmentation effectively improve the visibility of input haze images,\\rapproaching the clarity of clear nighttime images. Extensive experiments\\rdemonstrate that our NightHaze achieves state-of-the-art performance,\\routperforming existing nighttime image dehazing methods by a substantial margin\\rof 15.5% for MUSIQ and 23.5% for ClipIQA.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07408 ,  48121kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07420\\rDate: Tue, 12 Mar 2024 08:57:29 GMT   (43189kb,D)\\r\\rTitle: DragAnything: Motion Control for Anything using Entity Representation\\rAuthors: Wejia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao\\r  Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, Di Zhang\\rCategories: cs.CV\\rComments: The project website is at:\\r  https://weijiawu.github.io/draganything_page/ . The code is at:\\r  https://github.com/showlab/DragAnything\\r\\\\\\\\\\r  We introduce DragAnything, which utilizes a entity representation to achieve\\rmotion control for any object in controllable video generation. Comparison to\\rexisting motion control methods, DragAnything offers several advantages.\\rFirstly, trajectory-based is more userfriendly for interaction, when acquiring\\rother guidance signals (e.g., masks, depth maps) is labor-intensive. Users only\\rneed to draw a line (trajectory) during interaction. Secondly, our entity\\rrepresentation serves as an open-domain embedding capable of representing any\\robject, enabling the control of motion for diverse entities, including\\rbackground. Lastly, our entity representation allows simultaneous and distinct\\rmotion control for multiple objects. Extensive experiments demonstrate that our\\rDragAnything achieves state-of-the-art performance for FVD, FID, and User\\rStudy, particularly in terms of object motion control, where our method\\rsurpasses the previous methods (e.g., DragNUWA) by 26% in human voting.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07420 ,  43189kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07432\\rDate: Tue, 12 Mar 2024 09:15:19 GMT   (11169kb,D)\\r\\rTitle: Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for\\r  Scene Flow\\rAuthors: Hanyu Zhou, Yi Chang, Zhiwei Shi, Luxin Yan\\rCategories: cs.CV\\r\\\\\\\\\\r  Single RGB or LiDAR is the mainstream sensor for the challenging scene flow,\\rwhich relies heavily on visual features to match motion features. Compared with\\rsingle modality, existing methods adopt a fusion strategy to directly fuse the\\rcross-modal complementary knowledge in motion space. However, these direct\\rfusion methods may suffer the modality gap due to the visual intrinsic\\rheterogeneous nature between RGB and LiDAR, thus deteriorating motion features.\\rWe discover that event has the homogeneous nature with RGB and LiDAR in both\\rvisual and motion spaces. In this work, we bring the event as a bridge between\\rRGB and LiDAR, and propose a novel hierarchical visual-motion fusion framework\\rfor scene flow, which explores a homogeneous space to fuse the cross-modal\\rcomplementary knowledge for physical interpretation. In visual fusion, we\\rdiscover that event has a complementarity (relative v.s. absolute) in luminance\\rspace with RGB for high dynamic imaging, and has a complementarity (local\\rboundary v.s. global shape) in scene structure space with LiDAR for structure\\rintegrity. In motion fusion, we figure out that RGB, event and LiDAR are\\rcomplementary (spatial-dense, temporal-dense v.s. spatiotemporal-sparse) to\\reach other in correlation space, which motivates us to fuse their motion\\rcorrelations for motion continuity. The proposed hierarchical fusion can\\rexplicitly fuse the multimodal knowledge to progressively improve scene flow\\rfrom visual space to motion space. Extensive experiments have been performed to\\rverify the superiority of the proposed method.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07432 ,  11169kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07436\\rDate: Tue, 12 Mar 2024 09:22:52 GMT   (2019kb,D)\\r\\rTitle: JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object\\r  Detection\\rAuthors: Hanyu Zhou, Zhiwei Shi, Hao Dong, Shihan Peng, Yi Chang, Luxin Yan\\rCategories: cs.CV\\r\\\\\\\\\\r  Event-based moving object detection is a challenging task, where static\\rbackground and moving object are mixed together. Typically, existing methods\\rmainly align the background events to the same spatial coordinate system via\\rmotion compensation to distinguish the moving object. However, they neglect the\\rpotential spatial tailing effect of moving object events caused by excessive\\rmotion, which may affect the structure integrity of the extracted moving\\robject. We discover that the moving object has a complete columnar structure in\\rthe point cloud composed of motion-compensated events along the timestamp.\\rMotivated by this, we propose a novel joint spatio-temporal reasoning method\\rfor event-based moving object detection. Specifically, we first compensate the\\rmotion of background events using inertial measurement unit. In spatial\\rreasoning stage, we project the compensated events into the same image\\rcoordinate, discretize the timestamp of events to obtain a time image that can\\rreflect the motion confidence, and further segment the moving object through\\radaptive threshold on the time image. In temporal reasoning stage, we construct\\rthe events into a point cloud along timestamp, and use RANSAC algorithm to\\rextract the columnar shape in the cloud for peeling off the background.\\rFinally, we fuse the results from the two reasoning stages to extract the final\\rmoving object region. This joint spatio-temporal reasoning framework can\\reffectively detect the moving object from motion confidence and geometric\\rstructure. Moreover, we conduct extensive experiments on various datasets to\\rverify that the proposed method can improve the moving object detection\\raccuracy by 13\\\\%.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07436 ,  2019kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07437\\rDate: Tue, 12 Mar 2024 09:28:11 GMT   (5187kb,D)\\r\\rTitle: Category-Agnostic Pose Estimation for Point Clouds\\rAuthors: Bowen Liu, Wei Liu, Siang Chen, Pengwei Xie and Guijin Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  The goal of object pose estimation is to visually determine the pose of a\\rspecific object in the RGB-D input. Unfortunately, when faced with new\\rcategories, both instance-based and category-based methods are unable to deal\\rwith unseen objects of unseen categories, which is a challenge for pose\\restimation. To address this issue, this paper proposes a method to introduce\\rgeometric features for pose estimation of point clouds without requiring\\rcategory information. The method is based only on the patch feature of the\\rpoint cloud, a geometric feature with rotation invariance. After training\\rwithout category information, our method achieves as good results as other\\rcategory-based methods. Our method successfully achieved pose annotation of no\\rcategory information instances on the CAMERA25 dataset and ModelNet40 dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07437 ,  5187kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07469\\rDate: Tue, 12 Mar 2024 10:04:08 GMT   (3581kb,D)\\r\\rTitle: A Comprehensive Survey of 3D Dense Captioning: Localizing and Describing\\r  Objects in 3D Scenes\\rAuthors: Ting Yu, Xiaojun Lin, Shuhui Wang, Weiguo Sheng, Qingming Huang, Jun\\r  Yu\\rCategories: cs.CV\\rDOI: 10.1109/TCSVT.2023.3296889\\r\\\\\\\\\\r  Three-Dimensional (3D) dense captioning is an emerging vision-language\\rbridging task that aims to generate multiple detailed and accurate descriptions\\rfor 3D scenes. It presents significant potential and challenges due to its\\rcloser representation of the real world compared to 2D visual captioning, as\\rwell as complexities in data collection and processing of 3D point cloud\\rsources. Despite the popularity and success of existing methods, there is a\\rlack of comprehensive surveys summarizing the advancements in this field, which\\rhinders its progress. In this paper, we provide a comprehensive review of 3D\\rdense captioning, covering task definition, architecture classification,\\rdataset analysis, evaluation metrics, and in-depth prosperity discussions.\\rBased on a synthesis of previous literature, we refine a standard pipeline that\\rserves as a common paradigm for existing methods. We also introduce a clear\\rtaxonomy of existing models, summarize technologies involved in different\\rmodules, and conduct detailed experiment analysis. Instead of a chronological\\rorder introduction, we categorize the methods into different classes to\\rfacilitate exploration and analysis of the differences and connections among\\rexisting techniques. We also provide a reading guideline to assist readers with\\rdifferent backgrounds and purposes in reading efficiently. Furthermore, we\\rpropose a series of promising future directions for 3D dense captioning by\\ridentifying challenges and aligning them with the development of related tasks,\\roffering valuable insights and inspiring future research in this field. Our aim\\ris to provide a comprehensive understanding of 3D dense captioning, foster\\rfurther investigations, and contribute to the development of novel applications\\rin multimedia and related domains.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07469 ,  3581kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07487\\rDate: Tue, 12 Mar 2024 10:25:29 GMT   (2785kb,D)\\r\\rTitle: Motion Mamba: Efficient and Long Sequence Motion Generation with\\r  Hierarchical and Bidirectional Selective SSM\\rAuthors: Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, Hao\\r  Tang\\rCategories: cs.CV\\r\\\\\\\\\\r  Human motion generation stands as a significant pursuit in generative\\rcomputer vision, while achieving long-sequence and efficient motion generation\\rremains challenging. Recent advancements in state space models (SSMs), notably\\rMamba, have showcased considerable promise in long sequence modeling with an\\refficient hardware-aware design, which appears to be a promising direction to\\rbuild motion generation model upon it. Nevertheless, adapting SSMs to motion\\rgeneration faces hurdles since the lack of a specialized design architecture to\\rmodel motion sequence. To address these challenges, we propose Motion Mamba, a\\rsimple and efficient approach that presents the pioneering motion generation\\rmodel utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba\\r(HTM) block to process temporal data by ensemble varying numbers of isolated\\rSSM modules across a symmetric U-Net architecture aimed at preserving motion\\rconsistency between frames. We also design a Bidirectional Spatial Mamba (BSM)\\rblock to bidirectionally process latent poses, to enhance accurate motion\\rgeneration within a temporal frame. Our proposed method achieves up to 50% FID\\rimprovement and up to 4 times faster on the HumanML3D and KIT-ML datasets\\rcompared to the previous best diffusion-based method, which demonstrates strong\\rcapabilities of high-quality long sequence motion modeling and real-time human\\rmotion generation. See project website\\rhttps://steve-zeyu-zhang.github.io/MotionMamba/\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07487 ,  2785kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07500\\rDate: Tue, 12 Mar 2024 10:38:03 GMT   (8102kb,D)\\r\\rTitle: Block-wise LoRA: Revisiting Fine-grained LoRA for Effective\\r  Personalization and Stylization in Text-to-Image Generation\\rAuthors: Likun Li, Haoqi Zeng, Changpeng Yang, Haozhe Jia, Di Xu\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  The objective of personalization and stylization in text-to-image is to\\rinstruct a pre-trained diffusion model to analyze new concepts introduced by\\rusers and incorporate them into expected styles. Recently, parameter-efficient\\rfine-tuning (PEFT) approaches have been widely adopted to address this task and\\rhave greatly propelled the development of this field. Despite their popularity,\\rexisting efficient fine-tuning methods still struggle to achieve effective\\rpersonalization and stylization in T2I generation. To address this issue, we\\rpropose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained\\rfine-tuning for different blocks of SD, which can generate images faithful to\\rinput prompts and target identity and also with desired style. Extensive\\rexperiments demonstrate the effectiveness of the proposed method.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07500 ,  8102kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07508\\rDate: Tue, 12 Mar 2024 10:44:13 GMT   (6454kb,D)\\r\\rTitle: MoAI: Mixture of All Intelligence for Large Language and Vision Models\\rAuthors: Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro\\rCategories: cs.CV\\rComments: Code available: https://github.com/ByungKwanLee/MoAI\\r\\\\\\\\\\r  The rise of large language models (LLMs) and instruction tuning has led to\\rthe current trend of instruction-tuned large language and vision models\\r(LLVMs). This trend involves either meticulously curating numerous instruction\\rtuning datasets tailored to specific objectives or enlarging LLVMs to manage\\rvast amounts of vision language (VL) data. However, current LLVMs have\\rdisregarded the detailed and comprehensive real-world scene understanding\\ravailable from specialized computer vision (CV) models in visual perception\\rtasks such as segmentation, detection, scene graph generation (SGG), and\\roptical character recognition (OCR). Instead, the existing LLVMs rely mainly on\\rthe large capacity and emergent capabilities of their LLM backbones. Therefore,\\rwe present a new LLVM, Mixture of All Intelligence (MoAI), which leverages\\rauxiliary visual information obtained from the outputs of external\\rsegmentation, detection, SGG, and OCR models. MoAI operates through two newly\\rintroduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the\\routputs of the external CV models, the MoAI-Compressor aligns and condenses\\rthem to efficiently use relevant auxiliary visual information for VL tasks.\\rMoAI-Mixer then blends three types of intelligence (1) visual features, (2)\\rauxiliary features from the external CV models, and (3) language features by\\rutilizing the concept of Mixture of Experts. Through this integration, MoAI\\rsignificantly outperforms both open-source and closed-source LLVMs in numerous\\rzero-shot VL tasks, particularly those related to real-world scene\\runderstanding such as object existence, positions, relations, and OCR without\\renlarging the model size or curating extra visual instruction tuning datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07508 ,  6454kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07513\\rDate: Tue, 12 Mar 2024 10:47:29 GMT   (2102kb,D)\\r\\rTitle: Spatiotemporal Representation Learning for Short and Long Medical Image\\r  Time Series\\rAuthors: Chengzhi Shen, Martin J. Menten, Hrvoje Bogunovi\\\\'c, Ursula\\r  Schmidt-Erfurth, Hendrik Scholl, Sobha Sivaprasad, Andrew Lotery, Daniel\\r  Rueckert, Paul Hager, Robbie Holland\\rCategories: cs.CV\\r\\\\\\\\\\r  Analyzing temporal developments is crucial for the accurate prognosis of many\\rmedical conditions. Temporal changes that occur over short time scales are key\\rto assessing the health of physiological functions, such as the cardiac cycle.\\rMoreover, tracking longer term developments that occur over months or years in\\revolving processes, such as age-related macular degeneration (AMD), is\\ressential for accurate prognosis. Despite the importance of both short and long\\rterm analysis to clinical decision making, they remain understudied in medical\\rdeep learning. State of the art methods for spatiotemporal representation\\rlearning, developed for short natural videos, prioritize the detection of\\rtemporal constants rather than temporal developments. Moreover, they do not\\raccount for varying time intervals between acquisitions, which are essential\\rfor contextualizing observed changes. To address these issues, we propose two\\rapproaches. First, we combine clip-level contrastive learning with a novel\\rtemporal embedding to adapt to irregular time series. Second, we propose\\rmasking and predicting latent frame representations of the temporal sequence.\\rOur two approaches outperform all prior methods on temporally-dependent tasks\\rincluding cardiac output estimation and three prognostic AMD tasks. Overall,\\rthis enables the automated analysis of temporal patterns which are typically\\roverlooked in applications of deep learning to medicine.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07513 ,  2102kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07514\\rDate: Tue, 12 Mar 2024 10:47:45 GMT   (2511kb,D)\\r\\rTitle: Uncertainty-guided Contrastive Learning for Single Source Domain\\r  Generalisation\\rAuthors: Anastasios Arsenos and Dimitrios Kollias and Evangelos Petrongonas and\\r  Christos Skliros and Stefanos Kollias\\rCategories: cs.CV\\rComments: accepted at IEEE ICASSP 2024\\r\\\\\\\\\\r  In the context of single domain generalisation, the objective is for models\\rthat have been exclusively trained on data from a single domain to demonstrate\\rstrong performance when confronted with various unfamiliar domains. In this\\rpaper, we introduce a novel model referred to as Contrastive Uncertainty Domain\\rGeneralisation Network (CUDGNet). The key idea is to augment the source\\rcapacity in both input and label spaces through the fictitious domain generator\\rand jointly learn the domain invariant representation of each class through\\rcontrastive learning. Extensive experiments on two Single Source Domain\\rGeneralisation (SSDG) datasets demonstrate the effectiveness of our approach,\\rwhich surpasses the state-of-the-art single-DG methods by up to $7.08\\\\%$. Our\\rmethod also provides efficient uncertainty estimation at inference time from a\\rsingle forward pass through the generator subnetwork.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07514 ,  2511kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07516\\rDate: Tue, 12 Mar 2024 10:47:53 GMT   (20140kb,D)\\r\\rTitle: D4D: An RGBD diffusion model to boost monocular depth estimation\\rAuthors: L. Papa, P. Russo, and I. Amerini\\rCategories: cs.CV\\r\\\\\\\\\\r  Ground-truth RGBD data are fundamental for a wide range of computer vision\\rapplications; however, those labeled samples are difficult to collect and\\rtime-consuming to produce. A common solution to overcome this lack of data is\\rto employ graphic engines to produce synthetic proxies; however, those data do\\rnot often reflect real-world images, resulting in poor performance of the\\rtrained models at the inference step. In this paper we propose a novel training\\rpipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion\\rmodel able to generate realistic RGBD samples. We show the effectiveness of the\\rdeveloped solution in improving the performances of deep learning models on the\\rmonocular depth estimation task, where the correspondence between RGB and depth\\rmap is crucial to achieving accurate measurements. Our supervised training\\rpipeline, enriched by the generated samples, outperforms synthetic and original\\rdata performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%)\\rrespectively on the indoor NYU Depth v2 and the outdoor KITTI dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07516 ,  20140kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07518\\rDate: Tue, 12 Mar 2024 10:54:38 GMT   (850kb,D)\\r\\rTitle: Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and\\r  Margin Loss\\rAuthors: Xuhua Ren, Hengcan Shi, Jin Li\\rCategories: cs.CV\\r\\\\\\\\\\r  Scene text recognition is an important and challenging task in computer\\rvision. However, most prior works focus on recognizing pre-defined words, while\\rthere are various out-of-vocabulary (OOV) words in real-world applications.\\r  In this paper, we propose a novel open-vocabulary text recognition framework,\\rPseudo-OCR, to recognize OOV words. The key challenge in this task is the lack\\rof OOV training data. To solve this problem, we first propose a pseudo label\\rgeneration module that leverages character detection and image inpainting to\\rproduce substantial pseudo OOV training data from real-world images. Unlike\\rprevious synthetic data, our pseudo OOV data contains real characters and\\rbackgrounds to simulate real-world applications. Secondly, to reduce noises in\\rpseudo data, we present a semantic checking mechanism to filter semantically\\rmeaningful data. Thirdly, we introduce a quality-aware margin loss to boost the\\rtraining with pseudo data. Our loss includes a margin-based part to enhance the\\rclassification ability, and a quality-aware part to penalize low-quality\\rsamples in both real and pseudo data.\\r  Extensive experiments demonstrate that our approach outperforms the\\rstate-of-the-art on eight datasets and achieves the first rank in the ICDAR2022\\rchallenge.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07518 ,  850kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07532\\rDate: Tue, 12 Mar 2024 11:11:19 GMT   (42419kb,D)\\r\\rTitle: Open-World Semantic Segmentation Including Class Similarity\\rAuthors: Matteo Sodano, Federico Magistri, Lucas Nunes, Jens Behley, Cyrill\\r  Stachniss\\rCategories: cs.CV\\rComments: Accepted at CVPR 2024. Code at: https://github.com/PRBonn/ContMAV\\r\\\\\\\\\\r  Interpreting camera data is key for autonomously acting systems, such as\\rautonomous vehicles. Vision systems that operate in real-world environments\\rmust be able to understand their surroundings and need the ability to deal with\\rnovel situations. This paper tackles open-world semantic segmentation, i.e.,\\rthe variant of interpreting image data in which objects occur that have not\\rbeen seen during training. We propose a novel approach that performs accurate\\rclosed-world semantic segmentation and, at the same time, can identify new\\rcategories without requiring any additional training data. Our approach\\radditionally provides a similarity measure for every newly discovered class in\\ran image to a known category, which can be useful information in downstream\\rtasks such as planning or mapping. Through extensive experiments, we show that\\rour model achieves state-of-the-art results on classes known from training data\\ras well as for anomaly segmentation and can distinguish between different\\runknown classes.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07532 ,  42419kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07535\\rDate: Tue, 12 Mar 2024 11:18:35 GMT   (48033kb,D)\\r\\rTitle: Adaptive Fusion of Single-View and Multi-View Depth for Autonomous\\r  Driving\\rAuthors: JunDa Cheng, Wei Yin, Kaixuan Wang, Xiaozhi Chen, Shijie Wang, Xin\\r  Yang\\rCategories: cs.CV\\rComments: Accepted to CVPR 2024\\r\\\\\\\\\\r  Multi-view depth estimation has achieved impressive performance over various\\rbenchmarks. However, almost all current multi-view systems rely on given ideal\\rcamera poses, which are unavailable in many real-world scenarios, such as\\rautonomous driving. In this work, we propose a new robustness benchmark to\\revaluate the depth estimation system under various noisy pose settings.\\rSurprisingly, we find current multi-view depth estimation methods or\\rsingle-view and multi-view fusion methods will fail when given noisy pose\\rsettings. To address this challenge, we propose a single-view and multi-view\\rfused depth estimation system, which adaptively integrates high-confident\\rmulti-view and single-view results for both robust and accurate depth\\restimations. The adaptive fusion module performs fusion by dynamically\\rselecting high-confidence regions between two branches based on a wrapping\\rconfidence map. Thus, the system tends to choose the more reliable branch when\\rfacing textureless scenes, inaccurate calibration, dynamic objects, and other\\rdegradation or challenging conditions. Our method outperforms state-of-the-art\\rmulti-view and fusion methods under robustness testing. Furthermore, we achieve\\rstate-of-the-art performance on challenging benchmarks (KITTI and DDAD) when\\rgiven accurate pose estimations. Project website:\\rhttps://github.com/Junda24/AFNet/.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07535 ,  48033kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07536\\rDate: Tue, 12 Mar 2024 11:19:46 GMT   (10611kb)\\r\\rTitle: LaB-GATr: geometric algebra transformers for large biomedical surface\\r  and volume meshes\\rAuthors: Julian Suk, Baris Imre, Jelmer M. Wolterink\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Many anatomical structures can be described by surface or volume meshes.\\rMachine learning is a promising tool to extract information from these 3D\\rmodels. However, high-fidelity meshes often contain hundreds of thousands of\\rvertices, which creates unique challenges in building deep neural network\\rarchitectures. Furthermore, patient-specific meshes may not be canonically\\raligned which limits the generalisation of machine learning algorithms. We\\rpropose LaB-GATr, a transfomer neural network with geometric tokenisation that\\rcan effectively learn with large-scale (bio-)medical surface and volume meshes\\rthrough sequence compression and interpolation. Our method extends the recently\\rproposed geometric algebra transformer (GATr) and thus respects all Euclidean\\rsymmetries, i.e. rotation, translation and reflection, effectively mitigating\\rthe problem of canonical alignment between patients. LaB-GATr achieves\\rstate-of-the-art results on three tasks in cardiovascular hemodynamics\\rmodelling and neurodevelopmental phenotype prediction, featuring meshes of up\\rto 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful\\rarchitecture for learning with high-fidelity meshes which has the potential to\\renable interesting downstream applications. Our implementation is publicly\\ravailable.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07536 ,  10611kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07542\\rDate: Tue, 12 Mar 2024 11:29:40 GMT   (186kb,D)\\r\\rTitle: A Survey of Vision Transformers in Autonomous Driving: Current Trends\\r  and Future Directions\\rAuthors: Quoc-Vinh Lai-Dang\\rCategories: cs.CV cs.LG\\rComments: 9 pages, 3 figures\\r\\\\\\\\\\r  This survey explores the adaptation of visual transformer models in\\rAutonomous Driving, a transition inspired by their success in Natural Language\\rProcessing. Surpassing traditional Recurrent Neural Networks in tasks like\\rsequential image processing and outperforming Convolutional Neural Networks in\\rglobal context capture, as evidenced in complex scene recognition, Transformers\\rare gaining traction in computer vision. These capabilities are crucial in\\rAutonomous Driving for real-time, dynamic visual scene processing. Our survey\\rprovides a comprehensive overview of Vision Transformer applications in\\rAutonomous Driving, focusing on foundational concepts such as self-attention,\\rmulti-head attention, and encoder-decoder architecture. We cover applications\\rin object detection, segmentation, pedestrian detection, lane detection, and\\rmore, comparing their architectural merits and limitations. The survey\\rconcludes with future research directions, highlighting the growing role of\\rVision Transformers in Autonomous Driving.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07542 ,  186kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07547\\rDate: Tue, 12 Mar 2024 11:32:57 GMT   (9850kb,D)\\r\\rTitle: SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields\\rAuthors: Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee\\rCategories: cs.CV\\rComments: 25 pages, 10 figures, Code is available at\\r  https://github.com/Jho-Yonsei/SMURF\\r\\\\\\\\\\r  Neural radiance fields (NeRF) has attracted considerable attention for their\\rexceptional ability in synthesizing novel views with high fidelity. However,\\rthe presence of motion blur, resulting from slight camera movements during\\rextended shutter exposures, poses a significant challenge, potentially\\rcompromising the quality of the reconstructed 3D scenes. While recent studies\\rhave addressed this issue, they do not consider the continuous dynamics of\\rcamera movements during image acquisition, leading to inaccurate scene\\rreconstruction. Additionally, these methods are plagued by slow training and\\rrendering speed. To effectively handle these issues, we propose sequential\\rmotion understanding radiance fields (SMURF), a novel approach that employs\\rneural ordinary differential equation (Neural-ODE) to model continuous camera\\rmotion and leverages the explicit volumetric representation method for faster\\rtraining and robustness to motion-blurred input images. The core idea of the\\rSMURF is continuous motion blurring kernel (CMBK), a unique module designed to\\rmodel a continuous camera movements for processing blurry inputs. Our model,\\rrigorously evaluated against benchmark datasets, demonstrates state-of-the-art\\rperformance both quantitatively and qualitatively.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07547 ,  9850kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07560\\rDate: Tue, 12 Mar 2024 11:48:49 GMT   (11913kb,D)\\r\\rTitle: Unleashing Network Potentials for Semantic Scene Completion\\rAuthors: Fengyun Wang, Qianru Sun, Dong Zhang, and Jinhui Tang\\rCategories: cs.CV\\rComments: accepted by CVPR2024\\r\\\\\\\\\\r  Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy\\rand semantics from a single-view RGB-D image, and recent SSC methods commonly\\radopt multi-modal inputs. However, our investigation reveals two limitations:\\rineffective feature learning from single modalities and overfitting to limited\\rdatasets. To address these issues, this paper proposes a novel SSC framework -\\rAdversarial Modality Modulation Network (AMMNet) - with a fresh perspective of\\roptimizing gradient updates. The proposed AMMNet introduces two core modules: a\\rcross-modal modulation enabling the interdependence of gradient flows between\\rmodalities, and a customized adversarial training scheme leveraging dynamic\\rgradient competition. Specifically, the cross-modal modulation adaptively\\rre-calibrates the features to better excite representation potentials from each\\rsingle modality. The adversarial training employs a minimax game of evolving\\rgradients, with customized guidance to strengthen the generator's perception of\\rvisual fidelity from both geometric completeness and semantic correctness.\\rExtensive experimental results demonstrate that AMMNet outperforms\\rstate-of-the-art SSC methods by a large margin, providing a promising direction\\rfor improving the effectiveness and generalization of SSC methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07560 ,  11913kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07564\\rDate: Tue, 12 Mar 2024 11:51:59 GMT   (10544kb,D)\\r\\rTitle: RSBuilding: Towards General Remote Sensing Image Building Extraction and\\r  Change Detection with Foundation Model\\rAuthors: Mingze Wang, Keyan Chen, Lili Su, Cilin Yan, Sheng Xu, Haotian Zhang,\\r  Pengcheng Yuan, Xiaolong Jiang and Baochang Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  The intelligent interpretation of buildings plays a significant role in urban\\rplanning and management, macroeconomic analysis, population dynamics, etc.\\rRemote sensing image building interpretation primarily encompasses building\\rextraction and change detection. However, current methodologies often treat\\rthese two tasks as separate entities, thereby failing to leverage shared\\rknowledge. Moreover, the complexity and diversity of remote sensing image\\rscenes pose additional challenges, as most algorithms are designed to model\\rindividual small datasets, thus lacking cross-scene generalization. In this\\rpaper, we propose a comprehensive remote sensing image building understanding\\rmodel, termed RSBuilding, developed from the perspective of the foundation\\rmodel. RSBuilding is designed to enhance cross-scene generalization and task\\runiversality. Specifically, we extract image features based on the prior\\rknowledge of the foundation model and devise a multi-level feature sampler to\\raugment scale information. To unify task representation and integrate image\\rspatiotemporal clues, we introduce a cross-attention decoder with task prompts.\\rAddressing the current shortage of datasets that incorporate annotations for\\rboth tasks, we have developed a federated training strategy to facilitate\\rsmooth model convergence even when supervision for some tasks is missing,\\rthereby bolstering the complementarity of different tasks. Our model was\\rtrained on a dataset comprising up to 245,000 images and validated on multiple\\rbuilding extraction and change detection datasets. The experimental results\\rsubstantiate that RSBuilding can concurrently handle two structurally distinct\\rtasks and exhibits robust zero-shot generalization capabilities.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07564 ,  10544kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07570\\rDate: Tue, 12 Mar 2024 11:58:37 GMT   (1493kb)\\r\\rTitle: An Active Contour Model Driven By the Hybrid Signed Pressure Function\\rAuthors: Jing Zhao\\rCategories: cs.CV cs.NA math.NA\\r\\\\\\\\\\r  Due to the influence of imaging equipment and complex imaging environments,\\rmost images in daily life have features of intensity inhomogeneity and noise.\\rTherefore, many scholars have designed many image segmentation algorithms to\\raddress these issues. Among them, the active contour model is one of the most\\reffective image segmentation algorithms.This paper proposes an active contour\\rmodel driven by the hybrid signed pressure function that combines global and\\rlocal information construction. Firstly, a new global region-based signed\\rpressure function is introduced by combining the average intensity of the inner\\rand outer regions of the curve with the median intensity of the inner region of\\rthe evolution curve. Then, the paper uses the energy differences between the\\rinner and outer regions of the curve in the local region to design the signed\\rpressure function of the local term. Combine the two SPF function to obtain a\\rnew signed pressure function and get the evolution equation of the new model.\\rFinally, experiments and numerical analysis show that the model has excellent\\rsegmentation performance for both intensity inhomogeneous images and noisy\\rimages.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07570 ,  1493kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07576\\rDate: Tue, 12 Mar 2024 12:05:43 GMT   (413kb,D)\\r\\rTitle: FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine\\r  Tuning in High-resolution Medical Image Classification\\rAuthors: Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang\\rCategories: cs.CV\\r\\\\\\\\\\r  Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to\\rtransfer pre-trained models to downstream tasks, avoiding the high cost of\\rupdating entire large-scale pre-trained models (LPMs). In this work, we present\\rFine-grained Prompt Tuning (FPT), a novel PEFT method for medical image\\rclassification. FPT significantly reduces memory consumption compared to other\\rPEFT methods, especially in high-resolution contexts. To achieve this, we first\\rfreeze the weights of the LPM and construct a learnable lightweight side\\rnetwork. The frozen LPM takes high-resolution images as input to extract\\rfine-grained features, while the side network is fed low-resolution images to\\rreduce memory usage. To allow the side network to access pre-trained knowledge,\\rwe introduce fine-grained prompts that summarize information from the LPM\\rthrough a fusion module. Important tokens selection and preloading techniques\\rare employed to further reduce training cost and memory requirements. We\\revaluate FPT on four medical datasets with varying sizes, modalities, and\\rcomplexities. Experimental results demonstrate that FPT achieves comparable\\rperformance to fine-tuning the entire LPM while using only 1.8% of the\\rlearnable parameters and 13% of the memory costs of an encoder ViT-B model with\\ra 512 x 512 input resolution.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07576 ,  413kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07578\\rDate: Tue, 12 Mar 2024 12:07:00 GMT   (25941kb,D)\\r\\rTitle: AACP: Aesthetics assessment of children's paintings based on\\r  self-supervised learning\\rAuthors: Shiqi Jiang, Ning Li, Chen Shi, Liping Guo, Changbo Wang, Chenhui Li\\rCategories: cs.CV\\rComments: AAAI 2024\\r\\\\\\\\\\r  The Aesthetics Assessment of Children's Paintings (AACP) is an important\\rbranch of the image aesthetics assessment (IAA), playing a significant role in\\rchildren's education. This task presents unique challenges, such as limited\\ravailable data and the requirement for evaluation metrics from multiple\\rperspectives. However, previous approaches have relied on training large\\rdatasets and subsequently providing an aesthetics score to the image, which is\\rnot applicable to AACP. To solve this problem, we construct an aesthetics\\rassessment dataset of children's paintings and a model based on self-supervised\\rlearning. 1) We build a novel dataset composed of two parts: the first part\\rcontains more than 20k unlabeled images of children's paintings; the second\\rpart contains 1.2k images of children's paintings, and each image contains\\reight attributes labeled by multiple design experts. 2) We design a pipeline\\rthat includes a feature extraction module, perception modules and a\\rdisentangled evaluation module. 3) We conduct both qualitative and quantitative\\rexperiments to compare our model's performance with five other methods using\\rthe AACP dataset. Our experiments reveal that our method can accurately capture\\raesthetic features and achieve state-of-the-art performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07578 ,  25941kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07589\\rDate: Tue, 12 Mar 2024 12:19:05 GMT   (898kb,D)\\r\\rTitle: PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral\\r  Convolution\\rAuthors: Honghao Chen, Xiangxiang Chu, Yongjian Ren, Xin Zhao, Kaiqi Huang\\rCategories: cs.CV\\rComments: CVPR 2024\\r\\\\\\\\\\r  Recently, some large kernel convnets strike back with appealing performance\\rand efficiency. However, given the square complexity of convolution, scaling up\\rkernels can bring about an enormous amount of parameters and the proliferated\\rparameters can induce severe optimization problem. Due to these issues, current\\rCNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e.,\\r51x5 + 5x51) and start to saturate as the kernel size continues growing. In\\rthis paper, we delve into addressing these vital issues and explore whether we\\rcan continue scaling up kernels for more performance gains. Inspired by human\\rvision, we propose a human-like peripheral convolution that efficiently reduces\\rover 90% parameter count of dense grid convolution through parameter sharing,\\rand manage to scale up kernel size to extremely large. Our peripheral\\rconvolution behaves highly similar to human, reducing the complexity of\\rconvolution from O(K^2) to O(logK) without backfiring performance. Built on\\rthis, we propose Parameter-efficient Large Kernel Network (PeLK). Our PeLK\\routperforms modern vision Transformers and ConvNet architectures like Swin,\\rConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet\\rclassification, semantic segmentation on ADE20K and object detection on MS\\rCOCO. For the first time, we successfully scale up the kernel size of CNNs to\\ran unprecedented 101x101 and demonstrate consistent improvements.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07589 ,  898kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07592\\rDate: Tue, 12 Mar 2024 12:25:38 GMT   (32985kb,D)\\r\\rTitle: Accurate Spatial Gene Expression Prediction by integrating\\r  Multi-resolution features\\rAuthors: Youngmin Chung, Ji Hun Ha, Kyeong Chan Im, Joo Sang Lee\\rCategories: cs.CV\\r\\\\\\\\\\r  Recent advancements in Spatial Transcriptomics (ST) technology have\\rfacilitated detailed gene expression analysis within tissue contexts. However,\\rthe high costs and methodological limitations of ST necessitate a more robust\\rpredictive model. In response, this paper introduces TRIPLEX, a novel deep\\rlearning framework designed to predict spatial gene expression from Whole Slide\\rImages (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing\\rcellular morphology at individual spots, the local context around these spots,\\rand the global tissue organization. By integrating these features through an\\reffective fusion strategy, TRIPLEX achieves accurate gene expression\\rprediction. Our comprehensive benchmark study, conducted on three public ST\\rdatasets and supplemented with Visium data from 10X Genomics, demonstrates that\\rTRIPLEX outperforms current state-of-the-art models in Mean Squared Error\\r(MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC).\\rThe model's predictions align closely with ground truth gene expression\\rprofiles and tumor annotations, underscoring TRIPLEX's potential in advancing\\rcancer diagnosis and treatment.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07592 ,  32985kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07593\\rDate: Tue, 12 Mar 2024 12:25:54 GMT   (2333kb,D)\\r\\rTitle: MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D\\r  Sparse Convolutions\\rAuthors: J.J. Cabrera, A. Santo, A. Gil, C. Viegas and L. Pay\\\\'a\\rCategories: cs.CV\\r\\\\\\\\\\r  This paper presents MinkUNeXt, an effective and efficient architecture for\\rplace-recognition from point clouds entirely based on the new 3D MinkNeXt\\rBlock, a residual block composed of 3D sparse convolutions that follows the\\rphilosophy established by recent Transformers but purely using simple 3D\\rconvolutions. Feature extraction is performed at different scales by a U-Net\\rencoder-decoder network and the feature aggregation of those features into a\\rsingle descriptor is carried out by a Generalized Mean Pooling (GeM). The\\rproposed architecture demonstrates that it is possible to surpass the current\\rstate-of-the-art by only relying on conventional 3D sparse convolutions without\\rmaking use of more complex and sophisticated proposals such as Transformers,\\rAttention-Layers or Deformable Convolutions. A thorough assessment of the\\rproposal has been carried out using the Oxford RobotCar and the In-house\\rdatasets. As a result, MinkUNeXt proves to outperform other methods in the\\rstate-of-the-art.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07593 ,  2333kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07598\\rDate: Tue, 12 Mar 2024 12:35:12 GMT   (19329kb,D)\\r\\rTitle: Mondrian: On-Device High-Performance Video Analytics with Compressive\\r  Packed Inference\\rAuthors: Changmin Jeon, Seonjun Kim, Juheon Yi, Youngki Lee\\rCategories: cs.CV\\r\\\\\\\\\\r  In this paper, we present Mondrian, an edge system that enables\\rhigh-performance object detection on high-resolution video streams. Many\\rlightweight models and system optimization techniques have been proposed for\\rresource-constrained devices, but they do not fully utilize the potential of\\rthe accelerators over dynamic, high-resolution videos. To enable such\\rcapability, we devise a novel Compressive Packed Inference to minimize\\rper-pixel processing costs by selectively determining the necessary pixels to\\rprocess and combining them to maximize processing parallelism. In particular,\\rour system quickly extracts ROIs and dynamically shrinks them, reflecting the\\reffect of the fast-changing characteristics of objects and scenes. It then\\rintelligently combines such scaled ROIs into large canvases to maximize the\\rutilization of inference accelerators such as GPU. Evaluation across various\\rdatasets, models, and devices shows Mondrian outperforms state-of-the-art\\rbaselines (e.g., input rescaling, ROI extractions, ROI extractions+batching) by\\r15.0-19.7% higher accuracy, leading to $\\\\times$6.65 higher throughput than\\rframe-wise inference for processing various 1080p video streams. We will\\rrelease the code after the paper review.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07598 ,  19329kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07601\\rDate: Tue, 12 Mar 2024 12:40:08 GMT   (2621kb,D)\\r\\rTitle: Unified Source-Free Domain Adaptation\\rAuthors: Song Tang, Wenxin Su, Mao Ye, Jianwei Zhang and Xiatian Zhu\\rCategories: cs.CV\\r\\\\\\\\\\r  In the pursuit of transferring a source model to a target domain without\\raccess to the source training data, Source-Free Domain Adaptation (SFDA) has\\rbeen extensively explored across various scenarios, including closed-set,\\ropen-set, partial-set, and generalized settings. Existing methods, focusing on\\rspecific scenarios, not only address only a subset of challenges but also\\rnecessitate prior knowledge of the target domain, significantly limiting their\\rpractical utility and deployability. In light of these considerations, we\\rintroduce a more practical yet challenging problem, termed unified SFDA, which\\rcomprehensively incorporates all specific scenarios in a unified manner. To\\rtackle this unified SFDA problem, we propose a novel approach called Latent\\rCausal Factors Discovery (LCFD). In contrast to previous alternatives that\\remphasize learning the statistical description of reality, we formulate LCFD\\rfrom a causality perspective. The objective is to uncover the causal\\rrelationships between latent variables and model decisions, enhancing the\\rreliability and robustness of the learned model against domain shifts. To\\rintegrate extensive world knowledge, we leverage a pre-trained vision-language\\rmodel such as CLIP. This aids in the formation and discovery of latent causal\\rfactors in the absence of supervision in the variation of distribution and\\rsemantics, coupled with a newly designed information bottleneck with\\rtheoretical guarantees. Extensive experiments demonstrate that LCFD can achieve\\rnew state-of-the-art results in distinct SFDA settings, as well as source-free\\rout-of-distribution generalization.Our code and data are available at\\rhttps://github.com/tntek/source-free-domain-adaptation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07601 ,  2621kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07605\\rDate: Tue, 12 Mar 2024 12:44:34 GMT   (2312kb,D)\\r\\rTitle: Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in\\r  Text-To-Image Generation\\rAuthors: Michael Ogezi and Ning Shi\\rCategories: cs.CV cs.AI cs.LG\\r\\\\\\\\\\r  In text-to-image generation, using negative prompts, which describe\\rundesirable image characteristics, can significantly boost image quality.\\rHowever, producing good negative prompts is manual and tedious. To address\\rthis, we propose NegOpt, a novel method for optimizing negative prompt\\rgeneration toward enhanced image generation, using supervised fine-tuning and\\rreinforcement learning. Our combined approach results in a substantial increase\\rof 25% in Inception Score compared to other approaches and surpasses\\rground-truth negative prompts from the test set. Furthermore, with NegOpt we\\rcan preferentially optimize the metrics most important to us. Finally, we\\rconstruct Negative Prompts DB, a dataset of negative prompts.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07605 ,  2312kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07621\\rDate: Tue, 12 Mar 2024 13:04:37 GMT   (3452kb,D)\\r\\rTitle: Smartphone region-wise image indoor localization using deep learning for\\r  indoor tourist attraction\\rAuthors: Gabriel Toshio Hirokawa Higa, Rodrigo Stuqui Monzani, Jorge Fernando\\r  da Silva Cecatto, Maria Fernanda Balestieri Mariano de Souza, Vanessa\\r  Aparecida de Moraes Weber, Hemerson Pistori, Edson Takashi Matsubara\\rCategories: cs.CV\\r\\\\\\\\\\r  Smart indoor tourist attractions, such as smart museums and aquariums,\\rusually require a significant investment in indoor localization devices. The\\rsmartphone Global Positional Systems use is unsuitable for scenarios where\\rdense materials such as concrete and metal block weaken the GPS signals, which\\ris the most common scenario in an indoor tourist attraction. Deep learning\\rmakes it possible to perform region-wise indoor localization using smartphone\\rimages. This approach does not require any investment in infrastructure,\\rreducing the cost and time to turn museums and aquariums into smart museums or\\rsmart aquariums. This paper proposes using deep learning algorithms to classify\\rlocations using smartphone camera images for indoor tourism attractions. We\\revaluate our proposal in a real-world scenario in Brazil. We extensively\\rcollect images from ten different smartphones to classify biome-themed fish\\rtanks inside the Pantanal Biopark, creating a new dataset of 3654 images. We\\rtested seven state-of-the-art neural networks, three being transformer-based,\\rachieving precision around 90% on average and recall and f-score around 89% on\\raverage. The results indicate good feasibility of the proposal in a most indoor\\rtourist attractions.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07621 ,  3452kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07622\\rDate: Tue, 12 Mar 2024 13:05:51 GMT   (19627kb,D)\\r\\rTitle: Multiple Latent Space Mapping for Compressed Dark Image Enhancement\\rAuthors: Yi Zeng, Zhengning Wang, Yuxuan Liu, Tianjiao Zeng, Xuhang Liu,\\r  Xinglong Luo, Shuaicheng Liu, Shuyuan Zhu and Bing Zeng\\rCategories: cs.CV cs.AI eess.IV\\r\\\\\\\\\\r  Dark image enhancement aims at converting dark images to normal-light images.\\rExisting dark image enhancement methods take uncompressed dark images as inputs\\rand achieve great performance. However, in practice, dark images are often\\rcompressed before storage or transmission over the Internet. Current methods\\rget poor performance when processing compressed dark images. Artifacts hidden\\rin the dark regions are amplified by current methods, which results in\\runcomfortable visual effects for observers. Based on this observation, this\\rstudy aims at enhancing compressed dark images while avoiding compression\\rartifacts amplification. Since texture details intertwine with compression\\rartifacts in compressed dark images, detail enhancement and blocking artifacts\\rsuppression contradict each other in image space. Therefore, we handle the task\\rin latent space. To this end, we propose a novel latent mapping network based\\ron variational auto-encoder (VAE). Firstly, different from previous VAE-based\\rmethods with single-resolution features only, we exploit multiple latent spaces\\rwith multi-resolution features, to reduce the detail blur and improve image\\rfidelity. Specifically, we train two multi-level VAEs to project compressed\\rdark images and normal-light images into their latent spaces respectively.\\rSecondly, we leverage a latent mapping network to transform features from\\rcompressed dark space to normal-light space. Specifically, since the\\rdegradation models of darkness and compression are different from each other,\\rthe latent mapping process is divided mapping into enlightening branch and\\rdeblocking branch. Comprehensive experiments demonstrate that the proposed\\rmethod achieves state-of-the-art performance in compressed dark image\\renhancement.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07622 ,  19627kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07630\\rDate: Tue, 12 Mar 2024 13:11:58 GMT   (6780kb,D)\\r\\rTitle: Hunting Attributes: Context Prototype-Aware Learning for Weakly\\r  Supervised Semantic Segmentation\\rAuthors: Feilong Tang, Zhongxing Xu, Zhaojun Qu, Wei Feng, Xingjian Jiang,\\r  Zongyuan Ge\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Recent weakly supervised semantic segmentation (WSSS) methods strive to\\rincorporate contextual knowledge to improve the completeness of class\\ractivation maps (CAM). In this work, we argue that the knowledge bias between\\rinstances and contexts affects the capability of the prototype to sufficiently\\runderstand instance semantics. Inspired by prototype learning theory, we\\rpropose leveraging prototype awareness to capture diverse and fine-grained\\rfeature attributes of instances. The hypothesis is that contextual prototypes\\rmight erroneously activate similar and frequently co-occurring object\\rcategories due to this knowledge bias. Therefore, we propose to enhance the\\rprototype representation ability by mitigating the bias to better capture\\rspatial coverage in semantic object regions. With this goal, we present a\\rContext Prototype-Aware Learning (CPAL) strategy, which leverages semantic\\rcontext to enrich instance comprehension. The core of this method is to\\raccurately capture intra-class variations in object features through\\rcontext-aware prototypes, facilitating the adaptation to the semantic\\rattributes of various instances. We design feature distribution alignment to\\roptimize prototype awareness, aligning instance feature distributions with\\rdense features. In addition, a unified training framework is proposed to\\rcombine label-guided classification supervision and prototypes-guided\\rself-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show\\rthat CPAL significantly improves off-the-shelf methods and achieves\\rstate-of-the-art performance. The project is available at\\rhttps://github.com/Barrett-python/CPAL.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07630 ,  6780kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07636\\rDate: Tue, 12 Mar 2024 13:18:22 GMT   (3159kb,D)\\r\\rTitle: Decomposing Disease Descriptions for Enhanced Pathology Detection: A\\r  Multi-Aspect Vision-Language Matching Framework\\rAuthors: Minh Hieu Phan, Yutong Xie, Yuankai Qi, Lingqiao Liu, Liyang Liu,\\r  Bowen Zhang, Zhibin Liao, Qi Wu, Minh-Son To, Johan W. Verjans\\rCategories: cs.CV\\rComments: Accepted at CVPR2024. Pre-print before final camera-ready version\\r\\\\\\\\\\r  Medical vision language pre-training (VLP) has emerged as a frontier of\\rresearch, enabling zero-shot pathological recognition by comparing the query\\rimage with the textual descriptions for each disease. Due to the complex\\rsemantics of biomedical texts, current methods struggle to align medical images\\rwith key pathological findings in unstructured reports. This leads to the\\rmisalignment with the target disease's textual representation. In this paper,\\rwe introduce a novel VLP framework designed to dissect disease descriptions\\rinto their fundamental aspects, leveraging prior knowledge about the visual\\rmanifestations of pathologies. This is achieved by consulting a large language\\rmodel and medical experts. Integrating a Transformer module, our approach\\raligns an input image with the diverse elements of a disease, generating\\raspect-centric image representations. By consolidating the matches from each\\raspect, we improve the compatibility between an image and its associated\\rdisease. Additionally, capitalizing on the aspect-oriented representations, we\\rpresent a dual-head Transformer tailored to process known and unknown diseases,\\roptimizing the comprehensive detection efficacy. Conducting experiments on\\rseven downstream datasets, ours outperforms recent methods by up to 8.07% and\\r11.23% in AUC scores for seen and novel categories, respectively. Our code is\\rreleased at\\r\\\\href{https://github.com/HieuPhan33/MAVL}{https://github.com/HieuPhan33/MAVL}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07636 ,  3159kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07684\\rDate: Tue, 12 Mar 2024 14:21:30 GMT   (3297kb,D)\\r\\rTitle: Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for\\r  Video Adverse Weather Removal\\rAuthors: Yijun Yang, Hongtao Wu, Angelica I. Aviles-Rivero, Yulun Zhang, Jing\\r  Qin, Lei Zhu\\rCategories: cs.CV\\r\\\\\\\\\\r  Real-world vision tasks frequently suffer from the appearance of unexpected\\radverse weather conditions, including rain, haze, snow, and raindrops. In the\\rlast decade, convolutional neural networks and vision transformers have yielded\\routstanding results in single-weather video removal. However, due to the\\rabsence of appropriate adaptation, most of them fail to generalize to other\\rweather conditions. Although ViWS-Net is proposed to remove adverse weather\\rconditions in videos with a single set of pre-trained weights, it is seriously\\rblinded by seen weather at train-time and degenerates when coming to unseen\\rweather during test-time. In this work, we introduce test-time adaptation into\\radverse weather removal in videos, and propose the first framework that\\rintegrates test-time adaptation into the iterative diffusion reverse process.\\rSpecifically, we devise a diffusion-based network with a novel temporal noise\\rmodel to efficiently explore frame-correlated information in degraded video\\rclips at training stage. During inference stage, we introduce a proxy task\\rnamed Diffusion Tubelet Self-Calibration to learn the primer distribution of\\rtest video stream and optimize the model by approximating the temporal noise\\rmodel for online adaptation. Experimental results, on benchmark datasets,\\rdemonstrate that our Test-Time Adaptation method with Diffusion-based\\rnetwork(Diff-TTA) outperforms state-of-the-art methods in terms of restoring\\rvideos degraded by seen weather conditions. Its generalizable capability is\\ralso validated with unseen weather conditions in both synthesized and\\rreal-world videos.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07684 ,  3297kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07687\\rDate: Tue, 12 Mar 2024 14:27:17 GMT   (11363kb,D)\\r\\rTitle: Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model\\r  Performance and Annotation Cost\\rAuthors: Oana Ignat, Longju Bai, Joan Nwatu, Rada Mihalcea\\rCategories: cs.CV cs.AI cs.CL\\rComments: accepted at COLING 2024\\r\\\\\\\\\\r  Current foundation models have shown impressive performance across various\\rtasks. However, several studies have revealed that these models are not\\reffective for everyone due to the imbalanced geographical and economic\\rrepresentation of the data used in the training process. Most of this data\\rcomes from Western countries, leading to poor results for underrepresented\\rcountries. To address this issue, more data needs to be collected from these\\rcountries, but the cost of annotation can be a significant bottleneck. In this\\rpaper, we propose methods to identify the data to be annotated to balance model\\rperformance and annotation costs. Our approach first involves finding the\\rcountries with images of topics (objects and actions) most visually distinct\\rfrom those already in the training datasets used by current large\\rvision-language foundation models. Next, we identify countries with higher\\rvisual similarity for these topics and show that using data from these\\rcountries to supplement the training data improves model performance and\\rreduces annotation costs. The resulting lists of countries and corresponding\\rtopics are made available at\\rhttps://github.com/MichiganNLP/visual_diversity_budget.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07687 ,  11363kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07692\\rDate: Tue, 12 Mar 2024 14:36:52 GMT   (503kb,D)\\r\\rTitle: Masked AutoDecoder is Effective Multi-Task Vision Generalist\\rAuthors: Han Qiu, Jiaxing Huang, Peng Gao, Lewei Lu, Xiaoqin Zhang, Shijian Lu\\rCategories: cs.CV\\rComments: Accepted by CVPR 2024\\r\\\\\\\\\\r  Inspired by the success of general-purpose models in NLP, recent studies\\rattempt to unify different vision tasks in the same sequence format and employ\\rautoregressive Transformers for sequence prediction. They apply uni-directional\\rattention to capture sequential dependencies and generate task sequences\\rrecursively. However, such autoregressive Transformers may not fit vision tasks\\rwell, as vision task sequences usually lack the sequential dependencies\\rtypically observed in natural languages. In this work, we design Masked\\rAutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of\\rtwo core designs. First, we develop a parallel decoding framework that\\rintroduces bi-directional attention to capture contextual dependencies\\rcomprehensively and decode vision task sequences in parallel. Second, we design\\ra masked sequence modeling approach that learns rich task contexts by masking\\rand reconstructing task sequences. In this way, MAD handles all the tasks by a\\rsingle network branch and a simple cross-entropy loss with minimal\\rtask-specific designs. Extensive experiments demonstrate the great potential of\\rMAD as a new paradigm for unifying various vision tasks. MAD achieves superior\\rperformance and inference efficiency compared to autoregressive counterparts\\rwhile obtaining competitive accuracy with task-specific models. Code will be\\rreleased.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07692 ,  503kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07700\\rDate: Tue, 12 Mar 2024 14:46:03 GMT   (28589kb,D)\\r\\rTitle: CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive\\r  Self-Supervised Transformers\\rAuthors: Shahaf Arica, Or Rubin, Sapir Gershov, Shlomi Laufer\\rCategories: cs.CV\\rComments: Accepted to CVPR 2024\\r\\\\\\\\\\r  In this paper, we introduce VoteCut, an innovative method for unsupervised\\robject discovery that leverages feature representations from multiple\\rself-supervised models. VoteCut employs normalized-cut based graph\\rpartitioning, clustering and a pixel voting approach. Additionally, We present\\rCuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels,\\rgenerated by VoteCut, and a novel soft target loss to refine segmentation\\raccuracy. Through rigorous evaluations across multiple datasets and several\\runsupervised setups, our methods demonstrate significant improvements in\\rcomparison to previous state-of-the-art models. Our ablation studies further\\rhighlight the contributions of each component, revealing the robustness and\\refficacy of our approach. Collectively, VoteCut and CuVLER pave the way for\\rfuture advancements in image segmentation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07700 ,  28589kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07705\\rDate: Tue, 12 Mar 2024 14:50:05 GMT   (8527kb,D)\\r\\rTitle: Robust Synthetic-to-Real Transfer for Stereo Matching\\rAuthors: Jiawei Zhang, Jiahe Li, Lei Huang, Xiaohan Yu, Lin Gu, Jin Zheng, Xiao\\r  Bai\\rCategories: cs.CV\\rComments: Accepted at CVPR 2024\\r\\\\\\\\\\r  With advancements in domain generalized stereo matching networks, models\\rpre-trained on synthetic data demonstrate strong robustness to unseen domains.\\rHowever, few studies have investigated the robustness after fine-tuning them in\\rreal-world scenarios, during which the domain generalization ability can be\\rseriously degraded. In this paper, we explore fine-tuning stereo matching\\rnetworks without compromising their robustness to unseen domains. Our\\rmotivation stems from comparing Ground Truth (GT) versus Pseudo Label (PL) for\\rfine-tuning: GT degrades, but PL preserves the domain generalization ability.\\rEmpirically, we find the difference between GT and PL implies valuable\\rinformation that can regularize networks during fine-tuning. We also propose a\\rframework to utilize this difference for fine-tuning, consisting of a frozen\\rTeacher, an exponential moving average (EMA) Teacher, and a Student network.\\rThe core idea is to utilize the EMA Teacher to measure what the Student has\\rlearned and dynamically improve GT and PL for fine-tuning. We integrate our\\rframework with state-of-the-art networks and evaluate its effectiveness on\\rseveral real-world datasets. Extensive experiments show that our method\\reffectively preserves the domain generalization ability during fine-tuning.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07705 ,  8527kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07706\\rDate: Tue, 12 Mar 2024 14:51:23 GMT   (5390kb,D)\\r\\rTitle: Fast and Simple Explainability for Point Cloud Networks\\rAuthors: Meir Yossef Levi and Guy Gilboa\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  We propose a fast and simple explainable AI (XAI) method for point cloud\\rdata. It computes pointwise importance with respect to a trained network\\rdownstream task. This allows better understanding of the network properties,\\rwhich is imperative for safety-critical applications. In addition to debugging\\rand visualization, our low computational complexity facilitates online feedback\\rto the network at inference. This can be used to reduce uncertainty and to\\rincrease robustness. In this work, we introduce \\\\emph{Feature Based\\rInterpretability} (FBI), where we compute the features' norm, per point, before\\rthe bottleneck. We analyze the use of gradients and post- and pre-bottleneck\\rstrategies, showing pre-bottleneck is preferred, in terms of smoothness and\\rranking. We obtain at least three orders of magnitude speedup, compared to\\rcurrent XAI methods, thus, scalable for big point clouds or large-scale\\rarchitectures. Our approach achieves SOTA results, in terms of classification\\rexplainability. We demonstrate how the proposed measure is helpful in analyzing\\rand characterizing various aspects of 3D learning, such as rotation invariance,\\rrobustness to out-of-distribution (OOD) outliers or domain shift and dataset\\rbias.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07706 ,  5390kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07711\\rDate: Tue, 12 Mar 2024 14:53:56 GMT   (679kb,D)\\r\\rTitle: SSM Meets Video Diffusion Models: Efficient Video Generation with\\r  Structured State Spaces\\rAuthors: Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo\\rCategories: cs.CV cs.AI\\rComments: Accepted as workshop paper at ICLR 2024\\r\\\\\\\\\\r  Given the remarkable achievements in image generation through diffusion\\rmodels, the research community has shown increasing interest in extending these\\rmodels to video generation. Recent diffusion models for video generation have\\rpredominantly utilized attention layers to extract temporal features. However,\\rattention layers are limited by their memory consumption, which increases\\rquadratically with the length of the sequence. This limitation presents\\rsignificant challenges when attempting to generate longer video sequences using\\rdiffusion models. To overcome this challenge, we propose leveraging state-space\\rmodels (SSMs). SSMs have recently gained attention as viable alternatives due\\rto their linear memory consumption relative to sequence length. In the\\rexperiments, we first evaluate our SSM-based model with UCF101, a standard\\rbenchmark of video generation. In addition, to investigate the potential of\\rSSMs for longer video generation, we perform an experiment using the MineRL\\rNavigate dataset, varying the number of frames to 64 and 150. In these\\rsettings, our SSM-based model can considerably save memory consumption for\\rlonger sequences, while maintaining competitive FVD scores to the\\rattention-based models. Our codes are available at\\rhttps://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07711 ,  679kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07719\\rDate: Tue, 12 Mar 2024 14:58:51 GMT   (12420kb,D)\\r\\rTitle: Dynamic Graph Representation with Knowledge-aware Attention for\\r  Histopathology Whole Slide Image Analysis\\rAuthors: Jiawen Li, Yuxuan Chen, Hongbo Chu, Qiehe Sun, Tian Guan, Anjia Han,\\r  Yonghong He\\rCategories: cs.CV\\rComments: Accepted by CVPR 2024\\r\\\\\\\\\\r  Histopathological whole slide images (WSIs) classification has become a\\rfoundation task in medical microscopic imaging processing. Prevailing\\rapproaches involve learning WSIs as instance-bag representations, emphasizing\\rsignificant instances but struggling to capture the interactions between\\rinstances. Additionally, conventional graph representation methods utilize\\rexplicit spatial positions to construct topological structures but restrict the\\rflexible interaction capabilities between instances at arbitrary locations,\\rparticularly when spatially distant. In response, we propose a novel dynamic\\rgraph representation algorithm that conceptualizes WSIs as a form of the\\rknowledge graph structure. Specifically, we dynamically construct neighbors and\\rdirected edge embeddings based on the head and tail relationships between\\rinstances. Then, we devise a knowledge-aware attention mechanism that can\\rupdate the head node features by learning the joint attention score of each\\rneighbor and edge. Finally, we obtain a graph-level embedding through the\\rglobal pooling process of the updated head, serving as an implicit\\rrepresentation for the WSI classification. Our end-to-end graph representation\\rlearning approach has outperformed the state-of-the-art WSI analysis methods on\\rthree TCGA benchmark datasets and in-house test sets. Our code is available at\\rhttps://github.com/WonderLandxD/WiKG.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07719 ,  12420kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07720\\rDate: Tue, 12 Mar 2024 14:58:52 GMT   (1771kb,D)\\r\\rTitle: Multi-modal Auto-regressive Modeling via Visual Words\\rAuthors: Tianshuo Peng, Zuchao Li, Lefei Zhang, Hai Zhao, Ping Wang, and Bo Du\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Large Language Models (LLMs), benefiting from the auto-regressive modelling\\rapproach performed on massive unannotated texts corpora, demonstrates powerful\\rperceptual and reasoning capabilities. However, as for extending\\rauto-regressive modelling to multi-modal scenarios to build Large Multi-modal\\rModels (LMMs), there lies a great difficulty that the image information is\\rprocessed in the LMM as continuous visual embeddings, which cannot obtain\\rdiscrete supervised labels for classification. In this paper, we successfully\\rperform multi-modal auto-regressive modeling with a unified objective for the\\rfirst time. Specifically, we propose the concept of visual words, which maps\\rthe visual features to probability distributions over LLM's vocabulary,\\rproviding supervision information for visual modelling. We further explore the\\rdistribution of visual features in the semantic space within LMM and the\\rpossibility of using text embeddings to represent visual information.\\rExperimental results and ablation studies on 5 VQA tasks and 4 benchmark\\rtoolkits validate the powerful performance of our proposed approach.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07720 ,  1771kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07733\\rDate: Tue, 12 Mar 2024 15:13:12 GMT   (27216kb,D)\\r\\rTitle: DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven\\r  Segmentation\\rAuthors: Patrick Knab, Sascha Marton, Christian Bartelt\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Explainable Artificial Intelligence is critical in unraveling decision-making\\rprocesses in complex machine learning models. LIME (Local Interpretable\\rModel-agnostic Explanations) is a well-known XAI framework for image analysis.\\rIt utilizes image segmentation to create features to identify relevant areas\\rfor classification. Consequently, poor segmentation can compromise the\\rconsistency of the explanation and undermine the importance of the segments,\\raffecting the overall interpretability. Addressing these challenges, we\\rintroduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a\\rdata-driven segmentation for human-recognized feature generation, and ii) a\\rhierarchical segmentation procedure through composition. We benchmark DSEG-LIME\\ron pre-trained models with images from the ImageNet dataset - scenarios without\\rdomain-specific knowledge. The analysis includes a quantitative evaluation\\rusing established XAI metrics, complemented by a qualitative assessment through\\ra user study. Our findings demonstrate that DSEG outperforms in most of the XAI\\rmetrics and enhances the alignment of explanations with human-recognized\\rconcepts, significantly improving interpretability. The code is available\\runder: https://github. com/patrick-knab/DSEG-LIME\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07733 ,  27216kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07741\\rDate: Tue, 12 Mar 2024 15:19:25 GMT   (2839kb,D)\\r\\rTitle: Uncertainty Quantification with Deep Ensembles for 6D Object Pose\\r  Estimation\\rAuthors: Kira Wursthorn, Markus Hillemann, Markus Ulrich\\rCategories: cs.CV cs.AI\\rComments: 8 pages\\r\\\\\\\\\\r  The estimation of 6D object poses is a fundamental task in many computer\\rvision applications. Particularly, in high risk scenarios such as human-robot\\rinteraction, industrial inspection, and automation, reliable pose estimates are\\rcrucial. In the last years, increasingly accurate and robust\\rdeep-learning-based approaches for 6D object pose estimation have been\\rproposed. Many top-performing methods are not end-to-end trainable but consist\\rof multiple stages. In the context of deep uncertainty quantification, deep\\rensembles are considered as state of the art since they have been proven to\\rproduce well-calibrated and robust uncertainty estimates. However, deep\\rensembles can only be applied to methods that can be trained end-to-end. In\\rthis work, we propose a method to quantify the uncertainty of multi-stage 6D\\robject pose estimation approaches with deep ensembles. For the implementation,\\rwe choose SurfEmb as representative, since it is one of the top-performing 6D\\robject pose estimation approaches in the BOP Challenge 2022. We apply\\restablished metrics and concepts for deep uncertainty quantification to\\revaluate the results. Furthermore, we propose a novel uncertainty calibration\\rscore for regression tasks to quantify the quality of the estimated\\runcertainty.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07741 ,  2839kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07746\\rDate: Tue, 12 Mar 2024 15:28:51 GMT   (4763kb,D)\\r\\rTitle: Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified\\r  3D Perception\\rAuthors: Philipp Wolters, Johannes Gilg, Torben Teepe, Fabian Herzog, Anouar\\r  Laouichi, Martin Hofmann, Gerhard Rigoll\\rCategories: cs.CV\\rComments: 9 pages, 4 figures\\r\\\\\\\\\\r  Low-cost, vision-centric 3D perception systems for autonomous driving have\\rmade significant progress in recent years, narrowing the gap to expensive\\rLiDAR-based methods. The primary challenge in becoming a fully reliable\\ralternative lies in robust depth prediction capabilities, as camera-based\\rsystems struggle with long detection ranges and adverse lighting and weather\\rconditions. In this work, we introduce HyDRa, a novel camera-radar fusion\\rarchitecture for diverse 3D perception tasks. Building upon the principles of\\rdense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid\\rfusion approach to combine the strengths of complementary camera and radar\\rfeatures in two distinct representation spaces. Our Height Association\\rTransformer module leverages radar features already in the perspective view to\\rproduce more robust and accurate depth predictions. In the BEV, we refine the\\rinitial sparse representation by a Radar-weighted Depth Consistency. HyDRa\\rachieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and\\r58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new\\rsemantically rich and spatially accurate BEV features can be directly converted\\rinto a powerful occupancy representation, beating all previous camera-based\\rmethods on the Occ3D benchmark by an impressive 3.7 mIoU.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07746 ,  4763kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07750\\rDate: Tue, 12 Mar 2024 15:36:42 GMT   (5027kb,D)\\r\\rTitle: Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and\\r  Image Embeddings\\rAuthors: Sahand Sharifzadeh, Christos Kaplanis, Shreya Pathak, Dharshan\\r  Kumaran, Anastasija Ilic, Jovana Mitrovic, Charles Blundell, Andrea Banino\\rCategories: cs.CV cs.AI\\rComments: 9 pages, 6 figures\\r\\\\\\\\\\r  The creation of high-quality human-labeled image-caption datasets presents a\\rsignificant bottleneck in the development of Visual-Language Models (VLMs). We\\rpropose a novel approach that leverages the strengths of Large Language Models\\r(LLMs) and image generation models to create synthetic image-text pairs for\\refficient and effective VLM training. Our method employs pretraining a\\rtext-to-image model to synthesize image embeddings starting from captions\\rgenerated by an LLM. These synthetic pairs are then used to train a VLM.\\rExtensive experiments demonstrate that the VLM trained with synthetic data\\rexhibits comparable performance on image captioning, while requiring a fraction\\rof the data used by models trained solely on human-annotated data. In\\rparticular, we outperform the baseline by 17% through augmentation with a\\rsynthetic dataset. Furthermore, we show that synthesizing in the image\\rembedding space is 25% faster than in the pixel space. This research introduces\\ra promising technique for generating large-scale, customizable image datasets,\\rleading to enhanced VLM performance and wider applicability across various\\rdomains, all with improved data efficiency and resource utilization.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07750 ,  5027kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07752\\rDate: Tue, 12 Mar 2024 15:39:56 GMT   (8579kb,D)\\r\\rTitle: Vision-based Vehicle Re-identification in Bridge Scenario using Flock\\r  Similarity\\rAuthors: Chunfeng Zhang, Ping Wang\\rCategories: cs.CV\\rComments: 6 pages, 9 figures\\r\\\\\\\\\\r  Due to the needs of road traffic flow monitoring and public safety\\rmanagement, video surveillance cameras are widely distributed in urban roads.\\rHowever, the information captured directly by each camera is siloed, making it\\rdifficult to use it effectively. Vehicle re-identification refers to finding a\\rvehicle that appears under one camera in another camera, which can correlate\\rthe information captured by multiple cameras. While license plate recognition\\rplays an important role in some applications, there are some scenarios where\\rre-identification method based on vehicle appearance are more suitable. The\\rmain challenge is that the data of vehicle appearance has the characteristics\\rof high inter-class similarity and large intra-class differences. Therefore, it\\ris difficult to accurately distinguish between different vehicles by relying\\ronly on vehicle appearance information. At this time, it is often necessary to\\rintroduce some extra information, such as spatio-temporal information.\\rNevertheless, the relative position of the vehicles rarely changes when passing\\rthrough two adjacent cameras in the bridge scenario. In this paper, we present\\ra vehicle re-identification method based on flock similarity, which improves\\rthe accuracy of vehicle re-identification by utilizing vehicle information\\radjacent to the target vehicle. When the relative position of the vehicles\\rremains unchanged and flock size is appropriate, we obtain an average relative\\rimprovement of 204% on VeRi dataset in our experiments. Then, the effect of the\\rmagnitude of the relative position change of the vehicles as they pass through\\rtwo cameras is discussed. We present two metrics that can be used to quantify\\rthe difference and establish a connection between them. Although this\\rassumption is based on the bridge scenario, it is often true in other scenarios\\rdue to driving safety and camera location.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07752 ,  8579kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07764\\rDate: Tue, 12 Mar 2024 15:53:14 GMT   (40706kb,D)\\r\\rTitle: Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model\\rAuthors: Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia\\r  Li, Xu Tang, Yao Hu, Haibo Zhao\\rCategories: cs.CV\\r\\\\\\\\\\r  Current makeup transfer methods are limited to simple makeup styles, making\\rthem difficult to apply in real-world scenarios. In this paper, we introduce\\rStable-Makeup, a novel diffusion-based makeup transfer method capable of\\rrobustly transferring a wide range of real-world makeup, onto user-provided\\rfaces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a\\rDetail-Preserving (D-P) makeup encoder to encode makeup details. It also\\remploys content and structural control modules to preserve the content and\\rstructural information of the source image. With the aid of our newly added\\rmakeup cross-attention layers in U-Net, we can accurately transfer the detailed\\rmakeup to the corresponding position in the source image. After\\rcontent-structure decoupling training, Stable-Makeup can maintain content and\\rthe facial structure of the source image. Moreover, our method has demonstrated\\rstrong robustness and generalizability, making it applicable to varioustasks\\rsuch as cross-domain makeup transfer, makeup-guided text-to-image generation\\rand so on. Extensive experiments have demonstrated that our approach delivers\\rstate-of-the-art (SOTA) results among existing makeup transfer methods and\\rexhibits a highly promising with broad potential applications in various\\rrelated fields.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07764 ,  40706kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07773\\rDate: Tue, 12 Mar 2024 15:59:08 GMT   (46181kb,D)\\r\\rTitle: SemCity: Semantic Scene Generation with Triplane Diffusion\\rAuthors: Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, Sung-Eui\\r  Yoon\\rCategories: cs.CV\\rComments: Accepted to CVPR 2024\\r\\\\\\\\\\r  We present SemCity, a 3D diffusion model for semantic scene generation in\\rreal-world outdoor environments. Most 3D diffusion models focus on generating a\\rsingle object, synthetic indoor scenes, or synthetic outdoor scenes, while the\\rgeneration of real-world outdoor scenes is rarely addressed. In this paper, we\\rconcentrate on generating a real-outdoor scene through learning a diffusion\\rmodel on a real-world outdoor dataset. In contrast to synthetic data,\\rreal-outdoor datasets often contain more empty spaces due to sensor\\rlimitations, causing challenges in learning real-outdoor distributions. To\\raddress this issue, we exploit a triplane representation as a proxy form of\\rscene distributions to be learned by our diffusion model. Furthermore, we\\rpropose a triplane manipulation that integrates seamlessly with our triplane\\rdiffusion model. The manipulation improves our diffusion model's applicability\\rin a variety of downstream tasks related to outdoor scene generation such as\\rscene inpainting, scene outpainting, and semantic scene completion refinements.\\rIn experimental results, we demonstrate that our triplane diffusion model shows\\rmeaningful generation results compared with existing work in a real-outdoor\\rdataset, SemanticKITTI. We also show our triplane manipulation facilitates\\rseamlessly adding, removing, or modifying objects within a scene. Further, it\\ralso enables the expansion of scenes toward a city-level scale. Finally, we\\revaluate our method on semantic scene completion refinements where our\\rdiffusion model enhances predictions of semantic scene completion networks by\\rlearning scene distribution. Our code is available at\\rhttps://github.com/zoomin-lee/SemCity.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07773 ,  46181kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07798\\rDate: Tue, 12 Mar 2024 16:35:32 GMT   (3359kb,D)\\r\\rTitle: A Fourier Transform Framework for Domain Adaptation\\rAuthors: Le Luo, Bingrong Xu, Qingyong Zhang, Cheng Lian, Jie Luo\\rCategories: cs.CV\\rComments: 9 pages,5 figures\\r\\\\\\\\\\r  By using unsupervised domain adaptation (UDA), knowledge can be transferred\\rfrom a label-rich source domain to a target domain that contains relevant\\rinformation but lacks labels. Many existing UDA algorithms suffer from directly\\rusing raw images as input, resulting in models that overly focus on redundant\\rinformation and exhibit poor generalization capability. To address this issue,\\rwe attempt to improve the performance of unsupervised domain adaptation by\\remploying the Fourier method (FTF).Specifically, FTF is inspired by the\\ramplitude of Fourier spectra, which primarily preserves low-level statistical\\rinformation. In FTF, we effectively incorporate low-level information from the\\rtarget domain into the source domain by fusing the amplitudes of both domains\\rin the Fourier domain. Additionally, we observe that extracting features from\\rbatches of images can eliminate redundant information while retaining\\rclass-specific features relevant to the task. Building upon this observation,\\rwe apply the Fourier Transform at the data stream level for the first time. To\\rfurther align multiple sources of data, we introduce the concept of correlation\\ralignment. To evaluate the effectiveness of our FTF method, we conducted\\revaluations on four benchmark datasets for domain adaptation, including\\rOffice-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our results\\rdemonstrate superior performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07798 ,  3359kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07807\\rDate: Tue, 12 Mar 2024 16:44:52 GMT   (6805kb,D)\\r\\rTitle: StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting\\rAuthors: Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao,\\r  Shijian Lu\\rCategories: cs.CV\\r\\\\\\\\\\r  We introduce StyleGaussian, a novel 3D style transfer technique that allows\\rinstant transfer of any image's style to a 3D scene at 10 frames per second\\r(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style\\rtransfer without compromising its real-time rendering ability and multi-view\\rconsistency. It achieves instant style transfer with three steps: embedding,\\rtransfer, and decoding. Initially, 2D VGG scene features are embedded into\\rreconstructed 3D Gaussians. Next, the embedded features are transformed\\raccording to a reference style image. Finally, the transformed features are\\rdecoded into the stylized RGB. StyleGaussian has two novel designs. The first\\ris an efficient feature rendering strategy that first renders low-dimensional\\rfeatures and then maps them into high-dimensional features while embedding VGG\\rfeatures. It cuts the memory consumption significantly and enables 3DGS to\\rrender the high-dimensional memory-intensive features. The second is a\\rK-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized\\rfeatures, it eliminates the 2D CNN operations that compromise strict multi-view\\rconsistency. Extensive experiments show that StyleGaussian achieves instant 3D\\rstylization with superior stylization quality while preserving real-time\\rrendering and strict multi-view consistency. Project page:\\rhttps://kunhao-liu.github.io/StyleGaussian/\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07807 ,  6805kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07818\\rDate: Tue, 12 Mar 2024 16:57:56 GMT   (1419kb,D)\\r\\rTitle: Label Dropout: Improved Deep Learning Echocardiography Segmentation\\r  Using Multiple Datasets With Domain Shift and Partial Labelling\\rAuthors: Iman Islam (1), Esther Puyol-Ant\\\\'on (1), Bram Ruijsink (1), Andrew J.\\r  Reader (1), Andrew P. King (1) ((1) King's College London)\\rCategories: cs.CV cs.AI cs.LG\\rComments: 10 pages, 5 figures, submitted to MICCAI conference\\r\\\\\\\\\\r  Echocardiography (echo) is the first imaging modality used when assessing\\rcardiac function. The measurement of functional biomarkers from echo relies\\rupon the segmentation of cardiac structures and deep learning models have been\\rproposed to automate the segmentation process. However, in order to translate\\rthese tools to widespread clinical use it is important that the segmentation\\rmodels are robust to a wide variety of images (e.g. acquired from different\\rscanners, by operators with different levels of expertise etc.). To achieve\\rthis level of robustness it is necessary that the models are trained with\\rmultiple diverse datasets. A significant challenge faced when training with\\rmultiple diverse datasets is the variation in label presence, i.e. the combined\\rdata are often partially-labelled. Adaptations of the cross entropy loss\\rfunction have been proposed to deal with partially labelled data. In this paper\\rwe show that training naively with such a loss function and multiple diverse\\rdatasets can lead to a form of shortcut learning, where the model associates\\rlabel presence with domain characteristics, leading to a drop in performance.\\rTo address this problem, we propose a novel label dropout scheme to break the\\rlink between domain characteristics and the presence or absence of labels. We\\rdemonstrate that label dropout improves echo segmentation Dice score by 62% and\\r25% on two cardiac structures when training using multiple diverse partially\\rlabelled datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07818 ,  1419kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07839\\rDate: Tue, 12 Mar 2024 17:24:26 GMT   (3099kb,D)\\r\\rTitle: MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with\\r  Module-wise Pruning Error Metric\\rAuthors: Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying\\r  Wei, Zhenan Sun\\rCategories: cs.CV cs.AI cs.MM\\rComments: 18 pages, 8 figures, Published in CVPR2024\\rJournal-ref: In Proc. IEEE Conference on Computer Vision and Pattern\\r  Recognition (CVPR), 2024\\r\\\\\\\\\\r  Vision-language pre-trained models have achieved impressive performance on\\rvarious downstream tasks. However, their large model sizes hinder their\\rutilization on platforms with limited computational resources. We find that\\rdirectly using smaller pre-trained models and applying magnitude-based pruning\\ron CLIP models leads to inflexibility and inferior performance. Recent efforts\\rfor VLP compression either adopt uni-modal compression metrics resulting in\\rlimited performance or involve costly mask-search processes with learnable\\rmasks. In this paper, we first propose the Module-wise Pruning Error (MoPE)\\rmetric, accurately assessing CLIP module importance by performance decline on\\rcross-modal tasks. Using the MoPE metric, we introduce a unified pruning\\rframework applicable to both pre-training and task-specific fine-tuning\\rcompression stages. For pre-training, MoPE-CLIP effectively leverages knowledge\\rfrom the teacher model, significantly reducing pre-training costs while\\rmaintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning\\rfrom width to depth yields highly competitive task-specific models. Extensive\\rexperiments in two stages demonstrate the effectiveness of the MoPE metric, and\\rMoPE-CLIP outperforms previous state-of-the-art VLP compression methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07839 ,  3099kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07854\\rDate: Tue, 12 Mar 2024 17:44:45 GMT   (2030kb,D)\\r\\rTitle: Distilling the Knowledge in Data Pruning\\rAuthors: Emanuel Ben-Baruch, Adam Botach, Igor Kviatkovsky, Manoj Aggarwal,\\r  G\\\\'erard Medioni\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  With the increasing size of datasets used for training neural networks, data\\rpruning becomes an attractive field of research. However, most current data\\rpruning algorithms are limited in their ability to preserve accuracy compared\\rto models trained on the full data, especially in high pruning regimes. In this\\rpaper we explore the application of data pruning while incorporating knowledge\\rdistillation (KD) when training on a pruned subset. That is, rather than\\rrelying solely on ground-truth labels, we also use the soft predictions from a\\rteacher network pre-trained on the complete data. By integrating KD into\\rtraining, we demonstrate significant improvement across datasets, pruning\\rmethods, and on all pruning fractions. We first establish a theoretical\\rmotivation for employing self-distillation to improve training on pruned data.\\rThen, we empirically make a compelling and highly practical observation: using\\rKD, simple random pruning is comparable or superior to sophisticated pruning\\rmethods across all pruning regimes. On ImageNet for example, we achieve\\rsuperior accuracy despite training on a random subset of only 50% of the data.\\rAdditionally, we demonstrate a crucial connection between the pruning factor\\rand the optimal knowledge distillation weight. This helps mitigate the impact\\rof samples with noisy labels and low-quality images retained by typical pruning\\ralgorithms. Finally, we make an intriguing observation: when using lower\\rpruning fractions, larger teachers lead to accuracy degradation, while\\rsurprisingly, employing teachers with a smaller capacity than the student's may\\rimprove results. Our code will be made available.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07854 ,  2030kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07860\\rDate: Tue, 12 Mar 2024 17:50:11 GMT   (16552kb,D)\\r\\rTitle: Bridging Different Language Models and Generative Vision Models for\\r  Text-to-Image Generation\\rAuthors: Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong\\rCategories: cs.CV\\r\\\\\\\\\\r  Text-to-image generation has made significant advancements with the\\rintroduction of text-to-image diffusion models. These models typically consist\\rof a language model that interprets user prompts and a vision model that\\rgenerates corresponding images. As language and vision models continue to\\rprogress in their respective domains, there is a great potential in exploring\\rthe replacement of components in text-to-image diffusion models with more\\radvanced counterparts. A broader research objective would therefore be to\\rinvestigate the integration of any two unrelated language and generative vision\\rmodels for text-to-image generation. In this paper, we explore this objective\\rand propose LaVi-Bridge, a pipeline that enables the integration of diverse\\rpre-trained language models and generative vision models for text-to-image\\rgeneration. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and\\rplug-and-play approach without requiring modifications to the original weights\\rof the language and vision models. Our pipeline is compatible with various\\rlanguage models and generative vision models, accommodating different\\rstructures. Within this framework, we demonstrate that incorporating superior\\rmodules, such as more advanced language models or generative vision models,\\rresults in notable improvements in capabilities like text alignment or image\\rquality. Extensive evaluations have been conducted to verify the effectiveness\\rof LaVi-Bridge. Code is available at\\rhttps://github.com/ShihaoZhaoZSH/LaVi-Bridge.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07860 ,  16552kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07874\\rDate: Tue, 12 Mar 2024 17:59:51 GMT   (3993kb,D)\\r\\rTitle: Beyond Text: Frozen Large Language Models in Visual Signal Comprehension\\rAuthors: Lei Zhu, Fangyun Wei, Yanye Lu\\rCategories: cs.CV\\rComments: Accepted by CVPR 2024\\r\\\\\\\\\\r  In this work, we investigate the potential of a large language model (LLM) to\\rdirectly comprehend visual signals without the necessity of fine-tuning on\\rmulti-modal datasets. The foundational concept of our method views an image as\\ra linguistic entity, and translates it to a set of discrete words derived from\\rthe LLM's vocabulary. To achieve this, we present the Vision-to-Language\\rTokenizer, abbreviated as V2T Tokenizer, which transforms an image into a\\r``foreign language'' with the combined aid of an encoder-decoder, the LLM\\rvocabulary, and a CLIP model. With this innovative image encoding, the LLM\\rgains the ability not only for visual comprehension but also for image\\rdenoising and restoration in an auto-regressive fashion-crucially, without any\\rfine-tuning. We undertake rigorous experiments to validate our method,\\rencompassing understanding tasks like image recognition, image captioning, and\\rvisual question answering, as well as image denoising tasks like inpainting,\\routpainting, deblurring, and shift restoration. Code and models are available\\rat https://github.com/zh460045050/V2L-Tokenizer.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07874 ,  3993kb)\\r%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06988 (*cross-listing*)\\rDate: Wed, 7 Feb 2024 13:36:02 GMT   (128kb,D)\\r\\rTitle: Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\rAuthors: Luca Beurer-Kellner, Marc Fischer, Martin Vechev\\rCategories: cs.LG cs.CL\\r\\\\\\\\\\r  To ensure that text generated by large language models (LLMs) is in an\\rexpected format, constrained decoding proposes to enforce strict formal\\rlanguage constraints during generation. However, as we show in this work, not\\ronly do such methods incur performance overhead during generation, but many of\\rthem also significantly impair task accuracy, if they do not correctly align\\rthe underlying LLM sub-word vocabularies with external constraints. To address\\rthis, we present a novel decoding algorithm, DOMINO, that can enforce\\rconstraints in a fully subword-aligned fashion, while leveraging\\rpre-computation and speculative decoding to achieve virtually no overhead and\\rin some cases even almost 2$\\\\times$ speedup over unconstrained decoding --\\rthereby outperforming existing approaches by a wide margin.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06988 ,  128kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07008 (*cross-listing*)\\rDate: Sat, 9 Mar 2024 02:47:11 GMT   (1094kb,D)\\r\\rTitle: AutoEval Done Right: Using Synthetic Data for Model Evaluation\\rAuthors: Pierre Boyeau, Anastasios N. Angelopoulos, Nir Yosef, Jitendra Malik,\\r  Michael I. Jordan\\rCategories: cs.LG cs.AI cs.CL stat.ME\\r\\\\\\\\\\r  The evaluation of machine learning models using human-labeled validation data\\rcan be expensive and time-consuming. AI-labeled synthetic data can be used to\\rdecrease the number of human annotations required for this purpose in a process\\rcalled autoevaluation. We suggest efficient and statistically principled\\ralgorithms for this purpose that improve sample efficiency while remaining\\runbiased. These algorithms increase the effective human-labeled sample size by\\rup to 50% on experiments with GPT-4.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07008 ,  1094kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07179 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 21:44:54 GMT   (985kb,D)\\r\\rTitle: 3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of\\r  Molecular Graphs\\rAuthors: Huaisheng Zhu, Teng Xiao, Vasant G Honavar\\rCategories: cs.LG cs.CL q-bio.BM\\r\\\\\\\\\\r  Generating molecules with desired properties is a critical task with broad\\rapplications in drug discovery and materials design. Inspired by recent\\radvances in large language models, there is a growing interest in using natural\\rlanguage descriptions of molecules to generate molecules with the desired\\rproperties. Most existing methods focus on generating molecules that precisely\\rmatch the text description. However, practical applications call for methods\\rthat generate diverse, and ideally novel, molecules with the desired\\rproperties. We propose 3M-Diffusion, a novel multi-modal molecular graph\\rgeneration method, to address this challenge. 3M-Diffusion first encodes\\rmolecular graphs into a graph latent space aligned with text descriptions. It\\rthen reconstructs the molecular structure and atomic attributes based on the\\rgiven text descriptions using the molecule decoder. It then learns a\\rprobabilistic mapping from the text space to the latent molecular graph space\\rusing a diffusion model. The results of our extensive experiments on several\\rdatasets demonstrate that 3M-Diffusion can generate high-quality, novel and\\rdiverse molecular graphs that semantically match the textual description\\rprovided.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07179 ,  985kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07191 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 22:24:14 GMT   (8928kb,D)\\r\\rTitle: $\\\\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking\\r  Reinforcement Learning Algorithms in Generative Language Model\\rAuthors: Yufeng Zhang, Liyu Chen, Boyi Liu, Yingxiang Yang, Qiwen Cui, Yunzhe\\r  Tao, Hongxia Yang\\rCategories: cs.LG cs.AI cs.CL\\rComments: 8 pages\\r\\\\\\\\\\r  Recent advances in reinforcement learning (RL) algorithms aim to enhance the\\rperformance of language models at scale. Yet, there is a noticeable absence of\\ra cost-effective and standardized testbed tailored to evaluating and comparing\\rthese algorithms. To bridge this gap, we present a generalized version of the\\r24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a\\rtarget value $K$ with $N$ integers. We evaluate the effectiveness of\\restablished RL algorithms such as Proximal Policy Optimization (PPO), alongside\\rnovel approaches like Identity Policy Optimization (IPO) and Direct Policy\\rOptimization (DPO).\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07191 ,  8928kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07283 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 03:30:04 GMT   (8297kb,D)\\r\\rTitle: A Framework for Cost-Effective and Self-Adaptive LLM Shaking and\\r  Recovery Mechanism\\rAuthors: Zhiyu Chen, Yu Li, Suochao Zhang, Jingbo Zhou, Jiwen Zhou, Chenfu Bao,\\r  Dianhai Yu\\rCategories: cs.CR cs.CL cs.LG\\rComments: 9 pages\\r\\\\\\\\\\r  As Large Language Models (LLMs) gain great success in real-world\\rapplications, an increasing number of users are seeking to develop and deploy\\rtheir customized LLMs through cloud services. Nonetheless, in some specific\\rdomains, there are still concerns regarding cost and trade-offs between privacy\\rissues and accuracy. In this study, we introduce a cost-effective and\\rself-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With\\rcarefully designed horizontal and vertical shaking operators, we can achieve\\rcomparable accuracy results with SOTA privacy-preserving LLM schemes using\\rCryptography-based or Differential Privacy-based methods. Experiments also show\\rthat with the CypherTalk framework, users can achieve reliable accuracy when\\rusing optimized shaking operator settings. To our best knowledge, this is the\\rfirst work that considers cost, and trade-off between model utility and privacy\\rin LLM scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07283 ,  8297kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07300 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 04:04:38 GMT   (5925kb,D)\\r\\rTitle: Taming Pre-trained LLMs for Generalised Time Series Forecasting via\\r  Cross-modal Knowledge Distillation\\rAuthors: Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong\\r  Jiang, Shu-Tao Xia\\rCategories: cs.LG cs.CL\\r\\\\\\\\\\r  Multivariate time series forecasting has recently gained great success with\\rthe rapid growth of deep learning models. However, existing approaches usually\\rtrain models from scratch using limited temporal data, preventing their\\rgeneralization. Recently, with the surge of the Large Language Models (LLMs),\\rseveral works have attempted to introduce LLMs into time series forecasting.\\rDespite promising results, these methods directly take time series as the input\\rto LLMs, ignoring the inherent modality gap between temporal and text data. In\\rthis work, we propose a novel Large Language Models and time series alignment\\rframework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time\\rseries forecasting challenge. Based on cross-modal knowledge distillation, the\\rproposed method exploits both input-agnostic static knowledge and\\rinput-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers\\rthe forecasting model with favorable performance as well as strong\\rgeneralization abilities. Extensive experiments demonstrate the proposed method\\restablishes a new state of the art for both long- and short-term forecasting.\\rCode is available at \\\\url{https://github.com/Hank0626/LLaTA}.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07300 ,  5925kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07339 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 05:44:27 GMT   (863kb,D)\\r\\rTitle: IM-Unpack: Training and Inference with Arbitrarily Low Precision\\r  Integers\\rAuthors: Zhanpeng Zeng, Karthikeyan Sankaralingam, Vikas Singh\\rCategories: cs.LG cs.CL cs.CV\\r\\\\\\\\\\r  GEneral Matrix Multiply (GEMM) is a central operation in deep learning and\\rcorresponds to the largest chunk of the compute footprint. Therefore, improving\\rits efficiency is an active topic of ongoing research. A popular strategy is\\rthe use of low bit-width integers to approximate the original entries in a\\rmatrix. This allows efficiency gains, but often requires sophisticated\\rtechniques to control the rounding error incurred. In this work, we first\\rverify/check that when the low bit-width restriction is removed, for a variety\\rof Transformer-based models, whether integers are sufficient for all GEMMs need\\r-- for {\\\\em both} training and inference stages, and can achieve parity with\\rfloating point counterparts. No sophisticated techniques are needed. We find\\rthat while a large majority of entries in matrices (encountered in such models)\\rcan be easily represented by {\\\\em low} bit-width integers, the existence of a\\rfew heavy hitter entries make it difficult to achieve efficiency gains via the\\rexclusive use of low bit-width GEMMs alone. To address this issue, we develop a\\rsimple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\\\\em unpack} a\\rmatrix with large integer entries into a larger matrix whose entries all lie\\rwithin the representable range of arbitrarily low bit-width integers. This\\rallows {\\\\em equivalence} with the original GEMM, i.e., the exact result can be\\robtained using purely low bit-width integer GEMMs. This comes at the cost of\\radditional operations -- we show that for many popular models, this overhead is\\rquite small.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07339 ,  863kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07379 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 07:32:47 GMT   (15321kb,D)\\r\\rTitle: Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The\\r  Lengths, Bends, and Dead Ends\\rAuthors: Sidak Pal Singh, Bobby He, Thomas Hofmann, Bernhard Sch\\\\olkopf\\rCategories: cs.LG cs.CL stat.ML\\rComments: Preprint, 51 pages\\r\\\\\\\\\\r  We propose a fresh take on understanding the mechanisms of neural networks by\\ranalyzing the rich structure of parameters contained within their optimization\\rtrajectories. Towards this end, we introduce some natural notions of the\\rcomplexity of optimization trajectories, both qualitative and quantitative,\\rwhich reveal the inherent nuance and interplay involved between various\\roptimization choices, such as momentum, weight decay, and batch size. We use\\rthem to provide key hallmarks about the nature of optimization in deep neural\\rnetworks: when it goes right, and when it finds itself in a dead end. Further,\\rthanks to our trajectory perspective, we uncover an intertwined behaviour of\\rmomentum and weight decay that promotes directional exploration, as well as a\\rdirectional regularization behaviour of some others. We perform experiments\\rover large-scale vision and language settings, including large language models\\r(LLMs) with up to 12 billion parameters, to demonstrate the value of our\\rapproach.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07379 ,  15321kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07652 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 13:41:15 GMT   (7836kb,D)\\r\\rTitle: Harder Tasks Need More Experts: Dynamic Routing in MoE Models\\rAuthors: Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin,\\r  Kun Xu, Kun Xu, Liwei Chen, Songfang Huang, Yansong Feng\\rCategories: cs.LG cs.CL\\r\\\\\\\\\\r  In this paper, we introduce a novel dynamic expert selection framework for\\rMixture of Experts (MoE) models, aiming to enhance computational efficiency and\\rmodel performance by adjusting the number of activated experts based on input\\rdifficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing,\\rwhich activates a predetermined number of experts regardless of the input's\\rcomplexity, our method dynamically selects experts based on the confidence\\rlevel in expert selection for each input. This allows for a more efficient\\rutilization of computational resources, activating more experts for complex\\rtasks requiring advanced reasoning and fewer for simpler tasks. Through\\rextensive evaluations, our dynamic routing method demonstrates substantial\\rimprovements over conventional Top-2 routing across various benchmarks,\\rachieving an average improvement of 0.7% with less than 90% activated\\rparameters. Further analysis shows our model dispatches more experts to tasks\\rrequiring complex reasoning skills, like BBH, confirming its ability to\\rdynamically allocate computational resources in alignment with the input's\\rcomplexity. Our findings also highlight a variation in the number of experts\\rneeded across different layers of the transformer model, offering insights into\\rthe potential for designing heterogeneous MoE frameworks. The code and models\\rare available at https://github.com/ZhenweiAn/Dynamic_MoE.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07652 ,  7836kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07690 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 14:33:53 GMT   (536kb,D)\\r\\rTitle: SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted\\r  Technical Debt\\rAuthors: Edi Sutoyo, Andrea Capiluppi\\rCategories: cs.SE cs.CL\\rComments: Accepted to be published at the 21st IEEE/ACM International\\r  Conference on Mining Software Repositories (MSR 2024)\\r\\\\\\\\\\r  Self-admitted technical debt (SATD) refers to a form of technical debt in\\rwhich developers explicitly acknowledge and document the existence of technical\\rshortcuts, workarounds, or temporary solutions within the codebase. Over recent\\ryears, researchers have manually labeled datasets derived from various software\\rdevelopment artifacts: source code comments, messages from the issue tracker\\rand pull request sections, and commit messages. These datasets are designed for\\rtraining, evaluation, performance validation, and improvement of machine\\rlearning and deep learning models to accurately identify SATD instances.\\rHowever, class imbalance poses a serious challenge across all the existing\\rdatasets, particularly when researchers are interested in categorizing the\\rspecific types of SATD. In order to address the scarcity of labeled data for\\rSATD \\\\textit{identification} (i.e., whether an instance is SATD or not) and\\r\\\\textit{categorization} (i.e., which type of SATD is being classified) in\\rexisting datasets, we share the \\\\textit{SATDAUG} dataset, an augmented version\\rof existing SATD datasets, including source code comments, issue tracker, pull\\rrequests, and commit messages. These augmented datasets have been balanced in\\rrelation to the available artifacts and provide a much richer source of labeled\\rdata for training machine learning or deep learning models.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07690 ,  536kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07769 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 15:56:10 GMT   (558kb)\\r\\rTitle: Transforming Competition into Collaboration: The Revolutionary Role of\\r  Multi-Agent Systems and Language Models in Modern Organizations\\rAuthors: Carlos Jose Xavier Cruz\\rCategories: cs.AI cs.CL cs.CY cs.MA\\r\\\\\\\\\\r  This article explores the dynamic influence of computational entities based\\ron multi-agent systems theory (SMA) combined with large language models (LLM),\\rwhich are characterized by their ability to simulate complex human\\rinteractions, as a possibility to revolutionize human user interaction from the\\ruse of specialized artificial agents to support everything from operational\\rorganizational processes to strategic decision making based on applied\\rknowledge and human orchestration. Previous investigations reveal that there\\rare limitations, particularly in the autonomous approach of artificial agents,\\respecially when dealing with new challenges and pragmatic tasks such as\\rinducing logical reasoning and problem solving. It is also considered that\\rtraditional techniques, such as the stimulation of chains of thoughts, require\\rexplicit human guidance. In our approach we employ agents developed from large\\rlanguage models (LLM), each with distinct prototyping that considers behavioral\\relements, driven by strategies that stimulate the generation of knowledge based\\ron the use case proposed in the scenario (role-play) business, using a\\rdiscussion approach between agents (guided conversation). We demonstrate the\\rpotential of developing agents useful for organizational strategies, based on\\rmulti-agent system theories (SMA) and innovative uses based on large language\\rmodels (LLM based), offering a differentiated and adaptable experiment to\\rdifferent applications, complexities, domains, and capabilities from LLM.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07769 ,  558kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07809 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 16:46:54 GMT   (1242kb,D)\\r\\rTitle: pyvene: A Library for Understanding and Improving PyTorch Models via\\r  Interventions\\rAuthors: Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang,\\r  Noah D. Goodman, Christopher D. Manning, Christopher Potts\\rCategories: cs.LG cs.CL\\rComments: 8 pages, 3 figures\\r\\\\\\\\\\r  Interventions on model-internal states are fundamental operations in many\\rareas of AI, including model editing, steering, robustness, and\\rinterpretability. To facilitate such research, we introduce $\\\\textbf{pyvene}$,\\ran open-source Python library that supports customizable interventions on a\\rrange of different PyTorch modules. $\\\\textbf{pyvene}$ supports complex\\rintervention schemes with an intuitive configuration format, and its\\rinterventions can be static or include trainable parameters. We show how\\r$\\\\textbf{pyvene}$ provides a unified and extensible framework for performing\\rinterventions on neural models and sharing the intervened upon models with\\rothers. We illustrate the power of the library via interpretability analyses\\rusing causal abstraction and knowledge localization. We publish our library\\rthrough Python Package Index (PyPI) and provide code, documentation, and\\rtutorials at https://github.com/stanfordnlp/pyvene.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07809 ,  1242kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07030 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 03:34:14 GMT   (1092kb,D)\\r\\rTitle: AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge\\r  Distillation\\rAuthors: Zihao Tang, Zheqi Lv, Shengyu Zhang, Yifan Zhou, Xinyu Duan, Fei Wu,\\r  Kun Kuang\\rCategories: cs.LG cs.CV\\rComments: Accepted to ICLR 2024\\r\\\\\\\\\\r  Due to privacy or patent concerns, a growing number of large models are\\rreleased without granting access to their training data, making transferring\\rtheir knowledge inefficient and problematic. In response, Data-Free Knowledge\\rDistillation (DFKD) methods have emerged as direct solutions. However, simply\\radopting models derived from DFKD for real-world applications suffers\\rsignificant performance degradation, due to the discrepancy between teachers'\\rtraining data and real-world scenarios (student domain). The degradation stems\\rfrom the portions of teachers' knowledge that are not applicable to the student\\rdomain. They are specific to the teacher domain and would undermine students'\\rperformance. Hence, selectively transferring teachers' appropriate knowledge\\rbecomes the primary challenge in DFKD. In this work, we propose a simple but\\reffective method AuG-KD. It utilizes an uncertainty-guided and sample-specific\\ranchor to align student-domain data with the teacher domain and leverages a\\rgenerative method to progressively trade off the learning process between OOD\\rknowledge distillation and domain-specific information learning via mixup\\rlearning. Extensive experiments in 3 datasets and 8 settings demonstrate the\\rstability and superiority of our approach. Code available at\\rhttps://github.com/IshiKura-a/AuG-KD .\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07030 ,  1092kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07036 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 08:13:42 GMT   (1207kb,D)\\r\\rTitle: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN\\r  Inference at the Edge\\rAuthors: Hasanul Mahmud, Peng Kang, Kevin Desai, Palden Lama, Sushil Prasad\\rCategories: cs.LG cs.CV cs.DC\\rComments: 8 Pages, 8 Figures\\r\\\\\\\\\\r  Reducing inference time and energy usage while maintaining prediction\\raccuracy has become a significant concern for deep neural networks (DNN)\\rinference on resource-constrained edge devices. To address this problem, we\\rpropose a novel approach based on converting autoencoder and lightweight\\rDNNs. This improves upon recent work such as early-exiting framework and DNN\\rpartitioning. Early-exiting frameworks spend different amounts of computation\\rpower for different input data depending upon their complexity. However, they\\rcan be inefficient in real-world scenarios that deal with many hard image\\rsamples. On the other hand, DNN partitioning algorithms that utilize the\\rcomputation power of both the cloud and edge devices can be affected by network\\rdelays and intermittent connections between the cloud and the edge. We present\\rCBNet, a low-latency and energy-efficient DNN inference framework tailored for\\redge devices. It utilizes a converting autoencoder to efficiently transform\\rhard images into easy ones, which are subsequently processed by a lightweight\\rDNN for inference. To the best of our knowledge, such autoencoder has not been\\rproposed earlier. Our experimental results using three popular\\rimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\\ran instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\\rspeedup in inference latency and 79% reduction in energy usage compared to\\rcompeting techniques while maintaining similar or higher accuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07036 ,  1207kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07076 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 18:09:50 GMT   (3583kb,D)\\r\\rTitle: Mapping High-level Semantic Regions in Indoor Environments without\\r  Object Recognition\\rAuthors: Roberto Bigazzi, Lorenzo Baraldi, Shreyas Kousik, Rita Cucchiara,\\r  Marco Pavone\\rCategories: cs.RO cs.CV\\rComments: Accepted by IEEE International Conference on Robotics and Automation\\r  (ICRA 2024)\\r\\\\\\\\\\r  Robots require a semantic understanding of their surroundings to operate in\\ran efficient and explainable way in human environments. In the literature,\\rthere has been an extensive focus on object labeling and exhaustive scene graph\\rgeneration; less effort has been focused on the task of purely identifying and\\rmapping large semantic regions. The present work proposes a method for semantic\\rregion mapping via embodied navigation in indoor environments, generating a\\rhigh-level representation of the knowledge of the agent. To enable region\\ridentification, the method uses a vision-to-language model to provide scene\\rinformation for mapping. By projecting egocentric scene understanding into the\\rglobal frame, the proposed method generates a semantic map as a distribution\\rover possible region labels at each location. This mapping procedure is paired\\rwith a trained navigation policy to enable autonomous map generation. The\\rproposed method significantly outperforms a variety of baselines, including an\\robject-based system and a pretrained scene classifier, in experiments in a\\rphotorealistic simulator.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07076 ,  3583kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07078 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 18:11:00 GMT   (2816kb)\\r\\rTitle: Improving deep learning with prior knowledge and cognitive models: A\\r  survey on enhancing explainability, adversarial robustness and zero-shot\\r  learning\\rAuthors: Fuseinin Mumuni and Alhassan Mumuni\\rCategories: cs.LG cs.AI cs.CV\\rJournal-ref: Cognitive Systems Research, 84 (2024)\\rDOI: 10.1016/j.cogsys.2023.101188\\r\\\\\\\\\\r  We review current and emerging knowledge-informed and brain-inspired\\rcognitive systems for realizing adversarial defenses, eXplainable Artificial\\rIntelligence (XAI), and zero-shot or few-short learning. Data-driven deep\\rlearning models have achieved remarkable performance and demonstrated\\rcapabilities surpassing human experts in many applications. Yet, their\\rinability to exploit domain knowledge leads to serious performance limitations\\rin practical applications. In particular, deep learning systems are exposed to\\radversarial attacks, which can trick them into making glaringly incorrect\\rdecisions. Moreover, complex data-driven models typically lack interpretability\\ror explainability, i.e., their decisions cannot be understood by human\\rsubjects. Furthermore, models are usually trained on standard datasets with a\\rclosed-world assumption. Hence, they struggle to generalize to unseen cases\\rduring inference in practical open-world environments, thus, raising the zero-\\ror few-shot generalization problem. Although many conventional solutions exist,\\rexplicit domain knowledge, brain-inspired neural network and cognitive\\rarchitectures offer powerful new dimensions towards alleviating these problems.\\rPrior knowledge is represented in appropriate forms and incorporated in deep\\rlearning frameworks to improve performance. Brain-inspired cognition methods\\ruse computational models that mimic the human mind to enhance intelligent\\rbehavior in artificial agents and autonomous robots. Ultimately, these models\\rachieve better explainability, higher adversarial robustness and data-efficient\\rlearning, and can, in turn, provide insights for cognitive science and\\rneuroscience-that is, to deepen human understanding on how the brain works in\\rgeneral, and how it handles these problems.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07078 ,  2816kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07092 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 18:36:55 GMT   (381kb,D)\\r\\rTitle: A cascaded deep network for automated tumor detection and segmentation\\r  in clinical PET imaging of diffuse large B-cell lymphoma\\rAuthors: Shadab Ahamed, Natalia Dubljevic, Ingrid Bloise, Claire Gowdy, Patrick\\r  Martineau, Don Wilson, Carlos F. Uribe, Arman Rahmim, and Fereshteh\\r  Yousefirizi\\rCategories: eess.IV cs.CV cs.LG physics.med-ph\\rComments: 8 pages, 3 figures, 3 tables\\rJournal-ref: Proc. SPIE 12032, Medical Imaging 2022: Image Processing, 120323M\\r  (4 April 2022)\\rDOI: 10.1117/12.2612684\\r\\\\\\\\\\r  Accurate detection and segmentation of diffuse large B-cell lymphoma (DLBCL)\\rfrom PET images has important implications for estimation of total metabolic\\rtumor volume, radiomics analysis, surgical intervention and radiotherapy.\\rManual segmentation of tumors in whole-body PET images is time-consuming,\\rlabor-intensive and operator-dependent. In this work, we develop and validate a\\rfast and efficient three-step cascaded deep learning model for automated\\rdetection and segmentation of DLBCL tumors from PET images. As compared to a\\rsingle end-to-end network for segmentation of tumors in whole-body PET images,\\rour three-step model is more effective (improves 3D Dice score from 58.9% to\\r78.1%) since each of its specialized modules, namely the slice classifier, the\\rtumor detector and the tumor segmentor, can be trained independently to a high\\rdegree of skill to carry out a specific task, rather than a single network with\\rsuboptimal performance on overall segmentation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07092 ,  381kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07105 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 18:57:45 GMT   (22120kb,D)\\r\\rTitle: A slice classification neural network for automated classification of\\r  axial PET/CT slices from a multi-centric lymphoma dataset\\rAuthors: Shadab Ahamed, Yixi Xu, Ingrid Bloise, Joo H. O, Carlos F. Uribe,\\r  Rahul Dodhia, Juan L. Ferres, and Arman Rahmim\\rCategories: eess.IV cs.CV cs.LG physics.med-ph\\rComments: 10 pages, 6 figures, 2 tables\\rJournal-ref: Proc. SPIE 12464, Medical Imaging 2023: Image Processing, 124641Q\\r  (3 April 2023)\\rDOI: 10.1117/12.2652947\\r\\\\\\\\\\r  Automated slice classification is clinically relevant since it can be\\rincorporated into medical image segmentation workflows as a preprocessing step\\rthat would flag slices with a higher probability of containing tumors, thereby\\rdirecting physicians attention to the important slices. In this work, we train\\ra ResNet-18 network to classify axial slices of lymphoma PET/CT images\\r(collected from two institutions) depending on whether the slice intercepted a\\rtumor (positive slice) in the 3D image or if the slice did not (negative\\rslice). Various instances of the network were trained on 2D axial datasets\\rcreated in different ways: (i) slice-level split and (ii) patient-level split;\\rinputs of different types were used: (i) only PET slices and (ii) concatenated\\rPET and CT slices; and different training strategies were employed: (i)\\rcenter-aware (CAW) and (ii) center-agnostic (CAG). Model performances were\\rcompared using the area under the receiver operating characteristic curve\\r(AUROC) and the area under the precision-recall curve (AUPRC), and various\\rbinary classification metrics. We observe and describe a performance\\roverestimation in the case of slice-level split as compared to the\\rpatient-level split training. The model trained using patient-level split data\\rwith the network input containing only PET slices in the CAG training regime\\rwas the best performing/generalizing model on a majority of metrics. Our models\\rwere additionally more closely compared using the sensitivity metric on the\\rpositive slices from their respective test sets.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07105 ,  22120kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07116 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 19:14:51 GMT   (7880kb,D)\\r\\rTitle: Simulation-Based Segmentation of Blood Vessels in Cerebral 3D OCTA\\r  Images\\rAuthors: Bastian Wittmann, Lukas Glandorf, Johannes C. Paetzold, Tamaz\\r  Amiranashvili, Thomas W\\\\alchli, Daniel Razansky, Bjoern Menze\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Segmentation of blood vessels in murine cerebral 3D OCTA images is\\rfoundational for in vivo quantitative analysis of the effects of neurovascular\\rdisorders, such as stroke or Alzheimer's, on the vascular network. However, to\\raccurately segment blood vessels with state-of-the-art deep learning methods, a\\rvast amount of voxel-level annotations is required. Since cerebral 3D OCTA\\rimages are typically plagued by artifacts and generally have a low\\rsignal-to-noise ratio, acquiring manual annotations poses an especially\\rcumbersome and time-consuming task. To alleviate the need for manual\\rannotations, we propose utilizing synthetic data to supervise segmentation\\ralgorithms. To this end, we extract patches from vessel graphs and transform\\rthem into synthetic cerebral 3D OCTA images paired with their matching ground\\rtruth labels by simulating the most dominant 3D OCTA artifacts. In extensive\\rexperiments, we demonstrate that our approach achieves competitive results,\\renabling annotation-free blood vessel segmentation in cerebral 3D OCTA images.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07116 ,  7880kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07126 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 19:41:40 GMT   (1873kb,D)\\r\\rTitle: Heterogeneous Image-based Classification Using Distributional Data\\r  Analysis\\rAuthors: Alec Reinhardt, Newsha Nikzad, Raven J. Hollis, Galia Jacobson,\\r  Millicent A. Roach, Mohamed Badawy, Peter Chul Park, Laura Beretta, Prasun K\\r  Jalal, David T. Fuentes, Eugene J. Koay, and Suprateek Kundu\\rCategories: stat.AP cs.CV\\rComments: 16, 2 figures, 3 tables\\r\\\\\\\\\\r  Diagnostic imaging has gained prominence as potential biomarkers for early\\rdetection and diagnosis in a diverse array of disorders including cancer.\\rHowever, existing methods routinely face challenges arising from various\\rfactors such as image heterogeneity. We develop a novel imaging-based\\rdistributional data analysis (DDA) approach that incorporates the probability\\r(quantile) distribution of the pixel-level features as covariates. The proposed\\rapproach uses a smoothed quantile distribution (via a suitable basis\\rrepresentation) as functional predictors in a scalar-on-functional quantile\\rregression model. Some distinctive features of the proposed approach include\\rthe ability to: (i) account for heterogeneity within the image; (ii)\\rincorporate granular information spanning the entire distribution; and (iii)\\rtackle variability in image sizes for unregistered images in cancer\\rapplications. Our primary goal is risk prediction in Hepatocellular carcinoma\\rthat is achieved via predicting the change in tumor grades at post-diagnostic\\rvisits using pre-diagnostic enhancement pattern mapping (EPM) images of the\\rliver. Along the way, the proposed DDA approach is also used for case versus\\rcontrol diagnosis and risk stratification objectives. Our analysis reveals that\\rwhen coupled with global structural radiomics features derived from the\\rcorresponding T1-MRI scans, the proposed smoothed quantile distributions\\rderived from EPM images showed considerable improvements in sensitivity and\\rcomparable specificity in contrast to classification based on routinely used\\rsummary measures that do not account for image heterogeneity. Given that there\\rare limited predictive modeling approaches based on heterogeneous images in\\rcancer, the proposed method is expected to provide considerable advantages in\\rimage-based early detection and risk prediction.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07126 ,  1873kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07132 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 20:02:17 GMT   (597kb,D)\\r\\rTitle: A New Machine Learning Dataset of Bulldog Nostril Images for Stenosis\\r  Degree Classification\\rAuthors: Gabriel Toshio Hirokawa Higa, Joyce Katiuccia Medeiros Ramos Carvalho,\\r  Paolo Brito Pascoalini Zanoni, Gisele Braziliano de Andrade, Hemerson Pistori\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  Brachycephaly, a conformation trait in some dog breeds, causes BOAS, a\\rrespiratory disorder that affects the health and welfare of the dogs with\\rvarious symptoms. In this paper, a new annotated dataset composed of 190 images\\rof bulldogs' nostrils is presented. Three degrees of stenosis are approximately\\requally represented in the dataset: mild, moderate and severe stenosis. The\\rdataset also comprises a small quantity of non stenotic nostril images. To the\\rbest of our knowledge, this is the first image dataset addressing this problem.\\rFurthermore, deep learning is investigated as an alternative to automatically\\rinfer stenosis degree using nostril images. In this work, several neural\\rnetworks were tested: ResNet50, MobileNetV3, DenseNet201, SwinV2 and MaxViT.\\rFor this evaluation, the problem was modeled in two different ways: first, as a\\rthree-class classification problem (mild or open, moderate, and severe);\\rsecond, as a binary classification problem, with severe stenosis as target. For\\rthe multiclass classification, a maximum median f-score of 53.77\\\\% was achieved\\rby the MobileNetV3. For binary classification, a maximum median f-score of\\r72.08\\\\% has been reached by ResNet50, indicating that the problem is\\rchallenging but possibly tractable.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07132 ,  597kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07134 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 20:04:03 GMT   (667kb,D)\\r\\rTitle: COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization\\rAuthors: Aozhong Zhang, Zi Yang, Naigang Wang, Yingyong Qin, Jack Xin, Xin Li,\\r  Penghang Yin\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Post-training quantization (PTQ) has emerged as a practical approach to\\rcompress large neural networks, making them highly efficient for deployment.\\rHowever, effectively reducing these models to their low-bit counterparts\\rwithout compromising the original accuracy remains a key challenge. In this\\rpaper, we propose an innovative PTQ algorithm termed COMQ, which sequentially\\rconducts coordinate-wise minimization of the layer-wise reconstruction errors.\\rWe consider the widely used integer quantization, where every quantized weight\\rcan be decomposed into a shared floating-point scalar and an integer bit-code.\\rWithin a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as\\rthe variables of the reconstruction error. Every iteration improves this error\\ralong a single coordinate while keeping all other variables constant. COMQ is\\reasy to use and requires no hyper-parameter tuning. It instead involves only\\rdot products and rounding operations. We update these variables in a carefully\\rdesigned greedy order, significantly enhancing the accuracy. COMQ achieves\\rremarkable results in quantizing 4-bit Vision Transformers, with a negligible\\rloss of less than 1% in Top-1 accuracy. In 4-bit INT quantization of\\rconvolutional neural networks, COMQ maintains near-lossless accuracy with a\\rminimal drop of merely 0.3% in Top-1 accuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07134 ,  667kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07137 (*cross-listing*)\\rDate: Mon, 11 Mar 2024 20:07:05 GMT   (10106kb,D)\\r\\rTitle: Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution\\rAuthors: Alexandre de Oliveira Bezerra, Rodrigo Goncalves Mateus, Vanessa Ap.\\r  de Moraes Weber, Fabricio de Lima Weber, Yasmin Alves de Arruda, Rodrigo da\\r  Costa Gomes, Gabriel Toshio Hirokawa Higa, Hemerson Pistori\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  Assessing the biotype of cattle through human visual inspection is a very\\rcommon and important practice in precision cattle breeding. This paper presents\\rthe results of a correlation analysis between scores produced by humans for\\rNelore cattle and a variety of measurements that can be derived from images or\\rother instruments. It also presents a study using the k-means algorithm to\\rgenerate new ways of clustering a batch of cattle using the measurements that\\rmost correlate with the animal's body weight and visual scores.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07137 ,  10106kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07247 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 02:09:39 GMT   (3923kb,D)\\r\\rTitle: GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical\\r  structure Generation\\rAuthors: Linrui Dai, Rongzhao Zhang, Zhongzhen Huang, Xiaofan Zhang\\rCategories: eess.IV cs.CV cs.LG\\rComments: submitted to MICCAI2024\\r\\\\\\\\\\r  The annotation burden and extensive labor for gathering a large medical\\rdataset with images and corresponding labels are rarely cost-effective and\\rhighly intimidating. This results in a lack of abundant training data that\\rundermines downstream tasks and partially contributes to the challenge image\\ranalysis faces in the medical field. As a workaround, given the recent success\\rof generative neural models, it is now possible to synthesize image datasets at\\ra high fidelity guided by external constraints. This paper explores this\\rpossibility and presents \\\\textbf{GuideGen}: a pipeline that jointly generates\\rCT images and tissue masks for abdominal organs and colorectal cancer\\rconditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to\\rfit the discrete distribution of mask labels and generate low-resolution 3D\\rtissue masks. Secondly, our Conditional Image Generator autoregressively\\rgenerates CT slices conditioned on a corresponding mask slice to incorporate\\rboth style information and anatomical guidance. This pipeline guarantees high\\rfidelity and variability as well as exact alignment between generated CT\\rvolumes and tissue masks. Both qualitative and quantitative experiments on 3D\\rabdominal CTs demonstrate a high performance of our proposed pipeline, thereby\\rproving our method can serve as a dataset generator and provide potential\\rbenefits to downstream tasks. It is hoped that our work will offer a promising\\rsolution on the multimodality generation of CT and its anatomical mask. Our\\rsource code is publicly available at\\rhttps://github.com/OvO1111/JointImageGeneration.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07247 ,  3923kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07296 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 03:57:25 GMT   (524kb,D)\\r\\rTitle: Advancements in Continuous Glucose Monitoring: Integrating Deep Learning\\r  and ECG Signal\\rAuthors: MohammadReza Hosseinzadehketilateh, Banafsheh Adami, Nima Karimian\\rCategories: eess.SP cs.CV cs.HC\\r\\\\\\\\\\r  This paper presents a novel approach to noninvasive hyperglycemia monitoring\\rutilizing electrocardiograms (ECG) from an extensive database comprising 1119\\rsubjects. Previous research on hyperglycemia or glucose detection using ECG has\\rbeen constrained by challenges related to generalization and scalability,\\rprimarily due to using all subjects' ECG in training without considering unseen\\rsubjects as a critical factor for developing methods with effective\\rgeneralization. We designed a deep neural network model capable of identifying\\rsignificant features across various spatial locations and examining the\\rinterdependencies among different features within each convolutional layer. To\\rexpedite processing speed, we segment the ECG of each user to isolate one\\rheartbeat or one cycle of the ECG. Our model was trained using data from 727\\rsubjects, while 168 were used for validation. The testing phase involved 224\\runseen subjects, with a dataset consisting of 9,000 segments. The result\\rindicates that the proposed algorithm effectively detects hyperglycemia with a\\r91.60% area under the curve (AUC), 81.05% sensitivity, and 85.54% specificity.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07296 ,  524kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07303 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 04:10:06 GMT   (766kb,D)\\r\\rTitle: Dynamic U-Net: Adaptively Calibrate Features for Abdominal Multi-organ\\r  Segmentation\\rAuthors: Jin Yang, Daniel S. Marcus, and Aristeidis Sotiras\\rCategories: eess.IV cs.CV\\rComments: 11 pages, 3 figures, 2 tables\\r\\\\\\\\\\r  U-Net has been widely used for segmenting abdominal organs, achieving\\rpromising performance. However, when it is used for multi-organ segmentation,\\rfirst, it may be limited in exploiting global long-range contextual information\\rdue to the implementation of standard convolutions. Second, the use of\\rspatial-wise downsampling (e.g., max pooling or strided convolutions) in the\\rencoding path may lead to the loss of deformable or discriminative details.\\rThird, features upsampled from the higher level are concatenated with those\\rthat persevered via skip connections. However, repeated downsampling and\\rupsampling operations lead to misalignments between them and their\\rconcatenation degrades segmentation performance. To address these limitations,\\rwe propose Dynamically Calibrated Convolution (DCC), Dynamically Calibrated\\rDownsampling (DCD), and Dynamically Calibrated Upsampling (DCU) modules,\\rrespectively. The DCC module can utilize global inter-dependencies between\\rspatial and channel features to calibrate these features adaptively. The DCD\\rmodule enables networks to adaptively preserve deformable or discriminative\\rfeatures during downsampling. The DCU module can dynamically align and\\rcalibrate upsampled features to eliminate misalignments before concatenations.\\rWe integrated the proposed modules into a standard U-Net, resulting in a new\\rarchitecture, termed Dynamic U-Net. This architectural design enables U-Net to\\rdynamically adjust features for different organs. We evaluated Dynamic U-Net in\\rtwo abdominal multi-organ segmentation benchmarks. Dynamic U-Net achieved\\rstatistically improved segmentation accuracy compared with standard U-Net. Our\\rcode is available at https://github.com/sotiraslab/DynamicUNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07303 ,  766kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07314 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 05:00:38 GMT   (981kb)\\r\\rTitle: Customizable Avatars with Dynamic Facial Action Coded Expressions\\r  (CADyFACE) for Improved User Engagement\\rAuthors: Megan A. Witherow, Crystal Butler, Winston J. Shields, Furkan Ilgin,\\r  Norou Diawara, Janice Keener, John W. Harrington, and Khan M. Iftekharuddin\\rCategories: cs.HC cs.CV cs.LG\\rComments: 12 pages, 8 figures\\r\\\\\\\\\\r  Customizable 3D avatar-based facial expression stimuli may improve user\\rengagement in behavioral biomarker discovery and therapeutic intervention for\\rautism, Alzheimer's disease, facial palsy, and more. However, there is a lack\\rof customizable avatar-based stimuli with Facial Action Coding System (FACS)\\raction unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled,\\rcustomizable avatar-based expression stimuli for maintaining subjects'\\rengagement, (2) learning-based measurements that quantify subjects' facial\\rresponses to such stimuli, and (3) validation of constructs represented by\\rstimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial\\rAction Coded Expressions (CADyFACE) labeled with AUs by a certified FACS\\rexpert. To measure subjects' AUs in response to CADyFACE, we propose a novel\\rBeta-guided Correlation and Multi-task Expression learning neural network\\r(BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss\\rencourages feature correlation with AUs while discouraging correlation with\\rsubject identities for improved generalization. We train BeCoME-Net for\\runilateral and bilateral AU detection and compare with state-of-the-art\\rapproaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty\\rhealthy adult volunteers complete expression recognition and mimicry tasks in\\ran online feasibility study while webcam-based eye-tracking and video are\\rcollected. We test validity of multiple constructs, including face preference\\rduring recognition and AUs during mimicry.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07314 ,  981kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07355 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 06:28:41 GMT   (811kb)\\r\\rTitle: Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO\\r  Systems\\rAuthors: Junyong Shin, Yujin Kang, Yo-Seb Jeon\\rCategories: eess.SP cs.AI cs.CV\\r\\\\\\\\\\r  This paper presents a finite-rate deep-learning (DL)-based channel state\\rinformation (CSI) feedback method for massive multiple-input multiple-output\\r(MIMO) systems. The presented method provides a finite-bit representation of\\rthe latent vector based on a vector-quantized variational autoencoder (VQ-VAE)\\rframework while reducing its computational complexity based on shape-gain\\rvector quantization. In this method, the magnitude of the latent vector is\\rquantized using a non-uniform scalar codebook with a proper transformation\\rfunction, while the direction of the latent vector is quantized using a\\rtrainable Grassmannian codebook. A multi-rate codebook design strategy is also\\rdeveloped by introducing a codeword selection rule for a nested codebook along\\rwith the design of a loss function. Simulation results demonstrate that the\\rproposed method reduces the computational complexity associated with VQ-VAE\\rwhile improving CSI reconstruction performance under a given feedback overhead.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07355 ,  811kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07362 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 06:50:32 GMT   (27366kb,D)\\r\\rTitle: Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine\\r  Unlearning\\rAuthors: Chongyu Fan, Jiancheng Liu, Alfred Hero, Sijia Liu\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\\\r  The trustworthy machine learning (ML) community is increasingly recognizing\\rthe crucial need for models capable of selectively 'unlearning' data points\\rafter training. This leads to the problem of machine unlearning (MU), aiming to\\reliminate the influence of chosen data points on model performance, while still\\rmaintaining the model's utility post-unlearning. Despite various MU methods for\\rdata influence erasure, evaluations have largely focused on random data\\rforgetting, ignoring the vital inquiry into which subset should be chosen to\\rtruly gauge the authenticity of unlearning performance. To tackle this issue,\\rwe introduce a new evaluative angle for MU from an adversarial viewpoint. We\\rpropose identifying the data subset that presents the most significant\\rchallenge for influence erasure, i.e., pinpointing the worst-case forget set.\\rUtilizing a bi-level optimization principle, we amplify unlearning challenges\\rat the upper optimization level to emulate worst-case scenarios, while\\rsimultaneously engaging in standard training and unlearning at the lower level,\\rachieving a balance between data influence erasure and model utility. Our\\rproposal offers a worst-case evaluation of MU's resilience and effectiveness.\\rThrough extensive experiments across different datasets (including CIFAR-10,\\r100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image\\rclassifiers and generative models), we expose critical pros and cons in\\rexisting (approximate) unlearning strategies. Our results illuminate the\\rcomplex challenges of MU in practice, guiding the future development of more\\raccurate and robust unlearning algorithms. The code is available at\\rhttps://github.com/OPTML-Group/Unlearn-WorstCase.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07362 ,  27366kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07390 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 07:58:14 GMT   (38975kb,D)\\r\\rTitle: Learning Correction Errors via Frequency-Self Attention for Blind Image\\r  Super-Resolution\\rAuthors: Haochen Sun, Yan Yuan, Lijuan Su and Haotian Shao\\rCategories: eess.IV cs.CV\\rComments: 16 pages\\r\\\\\\\\\\r  Previous approaches for blind image super-resolution (SR) have relied on\\rdegradation estimation to restore high-resolution (HR) images from their\\rlow-resolution (LR) counterparts. However, accurate degradation estimation\\rposes significant challenges. The SR model's incompatibility with degradation\\restimation methods, particularly the Correction Filter, may significantly\\rimpair performance as a result of correction errors. In this paper, we\\rintroduce a novel blind SR approach that focuses on Learning Correction Errors\\r(LCE). Our method employs a lightweight Corrector to obtain a corrected\\rlow-resolution (CLR) image. Subsequently, within an SR network, we jointly\\roptimize SR performance by utilizing both the original LR image and the\\rfrequency learning of the CLR image. Additionally, we propose a new\\rFrequency-Self Attention block (FSAB) that enhances the global information\\rutilization ability of Transformer. This block integrates both self-attention\\rand frequency spatial attention mechanisms. Extensive ablation and comparison\\rexperiments conducted across various settings demonstrate the superiority of\\rour method in terms of visual quality and accuracy. Our approach effectively\\raddresses the challenges associated with degradation estimation and correction\\rerrors, paving the way for more accurate blind image SR.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07390 ,  38975kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07428 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 09:11:02 GMT   (2494kb,D)\\r\\rTitle: Input Data Adaptive Learning (IDAL) for Sub-acute Ischemic Stroke Lesion\\r  Segmentation\\rAuthors: Michael G\\\\otz, Christian Weber, Christoph Kolb, Klaus Maier-Hein\\rCategories: eess.IV cs.CV\\rJournal-ref: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic\\r  Brain Injuries. BrainLes 2015\\rDOI: 10.1007/978-3-319-30858-6_25\\r\\\\\\\\\\r  In machine learning larger databases are usually associated with higher\\rclassification accuracy due to better generalization. This generalization may\\rlead to non-optimal classifiers in some medical applications with highly\\rvariable expressions of pathologies. This paper presents a method for learning\\rfrom a large training base by adaptively selecting optimal training samples for\\rgiven input data. In this way heterogeneous databases are supported two-fold.\\rFirst, by being able to deal with sparsely annotated data allows a quick\\rinclusion of new data set and second, by training an input-dependent\\rclassifier. The proposed approach is evaluated using the SISS challenge. The\\rproposed algorithm leads to a significant improvement of the classification\\raccuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07428 ,  2494kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07434 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 09:17:21 GMT   (1574kb)\\r\\rTitle: DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated\\r  MR Images\\rAuthors: Michael G\\\\otz, Christian Weber, Franciszek Binczyk, Joanna Polanska,\\r  Rafal Tarnawski, Barbara Bobek-Billewicz, Ullrich K\\\\othe, Jens Kleesiek,\\r  Bram Stieltjes, Klaus H. Maier-Hein\\rCategories: eess.IV cs.CV\\rJournal-ref: IEEE Transactions on Medical Imaging ( Volume: 35, Issue: 1,\\r  January 2016)\\rDOI: 10.1109/TMI.2015.2463078\\r\\\\\\\\\\r  We propose a new method that employs transfer learning techniques to\\reffectively correct sampling selection errors introduced by sparse annotations\\rduring supervised learning for automated tumor segmentation. The practicality\\rof current learning-based automated tissue classification approaches is\\rseverely impeded by their dependency on manually segmented training databases\\rthat need to be recreated for each scenario of application, site, or\\racquisition setup. The comprehensive annotation of reference datasets can be\\rhighly labor-intensive, complex, and error-prone. The proposed method derives\\rhigh-quality classifiers for the different tissue classes from sparse and\\runambiguous annotations and employs domain adaptation techniques for\\reffectively correcting sampling selection errors introduced by the sparse\\rsampling. The new approach is validated on labeled, multi-modal MR images of 19\\rpatients with malignant gliomas and by comparative analysis on the BraTS 2013\\rchallenge data sets. Compared to training on fully labeled data, we reduced the\\rtime for labeling and training by a factor greater than 70 and 180 respectively\\rwithout sacrificing accuracy. This dramatically eases the establishment and\\rconstant extension of large annotated databases in various scenarios and\\rimaging setups and thus represents an important step towards practical\\rapplicability of learning-based approaches in tissue classification.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07434 ,  1574kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07463 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 09:59:34 GMT   (1192kb,D)\\r\\rTitle: Backdoor Attack with Mode Mixture Latent Modification\\rAuthors: Hongwei Zhang, Xiaoyin Xu, Dongsheng An, Xianfeng Gu and Min Zhang\\rCategories: cs.CR cs.CV\\r\\\\\\\\\\r  Backdoor attacks become a significant security concern for deep neural\\rnetworks in recent years. An image classification model can be compromised if\\rmalicious backdoors are injected into it. This corruption will cause the model\\rto function normally on clean images but predict a specific target label when\\rtriggers are present. Previous research can be categorized into two genres:\\rpoisoning a portion of the dataset with triggered images for users to train the\\rmodel from scratch, or training a backdoored model alongside a triggered image\\rgenerator. Both approaches require significant amount of attackable parameters\\rfor optimization to establish a connection between the trigger and the target\\rlabel, which may raise suspicions as more people become aware of the existence\\rof backdoor attacks. In this paper, we propose a backdoor attack paradigm that\\ronly requires minimal alterations (specifically, the output layer) to a clean\\rmodel in order to inject the backdoor under the guise of fine-tuning. To\\rachieve this, we leverage mode mixture samples, which are located between\\rdifferent modes in latent space, and introduce a novel method for conducting\\rbackdoor attacks. We evaluate the effectiveness of our method on four popular\\rbenchmark datasets: MNIST, CIFAR-10, GTSRB, and TinyImageNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07463 ,  1192kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07494 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 10:33:26 GMT   (1864kb,D)\\r\\rTitle: SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM\\rAuthors: Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, Hesheng Wang\\rCategories: cs.RO cs.CV\\r\\\\\\\\\\r  We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D\\rGaussian representation, that enables accurate 3D semantic mapping, robust\\rcamera tracking, and high-quality rendering in real-time. In this system, we\\rincorporate semantic feature embedding into 3D Gaussian representation, which\\reffectively encodes semantic information within the spatial layout of the\\renvironment for precise semantic scene representation. Furthermore, we propose\\rfeature-level loss for updating 3D Gaussian representation, enabling\\rhigher-level guidance for 3D Gaussian optimization. In addition, to reduce\\rcumulative drift and improve reconstruction accuracy, we introduce\\rsemantic-informed bundle adjustment leveraging semantic associations for joint\\roptimization of 3D Gaussian representation and camera poses, leading to more\\rrobust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates\\rsuperior performance over existing dense semantic SLAM methods in terms of\\rmapping and tracking accuracy on Replica and ScanNet datasets, while also\\rshowing excellent capabilities in novel-view semantic synthesis and 3D semantic\\rmapping.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07494 ,  1864kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07553 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 11:39:18 GMT   (3692kb,D)\\r\\rTitle: The future of document indexing: GPT and Donut revolutionize table of\\r  content processing\\rAuthors: Degaga Wolde Feyisa, Haylemicheal Berihun, Amanuel Zewdu, Mahsa\\r  Najimoghadam, Marzieh Zare\\rCategories: cs.IR cs.AI cs.CV\\rComments: Document AI, Document Classification, Information extraction, Large\\r  Language Models, OCR Models, Visual Document Understanding\\r\\\\\\\\\\r  Industrial projects rely heavily on lengthy, complex specification documents,\\rmaking tedious manual extraction of structured information a major bottleneck.\\rThis paper introduces an innovative approach to automate this process,\\rleveraging the capabilities of two cutting-edge AI models: Donut, a model that\\rextracts information directly from scanned documents without OCR, and OpenAI\\rGPT-3.5 Turbo, a robust large language model. The proposed methodology is\\rinitiated by acquiring the table of contents (ToCs) from construction\\rspecification documents and subsequently structuring the ToCs text into JSON\\rdata. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5\\rTurbo reaching 89% in effectively organizing the ToCs. This landmark\\rachievement represents a significant leap forward in document indexing,\\rdemonstrating the immense potential of AI to automate information extraction\\rtasks across diverse document types, boosting efficiency and liberating\\rcritical resources in various industries.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07553 ,  3692kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07563 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 11:51:55 GMT   (11715kb,D)\\r\\rTitle: Learning Generalizable Feature Fields for Mobile Manipulation\\rAuthors: Ri-Zhao Qiu, Yafei Hu, Ge Yang, Yuchen Song, Yang Fu, Jianglong Ye,\\r  Jiteng Mu, Ruihan Yang, Nikolay Atanasov, Sebastian Scherer, Xiaolong Wang\\rCategories: cs.RO cs.CV cs.LG\\rComments: Preprint. Project website is at: https://geff-b1.github.io/\\r\\\\\\\\\\r  An open problem in mobile manipulation is how to represent objects and scenes\\rin a unified manner, so that robots can use it both for navigating in the\\renvironment and manipulating objects. The latter requires capturing intricate\\rgeometry while understanding fine-grained semantics, whereas the former\\rinvolves capturing the complexity inherit to an expansive physical scale. In\\rthis work, we present GeFF (Generalizable Feature Fields), a scene-level\\rgeneralizable neural feature field that acts as a unified representation for\\rboth navigation and manipulation that performs in real-time. To do so, we treat\\rgenerative novel view synthesis as a pre-training task, and then align the\\rresulting rich scene priors with natural language via CLIP feature\\rdistillation. We demonstrate the effectiveness of this approach by deploying\\rGeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's\\rability to generalize to open-set objects as well as running time, when\\rperforming open-vocabulary mobile manipulation in dynamic scenes.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07563 ,  11715kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07569 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 11:56:50 GMT   (5042kb,D)\\r\\rTitle: Exploring Challenges in Deep Learning of Single-Station Ground Motion\\r  Records\\rAuthors: \\\\Umit Mert \\\\c{C}a\\\\u{g}lar, Baris Yilmaz, Melek T\\\\urkmen, Erdem\\r  Akag\\\\und\\\\uz, Salih Tileylioglu\\rCategories: eess.SP cs.CV cs.LG\\rComments: 9 Pages, 12 Figures, 5 Tables\\r\\\\\\\\\\r  Contemporary deep learning models have demonstrated promising results across\\rvarious applications within seismology and earthquake engineering. These models\\rrely primarily on utilizing ground motion records for tasks such as earthquake\\revent classification, localization, earthquake early warning systems, and\\rstructural health monitoring. However, the extent to which these models\\reffectively learn from these complex time-series signals has not been\\rthoroughly analyzed. In this study, our objective is to evaluate the degree to\\rwhich auxiliary information, such as seismic phase arrival times or seismic\\rstation distribution within a network, dominates the process of deep learning\\rfrom ground motion records, potentially hindering its effectiveness. We perform\\ra hyperparameter search on two deep learning models to assess their\\reffectiveness in deep learning from ground motion records while also examining\\rthe impact of auxiliary information on model performance. Experimental results\\rreveal a strong reliance on the highly correlated P and S phase arrival\\rinformation. Our observations highlight a potential gap in the field,\\rindicating an absence of robust methodologies for deep learning of\\rsingle-station ground motion recordings independent of any auxiliary\\rinformation.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07569 ,  5042kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07715 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 14:57:57 GMT   (2441kb,D)\\r\\rTitle: Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound\\rAuthors: Blake VanBerlo, Alexander Wong, Jesse Hoey, Robert Arntfield\\rCategories: eess.IV cs.CV\\rComments: 18 pages, 5 figures\\rACM-class: I.2.10; I.4.9; J.3\\r\\\\\\\\\\r  Self-supervised learning (SSL) is one strategy for addressing the paucity of\\rlabelled data in medical imaging by learning representations from unlabelled\\rimages. Contrastive and non-contrastive SSL methods produce learned\\rrepresentations that are similar for pairs of related images. Such pairs are\\rcommonly constructed by randomly distorting the same image twice. The\\rvideographic nature of ultrasound offers flexibility for defining the\\rsimilarity relationship between pairs of images. In this study, we investigated\\rthe effect of utilizing proximal, distinct images from the same B-mode\\rultrasound video as pairs for SSL. Additionally, we introduced a sample\\rweighting scheme that increases the weight of closer image pairs and\\rdemonstrated how it can be integrated into SSL objectives. Named Intra-Video\\rPositive Pairs (IVPP), the method surpassed previous ultrasound-specific\\rcontrastive learning methods' average test accuracy on COVID-19 classification\\rwith the POCUS dataset by $\\\\ge 1.3\\\\%$. Detailed investigations of IVPP's\\rhyperparameters revealed that some combinations of IVPP hyperparameters can\\rlead to improved or worsened performance, depending on the downstream task.\\rGuidelines for practitioners were synthesized based on the results, such as the\\rmerit of IVPP with task-specific hyperparameters, and the improved performance\\rof contrastive methods for ultrasound compared to non-contrastive counterparts.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07715 ,  2441kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07743 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 15:22:05 GMT   (54386kb,D)\\r\\rTitle: Equipping Computational Pathology Systems with Artifact Processing\\r  Pipelines: A Showcase for Computation and Performance Trade-offs\\rAuthors: Neel Kanwal, Farbod Khoraminia, Umay Kiraz, Andres Mosquera-Zamudio,\\r  Carlos Monteagudo, Emiel A.M. Janssen, Tahlita C.M. Zuiverloon, Chunmig Rong,\\r  and Kjersti Engan\\rCategories: eess.IV cs.AI cs.CV cs.LG\\rComments: Submitted to BMC Medical Informatics and Decision Making Journal\\r\\\\\\\\\\r  Histopathology is a gold standard for cancer diagnosis under a microscopic\\rexamination. However, histological tissue processing procedures result in\\rartifacts, which are ultimately transferred to the digitized version of glass\\rslides, known as whole slide images (WSIs). Artifacts are diagnostically\\rirrelevant areas and may result in wrong deep learning (DL) algorithms\\rpredictions. Therefore, detecting and excluding artifacts in the computational\\rpathology (CPATH) system is essential for reliable automated diagnosis. In this\\rpaper, we propose a mixture of experts (MoE) scheme for detecting five notable\\rartifacts, including damaged tissue, blur, folded tissue, air bubbles, and\\rhistologically irrelevant blood from WSIs. First, we train independent binary\\rDL models as experts to capture particular artifact morphology. Then, we\\rensemble their predictions using a fusion mechanism. We apply probabilistic\\rthresholding over the final probability distribution to improve the sensitivity\\rof the MoE. We developed DL pipelines using two MoEs and two multiclass models\\rof state-of-the-art deep convolutional neural networks (DCNNs) and vision\\rtransformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed\\rsimpler multiclass models and were tested on datasets from different hospitals\\rand cancer types, where MoE using DCNNs yielded the best results. The proposed\\rMoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining\\rless computational cost for inference than MoE using ViTs. This best\\rperformance of MoEs comes with relatively higher computational trade-offs than\\rmulticlass models. The proposed artifact detection pipeline will not only\\rensure reliable CPATH predictions but may also provide quality control.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07743 ,  54386kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07786 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 16:20:27 GMT   (33800kb,D)\\r\\rTitle: Generative deep learning-enabled ultra-large field-of-view lens-free\\r  imaging\\rAuthors: Ronald B. Liu, Zhe Liu, Max G.A. Wolf, Krishna P. Purohit, Gregor\\r  Fritz, Yi Feng, Carsten G. Hansen, Pierre O. Bagnaninchi, Xavier Casadevall i\\r  Solvas, Yunjie Yang\\rCategories: physics.optics cs.CV\\r\\\\\\\\\\r  Advancements in high-throughput biomedical applications necessitate\\rreal-time, large field-of-view (FOV) imaging capabilities. Conventional\\rlens-free imaging (LFI) systems, while addressing the limitations of physical\\rlenses, have been constrained by dynamic, hard-to-model optical fields,\\rresulting in a limited one-shot FOV of approximately 20 $mm^2$. This\\rrestriction has been a major bottleneck in applications like live-cell imaging\\rand automation of microfluidic systems for biomedical research. Here, we\\rpresent a deep-learning(DL)-based imaging framework -- GenLFI -- leveraging\\rgenerative artificial intelligence (AI) for holographic image reconstruction.\\rWe demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$,\\rsurpassing the current LFI system by more than 20-fold, and even larger than\\rthe world's largest confocal microscope by 1.76 times. The resolution is at the\\rsub-pixel level of 5.52 $\\\\mu m$, without the need for a shifting light source.\\rThe unsupervised learning-based reconstruction does not require optical field\\rmodeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics\\rand 3D cell models) in complex optical fields possible. This GenLFI framework\\runlocks the potential of LFI systems, offering a robust tool to tackle new\\rfrontiers in high-throughput biomedical applications such as drug discovery.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07786 ,  33800kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07788 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 16:23:49 GMT   (17395kb,D)\\r\\rTitle: DexCap: Scalable and Portable Mocap Data Collection System for Dexterous\\r  Manipulation\\rAuthors: Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C.\\r  Karen Liu\\rCategories: cs.RO cs.AI cs.CV cs.LG\\r\\\\\\\\\\r  Imitation learning from human hand motion data presents a promising avenue\\rfor imbuing robots with human-like dexterity in real-world manipulation tasks.\\rDespite this potential, substantial challenges persist, particularly with the\\rportability of existing hand motion capture (mocap) systems and the difficulty\\rof translating mocap data into effective control policies. To tackle these\\rissues, we introduce DexCap, a portable hand motion capture system, alongside\\rDexIL, a novel imitation algorithm for training dexterous robot skills directly\\rfrom human hand mocap data. DexCap offers precise, occlusion-resistant tracking\\rof wrist and finger motions based on SLAM and electromagnetic field together\\rwith 3D observations of the environment. Utilizing this rich dataset, DexIL\\remploys inverse kinematics and point cloud-based imitation learning to\\rreplicate human actions with robot hands. Beyond learning from human motion,\\rDexCap also offers an optional human-in-the-loop correction mechanism to refine\\rand further improve robot performance. Through extensive evaluation across six\\rdexterous manipulation tasks, our approach not only demonstrates superior\\rperformance but also showcases the system's capability to effectively learn\\rfrom in-the-wild mocap data, paving the way for future data collection methods\\rfor dexterous manipulation. More details can be found at\\rhttps://dex-cap.github.io\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07788 ,  17395kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07800 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 16:36:27 GMT   (1045kb,D)\\r\\rTitle: BraSyn 2023 challenge: Missing MRI synthesis and the effect of different\\r  learning objectives\\rAuthors: Ivo M. Baltruschat and Parvaneh Janbakhshi and Matthias Lenga\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  This work is addressing the Brain Magnetic Resonance Image Synthesis for\\rTumor Segmentation (BraSyn) challenge which was hosted as part of the Brain\\rTumor Segmentation challenge (BraTS) 2023. In this challenge researchers are\\rinvited to work on synthesizing a missing magnetic resonance image sequence\\rgiven other available sequences to facilitate tumor segmentation pipelines\\rtrained on complete sets of image sequences. This problem can be addressed\\rusing deep learning in the framework of paired images-to-image translation. In\\rthis work, we proposed to investigate the effectiveness of a commonly-used deep\\rlearning framework such as Pix2Pix trained under supervision of different\\rimage-quality loss functions. Our results indicate that using different loss\\rfunctions significantly affects the synthesis quality. We systematically study\\rthe impact of different loss functions in the multi-sequence MR image synthesis\\rsetting of the BraSyn challenge. Furthermore, we show how image synthesis\\rperformance can be optimized by beneficially combining different learning\\robjectives.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07800 ,  1045kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07834 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 17:17:20 GMT   (7128kb,D)\\r\\rTitle: When Eye-Tracking Meets Machine Learning: A Systematic Review on\\r  Applications in Medical Image Analysis\\rAuthors: Sahar Moradizeyveh, Mehnaz Tabassum, Sidong Liu, Robert Ahadizad\\r  Newport, Amin Beheshti, Antonio Di Ieva\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Eye-gaze tracking research offers significant promise in enhancing various\\rhealthcare-related tasks, above all in medical image analysis and\\rinterpretation. Eye tracking, a technology that monitors and records the\\rmovement of the eyes, provides valuable insights into human visual attention\\rpatterns. This technology can transform how healthcare professionals and\\rmedical specialists engage with and analyze diagnostic images, offering a more\\rinsightful and efficient approach to medical diagnostics. Hence, extracting\\rmeaningful features and insights from medical images by leveraging eye-gaze\\rdata improves our understanding of how radiologists and other medical experts\\rmonitor, interpret, and understand images for diagnostic purposes. Eye-tracking\\rdata, with intricate human visual attention patterns embedded, provides a\\rbridge to integrating artificial intelligence (AI) development and human\\rcognition. This integration allows novel methods to incorporate domain\\rknowledge into machine learning (ML) and deep learning (DL) approaches to\\renhance their alignment with human-like perception and decision-making.\\rMoreover, extensive collections of eye-tracking data have also enabled novel\\rML/DL methods to analyze human visual patterns, paving the way to a better\\runderstanding of human vision, attention, and cognition. This systematic review\\rinvestigates eye-gaze tracking applications and methodologies for enhancing\\rML/DL algorithms for medical image analysis in depth.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07834 ,  7128kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.07851 (*cross-listing*)\\rDate: Tue, 12 Mar 2024 17:43:20 GMT   (245kb,D)\\r\\rTitle: 12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning\\rAuthors: Yoga Esa Wibowo, Cristian Cioflan, Thorir Mar Ingolfsson, Michael\\r  Hersche, Leo Zhao, Abbas Rahimi, Luca Benini\\rCategories: cs.LG cs.CV\\rComments: 6 pages, 4 tables, 3 figures. Accepted at IEEE DATE 2024\\r\\\\\\\\\\r  Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems\\rto expand their inference capabilities to new classes using only a few labeled\\rexamples, without forgetting the previously learned classes. Classical\\rbackpropagation-based learning and its variants are often unsuitable for\\rbattery-powered, memory-constrained systems at the extreme edge. In this work,\\rwe introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a\\rlightweight model consisting of a pretrained and metalearned feature extractor\\rand an expandable explicit memory storing the class prototypes. The\\rarchitecture is pretrained with a novel feature orthogonality regularization\\rand metalearned with a multi-margin loss. For learning a new class, our\\rapproach extends the explicit memory with novel class prototypes, while the\\rremaining architecture is kept frozen. This allows learning previously unseen\\rclasses based on only a few examples with one single pass (hence online).\\rO-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,\\rachieving state-of-the-art results. Tailored for ultra-low-power platforms, we\\rimplement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online\\rlearning capabilities within just 12 mJ per new class.\\r\\\\\\\\ ( https://arxiv.org/abs/2403.07851 ,  245kb)\\r%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.04118\\rreplaced with revised version Tue, 12 Mar 2024 08:29:41 GMT   (1052kb,D)\\r\\rTitle: ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning\\rAuthors: Jinta Weng and Yifan Deng and d Donghao Li and Hao You and Yue Hu and\\r  Heyan Huang\\rCategories: cs.CL cs.AI\\rComments: 2 figures\\rJournal-ref: ICASSP2024\\r\\\\\\\\ ( https://arxiv.org/abs/2211.04118 ,  1052kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.07249\\rreplaced with revised version Tue, 12 Mar 2024 13:30:16 GMT   (9859kb,D)\\r\\rTitle: APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning\\rAuthors: Jiashuo Sun, Hang Zhang, Chen Lin, Xiangdong Su, Yeyun Gong, Jian Guo\\rCategories: cs.CL cs.LG\\rComments: Accepted by COLING 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2212.07249 ,  9859kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.16421\\rreplaced with revised version Tue, 12 Mar 2024 03:14:18 GMT   (210kb,D)\\r\\rTitle: ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of\\r  Commonsense Problem in Large Language Models\\rAuthors: Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, Ben He,\\r  Shanshan Jiang, Bin Dong\\rCategories: cs.CL\\rComments: Accepted by LREC-COLING 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2303.16421 ,  210kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.13040\\rreplaced with revised version Tue, 12 Mar 2024 08:52:02 GMT   (8341kb,D)\\r\\rTitle: SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented\\r  Dialogue Agents\\rAuthors: Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei\\r  Dai, Hangyu Li, Rui Yan, Fei Huang, Yongbin Li\\rCategories: cs.CL cs.AI\\rComments: NeurIPS 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2305.13040 ,  8341kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.08543\\rreplaced with revised version Tue, 12 Mar 2024 16:15:19 GMT   (309kb,D)\\r\\rTitle: Knowledge Distillation of Large Language Models\\rAuthors: Yuxian Gu, Li Dong, Furu Wei, Minlie Huang\\rCategories: cs.CL cs.AI\\rComments: Published as a conference paper in ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2306.08543 ,  309kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.06259\\rreplaced with revised version Tue, 12 Mar 2024 05:22:46 GMT   (2904kb,D)\\r\\rTitle: Self-Alignment with Instruction Backtranslation\\rAuthors: Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke\\r  Zettlemoyer, Jason Weston, Mike Lewis\\rCategories: cs.CL\\rComments: ICLR2024 camera ready\\r\\\\\\\\ ( https://arxiv.org/abs/2308.06259 ,  2904kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.00770\\rreplaced with revised version Tue, 12 Mar 2024 00:50:00 GMT   (826kb,D)\\r\\rTitle: Bias and Fairness in Large Language Models: A Survey\\rAuthors: Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim,\\r  Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed\\rCategories: cs.CL cs.AI cs.CY cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2309.00770 ,  826kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.10691\\rreplaced with revised version Tue, 12 Mar 2024 15:53:06 GMT   (1415kb,D)\\r\\rTitle: MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language\\r  Feedback\\rAuthors: Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao\\r  Peng, Heng Ji\\rCategories: cs.CL cs.AI cs.LG\\rComments: ICLR 2024. Code is available on our project website:\\r  https://xingyaoww.github.io/mint-bench\\r\\\\\\\\ ( https://arxiv.org/abs/2309.10691 ,  1415kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.11911\\rreplaced with revised version Tue, 12 Mar 2024 12:54:36 GMT   (1177kb,D)\\r\\rTitle: InstructERC: Reforming Emotion Recognition in Conversation with a\\r  Retrieval Multi-task LLMs Framework\\rAuthors: Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, Sirui Wang\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2309.11911 ,  1177kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.16319\\rreplaced with revised version Tue, 12 Mar 2024 03:43:57 GMT   (1553kb,D)\\r\\rTitle: Augmenting Transformers with Recursively Composed Multi-grained\\r  Representations\\rAuthors: Xiang Hu, Qingyang Zhu, Kewei Tu, Wei Wu\\rCategories: cs.CL cs.AI\\rComments: ICLR 2024 poster\\r\\\\\\\\ ( https://arxiv.org/abs/2309.16319 ,  1553kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.02129\\rreplaced with revised version Tue, 12 Mar 2024 16:58:53 GMT   (447kb,D)\\r\\rTitle: Unveiling the Pitfalls of Knowledge Editing for Large Language Models\\rAuthors: Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen\\rCategories: cs.CL cs.AI cs.CV cs.DB cs.LG\\rComments: ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.02129 ,  447kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.01070\\rreplaced with revised version Tue, 12 Mar 2024 14:50:30 GMT   (652kb,D)\\r\\rTitle: Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech\\r  Models via Language-Specific Experts\\rAuthors: Thomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, Vassilina\\r  Nikoulina\\rCategories: cs.CL cs.SD eess.AS\\rComments: Accepted to IEEE ICASSP 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.01070 ,  652kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.06602\\rreplaced with revised version Tue, 12 Mar 2024 16:54:57 GMT   (8825kb,D)\\r\\rTitle: BizBench: A Quantitative Reasoning Benchmark for Business and Finance\\rAuthors: Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy,\\r  Charles Lovering, Chris Tanner\\rCategories: cs.CL\\rComments: Work in progress\\r\\\\\\\\ ( https://arxiv.org/abs/2311.06602 ,  8825kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.11624\\rreplaced with revised version Tue, 12 Mar 2024 04:38:53 GMT   (183kb,D)\\r\\rTitle: In-context Learning with Retrieved Demonstrations for Language Models: A\\r  Survey\\rAuthors: Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi\\rCategories: cs.CL cs.AI cs.IR\\r\\\\\\\\ ( https://arxiv.org/abs/2401.11624 ,  183kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.11725\\rreplaced with revised version Tue, 12 Mar 2024 15:48:17 GMT   (2721kb,D)\\r\\rTitle: Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language\\r  Conversion for Language Models\\rAuthors: Yile Wang, Sijie Cheng, Zixin Sun, Peng Li, Yang Liu\\rCategories: cs.CL\\rComments: ICLR AGI Workshop 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2401.11725 ,  2721kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.13165\\rreplaced with revised version Tue, 12 Mar 2024 17:41:13 GMT   (282kb)\\r\\rTitle: Misgendering and Assuming Gender in Machine Translation when Working\\r  with Low-Resource Languages\\rAuthors: Sourojit Ghosh, Srishti Chatterjee\\rCategories: cs.CL\\rComments: Upcoming Publication, Gendered Technology in Translation and\\r  Interpreting Centering Rights in the Development of Language Technology,\\r  Routledge 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2401.13165 ,  282kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.00746\\rreplaced with revised version Tue, 12 Mar 2024 00:16:10 GMT   (1333kb,D)\\r\\rTitle: Health-LLM: Personalized Retrieval-Augmented Disease Prediction System\\rAuthors: Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Suiyuan Zhu, Mengnan Du,\\r  Yanda Meng, Yongfeng Zhang\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2402.00746 ,  1333kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.12243\\rreplaced with revised version Tue, 12 Mar 2024 13:52:13 GMT   (705kb,D)\\r\\rTitle: Understanding the Effects of Noise in Text-to-SQL: An Examination of the\\r  BIRD-Bench Benchmark\\rAuthors: Niklas Wretblad, Fredrik Gordh Riseby, Rahul Biswas, Amin Ahmadi,\\r  Oskar Holmstr\\\\om\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2402.12243 ,  705kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.19282\\rreplaced with revised version Tue, 12 Mar 2024 12:27:52 GMT   (107kb,D)\\r\\rTitle: WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset\\rAuthors: Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia\\r  Yu, ChaoBin Zhang, Zhenxiang Li, Pei Chu, Yuan Qu, Jin Shi, Lindong Lu, Runyu\\r  Peng, Zhiyuan Zeng, Huanze Tang, Zhikai Lei, Jiawei Hong, Keyu Chen, Zhaoye\\r  Fei, Ruiliang Xu, Wei Li, Zhongying Tu, Hang Yan and Conghui He\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2402.19282 ,  107kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01548\\rreplaced with revised version Tue, 12 Mar 2024 09:49:28 GMT   (1339kb,D)\\r\\rTitle: In-Context Sharpness as Alerts: An Inner Representation Perspective for\\r  Hallucination Mitigation\\rAuthors: Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang\\r  Gao, Junxian He\\rCategories: cs.CL cs.AI cs.LG\\rComments: code repo is available at:\\r  https://github.com/hkust-nlp/Activation_decoding.git\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01548 ,  1339kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01841\\rreplaced with revised version Tue, 12 Mar 2024 07:34:28 GMT   (1475kb,D)\\r\\rTitle: Making Pre-trained Language Models Great on Tabular Prediction\\rAuthors: Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Z. Chen, Jimeng\\r  Sun, Jian Wu, Jintai Chen\\rCategories: cs.CL cs.LG\\rComments: Accepted to ICLR 2024 as spotlight presentation (Notable Top 5%).\\r  OpenReview link is https://openreview.net/forum?id=anzIzGZuLi, codes will be\\r  available at https://github.com/jyansir/tp-berta\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01841 ,  1475kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.03102\\rreplaced with revised version Tue, 12 Mar 2024 05:33:16 GMT   (11338kb,D)\\r\\rTitle: In Dialogues We Learn: Towards Personalized Dialogue Without\\r  Pre-defined Profiles through In-Dialogue Learning\\rAuthors: Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zhengtao Yu,\\r  Rui Yan\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2403.03102 ,  11338kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.05326\\rreplaced with revised version Tue, 12 Mar 2024 12:12:36 GMT   (1927kb,D)\\r\\rTitle: ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in\\r  Dialogues\\rAuthors: Yiding Liu and Jingjing Wang and Jiamin Luo and Tao Zeng and Guodong\\r  Zhou\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2403.05326 ,  1927kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06412\\rreplaced with revised version Tue, 12 Mar 2024 10:33:06 GMT   (8186kb,D)\\r\\rTitle: CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in\\r  Korean\\rAuthors: Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, Alice\\r  Oh\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06412 ,  8186kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06914\\rreplaced with revised version Tue, 12 Mar 2024 15:52:14 GMT   (339kb,D)\\r\\rTitle: MEND: Meta dEmonstratioN Distillation for Efficient and Effective\\r  In-Context Learning\\rAuthors: Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo\\rCategories: cs.CL cs.AI\\rComments: ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06914 ,  339kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:1806.10866\\rreplaced with revised version Tue, 12 Mar 2024 14:21:12 GMT   (66kb,D)\\r\\rTitle: Exploring Architectures for CNN-Based Word Spotting\\rAuthors: Eugen Rusakov, Sebastian Sudholt, Fabian Wolf, Gernot A. Fink\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/1806.10866 ,  66kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2006.15524\\rreplaced with revised version Tue, 12 Mar 2024 12:53:14 GMT   (1823kb,D)\\r\\rTitle: MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot\\r  Class-Incremental Learning\\rAuthors: Hanbin Zhao, Yongjian Fu, Mintong Kang, Qi Tian, Fei Wu, Xi Li\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2006.15524 ,  1823kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2007.07924\\rreplaced with revised version Tue, 27 Feb 2024 08:20:57 GMT   (10643kb,D)\\r\\rTitle: Tracking Passengers and Baggage Items Using Multiple Overhead Cameras at\\r  Security Checkpoints\\rAuthors: Abubakar Siddique and Henry Medeiros\\rCategories: cs.CV\\rComments: 16 pages, 16 figures\\rJournal-ref: IEEE Transactions on Systems, Man, and Cybernetics: Systems (\\r  Volume: 53, Issue: 6, June 2023)\\rDOI: 10.1109/TSMC.2022.3225252\\r\\\\\\\\ ( https://arxiv.org/abs/2007.07924 ,  10643kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2101.09193\\rreplaced with revised version Tue, 12 Mar 2024 09:22:32 GMT   (4791kb,D)\\r\\rTitle: Dense outlier detection and open-set recognition based on training with\\r  noisy negative images\\rAuthors: Petra Bevandi\\\\'c, Ivan Kre\\\\v{s}o, Marin Or\\\\v{s}i\\\\'c, Sini\\\\v{s}a\\r  \\\\v{S}egvi\\\\'c\\rCategories: cs.CV\\rComments: Published in Image and Vision Computing\\rJournal-ref: Image and Vision Computing, Vol. 124, 2022, 104490\\rDOI: 10.1016/j.imavis.2022.104490\\r\\\\\\\\ ( https://arxiv.org/abs/2101.09193 ,  4791kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2108.00408\\rreplaced with revised version Mon, 11 Mar 2024 23:20:07 GMT   (785kb)\\r\\rTitle: CSC-Unet: A Novel Convolutional Sparse Coding Strategy Based Neural\\r  Network for Semantic Segmentation\\rAuthors: Haitong Tang, Shuang He, Mengduo Yang, Xia Lu, Qin Yu, Kaiyue Liu,\\r  Hongjie Yan and Nizhuan Wang\\rCategories: cs.CV cs.AI cs.LG\\rJournal-ref: IEEE Access,2024\\rDOI: 10.1109/ACCESS.2024.3373619\\r\\\\\\\\ ( https://arxiv.org/abs/2108.00408 ,  785kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2112.12989\\rreplaced with revised version Tue, 12 Mar 2024 14:47:47 GMT   (16856kb,D)\\r\\rTitle: Domain-Aware Continual Zero-Shot Learning\\rAuthors: Kai Yi, Paul Janson, Wenxuan Zhang, Mohamed Elhoseiny\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2112.12989 ,  16856kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2204.09957\\rreplaced with revised version Tue, 12 Mar 2024 08:13:27 GMT   (7003kb,D)\\r\\rTitle: Self-paced Multi-grained Cross-modal Interaction Modeling for Referring\\r  Expression Comprehension\\rAuthors: Peihan Miao, Wei Su, Gaoang Wang, Xuewei Li, Xi Li\\rCategories: cs.CV\\rComments: Accepted by TIP\\r\\\\\\\\ ( https://arxiv.org/abs/2204.09957 ,  7003kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2205.10766\\rreplaced with revised version Tue, 12 Mar 2024 16:29:18 GMT   (10286kb,D)\\r\\rTitle: Recent Advances in Embedding Methods for Multi-Object Tracking: A Survey\\rAuthors: Gaoang Wang, Mingli Song, Jenq-Neng Hwang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2205.10766 ,  10286kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.09292\\rreplaced with revised version Tue, 12 Mar 2024 02:08:37 GMT   (3889kb,D)\\r\\rTitle: Efficient Diffusion Models for Vision: A Survey\\rAuthors: Anwaar Ulhaq and Naveed Akhtar\\rCategories: cs.CV cs.AI\\rComments: 14 Pages, 5 Figures (in progress)\\r\\\\\\\\ ( https://arxiv.org/abs/2210.09292 ,  3889kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.10969\\rreplaced with revised version Tue, 12 Mar 2024 11:59:39 GMT   (759kb,D)\\r\\rTitle: SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic\\r  Retinopathy Grading\\rAuthors: Yijin Huang, Junyan Lyu, Pujin Cheng, Roger Tam, Xiaoying Tang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2210.10969 ,  759kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.02695\\rreplaced with revised version Tue, 12 Mar 2024 15:12:55 GMT   (1757kb,D)\\r\\rTitle: WaveNets: Wavelet Channel Attention Networks\\rAuthors: Hadi Salman, Caleb Parks, Shi Yin Hong, Justin Zhan\\rCategories: cs.CV cs.AI\\rComments: IEEE BigData2022 conference\\rDOI: 10.1109/BigData55660.2022.10020665\\r\\\\\\\\ ( https://arxiv.org/abs/2211.02695 ,  1757kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.10340\\rreplaced with revised version Tue, 12 Mar 2024 09:53:46 GMT   (17377kb,D)\\r\\rTitle: Weakly supervised training of universal visual concepts for multi-domain\\r  semantic segmentation\\rAuthors: Petra Bevandi\\\\'c, Marin Or\\\\v{s}i\\\\'c, Ivan Grubi\\\\v{s}i\\\\'c, Josip\\r  \\\\v{S}ari\\\\'c, Sini\\\\v{s}a \\\\v{S}egvi\\\\'c\\rCategories: cs.CV\\rComments: 27 pages, 16 figures, 10 tables, accepted to International Journal of\\r  Computer Vision\\rJournal-ref: International Journal of Computer Vision, 2024, 1-23\\rDOI: 10.1007/s11263-024-01986-z\\r\\\\\\\\ ( https://arxiv.org/abs/2212.10340 ,  17377kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.01060\\rreplaced with revised version Tue, 12 Mar 2024 06:55:20 GMT   (1910kb,D)\\r\\rTitle: Knowledge-guided Causal Intervention for Weakly-supervised Object\\r  Localization\\rAuthors: Feifei Shao, Yawei Luo, Fei Gao, Yi Yang, Jun Xiao\\rCategories: cs.CV\\rComments: 13 pages, 7 figures, 7 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2301.01060 ,  1910kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.14207\\rreplaced with revised version Tue, 12 Mar 2024 15:40:08 GMT   (17037kb,D)\\r\\rTitle: DiffuScene: Denoising Diffusion Models for Generative Indoor Scene\\r  Synthesis\\rAuthors: Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies,\\r  Matthias Nie{\\\\ss}ner\\rCategories: cs.CV\\rComments: CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2303.14207 ,  17037kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.17189\\rreplaced with revised version Tue, 12 Mar 2024 13:15:24 GMT   (12510kb,D)\\r\\rTitle: LayoutDiffusion: Controllable Diffusion Model for Layout-to-image\\r  Generation\\rAuthors: Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, Xi\\r  Li\\rCategories: cs.CV\\rComments: Accepted by CVPR2023\\r\\\\\\\\ ( https://arxiv.org/abs/2303.17189 ,  12510kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.12522\\rreplaced with revised version Mon, 11 Mar 2024 18:26:10 GMT   (42256kb,D)\\r\\rTitle: P-NOC: adversarial training of CAM generating networks for robust weakly\\r  supervised semantic segmentation priors\\rAuthors: Lucas David, Helio Pedrini, and Zanoni Dias\\rCategories: cs.CV cs.LG\\rComments: 19 pages, 10 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2305.12522 ,  42256kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.14527\\rreplaced with revised version Tue, 12 Mar 2024 14:45:02 GMT   (924kb,D)\\r\\rTitle: Slovo: Russian Sign Language Dataset\\rAuthors: Alexander Kapitanov, Karina Kvanchiani, Alexander Nagaev, Elizaveta\\r  Petrova\\rCategories: cs.CV\\rComments: russian sign language recognition dataset, open-source, 11 pages\\rDOI: 10.1007/978-3-031-44137-0_6\\r\\\\\\\\ ( https://arxiv.org/abs/2305.14527 ,  924kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.03403\\rreplaced with revised version Tue, 12 Mar 2024 12:41:04 GMT   (19721kb,D)\\r\\rTitle: SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic\\r  Segmentation\\rAuthors: Xuewei Li, Tao Wu, Zhongang Qi, Gaoang Wang, Ying Shan, Xi Li\\rCategories: cs.CV cs.AI cs.LG cs.MM\\rComments: Accepted by IJCAI 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2306.03403 ,  19721kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.06663\\rreplaced with revised version Tue, 12 Mar 2024 02:52:35 GMT   (25210kb,D)\\r\\rTitle: LF-PGVIO: A Visual-Inertial-Odometry Framework for Large Field-of-View\\r  Cameras using Points and Geodesic Segments\\rAuthors: Ze Wang, Kailun Yang, Hao Shi, Yufan Zhang, Zhijie Xu, Fei Gao, Kaiwei\\r  Wang\\rCategories: cs.CV cs.RO eess.IV\\rComments: Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The\\r  source code will be made publicly available at\\r  https://github.com/flysoaryun/LF-PGVIO\\r\\\\\\\\ ( https://arxiv.org/abs/2306.06663 ,  25210kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.13325\\rreplaced with revised version Tue, 12 Mar 2024 12:21:18 GMT   (33257kb,D)\\r\\rTitle: Differentiable Display Photometric Stereo\\rAuthors: Seokjun Choi, Seungwoo Yoon, Giljoo Nam, Seungyong Lee, Seung-Hwan\\r  Baek\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.13325 ,  33257kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.09696\\rreplaced with revised version Tue, 12 Mar 2024 15:29:56 GMT   (15057kb,D)\\r\\rTitle: Towards Saner Deep Image Registration\\rAuthors: Bin Duan and Ming Zhong and Yan Yan\\rCategories: cs.CV\\rComments: ICCV 2023, fix typos\\r\\\\\\\\ ( https://arxiv.org/abs/2307.09696 ,  15057kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.16140\\rreplaced with revised version Tue, 12 Mar 2024 07:23:51 GMT   (17457kb,D)\\r\\rTitle: Fully $1\\\\times1$ Convolutional Network for Lightweight Image\\r  Super-Resolution\\rAuthors: Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu\\rCategories: cs.CV cs.AI\\rComments: Accepted by Machine Intelligence Research, DOI:\\r  10.1007/s11633-024-1401-z\\rDOI: 10.1007/s11633-024-1401-z\\r\\\\\\\\ ( https://arxiv.org/abs/2307.16140 ,  17457kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.00759\\rreplaced with revised version Tue, 12 Mar 2024 12:58:06 GMT   (48423kb,D)\\r\\rTitle: Decomposition Ascribed Synergistic Learning for Unified Image\\r  Restoration\\rAuthors: Jinghao Zhang, Feng Zhao\\rCategories: cs.CV eess.IV\\rComments: 16 pages, 17 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2308.00759 ,  48423kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.05525\\rreplaced with revised version Tue, 12 Mar 2024 15:05:23 GMT   (4713kb,D)\\r\\rTitle: Robustifying Point Cloud Networks by Refocusing\\rAuthors: Meir Yossef Levi, Guy Gilboa\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2308.05525 ,  4713kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.07652\\rreplaced with revised version Tue, 12 Mar 2024 17:41:56 GMT   (4184kb,D)\\r\\rTitle: Geometry of the Visual Cortex with Applications to Image Inpainting and\\r  Enhancement\\rAuthors: Francesco Ballerin and Erlend Grong\\rCategories: cs.CV\\rComments: Associated python package available at\\r  https://github.com/ballerin/v1diffusion\\r\\\\\\\\ ( https://arxiv.org/abs/2308.07652 ,  4184kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.14286\\rreplaced with revised version Tue, 12 Mar 2024 09:29:51 GMT   (2199kb,D)\\r\\rTitle: Bridging Cross-task Protocol Inconsistency for Distillation in Dense\\r  Object Detection\\rAuthors: Longrong Yang, Xianpan Zhou, Xuewei Li, Liang Qiao, Zheyang Li, Ziwei\\r  Yang, Gaoang Wang, Xi Li\\rCategories: cs.CV\\rComments: Accepted by ICCV 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2308.14286 ,  2199kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.01786\\rreplaced with revised version Mon, 11 Mar 2024 20:33:12 GMT   (2771kb,D)\\r\\rTitle: Safe and Robust Watermark Injection with a Single OoD Image\\rAuthors: Shuyang Yu, Junyuan Hong, Haobo Zhang, Haotao Wang, Zhangyang Wang and\\r  Jiayu Zhou\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.01786 ,  2771kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.02001\\rreplaced with revised version Mon, 11 Mar 2024 18:31:09 GMT   (52kb,D)\\r\\rTitle: Analyzing domain shift when using additional data for the MICCAI KiTS23\\r  Challenge\\rAuthors: George Stoica, Mihaela Breaban and Vlad Barbu\\rCategories: cs.CV cs.AI cs.LG\\rComments: This preprint has not undergone peer review or any post-submission\\r  improvements or corrections. The Version of Record of this contribution is\\r  published in https://link.springer.com/book/10.1007/978-3-031-54806-2, and is\\r  available online at https://doi.org/10.1007/978-3-031-54806-2_4\\rJournal-ref: Kidney and Kidney Tumor Segmentation. KiTS 2023. Lecture Notes in\\r  Computer Science, vol 14540\\rDOI: 10.1007/978-3-031-54806-2_4\\r\\\\\\\\ ( https://arxiv.org/abs/2309.02001 ,  52kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.02031\\rreplaced with revised version Tue, 12 Mar 2024 10:33:20 GMT   (1462kb,D)\\r\\rTitle: A survey on efficient vision transformers: algorithms, techniques, and\\r  performance benchmarking\\rAuthors: Lorenzo Papa, Paolo Russo, Irene Amerini, and Luping Zhou\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.02031 ,  1462kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.06030\\rreplaced with revised version Tue, 12 Mar 2024 00:32:08 GMT   (2997kb,D)\\r\\rTitle: Federated Learning for Large-Scale Scene Modeling with Neural Radiance\\r  Fields\\rAuthors: Teppei Suzuki\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.06030 ,  2997kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.07537\\rreplaced with revised version Tue, 12 Mar 2024 10:46:33 GMT   (1925kb)\\r\\rTitle: Towards a universal mechanism for successful deep learning\\rAuthors: Yuval Meir, Yarden Tzach, Shiri Hodassman, Ofek Tevet and Ido Kanter\\rCategories: cs.CV\\rComments: 31 pages,7 figures, 9 tables. arXiv admin note: text overlap with\\r  arXiv:2305.18078\\rJournal-ref: Scientific Reports volume 14, Article number: 5881 (2024)\\rDOI: 10.1038/s41598-024-56609-x\\r\\\\\\\\ ( https://arxiv.org/abs/2309.07537 ,  1925kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.11525\\rreplaced with revised version Tue, 12 Mar 2024 03:48:13 GMT   (5794kb,D)\\r\\rTitle: Light Field Diffusion for Single-View Novel View Synthesis\\rAuthors: Yifeng Xiong, Haoyu Ma, Shanlin Sun, Kun Han, Hao Tang, Xiaohui Xie\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.11525 ,  5794kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.14865\\rreplaced with revised version Tue, 12 Mar 2024 17:45:45 GMT   (11248kb,D)\\r\\rTitle: Unsupervised Multi-Person 3D Human Pose Estimation From 2D Poses Alone\\rAuthors: Peter Hardy and Hansung Kim\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2309.14865 ,  11248kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.17336\\rreplaced with revised version Tue, 12 Mar 2024 09:24:29 GMT   (21777kb,D)\\r\\rTitle: Robust 3D Object Detection from LiDAR-Radar Point Clouds via Cross-Modal\\r  Feature Augmentation\\rAuthors: Jianning Deng, Gabriel Chan, Hantao Zhong, and Chris Xiaoxuan Lu\\rCategories: cs.CV cs.RO\\rComments: Accepted to ICRA 2024. 8 pages, 4 figures. Equal contribution for\\r  Gabriel Chan and Hantao Zhong, listed randomly\\r\\\\\\\\ ( https://arxiv.org/abs/2309.17336 ,  21777kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.18511\\rreplaced with revised version Tue, 12 Mar 2024 11:52:42 GMT   (23149kb,D)\\r\\rTitle: 3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for\\r  Compositional Recognition\\rAuthors: Habib Slim, Xiang Li, Yuchen Li, Mahmoud Ahmed, Mohamed Ayman, Ujjwal\\r  Upadhyay, Ahmed Abdelreheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka,\\r  Mohamed Elhoseiny\\rCategories: cs.CV cs.AI\\rComments: https://3dcompat-dataset.org/v2/\\r\\\\\\\\ ( https://arxiv.org/abs/2310.18511 ,  23149kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.00949\\rreplaced with revised version Tue, 12 Mar 2024 02:19:00 GMT   (26846kb,D)\\r\\rTitle: POS: A Prompts Optimization Suite for Augmenting Text-to-Video\\r  Generation\\rAuthors: Shijie Ma, Huayi Xu, Mengjian Li, Weidong Geng, Meng Wang, Yaxiong\\r  Wang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2311.00949 ,  26846kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08359\\rreplaced with revised version Tue, 12 Mar 2024 16:40:28 GMT   (30420kb,D)\\r\\rTitle: Rotation-Agnostic Image Representation Learning for Digital Pathology\\rAuthors: Saghir Alfasly, Abubakr Shafique, Peyman Nejat, Jibran Khan, Areej\\r  Alsaafin, Ghazal Alabtah, H.R. Tizhoosh\\rCategories: cs.CV\\rComments: CVPR 2024 - 23 pages, 10 figures, and 18 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08359 ,  30420kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.10356\\rreplaced with revised version Mon, 11 Mar 2024 19:27:35 GMT   (3525kb,D)\\r\\rTitle: Garment Recovery with Shape and Deformation Priors\\rAuthors: Ren Li, Corentin Dumery, Beno\\\\^it Guillard, Pascal Fua\\rCategories: cs.CV\\rComments: CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.10356 ,  3525kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.10601\\rreplaced with revised version Tue, 12 Mar 2024 13:06:14 GMT   (13374kb,D)\\r\\rTitle: Multimodal Indoor Localization Using Crowdsourced Radio Maps\\rAuthors: Zhaoguang Yi, Xiangyu Wen, Qiyue Xia, Peize Li, Francisco Zampella,\\r  Firas Alsehly, Chris Xiaoxuan Lu\\rCategories: cs.CV eess.SP\\rComments: 7 pages, 4 figures; ICRA'24 https://youtu.be/NTTKwJBFN5w\\r\\\\\\\\ ( https://arxiv.org/abs/2311.10601 ,  13374kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.11845\\rreplaced with revised version Tue, 12 Mar 2024 10:57:53 GMT   (11771kb,D)\\r\\rTitle: Entangled View-Epipolar Information Aggregation for Generalizable Neural\\r  Radiance Fields\\rAuthors: Zhiyuan Min, Yawei Luo, Wei Yang, Yuesong Wang, Yi Yang\\rCategories: cs.CV\\rComments: Accepted by CVPR-2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.11845 ,  11771kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.14282\\rreplaced with revised version Tue, 12 Mar 2024 12:14:51 GMT   (17868kb,D)\\r\\rTitle: Image Super-Resolution with Text Prompt Diffusion\\rAuthors: Zheng Chen, Yulun Zhang, Jinjin Gu, Xin Yuan, Linghe Kong, Guihai\\r  Chen, Xiaokang Yang\\rCategories: cs.CV\\rComments: Code is available at https://github.com/zhengchen1999/PromptSR\\r\\\\\\\\ ( https://arxiv.org/abs/2311.14282 ,  17868kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.15637\\rreplaced with revised version Tue, 12 Mar 2024 09:34:53 GMT   (4809kb,D)\\r\\rTitle: Neural 3D Strokes: Creating Stylized 3D Scenes with Vectorized 3D\\r  Strokes\\rAuthors: Hao-Bin Duan, Miao Wang, Yan-Xun Li and Yong-Liang Yang\\rCategories: cs.CV cs.GR\\rComments: Accepted to CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.15637 ,  4809kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.15732\\rreplaced with revised version Tue, 12 Mar 2024 01:07:14 GMT   (6502kb,D)\\r\\rTitle: GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?\\rAuthors: Wenhao Wu, Huanjin Yao, Mengxi Zhang, Yuxin Song, Wanli Ouyang,\\r  Jingdong Wang\\rCategories: cs.CV\\rComments: Technical report. Retest GPT-4V and update results\\r\\\\\\\\ ( https://arxiv.org/abs/2311.15732 ,  6502kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.16480\\rreplaced with revised version Tue, 12 Mar 2024 12:07:39 GMT   (2749kb,D)\\r\\rTitle: WsiCaption: Multiple Instance Generation of Pathology Reports for\\r  Gigapixel Whole-Slide Images\\rAuthors: Pingyi Chen, Honglin Li, Chenglu Zhu, Sunyi Zheng, Zhongyi Shui, Lin\\r  Yang\\rCategories: cs.CV cs.AI cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2311.16480 ,  2749kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.16510\\rreplaced with revised version Tue, 12 Mar 2024 14:41:31 GMT   (8205kb,D)\\r\\rTitle: Source-Free Domain Adaptation with Frozen Multimodal Foundation Model\\rAuthors: Song Tang, Wenxin Su, Mao Ye, and Xiatian Zhu\\rCategories: cs.CV\\rComments: Accepted at CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.16510 ,  8205kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.17600\\rreplaced with revised version Tue, 12 Mar 2024 04:27:19 GMT   (18224kb,D)\\r\\rTitle: MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large\\r  Language Models\\rAuthors: Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, Yu Qiao\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2311.17600 ,  18224kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.17911\\rreplaced with revised version Tue, 12 Mar 2024 05:59:46 GMT   (1840kb,D)\\r\\rTitle: OPERA: Alleviating Hallucination in Multi-Modal Large Language Models\\r  via Over-Trust Penalty and Retrospection-Allocation\\rAuthors: Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi\\r  Wang, Dahua Lin, Weiming Zhang, Nenghai Yu\\rCategories: cs.CV\\rComments: CVPR 2024, code is available at https://github.com/shikiw/OPERA\\r\\\\\\\\ ( https://arxiv.org/abs/2311.17911 ,  1840kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.00414\\rreplaced with revised version Tue, 12 Mar 2024 02:39:23 GMT   (13784kb,D)\\r\\rTitle: Vision-Language Models Learn Super Images for Efficient Partially\\r  Relevant Video Retrieval\\rAuthors: Taichi Nishimura and Shota Nakada and Masayoshi Kondo\\rCategories: cs.CV cs.MM\\rComments: 24 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2312.00414 ,  13784kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.02142\\rreplaced with revised version Tue, 12 Mar 2024 14:39:31 GMT   (26061kb,D)\\r\\rTitle: Object Recognition as Next Token Prediction\\rAuthors: Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein,\\r  Ser-Nam Lim\\rCategories: cs.CV\\rComments: CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.02142 ,  26061kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.03781\\rreplaced with revised version Tue, 12 Mar 2024 08:13:01 GMT   (41069kb,D)\\r\\rTitle: Lite-Mind: Towards Efficient and Robust Brain Representation Network\\rAuthors: Zixuan Gong, Qi Zhang, Duoqian Miao, Guangyin Bao, Liang Hu\\rCategories: cs.CV cs.AI\\rComments: 17 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2312.03781 ,  41069kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.06439\\rreplaced with revised version Tue, 12 Mar 2024 09:47:06 GMT   (13986kb,D)\\r\\rTitle: DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior\\rAuthors: Tianyu Huang, Yihan Zeng, Zhilu Zhang, Wan Xu, Hang Xu, Songcen Xu,\\r  Rynson W. H. Lau, Wangmeng Zuo\\rCategories: cs.CV\\rComments: Accepted by CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.06439 ,  13986kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11570\\rreplaced with revised version Tue, 12 Mar 2024 01:19:05 GMT   (5128kb,D)\\r\\rTitle: Understanding the Multi-modal Prompts of the Pre-trained Vision-Language\\r  Model\\rAuthors: Shuailei Ma, Chen-Wei Xie, Ying Wei, Siyang Sun, Jiaqi Fan, Xiaoyi\\r  Bao, Yuxin Guo, Yun Zheng\\rCategories: cs.CV\\rComments: We find that the statistical information in Figure 2 neglect the\\r  statistics for tSOS, so we make corrections. Additionally, we change the\\r  statistical samples to those where CLIP misidentify, but prompt tuning\\r  identify correctly. At the same time, we also revise some of the\\r  descriptions. The changes to the supplementary materials will be updated\\r  shortly. arXiv admin note: text overlap with arXiv:2307.06948 by other\\r  authors\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11570 ,  5128kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12028\\rreplaced with revised version Mon, 11 Mar 2024 20:29:50 GMT   (15472kb,D)\\r\\rTitle: EyePreserve: Identity-Preserving Iris Synthesis\\rAuthors: Siamul Karim Khan, Patrick Tinsley, Mahsa Mitcheff, Patrick Flynn,\\r  Kevin W. Bowyer, Adam Czajka\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12028 ,  15472kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.16145\\rreplaced with revised version Mon, 11 Mar 2024 18:13:54 GMT   (48209kb,D)\\r\\rTitle: One-Dimensional Adapter to Rule Them All: Concepts, Diffusion Models and\\r  Erasing Applications\\rAuthors: Mengyao Lyu, Yuhong Yang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He,\\r  Hui Xue, Jungong Han, Guiguang Ding\\rCategories: cs.CV cs.AI cs.LG\\rComments: CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.16145 ,  48209kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.04578\\rreplaced with revised version Tue, 12 Mar 2024 10:35:56 GMT   (11209kb,D)\\r\\rTitle: Effective pruning of web-scale datasets based on complexity of concept\\r  clusters\\rAuthors: Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika\\r  Chaudhuri, Ari S. Morcos\\rCategories: cs.CV\\rComments: Accepted at ICLR 2024, code available at\\r  https://github.com/amro-kamal/effective_pruning\\r\\\\\\\\ ( https://arxiv.org/abs/2401.04578 ,  11209kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.07629\\rreplaced with revised version Tue, 12 Mar 2024 04:09:02 GMT   (16107kb,D)\\r\\rTitle: Fine-Grained Prototypes Distillation for Few-Shot Object Detection\\rAuthors: Zichen Wang, Bo Yang, Haonan Yue, Zhenghao Ma\\rCategories: cs.CV\\rComments: Accepted by AAAI2024\\r\\\\\\\\ ( https://arxiv.org/abs/2401.07629 ,  16107kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.14168\\rreplaced with revised version Tue, 12 Mar 2024 14:45:49 GMT   (2703kb,D)\\r\\rTitle: Vivim: a Video Vision Mamba for Medical Video Object Segmentation\\rAuthors: Yijun Yang, Zhaohu Xing, Chunwang Huang, Lei Zhu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2401.14168 ,  2703kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.15886\\rreplaced with revised version Tue, 12 Mar 2024 04:15:35 GMT   (5841kb,D)\\r\\rTitle: Grey Level Texture Features for Segmentation of Chromogenic Dye RNAscope\\r  From Breast Cancer Tissue\\rAuthors: Andrew Davidson (1), Arthur Morley-Bunker (2), George Wiggins (2),\\r  Logan Walker (2), Gavin Harris (3), Ramakrishnan Mukundan (1), kConFab\\r  Investigators (4 and 5) ((1) University of Canterbury, (2) University of\\r  Otago, (3) Canterbury Health Laboratories, (4) The University of Melbourne,\\r  (5) Peter MacCallum Cancer Center)\\rCategories: cs.CV\\rComments: This preprint has not undergone peer review (when applicable) or any\\r  post-submission improvements or corrections. The Version of Record of this\\r  contribution is published in Proceedings of 2023 International Conference on\\r  Medical Imaging and Computer-Aided Diagnosis (MICAD 2023), and is available\\r  online at https://doi.org/10.1007/978-981-97-1335-6_7\\rDOI: 10.1007/978-981-97-1335-6_7\\r\\\\\\\\ ( https://arxiv.org/abs/2401.15886 ,  5841kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.01313\\rreplaced with revised version Tue, 12 Mar 2024 10:35:20 GMT   (2038kb,D)\\r\\rTitle: AutoGCN -- Towards Generic Human Activity Recognition with Neural\\r  Architecture Search\\rAuthors: Felix Tempel, Inga Str\\\\umke and Espen Alexander F. Ihlen\\rCategories: cs.CV\\rDOI: 10.1109/ACCESS.2024.3377103\\r\\\\\\\\ ( https://arxiv.org/abs/2402.01313 ,  2038kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.03227\\rreplaced with revised version Tue, 12 Mar 2024 11:28:20 GMT   (3069kb,D)\\r\\rTitle: IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of\\r  brain MR images\\rAuthors: Vincent Roca, Gr\\\\'egory Kuchcinski, Jean-Pierre Pruvo, Dorian\\r  Manouvriez, Renaud Lopes\\rCategories: cs.CV cs.AI cs.LG\\rComments: 23 pages, 8 figures; typos corrected\\r\\\\\\\\ ( https://arxiv.org/abs/2402.03227 ,  3069kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.03634\\rreplaced with revised version Tue, 12 Mar 2024 07:38:34 GMT   (9970kb,D)\\r\\rTitle: Ray Denoising: Depth-aware Hard Negative Sampling for Multi-view 3D\\r  Object Detection\\rAuthors: Feng Liu, Tengteng Huang, Qianjing Zhang, Haotian Yao, Chi Zhang, Fang\\r  Wan, Qixiang Ye, Yanzhao Zhou\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2402.03634 ,  9970kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.06809\\rreplaced with revised version Tue, 12 Mar 2024 00:59:34 GMT   (3759kb,D)\\r\\rTitle: Domain Adaptation Using Pseudo Labels\\rAuthors: Sachin Chhabra, Hemanth Venkateswara and Baoxin Li\\rCategories: cs.CV\\rComments: 8 pages + 3 pages of references\\r\\\\\\\\ ( https://arxiv.org/abs/2402.06809 ,  3759kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.08875\\rreplaced with revised version Mon, 11 Mar 2024 23:42:13 GMT   (3452kb,D)\\r\\rTitle: TikTokActions: A TikTok-Derived Video Dataset for Human Action\\r  Recognition\\rAuthors: Yang Qian, Yinan Sun, Ali Kargarandehkordi, Onur Cezmi Mutlu,\\r  Saimourya Surabhi, Pingyi Chen, Zain Jabbar, Dennis Paul Wall, Peter\\r  Washington\\rCategories: cs.CV\\rComments: 10 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2402.08875 ,  3452kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.09786\\rreplaced with revised version Tue, 12 Mar 2024 13:36:23 GMT   (29299kb,D)\\r\\rTitle: Examining Pathological Bias in a Generative Adversarial Network\\r  Discriminator: A Case Study on a StyleGAN3 Model\\rAuthors: Alvin Grissom II, Ryan F. Lei, Matt Gusdorff, Jeova Farias Sales Rocha\\r  Neto, Bailey Lin, Ryan Trotter\\rCategories: cs.CV cs.AI cs.CY cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2402.09786 ,  29299kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.13254\\rreplaced with revised version Tue, 12 Mar 2024 17:59:56 GMT   (10188kb,D)\\r\\rTitle: CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\\r  Compositional Reasoning via Counterfactual Examples\\rAuthors: Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee\\rCategories: cs.CV cs.AI cs.CL cs.LG\\rComments: 13 pages, 6 figures, 8 tables, Project Page:\\r  https://countercurate.github.io/\\r\\\\\\\\ ( https://arxiv.org/abs/2402.13254 ,  10188kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.16315\\rreplaced with revised version Mon, 11 Mar 2024 18:49:40 GMT   (28563kb,D)\\r\\rTitle: Finer: Investigating and Enhancing Fine-Grained Visual Concept\\r  Recognition in Large Vision Language Models\\rAuthors: Jeonghwan Kim and Heng Ji\\rCategories: cs.CV cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2402.16315 ,  28563kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.18817\\rreplaced with revised version Tue, 12 Mar 2024 01:54:21 GMT   (7624kb,D)\\r\\rTitle: Gradient Alignment for Cross-Domain Face Anti-Spoofing\\rAuthors: Binh M. Le, Simon S. Woo\\rCategories: cs.CV\\rJournal-ref: The IEEE/CVF Conference on Computer Vision and Pattern Recognition\\r  (CVPR) 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2402.18817 ,  7624kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.19344\\rreplaced with revised version Tue, 12 Mar 2024 16:49:56 GMT   (194kb,D)\\r\\rTitle: The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition\\rAuthors: Dimitrios Kollias and Panagiotis Tzirakis and Alan Cowen and Stefanos\\r  Zafeiriou and Irene Kotsia and Alice Baird and Chris Gagne and Chunchang Shao\\r  and Guanyu Hu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2402.19344 ,  194kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.01932\\rreplaced with revised version Tue, 12 Mar 2024 01:12:56 GMT   (0kb,I)\\r\\rTitle: Tree Counting by Bridging 3D Point Clouds with Imagery\\rAuthors: Lei Li, Tianfang Zhang, Zhongyu Jiang, Cheng-Yen Yang, Jenq-Neng\\r  Hwang, Stefan Oehmcke, Dimitri Pierre Johannes Gominski, Fabian Gieseke,\\r  Christian Igel\\rCategories: cs.CV\\rComments: need more experiments\\r\\\\\\\\ ( https://arxiv.org/abs/2403.01932 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.04765\\rreplaced with revised version Mon, 11 Mar 2024 23:42:14 GMT   (6220kb,D)\\r\\rTitle: Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like\\r  Speed\\rAuthors: Yifan Wang, Xingyi He, Sida Peng, Dongli Tan, Xiaowei Zhou\\rCategories: cs.CV\\rComments: CVPR 2024; Project page: https://zju3dv.github.io/efficientloftr\\r\\\\\\\\ ( https://arxiv.org/abs/2403.04765 ,  6220kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.05369\\rreplaced with revised version Tue, 12 Mar 2024 08:33:51 GMT   (22645kb,D)\\r\\rTitle: Frequency-Adaptive Dilated Convolution for Semantic Segmentation\\rAuthors: Linwei Chen, Lin Gu, Ying Fu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2403.05369 ,  22645kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.05854\\rreplaced with revised version Tue, 12 Mar 2024 16:26:39 GMT   (15414kb,D)\\r\\rTitle: LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content\\rAuthors: Qihao Zhao, Yalun Dai, Hao Li, Wei Hu, Fan Zhang, Jun Liu\\rCategories: cs.CV\\rComments: CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2403.05854 ,  15414kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.05930\\rreplaced with revised version Tue, 12 Mar 2024 14:15:50 GMT   (1403kb)\\r\\rTitle: Deep learning for multi-label classification of coral conditions in the\\r  Indo-Pacific via underwater photogrammetry\\rAuthors: Xinlei Shao and Hongruixuan Chen and Kirsty Magson and Jiaqi Wang and\\r  Jian Song and Jundong Chen and Jun Sasaki\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2403.05930 ,  1403kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.05949\\rreplaced with revised version Tue, 12 Mar 2024 03:23:45 GMT   (14157kb,D)\\r\\rTitle: General surgery vision transformer: A video pre-trained foundation model\\r  for general surgery\\rAuthors: Samuel Schmidgall, Ji Woong Kim, Jeffery Jopling, Axel Krieger\\rCategories: cs.CV cs.LG q-bio.TO\\r\\\\\\\\ ( https://arxiv.org/abs/2403.05949 ,  14157kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06025\\rreplaced with revised version Tue, 12 Mar 2024 17:35:29 GMT   (13284kb,D)\\r\\rTitle: CarbonNet: How Computer Vision Plays a Role in Climate Change?\\r  Application: Learning Geomechanics from Subsurface Geometry of CCS to\\r  Mitigate Global Warming\\rAuthors: Wei Chen, Yunan Li and Yuan Tian\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06025 ,  13284kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06154\\rreplaced with revised version Tue, 12 Mar 2024 02:44:36 GMT   (2579kb,D)\\r\\rTitle: GlanceVAD: Exploring Glance Supervision for Label-efficient Video\\r  Anomaly Detection\\rAuthors: Huaxin Zhang, Xiang Wang, Xiaohao Xu, Xiaonan Huang, Chuchu Han,\\r  Yuehuan Wang, Changxin Gao, Shanjun Zhang, Nong Sang\\rCategories: cs.CV\\rComments: 21 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06154 ,  2579kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06365\\rreplaced with revised version Tue, 12 Mar 2024 03:12:29 GMT   (18690kb,D)\\r\\rTitle: Style2Talker: High-Resolution Talking Head Generation with Emotion Style\\r  and Art Style\\rAuthors: Shuai Tan, Bin Ji, Ye Pan\\rCategories: cs.CV\\rComments: 9 pages, 5 figures, conference\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06365 ,  18690kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06375\\rreplaced with revised version Tue, 12 Mar 2024 02:31:12 GMT   (12324kb,D)\\r\\rTitle: FlowVQTalker: High-Quality Emotional Talking Face Generation through\\r  Normalizing Flow and Quantization\\rAuthors: Shuai Tan, Bin Ji, Ye Pan\\rCategories: cs.CV\\rComments: 11 pages, 11 figures, conference\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06375 ,  12324kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06495\\rreplaced with revised version Tue, 12 Mar 2024 12:52:14 GMT   (18300kb,D)\\r\\rTitle: Toward Generalist Anomaly Detection via In-context Residual Learning\\r  with Few-shot Sample Prompts\\rAuthors: Jiawen Zhu and Guansong Pang\\rCategories: cs.CV\\rComments: Accepted to CVPR 2024; 17 pages; 5 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06495 ,  18300kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06670\\rreplaced with revised version Tue, 12 Mar 2024 03:04:15 GMT   (6526kb,D)\\r\\rTitle: CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar\\r  Class-Incremental Learning\\rAuthors: Xinyuan Gao, Songlin Dong, Yuhang He, Xing Wei, Yihong Gong\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06670 ,  6526kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06866\\rreplaced with revised version Tue, 12 Mar 2024 08:28:39 GMT   (45178kb,D)\\r\\rTitle: QUASAR: QUality and Aesthetics Scoring with Advanced Representations\\rAuthors: Sergey Kastryulin, Denis Prokopenko, Artem Babenko, Dmitry V. Dylov\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06866 ,  45178kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06951\\rreplaced with revised version Tue, 12 Mar 2024 03:38:13 GMT   (20494kb,D)\\r\\rTitle: DEADiff: An Efficient Stylization Diffusion Model with Disentangled\\r  Representations\\rAuthors: Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang\\r  Chen, Qian He, Yongdong Zhang\\rCategories: cs.CV\\rComments: Accepted by CVPR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06951 ,  20494kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06977\\rreplaced with revised version Tue, 12 Mar 2024 15:22:52 GMT   (1889kb,D)\\r\\rTitle: VideoMamba: State Space Model for Efficient Video Understanding\\rAuthors: Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and\\r  Yu Qiao\\rCategories: cs.CV\\rComments: 19 Pages, 7 Figures, 8 Tables\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06977 ,  1889kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.09582\\rreplaced with revised version Tue, 12 Mar 2024 14:55:29 GMT   (6341kb)\\r\\rTitle: Language-Specific Representation of Emotion-Concept Knowledge Causally\\r  Supports Emotion Inference\\rAuthors: Ming Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao\\r  Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Kristen A. Lindquist, Zhiyuan\\r  Liu, Dan Zhang\\rCategories: cs.AI cs.CL\\rComments: 44 pages, 14 figures, 2 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2302.09582 ,  6341kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.09476\\rreplaced with revised version Tue, 12 Mar 2024 07:00:02 GMT   (487kb,D)\\r\\rTitle: Overthinking the Truth: Understanding how Language Models Process False\\r  Demonstrations\\rAuthors: Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2307.09476 ,  487kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.11432\\rreplaced with revised version Tue, 12 Mar 2024 09:51:35 GMT   (4914kb,D)\\r\\rTitle: A Survey on Large Language Model based Autonomous Agents\\rAuthors: Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and\\r  Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and\\r  Wayne Xin Zhao and Zhewei Wei and Ji-Rong Wen\\rCategories: cs.AI cs.CL\\rComments: 35 pages, 5 figures, 3 tables, has been accepted by frontiers of\\r  computer science (FCS), doi={10.1007/s11704-024-40231-1}\\r\\\\\\\\ ( https://arxiv.org/abs/2308.11432 ,  4914kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.02832\\rreplaced with revised version Mon, 11 Mar 2024 18:18:41 GMT   (901kb,D)\\r\\rTitle: Out-of-Distribution Detection by Leveraging Between-Layer Transformation\\r  Smoothness\\rAuthors: Fran Jeleni\\\\'c, Josip Juki\\\\'c, Martin Tutek, Mate Puljiz, Jan\\r  \\\\v{S}najder\\rCategories: cs.LG cs.CL\\rComments: International Conference on Learning Representations: ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.02832 ,  901kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.03646\\rreplaced with revised version Tue, 12 Mar 2024 13:38:31 GMT   (155kb,D)\\r\\rTitle: TRAM: Bridging Trust Regions and Sharpness Aware Minimization\\rAuthors: Tom Sherborne, Naomi Saphra, Pradeep Dasigi, Hao Peng\\rCategories: cs.LG cs.CL\\rComments: Camera Ready for ICLR 2024 (Accepted as Spotlight). 21 pages, 14\\r  tables, 2 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2310.03646 ,  155kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.06117\\rreplaced with revised version Tue, 12 Mar 2024 04:38:27 GMT   (763kb,D)\\r\\rTitle: Take a Step Back: Evoking Reasoning via Abstraction in Large Language\\r  Models\\rAuthors: Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed\\r  H. Chi, Quoc V Le and Denny Zhou\\rCategories: cs.LG cs.AI cs.CL\\rComments: ICLR 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.06117 ,  763kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.09618 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 04:14:18 GMT   (15305kb,D)\\r\\rTitle: Simulating Opinion Dynamics with Networks of LLM-based Agents\\rAuthors: Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh,\\r  Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers\\rCategories: physics.soc-ph cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2311.09618 ,  15305kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.15479\\rreplaced with revised version Mon, 11 Mar 2024 23:37:36 GMT   (3565kb,D)\\r\\rTitle: Navigating the Post-API Dilemma Search Engine Results Pages Present a\\r  Biased View of Social Media Data\\rAuthors: Amrit Poudel, Tim Weninger\\rCategories: cs.IR cs.CL cs.SI\\rComments: Proceedings of the ACM Web Conference 2024 (WWW '24)\\r\\\\\\\\ ( https://arxiv.org/abs/2401.15479 ,  3565kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.05359\\rreplaced with revised version Mon, 11 Mar 2024 23:15:10 GMT   (585kb,D)\\r\\rTitle: Guiding Large Language Models with Divide-and-Conquer Program for\\r  Discerning Problem Solving\\rAuthors: Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu\\rCategories: cs.AI cs.CL cs.LG\\rComments: Preprint\\r\\\\\\\\ ( https://arxiv.org/abs/2402.05359 ,  585kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.12177\\rreplaced with revised version Tue, 12 Mar 2024 16:04:23 GMT   (60kb)\\r\\rTitle: Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning\\rAuthors: Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2402.12177 ,  60kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.18603\\rreplaced with revised version Tue, 12 Mar 2024 16:35:25 GMT   (946kb,D)\\r\\rTitle: MMSR: Symbolic Regression is a Multimodal Task\\rAuthors: Yanjie Li, Jingyi Liu, Weijun Li, Lina Yu, Min Wu, Wenqiang Li, Meilan\\r  Hao, Su Wei, Yusong Deng\\rCategories: cs.LG cs.AI cs.CL\\rComments: 12 page\\r\\\\\\\\ ( https://arxiv.org/abs/2402.18603 ,  946kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.04964\\rreplaced with revised version Mon, 11 Mar 2024 18:41:29 GMT   (290kb)\\r\\rTitle: Tell me the truth: A system to measure the trustworthiness of Large\\r  Language Models\\rAuthors: Carlo Lipizzi\\rCategories: cs.AI cs.CL cs.CY\\r\\\\\\\\ ( https://arxiv.org/abs/2403.04964 ,  290kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.05527\\rreplaced with revised version Mon, 11 Mar 2024 18:55:40 GMT   (2567kb,D)\\r\\rTitle: GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\\r  Generative Inference of LLM\\rAuthors: Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu,\\r  Tushar Krishna, Tuo Zhao\\rCategories: cs.LG cs.AI cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2403.05527 ,  2567kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.05820\\rreplaced with revised version Tue, 12 Mar 2024 11:26:07 GMT   (277kb,D)\\r\\rTitle: An Audio-textual Diffusion Model For Converting Speech Signals Into\\r  Ultrasound Tongue Imaging Data\\rAuthors: Yudong Yang, Rongfeng Su, Xiaokang Liu, Nan Yan, and Lan Wang\\rCategories: cs.SD cs.CL eess.AS\\rComments: ICASSP2024 Accept\\r\\\\\\\\ ( https://arxiv.org/abs/2403.05820 ,  277kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2006.14347\\rreplaced with revised version Tue, 12 Mar 2024 13:00:14 GMT   (1542kb,D)\\r\\rTitle: Epoch-evolving Gaussian Process Guided Learning\\rAuthors: Jiabao Cui, Xuewei Li, Bin Li, Hanbin Zhao, Bourahla Omar, and Xi Li\\rCategories: cs.LG cs.CV stat.ML\\r\\\\\\\\ ( https://arxiv.org/abs/2006.14347 ,  1542kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2112.15411\\rreplaced with revised version Tue, 12 Mar 2024 16:16:00 GMT   (13822kb,D)\\r\\rTitle: Disjoint Contrastive Regression Learning for Multi-Sourced Annotations\\rAuthors: Xiaoqian Ruan, Gaoang Wang\\rCategories: cs.LG cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2112.15411 ,  13822kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2203.02158 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 13:57:45 GMT   (1109kb,D)\\r\\rTitle: Transformations in Learned Image Compression from a Modulation\\r  Perspective\\rAuthors: Youneng Bao, Fangyang Meng, Wen Tan, Chao Li, Yonghong Tian and\\r  Yongsheng Liang\\rCategories: eess.IV cs.AI cs.CV\\rComments: 10 pages, 8 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2203.02158 ,  1109kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2204.14100 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 16:36:06 GMT   (13642kb,D)\\r\\rTitle: Adversarial Distortion Learning for Medical Image Denoising\\rAuthors: Morteza Ghahremani, Mohammad Khateri, Alejandra Sierra, and Jussi\\r  Tohka\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2204.14100 ,  13642kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.03267\\rreplaced with revised version Tue, 12 Mar 2024 09:01:54 GMT   (7913kb,D)\\r\\rTitle: Prompter: Utilizing Large Language Model Prompting for a Data Efficient\\r  Embodied Instruction Following\\rAuthors: Yuki Inoue and Hiroki Ohashi\\rCategories: cs.RO cs.CV\\rComments: 8 pages, 3 figures, rejected by IROS2023\\r\\\\\\\\ ( https://arxiv.org/abs/2211.03267 ,  7913kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.07864\\rreplaced with revised version Tue, 12 Mar 2024 05:23:54 GMT   (1429kb,D)\\r\\rTitle: Federated Adaptive Prompt Tuning for Multi-Domain Collaborative Learning\\rAuthors: Shangchao Su and Mingzhao Yang and Bin Li and Xiangyang Xue\\rCategories: cs.LG cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2211.07864 ,  1429kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.10535 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 11:57:00 GMT   (1326kb,D)\\r\\rTitle: ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging\\rAuthors: {\\\\L}ukasz Struski, Dawid Rymarczyk, Arkadiusz Lewicki, Robert\\r  Sabiniewicz, Jacek Tabor, Bartosz Zieli\\\\'nski\\rCategories: eess.IV cs.CV cs.LG\\rComments: Accepted Paper to European Conference on Artificial Intelligence\\r  (ECAI 2023)\\r\\\\\\\\ ( https://arxiv.org/abs/2306.10535 ,  1326kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.03293 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 16:15:45 GMT   (13407kb,D)\\r\\rTitle: CheXmask: a large-scale dataset of anatomical segmentation masks for\\r  multi-center chest x-ray images\\rAuthors: Nicol\\\\'as Gaggion, Candelaria Mosquera, Lucas Mansilla, Julia Mariel\\r  Saidman, Martina Aineseder, Diego H. Milone, Enzo Ferrante\\rCategories: eess.IV cs.CV physics.med-ph\\rComments: The CheXmask dataset is publicly available at\\r  https://physionet.org/content/chexmask-cxr-segmentation-data/\\r\\\\\\\\ ( https://arxiv.org/abs/2307.03293 ,  13407kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.06067 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 11:24:51 GMT   (2471kb)\\r\\rTitle: Implicit Neural Representation for MRI Parallel Imaging Reconstruction\\rAuthors: Hao Li, Yusheng Zhou, Jianan Liu, Xiling Liu, Tao Huang, and Zhihan Lv\\rCategories: eess.IV cs.CV physics.med-ph\\r\\\\\\\\ ( https://arxiv.org/abs/2309.06067 ,  2471kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.07096 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 16:30:34 GMT   (3747kb)\\r\\rTitle: Computational limits to the legibility of the imaged human brain\\rAuthors: James K Ruffle, Robert J Gray, Samia Mohinta, Guilherme Pombo,\\r  Chaitanya Kaul, Harpreet Hyare, Geraint Rees, Parashkev Nachev\\rCategories: q-bio.NC cs.CV eess.IV\\rComments: 38 pages, 6 figures, 1 table, 2 supplementary figures, 1\\r  supplementary table\\r\\\\\\\\ ( https://arxiv.org/abs/2309.07096 ,  3747kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.12553\\rreplaced with revised version Mon, 11 Mar 2024 19:14:33 GMT   (4973kb,D)\\r\\rTitle: Explanation-based Training with Differentiable Insertion/Deletion\\r  Metric-aware Regularizers\\rAuthors: Yuya Yoshikawa, Tomoharu Iwata\\rCategories: cs.LG cs.CV stat.ML\\rComments: Accepted to AISTATS 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2310.12553 ,  4973kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.02058\\rreplaced with revised version Tue, 12 Mar 2024 17:23:55 GMT   (2772kb,D)\\r\\rTitle: LOTUS: Continual Imitation Learning for Robot Manipulation Through\\r  Unsupervised Skill Discovery\\rAuthors: Weikang Wan, Yifeng Zhu, Rutav Shah, Yuke Zhu\\rCategories: cs.RO cs.CV cs.LG\\rComments: ICRA 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.02058 ,  2772kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.04079 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 17:47:59 GMT   (774kb,D)\\r\\rTitle: RudolfV: A Foundation Model by Pathologists for Pathologists\\rAuthors: Jonas Dippel, Barbara Feulner, Tobias Winterhoff, Simon Schallenberg,\\r  Gabriel Dernbach, Andreas Kunft, Stephan Tietz, Timo Milbich, Simon Heinke,\\r  Marie-Lisa Eich, Julika Ribbat-Idel, Rosemarie Krupar, Philipp Jurmeister,\\r  David Horst, Lukas Ruff, Klaus-Robert M\\\\uller, Frederick Klauschen,\\r  Maximilian Alber\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2401.04079 ,  774kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2401.06187\\rreplaced with revised version Tue, 12 Mar 2024 02:38:33 GMT   (6409kb,D)\\r\\rTitle: Scissorhands: Scrub Data Influence via Connection Sensitivity in\\r  Networks\\rAuthors: Jing Wu and Mehrtash Harandi\\rCategories: cs.LG cs.CV\\rComments: Machine Unlearning, Deep Learning\\r\\\\\\\\ ( https://arxiv.org/abs/2401.06187 ,  6409kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.09430 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 11:48:02 GMT   (11432kb,D)\\r\\rTitle: WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing\\rAuthors: Shuokang Huang, Kaihan Li, Di You, Yichong Chen, Arvin Lin, Siying\\r  Liu, Xiaohui Li, Julie A. McCann\\rCategories: eess.SP cs.AI cs.CV cs.MM\\rComments: We present WiMANS, to our knowledge, the first dataset for multi-user\\r  activity sensing based on WiFi\\r\\\\\\\\ ( https://arxiv.org/abs/2402.09430 ,  11432kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2402.10885\\rreplaced with revised version Mon, 11 Mar 2024 22:05:00 GMT   (28406kb,D)\\r\\rTitle: 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations\\rAuthors: Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki\\rCategories: cs.RO cs.AI cs.CV cs.LG\\rComments: First two authors contributed equally\\r\\\\\\\\ ( https://arxiv.org/abs/2402.10885 ,  28406kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.04558\\rreplaced with revised version Tue, 12 Mar 2024 11:42:06 GMT   (1167kb,D)\\r\\rTitle: Reducing self-supervised learning complexity improves weakly-supervised\\r  classification performance in computational pathology\\rAuthors: Tim Lenz, Omar S. M. El Nahhas, Marta Ligero, Jakob Nikolas Kather\\rCategories: cs.LG cs.AI cs.CV\\rComments: Submitted to MICCAI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2403.04558 ,  1167kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06054 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 03:22:13 GMT   (13981kb,D)\\r\\rTitle: Decoupled Data Consistency with Diffusion Purification for Image\\r  Restoration\\rAuthors: Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishanka, Qing\\r  Qu\\rCategories: eess.IV cs.AI cs.CV cs.LG eess.SP\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06054 ,  13981kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2403.06242 (*cross-listing*)\\rreplaced with revised version Tue, 12 Mar 2024 10:54:57 GMT   (2301kb,D)\\r\\rTitle: COVID-19 Computer-aided Diagnosis through AI-assisted CT Imaging\\r  Analysis: Deploying a Medical AI System\\rAuthors: Demetris Gerogiannis and Anastasios Arsenos and Dimitrios Kollias and\\r  Dimitris Nikitopoulos and Stefanos Kollias\\rCategories: eess.IV cs.CV\\rComments: accepted at IEEE ISBI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2403.06242 ,  2301kb)\\r%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%--- ------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\rSend any comments regarding submissions directly to submitter.\\r------------------------------------------------------------------------------\\rArchives at http://arxiv.org/\\rTo unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel\\r------------------------------------------------------------------------------\\r Submissions to:\\rComputation and Language\\rComputer Vision and Pattern Recognition\\rGraphics\\r received from  Tue 19 Dec 23 19:00:00 GMT  to  Wed 20 Dec 23 19:00:00 GMT\\r------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12458\\rDate: Sat, 16 Dec 2023 17:13:08 GMT   (12546kb,D)\\r\\rTitle: When Parameter-efficient Tuning Meets General-purpose Vision-language\\r  Models\\rAuthors: Yihang Zhai, Haixin Wang, Jianlong Chang, Xinlong Yang, Jinan Sun,\\r  Shikun Zhang, Qi Tian\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Instruction tuning has shown promising potential for developing\\rgeneral-purpose AI capabilities by using large-scale pre-trained models and\\rboosts growing research to integrate multimodal information for creative\\rapplications. However, existing works still face two main limitations: the high\\rtraining costs and heavy computing resource dependence of full model\\rfine-tuning, and the lack of semantic information in instructions, which\\rhinders multimodal alignment. Addressing these challenges, this paper proposes\\ra novel approach to utilize Parameter-Efficient Tuning for generAl-purpose\\rvision-Language models, namely PETAL. PETAL revolutionizes the training process\\rby requiring only 0.5% of the total parameters, achieved through a unique mode\\rapproximation technique, which significantly reduces the training costs and\\rreliance on heavy computing resources. Furthermore, PETAL enhances the semantic\\rdepth of instructions in two innovative ways: 1) by introducing adaptive\\rinstruction mixture-of-experts(MOEs), and 2) by fortifying the score-based\\rlinkage between parameter-efficient tuning and mutual information. Our\\rextensive experiments across five multimodal downstream benchmarks reveal that\\rPETAL not only outperforms current state-of-the-art methods in most scenarios\\rbut also surpasses full fine-tuning models in effectiveness. Additionally, our\\rapproach demonstrates remarkable advantages in few-shot settings, backed by\\rcomprehensive visualization analyses. Our source code is available at:\\rhttps://github. com/melonking32/PETAL.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12458 ,  12546kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12588\\rDate: Tue, 19 Dec 2023 20:35:08 GMT   (7323kb,D)\\r\\rTitle: An Empirical study of Unsupervised Neural Machine Translation: analyzing\\r  NMT output, model's behavior and sentences' contribution\\rAuthors: Isidora Chara Tourni, Derry Wijaya\\rCategories: cs.CL\\r\\\\\\\\\\r  Unsupervised Neural Machine Translation (UNMT) focuses on improving NMT\\rresults under the assumption there is no human translated parallel data, yet\\rlittle work has been done so far in highlighting its advantages compared to\\rsupervised methods and analyzing its output in aspects other than translation\\raccuracy. We focus on three very diverse languages, French, Gujarati, and\\rKazakh, and train bilingual NMT models, to and from English, with various\\rlevels of supervision, in high- and low- resource setups, measure quality of\\rthe NMT output and compare the generated sequences' word order and semantic\\rsimilarity to source and reference sentences. We also use Layer-wise Relevance\\rPropagation to evaluate the source and target sentences' contribution to the\\rresult, expanding the findings of previous works to the UNMT paradigm.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12588 ,  7323kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12624\\rDate: Tue, 19 Dec 2023 22:01:01 GMT   (2117kb,D)\\r\\rTitle: Building a Llama2-finetuned LLM for Odia Language Utilizing Domain\\r  Knowledge Instruction Set\\rAuthors: Guneet Singh Kohli, Shantipriya Parida, Sambit Sekhar, Samirit Saha,\\r  Nipun B Nair, Parul Agarwal, Sonal Khosla, Kusumlata Patiyal, Debasish Dhal\\rCategories: cs.CL\\r\\\\\\\\\\r  Building LLMs for languages other than English is in great demand due to the\\runavailability and performance of multilingual LLMs, such as understanding the\\rlocal context. The problem is critical for low-resource languages due to the\\rneed for instruction sets. In a multilingual country like India, there is a\\rneed for LLMs supporting Indic languages to provide generative AI and LLM-based\\rtechnologies and services to its citizens.\\r  This paper presents our approach of i) generating a large Odia instruction\\rset, including domain knowledge data suitable for LLM fine-tuning, and ii)\\rbuilding a Llama2-finetuned model tailored for enhanced performance in the Odia\\rdomain. The proposed work will help researchers build an instruction set and\\rLLM, particularly for Indic languages. We will release the model and\\rinstruction set for the public for research and noncommercial purposes.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12624 ,  2117kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12660\\rDate: Tue, 19 Dec 2023 23:21:19 GMT   (1462kb)\\r\\rTitle: Is post-editing really faster than human translation?\\rAuthors: Silvia Terribile\\rCategories: cs.CL\\rComments: 30 pages, 11 tables, 7 figures. This article has been published in\\r  Translation Spaces. This is the author accepted manuscript. Please find the\\r  published version at: https://doi.org/10.1075/ts.22044.ter\\rDOI: 10.1075/ts.22044.ter\\r\\\\\\\\\\r  Time efficiency is paramount for the localisation industry, which demands\\rever-faster turnaround times. However, translation speed is largely\\runderresearched, and there is a lack of clarity about how language service\\rproviders (LSPs) can evaluate the performance of their post-editing (PE) and\\rhuman translation (HT) services. This study constitutes the first large-scale\\rinvestigation of translation and revision speed in HT and in the PE of neural\\rmachine translation, based on real-world data from an LSP. It uses an\\rexploratory data analysis approach to investigate data for 90 million words\\rtranslated by 879 linguists across 11 language pairs, over 2.5 years. The\\rresults of this research indicate that (a) PE is usually but not always faster\\rthan HT; (b) average speed values may be misleading; (c) translation speed is\\rhighly variable; and (d) edit distance cannot be used as a proxy for\\rpost-editing productivity, because it does not correlate strongly with speed.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12660 ,  1462kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12681\\rDate: Wed, 20 Dec 2023 00:45:27 GMT   (3531kb,D)\\r\\rTitle: Imitation of Life: A Search Engine for Biologically Inspired Design\\rAuthors: Hen Emuna, Nadav Borenstein, Xin Qian, Hyeonsu Kang, Joel Chan, Aniket\\r  Kittur, Dafna Shahaf\\rCategories: cs.CL cs.AI\\rComments: To be published in the AAAI 2024 Proceedings Main Track\\r\\\\\\\\\\r  Biologically Inspired Design (BID), or Biomimicry, is a problem-solving\\rmethodology that applies analogies from nature to solve engineering challenges.\\rFor example, Speedo engineers designed swimsuits based on shark skin. Finding\\rrelevant biological solutions for real-world problems poses significant\\rchallenges, both due to the limited biological knowledge engineers and\\rdesigners typically possess and to the limited BID resources. Existing BID\\rdatasets are hand-curated and small, and scaling them up requires costly human\\rannotations.\\r  In this paper, we introduce BARcode (Biological Analogy Retriever), a search\\rengine for automatically mining bio-inspirations from the web at scale. Using\\radvances in natural language understanding and data programming, BARcode\\ridentifies potential inspirations for engineering challenges. Our experiments\\rdemonstrate that BARcode can retrieve inspirations that are valuable to\\rengineers and designers tackling real-world problems, as well as recover famous\\rhistorical BID examples. We release data and code; we view BARcode as a step\\rtowards addressing the challenges that have historically hindered the practical\\rapplication of BID to engineering innovation.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12681 ,  3531kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12682\\rDate: Wed, 20 Dec 2023 00:48:13 GMT   (1434kb,D)\\r\\rTitle: Mini-GPTs: Efficient Large Language Models through Contextual Pruning\\rAuthors: Tim Valicenti, Justice Vidal, Ritik Patnaik\\rCategories: cs.CL cs.AI\\rComments: 7 pages, 4 figures, Neurips 2023 styling\\rACM-class: I.2.7\\r\\\\\\\\\\r  In AI research, the optimization of Large Language Models (LLMs) remains a\\rsignificant challenge, crucial for advancing the field's practical applications\\rand sustainability. Building upon the foundational work of Professor Song Han's\\rlab at MIT, this paper introduces a novel approach in developing Mini-GPTs via\\rcontextual pruning. Our methodology strategically prunes the computational\\rarchitecture of traditional LLMs, like Phi-1.5, focusing on retaining core\\rfunctionalities while drastically reducing model sizes. We employ the technique\\racross diverse and complex datasets, including US law, Medical Q&A, Skyrim\\rdialogue, English-Taiwanese translation, and Economics articles. The results\\runderscore the efficiency and effectiveness of contextual pruning, not merely\\ras a theoretical concept but as a practical tool in developing domain-specific,\\rresource-efficient LLMs. Contextual pruning is a promising method for building\\rdomain-specific LLMs, and this research is a building block towards future\\rdevelopment with more hardware compute, refined fine-tuning, and quantization.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12682 ,  1434kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12683\\rDate: Wed, 20 Dec 2023 00:49:52 GMT   (9751kb,D)\\r\\rTitle: Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is\\r  Needed?\\rAuthors: Tannon Kew, Florian Schottmann, Rico Sennrich\\rCategories: cs.CL\\r\\\\\\\\\\r  The vast majority of today's large language models are English-centric,\\rhaving been pretrained predominantly on English text. Yet, in order to meet\\ruser expectations, models need to be able to respond appropriately in multiple\\rlanguages once deployed in downstream applications. Given limited exposure to\\rother languages during pretraining, cross-lingual transfer is important for\\rachieving decent performance in non-English settings. In this work, we\\rinvestigate just how much multilinguality is required during finetuning to\\relicit strong cross-lingual generalisation across a range of tasks and target\\rlanguages. We find that, compared to English-only finetuning, multilingual\\rinstruction tuning with as few as three languages significantly improves a\\rmodel's cross-lingual transfer abilities on generative tasks that assume\\rinput/output language agreement, while being of less importance for highly\\rstructured tasks. Our code and data is available at\\rhttps://github.com/ZurichNLP/multilingual-instruction-tuning.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12683 ,  9751kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12713\\rDate: Wed, 20 Dec 2023 02:19:54 GMT   (327kb,D)\\r\\rTitle: Response Enhanced Semi-Supervised Dialogue Query Generation\\rAuthors: Jianheng Huang, Ante Wang, Linfeng Gao, Linfeng Song, Jinsong Su\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Leveraging vast and continually updated knowledge from the Internet has been\\rconsidered an important ability for a dialogue system. Therefore, the dialogue\\rquery generation task is proposed for generating search queries from dialogue\\rhistories, which will be submitted to a search engine for retrieving relevant\\rwebsites on the Internet. In this regard, previous efforts were devoted to\\rcollecting conversations with annotated queries and training a query producer\\r(QP) via standard supervised learning. However, these studies still face the\\rchallenges of data scarcity and domain adaptation. To address these issues, in\\rthis paper, we propose a semi-supervised learning framework -- SemiDQG, to\\rimprove model performance with unlabeled conversations. Based on the\\robservation that the search query is typically related to the topic of dialogue\\rresponse, we train a response-augmented query producer (RA) to provide rich and\\reffective training signals for QP. We first apply a similarity-based query\\rselection strategy to select high-quality RA-generated pseudo queries, which\\rare used to construct pseudo instances for training QP and RA. Then, we adopt\\rthe REINFORCE algorithm to further enhance QP, with RA-provided rewards as\\rfine-grained training signals. Experimental results and in-depth analysis of\\rthree benchmarks show the effectiveness of our framework in cross-domain and\\rlow-resource scenarios. Particularly, SemiDQG significantly surpasses ChatGPT\\rand competitive baselines. Our code is available at\\r\\\\url{https://github.com/DeepLearnXMU/SemiDQG}.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12713 ,  327kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12736\\rDate: Wed, 20 Dec 2023 03:18:50 GMT   (389kb,D)\\r\\rTitle: Learning and Forgetting Unsafe Examples in Large Language Models\\rAuthors: Jiachen Zhao, Zhun Deng, David Madras, James Zou, Mengye Ren\\rCategories: cs.CL cs.LG\\r\\\\\\\\\\r  As the number of large language models (LLMs) released to the public grows,\\rthere is a pressing need to understand the safety implications associated with\\rthese models learning from third-party custom finetuning data. We explore the\\rbehavior of LLMs finetuned on noisy custom data containing unsafe content,\\rrepresented by datasets that contain biases, toxicity, and harmfulness, finding\\rthat while aligned LLMs can readily learn this unsafe content, they also tend\\rto forget it more significantly than other examples when subsequently finetuned\\ron safer content. Drawing inspiration from the discrepancies in forgetting, we\\rintroduce the ForgetFilter algorithm, which filters unsafe data based on how\\rstrong the model's forgetting signal is for that data. We demonstrate that the\\rForgetFilter algorithm ensures safety in customized finetuning without\\rcompromising downstream task performance, unlike sequential safety finetuning.\\rForgetFilter outperforms alternative strategies like replay and moral\\rself-correction in curbing LLMs' ability to assimilate unsafe content during\\rcustom finetuning, e.g. 75% lower than not applying any safety measures and 62%\\rlower than using self-correction in toxicity score.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12736 ,  389kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12740\\rDate: Wed, 20 Dec 2023 03:21:48 GMT   (222kb)\\r\\rTitle: Fine-tuning Large Language Models for Adaptive Machine Translation\\rAuthors: Yasmin Moslem, Rejwanul Haque, Andy Way\\rCategories: cs.CL cs.IR\\r\\\\\\\\\\r  This paper presents the outcomes of fine-tuning Mistral 7B, a general-purpose\\rlarge language model (LLM), for adaptive machine translation (MT). The\\rfine-tuning process involves utilising a combination of zero-shot and one-shot\\rtranslation prompts within the medical domain. The primary objective is to\\renhance real-time adaptive MT capabilities of Mistral 7B, enabling it to adapt\\rtranslations to the required domain at inference time. The results,\\rparticularly for Spanish-to-English MT, showcase the efficacy of the fine-tuned\\rmodel, demonstrating quality improvements in both zero-shot and one-shot\\rtranslation scenarios, surpassing Mistral 7B's baseline performance. Notably,\\rthe fine-tuned Mistral outperforms ChatGPT gpt-3.5-turbo in zero-shot\\rtranslation while achieving comparable one-shot translation quality. Moreover,\\rthe zero-shot translation of the fine-tuned Mistral matches NLLB 3.3B's\\rperformance, and its one-shot translation quality surpasses that of NLLB 3.3B.\\rThese findings emphasise the significance of fine-tuning efficient LLMs like\\rMistral 7B to yield high-quality zero-shot translations comparable to\\rtask-oriented models like NLLB 3.3B. Additionally, the adaptive gains achieved\\rin one-shot translation are comparable to those of commercial LLMs such as\\rChatGPT. Our experiments demonstrate that, with a relatively small dataset of\\r20,000 segments that incorporate a mix of zero-shot and one-shot prompts,\\rfine-tuning significantly enhances Mistral's in-context learning ability,\\respecially for real-time adaptive MT.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12740 ,  222kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12746\\rDate: Wed, 20 Dec 2023 03:40:45 GMT   (2036kb,D)\\r\\rTitle: ChatFDA: Medical Records Risk Assessment\\rAuthors: M Tran, C Sun\\rCategories: cs.CL cs.CY\\r\\\\\\\\\\r  In healthcare, the emphasis on patient safety and the minimization of medical\\rerrors cannot be overstated. Despite concerted efforts, many healthcare\\rsystems, especially in low-resource regions, still grapple with preventing\\rthese errors effectively. This study explores a pioneering application aimed at\\raddressing this challenge by assisting caregivers in gauging potential risks\\rderived from medical notes. The application leverages data from openFDA,\\rdelivering real-time, actionable insights regarding prescriptions. Preliminary\\ranalyses conducted on the MIMIC-III \\\\cite{mimic} dataset affirm a proof of\\rconcept highlighting a reduction in medical errors and an amplification in\\rpatient safety. This tool holds promise for drastically enhancing healthcare\\routcomes in settings with limited resources. To bolster reproducibility and\\rfoster further research, the codebase underpinning our methodology is\\raccessible on\\rhttps://github.com/autonlab/2023.hackAuton/tree/main/prescription_checker. This\\ris a submission for the 30th HackAuton CMU.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12746 ,  2036kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12806\\rDate: Wed, 20 Dec 2023 07:01:49 GMT   (1848kb,D)\\r\\rTitle: MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large\\r  Language Models\\rAuthors: Yan Cai, Linlin Wang, Ye Wang, Gerard de Melo, Ya Zhang, Yanfeng Wang,\\r  Liang He\\rCategories: cs.CL cs.AI\\rComments: accepted by AAAI-24\\r\\\\\\\\\\r  The emergence of various medical large language models (LLMs) in the medical\\rdomain has highlighted the need for unified evaluation standards, as manual\\revaluation of LLMs proves to be time-consuming and labor-intensive. To address\\rthis issue, we introduce MedBench, a comprehensive benchmark for the Chinese\\rmedical domain, comprising 40,041 questions sourced from authentic examination\\rexercises and medical reports of diverse branches of medicine. In particular,\\rthis benchmark is composed of four key components: the Chinese Medical\\rLicensing Examination, the Resident Standardization Training Examination, the\\rDoctor In-Charge Qualification Examination, and real-world clinic cases\\rencompassing examinations, diagnoses, and treatments. MedBench replicates the\\reducational progression and clinical practice experiences of doctors in\\rMainland China, thereby establishing itself as a credible benchmark for\\rassessing the mastery of knowledge and reasoning abilities in medical language\\rlearning models. We perform extensive experiments and conduct an in-depth\\ranalysis from diverse perspectives, which culminate in the following findings:\\r(1) Chinese medical LLMs underperform on this benchmark, highlighting the need\\rfor significant advances in clinical knowledge and diagnostic precision. (2)\\rSeveral general-domain LLMs surprisingly possess considerable medical\\rknowledge. These findings elucidate both the capabilities and limitations of\\rLLMs within the context of MedBench, with the ultimate goal of aiding the\\rmedical research community.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12806 ,  1848kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12808\\rDate: Wed, 20 Dec 2023 07:15:04 GMT   (197kb,D)\\r\\rTitle: Enhancing Consistency in Multimodal Dialogue System Using LLM with\\r  Dialogue Scenario\\rAuthors: Hiroki Onozeki, Zhiyang Qi, Kazuma Akiyama, Ryutaro Asahara, Takumasa\\r  Kaneko, Michimasa Inaba\\rCategories: cs.CL\\rComments: This paper is part of the proceedings of the Dialogue Robot\\r  Competition 2023\\r\\\\\\\\\\r  This paper describes our dialogue system submitted to Dialogue Robot\\rCompetition 2023. The system's task is to help a user at a travel agency decide\\ron a plan for visiting two sightseeing spots in Kyoto City that satisfy the\\ruser. Our dialogue system is flexible and stable and responds to user\\rrequirements by controlling dialogue flow according to dialogue scenarios. We\\ralso improved user satisfaction by introducing motion and speech control based\\ron system utterances and user situations. In the preliminary round, our system\\rwas ranked fifth in the impression evaluation and sixth in the plan evaluation\\ramong all 12 teams.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12808 ,  197kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12832\\rDate: Wed, 20 Dec 2023 08:28:36 GMT   (530kb,D)\\r\\rTitle: Turning Dust into Gold: Distilling Complex Reasoning Capabilities from\\r  LLMs by Leveraging Negative Data\\rAuthors: Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin\\r  Wang, Heda Wang, Kan Li\\rCategories: cs.CL cs.AI\\rComments: AAAI 2024\\r\\\\\\\\\\r  Large Language Models (LLMs) have performed well on various reasoning tasks,\\rbut their inaccessibility and numerous parameters hinder wide application in\\rpractice. One promising way is distilling the reasoning ability from LLMs to\\rsmall models by the generated chain-of-thought reasoning paths. In some cases,\\rhowever, LLMs may produce incorrect reasoning chains, especially when facing\\rcomplex mathematical problems. Previous studies only transfer knowledge from\\rpositive samples and drop the synthesized data with wrong answers. In this\\rwork, we illustrate the merit of negative data and propose a model\\rspecialization framework to distill LLMs with negative samples besides positive\\rones. The framework consists of three progressive steps, covering from training\\rto inference stages, to absorb knowledge from negative data. We conduct\\rextensive experiments across arithmetic reasoning tasks to demonstrate the role\\rof negative data in distillation from LLM.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12832 ,  530kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12850\\rDate: Wed, 20 Dec 2023 09:01:01 GMT   (203kb)\\r\\rTitle: A Stochastic Analysis of the Linguistic Provenance of English Place\\r  Names\\rAuthors: Michael Dalvean\\rCategories: cs.CL\\rMSC-class: 68T50\\rACM-class: I.2.7\\r\\\\\\\\\\r  In English place name analysis, meanings are often derived from the\\rresemblance of roots in place names to topographical features, proper names\\rand/or habitation terms in one of the languages that have had an influence on\\rEnglish place names. The problem here is that it is sometimes difficult to\\rdetermine the base language to use to interpret the roots. The purpose of this\\rpaper is to stochastically determine the resemblance between 18799 English\\rplace names and 84685 place names from Ireland, Scotland, Wales, Denmark,\\rNorway, Sweden, France, Germany, the Netherlands and Ancient Rome. Each English\\rplace name is ranked according to the extent to which it resembles place names\\rfrom the other countries, and this provides a basis for determining the likely\\rlanguage to use to interpret the place name. A number of observations can be\\rmade using the ranking provided. In particular, it is found that `Didlington'\\ris the most archetypically English place name in the English sample, and `Anna'\\ris the least. Furthermore, it is found that the place names in the non-English\\rdatasets are most similar to Norwegian place names and least similar to Welsh\\rplace names.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12850 ,  203kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12852\\rDate: Wed, 20 Dec 2023 09:06:06 GMT   (7658kb,D)\\r\\rTitle: Language Resources for Dutch Large Language Modelling\\rAuthors: Bram Vanroy\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Despite the rapid expansion of types of large language models, there remains\\ra notable gap in models specifically designed for the Dutch language. This gap\\ris not only a shortage in terms of pretrained Dutch models but also in terms of\\rdata, and benchmarks and leaderboards. This work provides a small step to\\rimprove the situation. First, we introduce two fine-tuned variants of the Llama\\r2 13B model. We first fine-tuned Llama 2 using Dutch-specific web-crawled data\\rand subsequently refined this model further on multiple synthetic instruction\\rand chat datasets. These datasets as well as the model weights are made\\ravailable. In addition, we provide a leaderboard to keep track of the\\rperformance of (Dutch) models on a number of generation tasks, and we include\\rresults of a number of state-of-the-art models, including our own. Finally we\\rprovide a critical conclusion on what we believe is needed to push forward\\rDutch language models and the whole eco-system around the models.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12852 ,  7658kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12853\\rDate: Wed, 20 Dec 2023 09:06:18 GMT   (5646kb,D)\\r\\rTitle: CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks\\r  for Chinese Large Language Models\\rAuthors: Dan Shi, Chaobin You, Jiantao Huang, Taihao Li, Deyi Xiong\\rCategories: cs.CL\\rComments: AAAI 2024\\r\\\\\\\\\\r  As an indispensable ingredient of intelligence, commonsense reasoning is\\rcrucial for large language models (LLMs) in real-world scenarios. In this\\rpaper, we propose CORECODE, a dataset that contains abundant commonsense\\rknowledge manually annotated on dyadic dialogues, to evaluate the commonsense\\rreasoning and commonsense conflict detection capabilities of Chinese LLMs. We\\rcategorize commonsense knowledge in everyday conversations into three\\rdimensions: entity, event, and social interaction. For easy and consistent\\rannotation, we standardize the form of commonsense knowledge annotation in\\ropen-domain dialogues as domain: slot = value. A total of 9 domains and 37\\rslots are defined to capture diverse commonsense knowledge. With these\\rpre-defined domains and slots, we collect 76,787 commonsense knowledge\\rannotations from 19,700 dialogues through crowdsourcing. To evaluate and\\renhance the commonsense reasoning capability for LLMs on the curated dataset,\\rwe establish a series of dialogue-level reasoning and detection tasks,\\rincluding commonsense knowledge filling, commonsense knowledge generation,\\rcommonsense conflict phrase detection, domain identification, slot\\ridentification, and event causal inference. A wide variety of existing\\ropen-source Chinese LLMs are evaluated with these tasks on our dataset.\\rExperimental results demonstrate that these models are not competent to predict\\rCORECODE's plentiful reasoning content, and even ChatGPT could only achieve\\r0.275 and 0.084 accuracy on the domain identification and slot identification\\rtasks under the zero-shot setting. We release the data and codes of CORECODE at\\rhttps://github.com/danshi777/CORECODE to promote commonsense reasoning\\revaluation and study of LLMs in the context of daily conversations.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12853 ,  5646kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12918\\rDate: Wed, 20 Dec 2023 10:53:53 GMT   (7038kb,D)\\r\\rTitle: Assaying on the Robustness of Zero-Shot Machine-Generated Text Detectors\\rAuthors: Yi-Fan Zhang and Zhang Zhang and Liang Wang and Rong Jin\\rCategories: cs.CL\\rComments: 8 pages, 3 figures, AAAI 2024 Workshop on Responsible Language Models\\r\\\\\\\\\\r  To combat the potential misuse of Natural Language Generation (NLG)\\rtechnology, a variety of algorithms have been developed for the detection of\\rAI-generated texts. Traditionally, this task is treated as a binary\\rclassification problem. Although supervised learning has demonstrated promising\\rresults, acquiring labeled data for detection purposes poses real-world\\rchallenges and the risk of overfitting. In an effort to address these issues,\\rwe delve into the realm of zero-shot machine-generated text detection. Existing\\rzero-shot detectors, typically designed for specific tasks or topics, often\\rassume uniform testing scenarios, limiting their practicality. In our research,\\rwe explore various advanced Large Language Models (LLMs) and their specialized\\rvariants, contributing to this field in several ways. In empirical studies, we\\runcover a significant correlation between topics and detection performance.\\rSecondly, we delve into the influence of topic shifts on zero-shot detectors.\\rThese investigations shed light on the adaptability and robustness of these\\rdetection methods across diverse topics.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12918 ,  7038kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12999\\rDate: Wed, 20 Dec 2023 12:59:31 GMT   (4886kb,D)\\r\\rTitle: Machine Mindset: An MBTI Exploration of Large Language Models\\rAuthors: Jiaxi Cui, Liuzhenghao Lv, Jing Wen, Jing Tang, YongHong Tian, Li Yuan\\rCategories: cs.CL\\r\\\\\\\\\\r  We present a novel approach for integrating Myers-Briggs Type Indicator\\r(MBTI) personality traits into large language models (LLMs), addressing the\\rchallenges of personality consistency in personalized AI. Our method, Machine\\rMindset, involves a two-phase fine-tuning and Direct Preference Optimization\\r(DPO) to embed MBTI traits into LLMs. This approach ensures that models\\rinternalize these traits, offering a stable and consistent personality profile.\\rWe demonstrate the effectiveness of our models across various domains, showing\\ralignment between model performance and their respective MBTI traits. The paper\\rhighlights significant contributions in the development of personality datasets\\rand a new training methodology for personality integration in LLMs, enhancing\\rthe potential for personalized AI applications. We also open-sourced our model\\rand part of the data at \\\\url{https://github.com/PKU-YuanGroup/Machine-Mindset}.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12999 ,  4886kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13010\\rDate: Wed, 20 Dec 2023 13:22:41 GMT   (1549kb,D)\\r\\rTitle: AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and\\r  Optimisation\\rAuthors: Dong Huang, Qingwen Bu, Jie M.Zhang, Michael Luck, and Heming Cui\\rCategories: cs.CL\\rComments: 21 pages, 12 figures\\r\\\\\\\\\\r  The advancement of natural language processing (NLP) has been significantly\\rboosted by the development of transformer-based large language models (LLMs).\\rThese models have revolutionized NLP tasks, particularly in code generation,\\raiding developers in creating software with enhanced efficiency. Despite their\\radvancements, challenges in balancing code snippet generation with effective\\rtest case generation and execution persist. To address these issues, this paper\\rintroduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution\\rcomprising a multi-agent framework with specialized agents: the programmer\\ragent, the test designer agent, and the test executor agent. During the coding\\rprocedure, the programmer agent will focus on the code generation and\\rrefinement based on the test executor agent's feedback. The test designer agent\\rwill generate test cases for the generated code, and the test executor agent\\rwill run the code with the test cases and write the feedback to the programmer.\\rThis collaborative system ensures robust code generation, surpassing the\\rlimitations of single-agent models and traditional methodologies. Our extensive\\rexperiments on 9 code generation models and 12 enhancement approaches showcase\\rAgentCoder's superior performance over existing code generation models and\\rprompt engineering techniques across various benchmarks. For example,\\rAgentCoder achieves 77.4% and 89.1% pass@1 in HumanEval-ET and MBPP-ET with\\rGPT-3.5, while SOTA baselines obtain only 69.5% and 63.0%.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13010 ,  1549kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13040\\rDate: Wed, 20 Dec 2023 14:08:58 GMT   (8613kb,D)\\r\\rTitle: Retrieval-augmented Multilingual Knowledge Editing\\rAuthors: Weixuan Wang, Barry Haddow, Alexandra Birch\\rCategories: cs.CL\\r\\\\\\\\\\r  Knowledge represented in Large Language Models (LLMs) is quite often\\rincorrect and can also become obsolete over time. Updating knowledge via\\rfine-tuning is computationally resource-hungry and not reliable, and so\\rknowledge editing (KE) has developed as an effective and economical alternative\\rto inject new knowledge or to fix factual errors in LLMs. Although there has\\rbeen considerable interest in this area, current KE research exclusively\\rfocuses on the monolingual setting, typically in English. However, what happens\\rif the new knowledge is supplied in one language, but we would like to query\\rthe LLM in a different language? To address the problem of multilingual\\rknowledge editing, we propose Retrieval-augmented Multilingual Knowledge Editor\\r(ReMaKE) to update new knowledge in LLMs. ReMaKE can perform model-agnostic\\rknowledge editing in multilingual settings. ReMaKE concatenates the new\\rknowledge retrieved from a multilingual knowledge base with prompts. Our\\rexperimental results show that ReMaKE outperforms baseline knowledge editing\\rmethods by a significant margin and is the first KE method to work in a\\rmultilingual setting. We provide our multilingual knowledge editing dataset\\r(MzsRE) in 12 languages, which along with code, and additional project\\rinformation is available at https://github.com/Vicky-Wil/ReMaKE.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13040 ,  8613kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13096\\rDate: Wed, 20 Dec 2023 15:17:03 GMT   (2780kb)\\r\\rTitle: In Generative AI we Trust: Can Chatbots Effectively Verify Political\\r  Information?\\rAuthors: Elizaveta Kuznetsova, Mykola Makhortykh, Victoria Vziatysheva, Martha\\r  Stolze, Ani Baghumyan, Aleksandra Urman\\rCategories: cs.CL cs.CY\\rComments: 22 pages, 8 figures\\r\\\\\\\\\\r  This article presents a comparative analysis of the ability of two large\\rlanguage model (LLM)-based chatbots, ChatGPT and Bing Chat, recently rebranded\\rto Microsoft Copilot, to detect veracity of political information. We use AI\\rauditing methodology to investigate how chatbots evaluate true, false, and\\rborderline statements on five topics: COVID-19, Russian aggression against\\rUkraine, the Holocaust, climate change, and LGBTQ+ related debates. We compare\\rhow the chatbots perform in high- and low-resource languages by using prompts\\rin English, Russian, and Ukrainian. Furthermore, we explore the ability of\\rchatbots to evaluate statements according to political communication concepts\\rof disinformation, misinformation, and conspiracy theory, using\\rdefinition-oriented prompts. We also systematically test how such evaluations\\rare influenced by source bias which we model by attributing specific claims to\\rvarious political and social actors. The results show high performance of\\rChatGPT for the baseline veracity evaluation task, with 72 percent of the cases\\revaluated correctly on average across languages without pre-training. Bing Chat\\rperformed worse with a 67 percent accuracy. We observe significant disparities\\rin how chatbots evaluate prompts in high- and low-resource languages and how\\rthey adapt their evaluations to political communication concepts with ChatGPT\\rproviding more nuanced outputs than Bing Chat. Finally, we find that for some\\rveracity detection-related tasks, the performance of chatbots varied depending\\ron the topic of the statement or the source to which it is attributed. These\\rfindings highlight the potential of LLM-based chatbots in tackling different\\rforms of false information in online environments, but also points to the\\rsubstantial variation in terms of how such potential is realized due to\\rspecific factors, such as language of the prompt or the topic.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13096 ,  2780kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13103\\rDate: Wed, 20 Dec 2023 15:20:33 GMT   (810kb)\\r\\rTitle: Exploring Multimodal Large Language Models for Radiology Report\\r  Error-checking\\rAuthors: Jinge Wu, Yunsoo Kim, Eva C. Keller, Jamie Chow, Adam P. Levine,\\r  Nikolas Pontikos, Zina Ibrahim, Paul Taylor, Michelle C. Williams, Honghan Wu\\rCategories: cs.CL cs.CV\\r\\\\\\\\\\r  This paper proposes one of the first clinical applications of multimodal\\rlarge language models (LLMs) as an assistant for radiologists to check errors\\rin their reports. We created an evaluation dataset from two real-world\\rradiology datasets (MIMIC-CXR and IU-Xray), with 1,000 subsampled reports each.\\rA subset of original reports was modified to contain synthetic errors by\\rintroducing various type of mistakes. The evaluation contained two difficulty\\rlevels: SIMPLE for binary error-checking and COMPLEX for identifying error\\rtypes. LLaVA (Large Language and Visual Assistant) variant models, including\\rour instruction-tuned model, were used for the evaluation. Additionally, a\\rdomain expert evaluation was conducted on a small test set. At the SIMPLE\\rlevel, the LLaVA v1.5 model outperformed other publicly available models.\\rInstruction tuning significantly enhanced performance by 47.4% and 25.4% on\\rMIMIC-CXR and IU-Xray data, respectively. The model also surpassed the domain\\rexperts accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets\\r(N=21) of the test set where a clinician did not achieve the correct\\rconclusion, the LLaVA ensemble mode correctly identified 71.4% of these cases.\\rThis study marks a promising step toward utilizing multi-modal LLMs to enhance\\rdiagnostic accuracy in radiology. The ensemble model demonstrated comparable\\rperformance to clinicians, even capturing errors overlooked by humans.\\rNevertheless, future work is needed to improve the model ability to identify\\rthe types of inconsistency.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13103 ,  810kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13179\\rDate: Wed, 20 Dec 2023 16:40:33 GMT   (218kb,D)\\r\\rTitle: Contextual Code Switching for Machine Translation using Language Models\\rAuthors: Arshad Kaji, Manan Shah\\rCategories: cs.CL\\rComments: 4 pages, 1 figure, 2 tables\\r\\\\\\\\\\r  Large language models (LLMs) have exerted a considerable impact on diverse\\rlanguage-related tasks in recent years. Their demonstrated state-of-the-art\\rperformance is achieved through methodologies such as zero-shot or few-shot\\rprompting. These models undergo training on extensive datasets that encompass\\rsegments of the Internet and subsequently undergo fine-tuning tailored to\\rspecific tasks. Notably, they exhibit proficiency in tasks such as translation,\\rsummarization, question answering, and creative writing, even in the absence of\\rexplicit training for those particular tasks. While they have shown substantial\\rimprovement in the multilingual tasks their performance in the code switching,\\respecially for machine translation remains relatively uncharted. In this paper,\\rwe present an extensive study on the code switching task specifically for the\\rmachine translation task comparing multiple LLMs. Our results indicate that\\rdespite the LLMs having promising results in the certain tasks, the models with\\rrelatively lesser complexity outperform the multilingual large language models\\rin the machine translation task. We posit that the efficacy of multilingual\\rlarge language models in contextual code switching is constrained by their\\rtraining methodologies. In contrast, relatively smaller models, when trained\\rand fine-tuned on bespoke datasets, may yield superior results in comparison to\\rthe majority of multilingual models.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13179 ,  218kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13193\\rDate: Wed, 20 Dec 2023 17:05:46 GMT   (336kb)\\r\\rTitle: HCDIR: End-to-end Hate Context Detection, and Intensity Reduction model\\r  for online comments\\rAuthors: Neeraj Kumar Singh, Koyel Ghosh, Joy Mahapatra, Utpal Garain,\\r  Apurbalal Senapati\\rCategories: cs.CL cs.AI\\r\\\\\\\\\\r  Warning: This paper contains examples of the language that some people may\\rfind offensive.\\r  Detecting and reducing hateful, abusive, offensive comments is a critical and\\rchallenging task on social media. Moreover, few studies aim to mitigate the\\rintensity of hate speech. While studies have shown that context-level semantics\\rare crucial for detecting hateful comments, most of this research focuses on\\rEnglish due to the ample datasets available. In contrast, low-resource\\rlanguages, like Indian languages, remain under-researched because of limited\\rdatasets. Contrary to hate speech detection, hate intensity reduction remains\\runexplored in high-resource and low-resource languages. In this paper, we\\rpropose a novel end-to-end model, HCDIR, for Hate Context Detection, and Hate\\rIntensity Reduction in social media posts. First, we fine-tuned several\\rpre-trained language models to detect hateful comments to ascertain the\\rbest-performing hateful comments detection model. Then, we identified the\\rcontextual hateful words. Identification of such hateful words is justified\\rthrough the state-of-the-art explainable learning model, i.e., Integrated\\rGradient (IG). Lastly, the Masked Language Modeling (MLM) model has been\\remployed to capture domain-specific nuances to reduce hate intensity. We masked\\rthe 50\\\\% hateful words of the comments identified as hateful and predicted the\\ralternative words for these masked terms to generate convincing sentences. An\\roptimal replacement for the original hate comments from the feasible sentences\\ris preferred. Extensive experiments have been conducted on several recent\\rdatasets using automatic metric-based evaluation (BERTScore) and thorough human\\revaluation. To enhance the faithfulness in human evaluation, we arranged a\\rgroup of three human annotators with varied expertise.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13193 ,  336kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13208\\rDate: Wed, 20 Dec 2023 17:25:23 GMT   (9625kb,D)\\r\\rTitle: LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent\\r  Sentence Spaces\\rAuthors: Yingji Zhang, Danilo S. Carvalho, Ian Pratt-Hartmann, Andr\\\\'e Freitas\\rCategories: cs.CL\\r\\\\\\\\\\r  Deep generative neural networks, such as Variational AutoEncoders (VAEs),\\roffer an opportunity to better understand and control language models from the\\rperspective of sentence-level latent spaces. To combine the controllability of\\rVAE latent spaces with the state-of-the-art performance of recent large\\rlanguage models (LLMs), we present in this work LlaMaVAE, which combines\\rexpressive encoder and decoder models (sentenceT5 and LlaMA) with a VAE\\rarchitecture, aiming to provide better text generation control to LLMs. In\\raddition, to conditionally guide the VAE generation, we investigate a new\\rapproach based on flow-based invertible neural networks (INNs) named Invertible\\rCVAE. Experimental results reveal that LlaMaVAE can outperform the previous\\rstate-of-the-art VAE language model, Optimus, across various tasks, including\\rlanguage modelling, semantic textual similarity and definition modelling.\\rQualitative analysis on interpolation and traversal experiments also indicates\\ran increased degree of semantic clustering and geometric consistency, which\\renables better generation control.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13208 ,  9625kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13211\\rDate: Wed, 20 Dec 2023 17:27:25 GMT   (733kb,D)\\r\\rTitle: DSFormer: Effective Compression of Text-Transformers by Dense-Sparse\\r  Weight Factorization\\rAuthors: Rahul Chand, Yashoteja Prabhu, Pratyush Kumar\\rCategories: cs.CL\\rComments: 9 page main paper. 1 page appendix\\r\\\\\\\\\\r  With the tremendous success of large transformer models in natural language\\runderstanding, down-sizing them for cost-effective deployments has become\\rcritical. Recent studies have explored the low-rank weight factorization\\rtechniques which are efficient to train, and apply out-of-the-box to any\\rtransformer architecture. Unfortunately, the low-rank assumption tends to be\\rover-restrictive and hinders the expressiveness of the compressed model. This\\rpaper proposes, DSFormer, a simple alternative factorization scheme which\\rexpresses a target weight matrix as the product of a small dense and a\\rsemi-structured sparse matrix. The resulting approximation is more faithful to\\rthe weight distribution in transformers and therefore achieves a stronger\\refficiency-accuracy trade-off. Another concern with existing factorizers is\\rtheir dependence on a task-unaware initialization step which degrades the\\raccuracy of the resulting model. DSFormer addresses this issue through a novel\\rStraight-Through Factorizer (STF) algorithm that jointly learns all the weight\\rfactorizations to directly maximize the final task accuracy. Extensive\\rexperiments on multiple natural language understanding benchmarks demonstrate\\rthat DSFormer obtains up to 40% better compression than the state-of-the-art\\rlow-rank factorizers, leading semi-structured sparsity baselines and popular\\rknowledge distillation approaches. Our approach is also orthogonal to\\rmainstream compressors and offers up to 50% additional compression when added\\rto popular distilled, layer-shared and quantized transformers. We empirically\\revaluate the benefits of STF over conventional optimization practices.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13211 ,  733kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13264\\rDate: Wed, 20 Dec 2023 18:41:44 GMT   (7402kb,D)\\r\\rTitle: dIR -- Discrete Information Retrieval: Conversational Search over\\r  Unstructured (and Structured) Data with Large Language Models\\rAuthors: Pablo M. Rodriguez Bertorello and Jean Rodmond Junior Laguerre\\r  (Computer Science Department, Stanford University)\\rCategories: cs.CL cs.AI cs.DB cs.IR cs.LG\\rComments: 8 pages, 5 figures, Association for Computational Linguistics\\r\\\\\\\\\\r  Data is stored in both structured and unstructured form. Querying both, to\\rpower natural language conversations, is a challenge. This paper introduces\\rdIR, Discrete Information Retrieval, providing a unified interface to query\\rboth free text and structured knowledge. Specifically, a Large Language Model\\r(LLM) transforms text into expressive representation. After the text is\\rextracted into columnar form, it can then be queried via a text-to-SQL Semantic\\rParser, with an LLM converting natural language into SQL. Where desired, such\\rconversation may be effected by a multi-step reasoning conversational agent. We\\rvalidate our approach via a proprietary question/answer data set, concluding\\rthat dIR makes a whole new class of queries on free text possible when compared\\rto traditionally fine-tuned dense-embedding-model-based Information Retrieval\\r(IR) and SQL-based Knowledge Bases (KB). For sufficiently complex queries, dIR\\rcan succeed where no other method stands a chance.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13264 ,  7402kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12439\\rDate: Fri, 20 Oct 2023 13:03:48 GMT   (24170kb,D)\\r\\rTitle: Single-pixel 3D imaging based on fusion temporal data of single photon\\r  detector and millimeter-wave radar\\rAuthors: Tingqin Lai, Xiaolin Liang, Yi Zhu, Xinyi Wu, Lianye Liao, Xuelin\\r  Yuan, Ping Su and Shihai Sun\\rCategories: cs.CV physics.optics\\rComments: Accepted by Chinese Optics Letters, and comments are welcome\\r\\\\\\\\\\r  Recently, there has been increased attention towards 3D imaging using\\rsingle-pixel single-photon detection (also known as temporal data) due to its\\rpotential advantages in terms of cost and power efficiency. However, to\\reliminate the symmetry blur in the reconstructed images, a fixed background is\\rrequired. This paper proposes a fusion-data-based 3D imaging method that\\rutilizes a single-pixel single-photon detector and a millimeter-wave radar to\\rcapture temporal histograms of a scene from multiple perspectives.\\rSubsequently, the 3D information can be reconstructed from the one-dimensional\\rfusion temporal data by using Artificial Neural Network (ANN). Both the\\rsimulation and experimental results demonstrate that our fusion method\\reffectively eliminates symmetry blur and improves the quality of the\\rreconstructed images.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12439 ,  24170kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12441\\rDate: Sun, 29 Oct 2023 15:26:37 GMT   (6322kb,D)\\r\\rTitle: DiffSpectralNet : Unveiling the Potential of Diffusion Models for\\r  Hyperspectral Image Classification\\rAuthors: Neetu Sigger, Tuan Thanh Nguyen, Gianluca Tozzi, Quoc-Tuan Vien, Sinh\\r  Van Nguyen\\rCategories: cs.CV cs.LG\\rComments: 18 pages\\r\\\\\\\\\\r  Hyperspectral images (HSI) have become popular for analysing remotely sensed\\rimages in multiple domain like agriculture, medical. However, existing models\\rstruggle with complex relationships and characteristics of spectral-spatial\\rdata due to the multi-band nature and data redundancy of hyperspectral data. To\\raddress this limitation, we propose a new network called DiffSpectralNet, which\\rcombines diffusion and transformer techniques. Our approach involves a two-step\\rprocess. First, we use an unsupervised learning framework based on the\\rdiffusion model to extract both high-level and low-level spectral-spatial\\rfeatures. The diffusion method is capable of extracting diverse and meaningful\\rspectral-spatial features, leading to improvement in HSI classification. Then,\\rwe employ a pretrained denoising U-Net to extract intermediate hierarchical\\rfeatures for classification. Finally, we use a supervised transformer-based\\rclassifier to perform the HSI classification. Through comprehensive experiments\\ron HSI datasets, we evaluate the classification performance of DiffSpectralNet.\\rThe results demonstrate that our framework significantly outperforms existing\\rapproaches, achieving state-of-the-art performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12441 ,  6322kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12442\\rDate: Thu, 2 Nov 2023 18:37:45 GMT   (1052kb)\\r\\rTitle: Hierarchical Classification System for Breast Cancer Specimen Report\\r  (HCSBC) -- an end-to-end model for characterizing severity and diagnosis\\rAuthors: Thiago Santos, Harish Kamath, Christopher R. McAdams, Mary S. Newell,\\r  Marina Mosunjac, Gabriela Oprea-Ilies, Geoffrey Smith, Constance Lehman, Judy\\r  Gichoya, Imon Banerjee, Hari Trivedi\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Automated classification of cancer pathology reports can extract information\\rfrom unstructured reports and categorize each report into structured diagnosis\\rand severity categories. Thus, such system can reduce the burden for populating\\rtumor registries, help registration for clinical trial as well as developing\\rlarge dataset for deep learning model development using true pathologic ground\\rtruth. However, the content of breast pathology reports can be difficult for\\rcategorize due to the high linguistic variability in content and wide variety\\rof potential diagnoses >50. Existing NLP models are primarily focused on\\rdeveloping classifier for primary breast cancer types (e.g. IDC, DCIS, ILC) and\\rtumor characteristics, and ignore the rare diagnosis of cancer subtypes. We\\rthen developed a hierarchical hybrid transformer-based pipeline (59 labels) -\\rHierarchical Classification System for Breast Cancer Specimen Report (HCSBC),\\rwhich utilizes the potential of the transformer context-preserving NLP\\rtechnique and compared our model to several state of the art ML and DL models.\\rWe trained the model on the EUH data and evaluated our model's performance on\\rtwo external datasets - MGH and Mayo Clinic. We publicly release the code and a\\rlive application under Huggingface spaces repository\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12442 ,  1052kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12444\\rDate: Fri, 3 Nov 2023 18:09:08 GMT   (6209kb,D)\\r\\rTitle: What Makes Pre-Trained Visual Representations Successful for Robust\\r  Manipulation?\\rAuthors: Kaylee Burns, Zach Witzel, Jubayer Ibn Hamid, Tianhe Yu, Chelsea Finn,\\r  Karol Hausman\\rCategories: cs.CV cs.AI cs.RO\\rComments: 20 pages, 12 figures\\r\\\\\\\\\\r  Inspired by the success of transfer learning in computer vision, roboticists\\rhave investigated visual pre-training as a means to improve the learning\\refficiency and generalization ability of policies learned from pixels. To that\\rend, past work has favored large object interaction datasets, such as\\rfirst-person videos of humans completing diverse tasks, in pursuit of\\rmanipulation-relevant features. Although this approach improves the efficiency\\rof policy learning, it remains unclear how reliable these representations are\\rin the presence of distribution shifts that arise commonly in robotic\\rapplications. Surprisingly, we find that visual representations designed for\\rmanipulation and control tasks do not necessarily generalize under subtle\\rchanges in lighting and scene texture or the introduction of distractor\\robjects. To understand what properties do lead to robust representations, we\\rcompare the performance of 15 pre-trained vision models under different visual\\rappearances. We find that emergent segmentation ability is a strong predictor\\rof out-of-distribution generalization among ViT models. The rank order induced\\rby this metric is more predictive than metrics that have previously guided\\rgeneralization research within computer vision and machine learning, such as\\rdownstream ImageNet accuracy, in-domain accuracy, or shape-bias as evaluated by\\rcue-conflict performance. We test this finding extensively on a suite of\\rdistribution shifts in ten tasks across two simulated manipulation\\renvironments. On the ALOHA setup, segmentation score predicts real-world\\rperformance after offline training with 50 demonstrations.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12444 ,  6209kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12463\\rDate: Mon, 18 Dec 2023 19:02:07 GMT   (24351kb,D)\\r\\rTitle: Open Vocabulary Semantic Scene Sketch Understanding\\rAuthors: Ahmed Bourouis, Judith Ellen Fan, Yulia Gryaditskaya\\rCategories: cs.CV\\r\\\\\\\\\\r  We study the underexplored but fundamental vision problem of machine\\runderstanding of abstract freehand scene sketches. We introduce a sketch\\rencoder that results in semantically-aware feature space, which we evaluate by\\rtesting its performance on a semantic sketch segmentation task. To train our\\rmodel we rely only on the availability of bitmap sketches with their brief\\rcaptions and do not require any pixel-level annotations. To obtain\\rgeneralization to a large set of sketches and categories, we build on a vision\\rtransformer encoder pretrained with the CLIP model. We freeze the text encoder\\rand perform visual-prompt tuning of the visual encoder branch while introducing\\ra set of critical modifications. Firstly, we augment the classical key-query\\r(k-q) self-attention blocks with value-value (v-v) self-attention blocks.\\rCentral to our model is a two-level hierarchical network design that enables\\refficient semantic disentanglement: The first level ensures holistic scene\\rsketch encoding, and the second level focuses on individual categories. We,\\rthen, in the second level of the hierarchy, introduce a cross-attention between\\rtextual and visual branches. Our method outperforms zero-shot CLIP pixel\\raccuracy of segmentation results by 37 points, reaching an accuracy of $85.5\\\\%$\\ron the FS-COCO sketch dataset. Finally, we conduct a user study that allows us\\rto identify further improvements needed over our method to reconcile machine\\rand human understanding of scene sketches.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12463 ,  24351kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12468\\rDate: Tue, 19 Dec 2023 07:05:39 GMT   (32301kb,D)\\r\\rTitle: MaskINT: Video Editing via Interpolative Non-autoregressive Masked\\r  Transformers\\rAuthors: Haoyu Ma, Shahin Mahdizadehaghdam, Bichen Wu, Zhipeng Fan, Yuchao Gu,\\r  Wenliang Zhao, Lior Shapira, Xiaohui Xie\\rCategories: cs.CV\\r\\\\\\\\\\r  Recent advances in generative AI have significantly enhanced image and video\\rediting, particularly in the context of text prompt control. State-of-the-art\\rapproaches predominantly rely on diffusion models to accomplish these tasks.\\rHowever, the computational demands of diffusion-based methods are substantial,\\roften necessitating large-scale paired datasets for training, and therefore\\rchallenging the deployment in practical applications. This study addresses this\\rchallenge by breaking down the text-based video editing process into two\\rseparate stages. In the first stage, we leverage an existing text-to-image\\rdiffusion model to simultaneously edit a few keyframes without additional\\rfine-tuning. In the second stage, we introduce an efficient model called\\rMaskINT, which is built on non-autoregressive masked generative transformers\\rand specializes in frame interpolation between the keyframes, benefiting from\\rstructural guidance provided by intermediate frames. Our comprehensive set of\\rexperiments illustrates the efficacy and efficiency of MaskINT when compared to\\rother diffusion-based methodologies. This research offers a practical solution\\rfor text-based video editing and showcases the potential of non-autoregressive\\rmasked generative transformers in this domain.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12468 ,  32301kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12470\\rDate: Tue, 19 Dec 2023 08:14:14 GMT   (4354kb,D)\\r\\rTitle: Rotated Multi-Scale Interaction Network for Referring Remote Sensing\\r  Image Segmentation\\rAuthors: Sihan Liu, Yiwei Ma, Xiaoqing Zhang, Haowei Wang, Jiayi Ji, Xiaoshuai\\r  Sun, Rongrong Ji\\rCategories: cs.CV\\r\\\\\\\\\\r  Referring Remote Sensing Image Segmentation (RRSIS) is a new challenge that\\rcombines computer vision and natural language processing, delineating specific\\rregions in aerial images as described by textual queries. Traditional Referring\\rImage Segmentation (RIS) approaches have been impeded by the complex spatial\\rscales and orientations found in aerial imagery, leading to suboptimal\\rsegmentation results. To address these challenges, we introduce the Rotated\\rMulti-Scale Interaction Network (RMSIN), an innovative approach designed for\\rthe unique demands of RRSIS. RMSIN incorporates an Intra-scale Interaction\\rModule (IIM) to effectively address the fine-grained detail required at\\rmultiple scales and a Cross-scale Interaction Module (CIM) for integrating\\rthese details coherently across the network. Furthermore, RMSIN employs an\\rAdaptive Rotated Convolution (ARC) to account for the diverse orientations of\\robjects, a novel contribution that significantly enhances segmentation\\raccuracy. To assess the efficacy of RMSIN, we have curated an expansive dataset\\rcomprising 17,402 image-caption-mask triplets, which is unparalleled in terms\\rof scale and variety. This dataset not only presents the model with a wide\\rrange of spatial and rotational scenarios but also establishes a stringent\\rbenchmark for the RRSIS task, ensuring a rigorous evaluation of performance.\\rOur experimental evaluations demonstrate the exceptional performance of RMSIN,\\rsurpassing existing state-of-the-art models by a significant margin. All\\rdatasets and code are made available at https://github.com/Lsan2401/RMSIN.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12470 ,  4354kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12471\\rDate: Tue, 19 Dec 2023 08:56:33 GMT   (12202kb,D)\\r\\rTitle: Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion\\rAuthors: Fan Zhang, Shaodi You, Yu Li, Ying Fu\\rCategories: cs.CV\\rComments: 10 pages\\r\\\\\\\\\\r  Monocular depth estimation has experienced significant progress on\\rterrestrial images in recent years, largely due to deep learning advancements.\\rHowever, it remains inadequate for underwater scenes, primarily because of data\\rscarcity. Given the inherent challenges of light attenuation and backscattering\\rin water, acquiring clear underwater images or precise depth information is\\rnotably difficult and costly. Consequently, learning-based approaches often\\rrely on synthetic data or turn to unsupervised or self-supervised methods to\\rmitigate this lack of data. Nonetheless, the performance of these methods is\\roften constrained by the domain gap and looser constraints. In this paper, we\\rpropose a novel pipeline for generating photorealistic underwater images using\\raccurate terrestrial depth data. This approach facilitates the training of\\rsupervised models for underwater depth estimation, effectively reducing the\\rperformance disparity between terrestrial and underwater environments. Contrary\\rto prior synthetic datasets that merely apply style transfer to terrestrial\\rimages without altering the scene content, our approach uniquely creates\\rvibrant, non-existent underwater scenes by leveraging terrestrial depth data\\rthrough the innovative Stable Diffusion model. Specifically, we introduce a\\runique Depth2Underwater ControlNet, trained on specially prepared \\\\{Underwater,\\rDepth, Text\\\\} data triplets, for this generation task. Our newly developed\\rdataset enables terrestrial depth estimation models to achieve considerable\\rimprovements, both quantitatively and qualitatively, on unseen underwater\\rimages, surpassing their terrestrial pre-trained counterparts. Moreover, the\\renhanced depth accuracy for underwater scenes also aids underwater image\\rrestoration techniques that rely on depth maps, further demonstrating our\\rdataset's utility. The dataset will be available at\\rhttps://github.com/zkawfanx/Atlantis.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12471 ,  12202kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12478\\rDate: Tue, 19 Dec 2023 14:39:11 GMT   (2838kb,D)\\r\\rTitle: ProS: Prompting-to-simulate Generalized knowledge for Universal\\r  Cross-Domain Retrieval\\rAuthors: Kaipeng Fang, Jingkuan Song, Lianli Gao, Pengpeng Zeng, Zhi-Qi Cheng,\\r  Xiyao Li, Heng Tao Shen\\rCategories: cs.CV\\r\\\\\\\\\\r  The goal of Universal Cross-Domain Retrieval (UCDR) is to achieve robust\\rperformance in generalized test scenarios, wherein data may belong to strictly\\runknown domains and categories during training. Recently, pre-trained models\\rwith prompt tuning have shown strong generalization capabilities and attained\\rnoteworthy achievements in various downstream tasks, such as few-shot learning\\rand video-text retrieval. However, applying them directly to UCDR may not\\rsufficiently to handle both domain shift (i.e., adapting to unfamiliar domains)\\rand semantic shift (i.e., transferring to unknown categories). To this end, we\\rpropose Prompting-to-Simulate (ProS), the first method to apply prompt tuning\\rfor UCDR. ProS employs a two-step process to simulate Content-aware Dynamic\\rPrompts (CaDP) which can impact models to produce generalized features for\\rUCDR. Concretely, in Prompt Units Learning stage, we introduce two Prompt Units\\rto individually capture domain and semantic knowledge in a mask-and-align way.\\rThen, in Context-aware Simulator Learning stage, we train a Content-aware\\rPrompt Simulator under a simulated test scenarios to produce the corresponding\\rCaDP. Extensive experiments conducted on three benchmark datasets show that our\\rmethod achieves new state-of-the-art performance without bringing excessive\\rparameters. Our method is publicly available at\\rhttps://anonymous.4open.science/r/ProS\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12478 ,  2838kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12479\\rDate: Tue, 19 Dec 2023 14:44:02 GMT   (9606kb,D)\\r\\rTitle: Zero-shot Building Attribute Extraction from Large-Scale Vision and\\r  Language Models\\rAuthors: Fei Pan, Sangryul Jeon, Brian Wang, Frank Mckenna, Stella X. Yu\\rCategories: cs.CV\\rComments: Accepted to WACV 2024, Project Page:\\r  https://sites.google.com/view/zobae/home\\r\\\\\\\\\\r  Existing building recognition methods, exemplified by BRAILS, utilize\\rsupervised learning to extract information from satellite and street-view\\rimages for classification and segmentation. However, each task module requires\\rhuman-annotated data, hindering the scalability and robustness to regional\\rvariations and annotation imbalances. In response, we propose a new zero-shot\\rworkflow for building attribute extraction that utilizes large-scale vision and\\rlanguage models to mitigate reliance on external annotations. The proposed\\rworkflow contains two key components: image-level captioning and segment-level\\rcaptioning for the building images based on the vocabularies pertinent to\\rstructural and civil engineering. These two components generate descriptive\\rcaptions by computing feature representations of the image and the\\rvocabularies, and facilitating a semantic match between the visual and textual\\rrepresentations. Consequently, our framework offers a promising avenue to\\renhance AI-driven captioning for building attribute extraction in the\\rstructural and civil engineering domains, ultimately reducing reliance on human\\rannotations while bolstering performance and adaptability.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12479 ,  9606kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12480\\rDate: Tue, 19 Dec 2023 15:34:52 GMT   (2276kb,D)\\r\\rTitle: Adaptive Distribution Masked Autoencoders for Continual Test-Time\\r  Adaptation\\rAuthors: Jiaming Liu, Ran Xu, Senqiao Yang, Renrui Zhang, Qizhe Zhang, Zehui\\r  Chen, Yandong Guo, and Shanghang Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  Continual Test-Time Adaptation (CTTA) is proposed to migrate a source\\rpre-trained model to continually changing target distributions, addressing\\rreal-world dynamism. Existing CTTA methods mainly rely on entropy minimization\\ror teacher-student pseudo-labeling schemes for knowledge extraction in\\runlabeled target domains. However, dynamic data distributions cause\\rmiscalibrated predictions and noisy pseudo-labels in existing self-supervised\\rlearning methods, hindering the effective mitigation of error accumulation and\\rcatastrophic forgetting problems during the continual adaptation process. To\\rtackle these issues, we propose a continual self-supervised method, Adaptive\\rDistribution Masked Autoencoders (ADMA), which enhances the extraction of\\rtarget domain knowledge while mitigating the accumulation of distribution\\rshifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism\\rto adaptively sample masked positions, followed by establishing consistency\\rconstraints between the masked target samples and the original target samples.\\rAdditionally, for masked tokens, we utilize an efficient decoder to reconstruct\\ra hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients),\\rleveraging its invariant properties to boost task-relevant representations.\\rThrough conducting extensive experiments on four widely recognized benchmarks,\\rour proposed method attains state-of-the-art performance in both classification\\rand segmentation CTTA tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12480 ,  2276kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12481\\rDate: Tue, 19 Dec 2023 16:03:04 GMT   (4591kb,D)\\r\\rTitle: Unveiling Spaces: Architecturally meaningful semantic descriptions from\\r  images of interior spaces\\rAuthors: Demircan Tas, Rohit Priyadarshi Sanatani\\rCategories: cs.CV\\rComments: Written for 6.869, Advances in Computer Vision at MIT, Spring 2022. 9\\r  pages, 13 figures\\r\\\\\\\\\\r  There has been a growing adoption of computer vision tools and technologies\\rin architectural design workflows over the past decade. Notable use cases\\rinclude point cloud generation, visual content analysis, and spatial awareness\\rfor robotic fabrication. Multiple image classification, object detection, and\\rsemantic pixel segmentation models have become popular for the extraction of\\rhigh-level symbolic descriptions and semantic content from two-dimensional\\rimages and videos. However, a major challenge in this regard has been the\\rextraction of high-level architectural structures (walls, floors, ceilings\\rwindows etc.) from diverse imagery where parts of these elements are occluded\\rby furniture, people, or other non-architectural elements. This project aims to\\rtackle this problem by proposing models that are capable of extracting\\rarchitecturally meaningful semantic descriptions from two-dimensional scenes of\\rpopulated interior spaces. 1000 virtual classrooms are parametrically\\rgenerated, randomized along key spatial parameters such as length, width,\\rheight, and door/window positions. The positions of cameras, and\\rnon-architectural visual obstructions (furniture/objects) are also randomized.\\rA Generative Adversarial Network (GAN) for image-to-image translation (Pix2Pix)\\ris trained on synthetically generated rendered images of these enclosures,\\ralong with corresponding image abstractions representing high-level\\rarchitectural structure. The model is then tested on unseen synthetic imagery\\rof new enclosures, and outputs are compared to ground truth using pixel-wise\\rcomparison for evaluation. A similar model evaluation is also carried out on\\rphotographs of existing indoor enclosures, to measure its performance in\\rreal-world settings.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12481 ,  4591kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12486\\rDate: Tue, 19 Dec 2023 17:08:08 GMT   (656kb)\\r\\rTitle: Vision-Based Automatic Groceries Tracking System -- Smart Homes\\rAuthors: Divya Mereddy\\rCategories: cs.CV cs.AI\\rComments: 2023 IEEE International Conference on Web Intelligence and\\r  Intelligent Agent Technology (WI-IAT)\\r\\\\\\\\\\r  With advanced AI, while every industry is growing at rocket speed, the smart\\rhome industry has not reached the next generation. There is still a huge leap\\rof innovation that needs to happen before we call a home a Smart home. A Smart\\rhome should predict residents' needs and fulfill them in a timely manner. One\\rof the important tasks of maintaining a home is timely grocery tracking and\\rsupply maintenance. Grocery tracking models are very famous in the retail\\rindustry but they are nonexistent in the common household. Groceries detection\\rin household refrigerators or storage closets is very complicated compared to\\rretail shelving data. In this paper, home grocery tracking problem is resolved\\rby combining retail shelving data and fruits dataset with real-time 360 view\\rdata points collected from home groceries storage. By integrating this\\rvision-based object detection system along with supply chain and user food\\rinterest prediction systems, complete automation of groceries ordering can be\\rachieved.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12486 ,  656kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12490\\rDate: Tue, 19 Dec 2023 17:55:16 GMT   (11130kb,D)\\r\\rTitle: InstructVideo: Instructing Video Diffusion Models with Human Feedback\\rAuthors: Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining\\r  Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, Dong Ni\\rCategories: cs.CV cs.AI cs.LG cs.MM\\rComments: Project page: https://instructvideo.github.io/\\r\\\\\\\\\\r  Diffusion models have emerged as the de facto paradigm for video generation.\\rHowever, their reliance on web-scale data of varied quality often yields\\rresults that are visually unappealing and misaligned with the textual prompts.\\rTo tackle this problem, we propose InstructVideo to instruct text-to-video\\rdiffusion models with human feedback by reward fine-tuning. InstructVideo has\\rtwo key ingredients: 1) To ameliorate the cost of reward fine-tuning induced by\\rgenerating through the full DDIM sampling chain, we recast reward fine-tuning\\ras editing. By leveraging the diffusion process to corrupt a sampled video,\\rInstructVideo requires only partial inference of the DDIM sampling chain,\\rreducing fine-tuning cost while improving fine-tuning efficiency. 2) To\\rmitigate the absence of a dedicated video reward model for human preferences,\\rwe repurpose established image reward models, e.g., HPSv2. To this end, we\\rpropose Segmental Video Reward, a mechanism to provide reward signals based on\\rsegmental sparse sampling, and Temporally Attenuated Reward, a method that\\rmitigates temporal modeling degradation during fine-tuning. Extensive\\rexperiments, both qualitative and quantitative, validate the practicality and\\refficacy of using image reward models in InstructVideo, significantly enhancing\\rthe visual quality of generated videos without compromising generalization\\rcapabilities. Code and models will be made publicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12490 ,  11130kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12491\\rDate: Tue, 19 Dec 2023 18:18:33 GMT   (13436kb,D)\\r\\rTitle: StreamDiffusion: A Pipeline-level Solution for Real-time Interactive\\r  Generation\\rAuthors: Akio Kodaira, Chenfeng Xu, Toshiki Hazama, Takanori Yoshimoto, Kohei\\r  Ohno, Shogo Mitsuhori, Soichi Sugano, Hanying Cho, Zhijian Liu, Kurt Keutzer\\rCategories: cs.CV cs.GR cs.LG\\rComments: tech report, the code is available at\\r  https://github.com/cumulo-autumn/StreamDiffusion\\r\\\\\\\\\\r  We introduce StreamDiffusion, a real-time diffusion pipeline designed for\\rinteractive image generation. Existing diffusion models are adept at creating\\rimages from text or image prompts, yet they often fall short in real-time\\rinteraction. This limitation becomes particularly evident in scenarios\\rinvolving continuous input, such as Metaverse, live video streaming, and\\rbroadcasting, where high throughput is imperative. To address this, we present\\ra novel approach that transforms the original sequential denoising into the\\rbatching denoising process. Stream Batch eliminates the conventional\\rwait-and-interact approach and enables fluid and high throughput streams. To\\rhandle the frequency disparity between data input and model throughput, we\\rdesign a novel input-output queue for parallelizing the streaming process.\\rMoreover, the existing diffusion pipeline uses classifier-free guidance(CFG),\\rwhich requires additional U-Net computation. To mitigate the redundant\\rcomputations, we propose a novel residual classifier-free guidance (RCFG)\\ralgorithm that reduces the number of negative conditional denoising steps to\\ronly one or even zero. Besides, we introduce a stochastic similarity\\rfilter(SSF) to optimize power consumption. Our Stream Batch achieves around\\r1.5x speedup compared to the sequential denoising method at different denoising\\rlevels. The proposed RCFG leads to speeds up to 2.05x higher than the\\rconventional CFG. Combining the proposed strategies and existing mature\\racceleration tools makes the image-to-image generation achieve up-to 91.07fps\\ron one RTX4090, improving the throughputs of AutoPipline developed by Diffusers\\rover 59.56x. Furthermore, our proposed StreamDiffusion also significantly\\rreduces the energy consumption by 2.39x on one RTX3060 and 1.99x on one\\rRTX4090, respectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12491 ,  13436kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12494\\rDate: Tue, 19 Dec 2023 18:54:40 GMT   (20204kb,D)\\r\\rTitle: DDOS: The Drone Depth and Obstacle Segmentation Dataset\\rAuthors: Benedikt Kolbeinsson and Krystian Mikolajczyk\\rCategories: cs.CV\\r\\\\\\\\\\r  Accurate depth and semantic segmentation are crucial for various computer\\rvision tasks. However, the scarcity of annotated real-world aerial datasets\\rposes a significant challenge for training and evaluating robust models.\\rAdditionally, the detection and segmentation of thin objects, such as wires,\\rcables, and fences, present a critical concern for ensuring the safe operation\\rof drones. To address these limitations, we present a novel synthetic dataset\\rspecifically designed for depth and semantic segmentation tasks in aerial\\rviews. Leveraging photo-realistic rendering techniques, our dataset provides a\\rvaluable resource for training models using a synthetic-supervision training\\rscheme while introducing new drone-specific metrics for depth accuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12494 ,  20204kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12540\\rDate: Tue, 19 Dec 2023 19:19:19 GMT   (17183kb,D)\\r\\rTitle: Fixed-point Inversion for Text-to-image diffusion models\\rAuthors: Barak Meiri, Dvir Samuel, Nir Darshan, Gal Chechik, Shai Avidan, Rami\\r  Ben-Ari\\rCategories: cs.CV\\r\\\\\\\\\\r  Text-guided diffusion models offer powerful new ways to generate and\\rmanipulate images. Several applications of these models, including image\\rediting interpolation, and semantic augmentation, require diffusion inversion.\\rThis is the process of finding a noise seed that can be used to generate a\\rgiven image. Current techniques for inverting a given image can be slow or\\rinaccurate. The technical challenge for inverting the diffusion process arises\\rfrom an implicit equation over the latent that cannot be solved in closed form.\\rPrevious approaches proposed to solve this issue by approximation or various\\rlearning schemes. Here, we formulate the problem as a fixed-point equation\\rproblem and solve it using fixed-point iterations, a well-studied approach in\\rnumerical analysis. We further identify a source of inconsistency that\\rsignificantly hurts the inversion of real images encoded to the latent space.\\rWe show how to correct it by applying a prompt-aware adjustment of the\\rencoding. Our solution, Fixed-point inversion, is much faster than previous\\rtechniques like EDICT and Null-text, with similar inversion quality. It can be\\rcombined with any pretrained diffusion model and requires no model training,\\rprompt tuning, or additional parameters. In a series of experiments, we find\\rthat Fixed-point inversion shows improved results in several downstream tasks:\\rimage editing, image interpolation, and generation of rare objects.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12540 ,  17183kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12619\\rDate: Tue, 19 Dec 2023 21:53:12 GMT   (4977kb,D)\\r\\rTitle: Hierarchical Vision Transformers for Context-Aware Prostate Cancer\\r  Grading in Whole Slide Images\\rAuthors: Cl\\\\'ement Grisi, Geert Litjens, Jeroen van der Laak\\rCategories: cs.CV cs.AI\\rComments: Accepted at Medical Imaging meets NeurIPS 2023 workshop\\rMSC-class: 68T07\\rACM-class: I.2.10\\r\\\\\\\\\\r  Vision Transformers (ViTs) have ushered in a new era in computer vision,\\rshowcasing unparalleled performance in many challenging tasks. However, their\\rpractical deployment in computational pathology has largely been constrained by\\rthe sheer size of whole slide images (WSIs), which result in lengthy input\\rsequences. Transformers faced a similar limitation when applied to long\\rdocuments, and Hierarchical Transformers were introduced to circumvent it.\\rGiven the analogous challenge with WSIs and their inherent hierarchical\\rstructure, Hierarchical Vision Transformers (H-ViTs) emerge as a promising\\rsolution in computational pathology. This work delves into the capabilities of\\rH-ViTs, evaluating their efficiency for prostate cancer grading in WSIs. Our\\rresults show that they achieve competitive performance against existing\\rstate-of-the-art solutions.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12619 ,  4977kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12634\\rDate: Tue, 19 Dec 2023 22:33:17 GMT   (2157kb,D)\\r\\rTitle: MotionScript: Natural Language Descriptions for Expressive 3D Human\\r  Motions\\rAuthors: Payam Jome Yazdian, Eric Liu, Li Cheng, Angelica Lim\\rCategories: cs.CV cs.AI cs.CL cs.RO\\r\\\\\\\\\\r  This paper proposes MotionScript, a motion-to-text conversion algorithm and\\rnatural language representation for human body motions. MotionScript aims to\\rdescribe movements in greater detail and with more accuracy than previous\\rnatural language approaches. Many motion datasets describe relatively objective\\rand simple actions with little variation on the way they are expressed (e.g.\\rsitting, walking, dribbling a ball). But for expressive actions that contain a\\rdiversity of movements in the class (e.g. being sad, dancing), or for actions\\routside the domain of standard motion capture datasets (e.g. stylistic walking,\\rsign-language), more specific and granular natural language descriptions are\\rneeded. Our proposed MotionScript descriptions differ from existing natural\\rlanguage representations in that it provides direct descriptions in natural\\rlanguage instead of simple action labels or high-level human captions. To the\\rbest of our knowledge, this is the first attempt at translating 3D motions to\\rnatural language descriptions without requiring training data. Our experiments\\rshow that when MotionScript representations are used in a text-to-motion neural\\rtask, body movements are more accurately reconstructed, and large language\\rmodels can be used to generate unseen complex motions.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12634 ,  2157kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12635\\rDate: Tue, 19 Dec 2023 22:33:42 GMT   (3479kb,D)\\r\\rTitle: RealCraft: Attention Control as A Solution for Zero-shot Long Video\\r  Editing\\rAuthors: Shutong Jin, Ruiyu Wang, Florian T. Pokorny\\rCategories: cs.CV\\r\\\\\\\\\\r  Although large-scale text-to-image generative models have shown promising\\rperformance in synthesizing high-quality images, directly applying these models\\rto image editing remains a significant challenge. This challenge is further\\ramplified in video editing due to the additional dimension of time. Especially\\rfor editing real videos as it necessitates maintaining a stable semantic layout\\racross the frames while executing localized edits precisely without disrupting\\rthe existing backgrounds. In this paper, we propose \\\\textit{RealCraft}, an\\rattention-control-based method for zero-shot editing in real videos. By\\remploying the object-centric manipulation of cross-attention between prompts\\rand frames and spatial-temporal attention within the frames, we achieve precise\\rshape-wise editing along with enhanced consistency. Our model can be used\\rdirectly with Stable Diffusion and operates without the need for additional\\rlocalized information. We showcase our zero-shot attention-control-based method\\racross a range of videos, demonstrating localized, high-fidelity, shape-precise\\rand time-consistent editing in videos of various lengths, up to 64 frames.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12635 ,  3479kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12659\\rDate: Tue, 19 Dec 2023 23:11:06 GMT   (1797kb,D)\\r\\rTitle: Expediting Contrastive Language-Image Pretraining via Self-distilled\\r  Encoders\\rAuthors: Bumsoo Kim, Jinhyung Kim, Yeonsik Jo, Seung Hwan Kim\\rCategories: cs.CV\\rComments: AAAI 2024\\r\\\\\\\\\\r  Recent advances in vision language pretraining (VLP) have been largely\\rattributed to the large-scale data collected from the web. However, uncurated\\rdataset contains weakly correlated image-text pairs, causing data inefficiency.\\rTo address the issue, knowledge distillation have been explored at the expense\\rof extra image and text momentum encoders to generate teaching signals for\\rmisaligned image-text pairs. In this paper, our goal is to resolve the\\rmisalignment problem with an efficient distillation framework. To this end, we\\rpropose ECLIPSE: Expediting Contrastive Language-Image Pretraining with\\rSelf-distilled Encoders. ECLIPSE features a distinctive distillation\\rarchitecture wherein a shared text encoder is utilized between an online image\\rencoder and a momentum image encoder. This strategic design choice enables the\\rdistillation to operate within a unified projected space of text embedding,\\rresulting in better performance. Based on the unified text embedding space,\\rECLIPSE compensates for the additional computational cost of the momentum image\\rencoder by expediting the online image encoder. Through our extensive\\rexperiments, we validate that there is a sweet spot between expedition and\\rdistillation where the partial view from the expedited online image encoder\\rinteracts complementarily with the momentum teacher. As a result, ECLIPSE\\routperforms its counterparts while achieving substantial acceleration in\\rinference speed.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12659 ,  1797kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12661\\rDate: Tue, 19 Dec 2023 23:22:47 GMT   (856kb,D)\\r\\rTitle: Misalign, Contrast then Distill: Rethinking Misalignments in\\r  Language-Image Pretraining\\rAuthors: Bumsoo Kim, Yeonsik Jo, Jinhyung Kim, Seung Hwan Kim\\rCategories: cs.CV\\rComments: ICCV 2023\\r\\\\\\\\\\r  Contrastive Language-Image Pretraining has emerged as a prominent approach\\rfor training vision and text encoders with uncurated image-text pairs from the\\rweb. To enhance data-efficiency, recent efforts have introduced additional\\rsupervision terms that involve random-augmented views of the image. However,\\rsince the image augmentation process is unaware of its text counterpart, this\\rprocedure could cause various degrees of image-text misalignments during\\rtraining. Prior methods either disregarded this discrepancy or introduced\\rexternal models to mitigate the impact of misalignments during training. In\\rcontrast, we propose a novel metric learning approach that capitalizes on these\\rmisalignments as an additional training source, which we term Misalign,\\rContrast then Distill (MCD). Unlike previous methods that treat augmented\\rimages and their text counterparts as simple positive pairs, MCD predicts the\\rcontinuous scales of misalignment caused by the augmentation. Our extensive\\rexperimental results show that our proposed MCD achieves state-of-the-art\\rtransferability in multiple classification and retrieval downstream datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12661 ,  856kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12664\\rDate: Tue, 19 Dec 2023 23:34:43 GMT   (15183kb,D)\\r\\rTitle: UnionDet: Union-Level Detector Towards Real-Time Human-Object\\r  Interaction Detection\\rAuthors: Bumsoo Kim, Taeho Choi, Jaewoo Kang, Hyunwoo J. Kim\\rCategories: cs.CV\\rComments: ECCV 2020\\r\\\\\\\\\\r  Recent advances in deep neural networks have achieved significant progress in\\rdetecting individual objects from an image. However, object detection is not\\rsufficient to fully understand a visual scene. Towards a deeper visual\\runderstanding, the interactions between objects, especially humans and objects\\rare essential. Most prior works have obtained this information with a bottom-up\\rapproach, where the objects are first detected and the interactions are\\rpredicted sequentially by pairing the objects. This is a major bottleneck in\\rHOI detection inference time. To tackle this problem, we propose UnionDet, a\\rone-stage meta-architecture for HOI detection powered by a novel union-level\\rdetector that eliminates this additional inference stage by directly capturing\\rthe region of interaction. Our one-stage detector for human-object interaction\\rshows a significant reduction in interaction prediction time 4x~14x while\\routperforming state-of-the-art methods on two public datasets: V-COCO and\\rHICO-DET.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12664 ,  15183kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12680\\rDate: Wed, 20 Dec 2023 00:44:04 GMT   (826kb)\\r\\rTitle: Trajectory Approximation of Video Based on Phase Correlation for Forward\\r  Facing Camera\\rAuthors: Abdulkadhem A. Abdulkadhem\\rCategories: cs.CV cs.GR cs.MM cs.RO\\r\\\\\\\\\\r  In this paper, we introduce an innovative approach for extracting\\rtrajectories from a camera sensor in GPS-denied environments, leveraging visual\\rodometry. The system takes video footage captured by a forward-facing camera\\rmounted on a vehicle as input, with the output being a chain code representing\\rthe camera's trajectory. The proposed methodology involves several key steps.\\rFirstly, we employ phase correlation between consecutive frames of the video to\\rextract essential information. Subsequently, we introduce a novel chain code\\rmethod termed dynamic chain code, which is based on the x-shift values\\rderived from the phase correlation. The third step involves determining\\rdirectional changes (forward, left, right) by establishing thresholds and\\rextracting the corresponding chain code. This extracted code is then stored in\\ra buffer for further processing. Notably, our system outperforms traditional\\rmethods reliant on spatial features, exhibiting greater speed and robustness in\\rnoisy environments. Importantly, our approach operates without external camera\\rcalibration information. Moreover, by incorporating visual odometry, our system\\renhances its accuracy in estimating camera motion, providing a more\\rcomprehensive understanding of trajectory dynamics. Finally, the system\\rculminates in the visualization of the normalized camera motion trajectory.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12680 ,  826kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12716\\rDate: Wed, 20 Dec 2023 02:22:49 GMT   (1146kb,D)\\r\\rTitle: BloomVQA: Assessing Hierarchical Multi-modal Comprehension\\rAuthors: Yunye Gong, Robik Shrestha, Jared Claypoole, Michael Cogswell, Arijit\\r  Ray, Christopher Kanan, Ajay Divakaran\\rCategories: cs.CV cs.CL cs.LG\\r\\\\\\\\\\r  We propose a novel VQA dataset, based on picture stories designed for\\reducating young children, that aims to facilitate comprehensive evaluation and\\rcharacterization of vision-language models on comprehension tasks. Unlike\\rcurrent VQA datasets that often focus on fact-based memorization and simple\\rreasoning tasks without principled scientific grounding, we collect data\\rcontaining tasks reflecting different levels of comprehension and underlying\\rcognitive processes, as laid out in Bloom's Taxonomy, a classic framework\\rwidely adopted in education research. The proposed BloomVQA dataset can be\\rmapped to a hierarchical graph-based representation of visual stories, enabling\\rautomatic data augmentation and novel measures characterizing model consistency\\racross the underlying taxonomy. We demonstrate graded evaluation and\\rreliability analysis based on our proposed consistency metrics on\\rstate-of-the-art vision-language models. Our results suggest that, while\\rcurrent models achieve the most gain on low-level comprehension tasks, they\\rgenerally fall short on high-level tasks requiring more advanced comprehension\\rand cognitive skills, as 38.0% drop in VQA accuracy is observed comparing\\rlowest and highest level tasks. Furthermore, current models show consistency\\rpatterns misaligned with human comprehension in various scenarios, suggesting\\remergent structures of model behaviors.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12716 ,  1146kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12720\\rDate: Wed, 20 Dec 2023 02:29:31 GMT   (4393kb,D)\\r\\rTitle: AdvST: Revisiting Data Augmentations for Single Domain Generalization\\rAuthors: Guangtao Zheng, Mengdi Huai, Aidong Zhang\\rCategories: cs.CV\\rComments: Accepted to AAAI 2024\\r\\\\\\\\\\r  Single domain generalization (SDG) aims to train a robust model against\\runknown target domain shifts using data from a single source domain. Data\\raugmentation has been proven an effective approach to SDG. However, the utility\\rof standard augmentations, such as translate, or invert, has not been fully\\rexploited in SDG; practically, these augmentations are used as a part of a data\\rpreprocessing procedure. Although it is intuitive to use many such\\raugmentations to boost the robustness of a model to out-of-distribution domain\\rshifts, we lack a principled approach to harvest the benefit brought from\\rmultiple these augmentations. Here, we conceptualize standard data\\raugmentations with learnable parameters as semantics transformations that can\\rmanipulate certain semantics of a sample, such as the geometry or color of an\\rimage. Then, we propose Adversarial learning with Semantics Transformations\\r(AdvST) that augments the source domain data with semantics transformations and\\rlearns a robust model with the augmented data. We theoretically show that AdvST\\ressentially optimizes a distributionally robust optimization objective defined\\ron a set of semantics distributions induced by the parameters of semantics\\rtransformations. We demonstrate that AdvST can produce samples that expand the\\rcoverage on target domain data. Compared with the state-of-the-art methods,\\rAdvST, despite being a simple method, is surprisingly competitive and achieves\\rthe best average SDG performance on the Digits, PACS, and DomainNet datasets.\\rOur code is available at https://github.com/gtzheng/AdvST.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12720 ,  4393kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12721\\rDate: Wed, 20 Dec 2023 02:30:39 GMT   (4153kb,D)\\r\\rTitle: Cross-Modal Reasoning with Event Correlation for Video Question\\r  Answering\\rAuthors: Chengxiang Yin, Zhengping Che, Kun Wu, Zhiyuan Xu, Qinru Qiu, Jian\\r  Tang\\rCategories: cs.CV\\r\\\\\\\\\\r  Video Question Answering (VideoQA) is a very attractive and challenging\\rresearch direction aiming to understand complex semantics of heterogeneous data\\rfrom two domains, i.e., the spatio-temporal video content and the word sequence\\rin question. Although various attention mechanisms have been utilized to manage\\rcontextualized representations by modeling intra- and inter-modal relationships\\rof the two modalities, one limitation of the predominant VideoQA methods is the\\rlack of reasoning with event correlation, that is, sensing and analyzing\\rrelationships among abundant and informative events contained in the video. In\\rthis paper, we introduce the dense caption modality as a new auxiliary and\\rdistill event-correlated information from it to infer the correct answer. To\\rthis end, we propose a novel end-to-end trainable model, Event-Correlated Graph\\rNeural Networks (EC-GNNs), to perform cross-modal reasoning over information\\rfrom the three modalities (i.e., caption, video, and question). Besides the\\rexploitation of a brand new modality, we employ cross-modal reasoning modules\\rfor explicitly modeling inter-modal relationships and aggregating relevant\\rinformation across different modalities, and we propose a question-guided\\rself-adaptive multi-modal fusion module to collect the question-oriented and\\revent-correlated evidence through multi-step reasoning. We evaluate our model\\ron two widely-used benchmark datasets and conduct an ablation study to justify\\rthe effectiveness of each proposed component.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12721 ,  4153kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12722\\rDate: Wed, 20 Dec 2023 02:34:11 GMT   (761kb,D)\\r\\rTitle: Fine-Grained Knowledge Selection and Restoration for Non-Exemplar Class\\r  Incremental Learning\\rAuthors: Jiang-Tian Zhai, Xialei Liu, Lu Yu, Ming-Ming Cheng\\rCategories: cs.CV\\rComments: to appear at AAAI 2024\\r\\\\\\\\\\r  Non-exemplar class incremental learning aims to learn both the new and old\\rtasks without accessing any training data from the past. This strict\\rrestriction enlarges the difficulty of alleviating catastrophic forgetting\\rsince all techniques can only be applied to current task data. Considering this\\rchallenge, we propose a novel framework of fine-grained knowledge selection and\\rrestoration. The conventional knowledge distillation-based methods place too\\rstrict constraints on the network parameters and features to prevent\\rforgetting, which limits the training of new tasks. To loose this constraint,\\rwe proposed a novel fine-grained selective patch-level distillation to\\radaptively balance plasticity and stability. Some task-agnostic patches can be\\rused to preserve the decision boundary of the old task. While some patches\\rcontaining the important foreground are favorable for learning the new task.\\r  Moreover, we employ a task-agnostic mechanism to generate more realistic\\rprototypes of old tasks with the current task sample for reducing classifier\\rbias for fine-grained knowledge restoration. Extensive experiments on CIFAR100,\\rTinyImageNet and ImageNet-Subset demonstrate the effectiveness of our method.\\rCode is available at https://github.com/scok30/vit-cil.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12722 ,  761kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12723\\rDate: Wed, 20 Dec 2023 02:35:18 GMT   (2236kb,D)\\r\\rTitle: Multi-Clue Reasoning with Memory Augmentation for Knowledge-based Visual\\r  Question Answering\\rAuthors: Chengxiang Yin, Zhengping Che, Kun Wu, Zhiyuan Xu, Jian Tang\\rCategories: cs.CV\\r\\\\\\\\\\r  Visual Question Answering (VQA) has emerged as one of the most challenging\\rtasks in artificial intelligence due to its multi-modal nature. However, most\\rexisting VQA methods are incapable of handling Knowledge-based Visual Question\\rAnswering (KB-VQA), which requires external knowledge beyond visible contents\\rto answer questions about a given image. To address this issue, we propose a\\rnovel framework that endows the model with capabilities of answering more\\rgeneral questions, and achieves a better exploitation of external knowledge\\rthrough generating Multiple Clues for Reasoning with Memory Neural Networks\\r(MCR-MemNN). Specifically, a well-defined detector is adopted to predict\\rimage-question related relation phrases, each of which delivers two\\rcomplementary clues to retrieve the supporting facts from external knowledge\\rbase (KB), which are further encoded into a continuous embedding space using a\\rcontent-addressable memory. Afterwards, mutual interactions between\\rvisual-semantic representation and the supporting facts stored in memory are\\rcaptured to distill the most relevant information in three modalities (i.e.,\\rimage, question, and KB). Finally, the optimal answer is predicted by choosing\\rthe supporting fact with the highest score. We conduct extensive experiments on\\rtwo widely-used benchmarks. The experimental results well justify the\\reffectiveness of MCR-MemNN, as well as its superiority over other KB-VQA\\rmethods.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12723 ,  2236kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12726\\rDate: Wed, 20 Dec 2023 02:50:03 GMT   (1716kb,D)\\r\\rTitle: Reducing Shape-Radiance Ambiguity in Radiance Fields with a Closed-Form\\r  Color Estimation Method\\rAuthors: Qihang Fang, Yafei Song, Keqiang Li, Liefeng Bo\\rCategories: cs.CV\\rComments: This work has been published in NeurIPS 2023\\r\\\\\\\\\\r  Neural radiance field (NeRF) enables the synthesis of cutting-edge realistic\\rnovel view images of a 3D scene. It includes density and color fields to model\\rthe shape and radiance of a scene, respectively. Supervised by the photometric\\rloss in an end-to-end training manner, NeRF inherently suffers from the\\rshape-radiance ambiguity problem, i.e., it can perfectly fit training views but\\rdoes not guarantee decoupling the two fields correctly. To deal with this\\rissue, existing works have incorporated prior knowledge to provide an\\rindependent supervision signal for the density field, including total variation\\rloss, sparsity loss, distortion loss, etc. These losses are based on general\\rassumptions about the density field, e.g., it should be smooth, sparse, or\\rcompact, which are not adaptive to a specific scene. In this paper, we propose\\ra more adaptive method to reduce the shape-radiance ambiguity. The key is a\\rrendering method that is only based on the density field. Specifically, we\\rfirst estimate the color field based on the density field and posed images in a\\rclosed form. Then NeRF's rendering process can proceed. We address the problems\\rin estimating the color field, including occlusion and non-uniformly\\rdistributed views. Afterward, it is applied to regularize NeRF's density field.\\rAs our regularization is guided by photometric loss, it is more adaptive\\rcompared to existing ones. Experimental results show that our method improves\\rthe density field of NeRF both qualitatively and quantitatively. Our code is\\ravailable at https://github.com/qihangGH/Closed-form-color-field.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12726 ,  1716kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12729\\rDate: Wed, 20 Dec 2023 02:57:21 GMT   (2500kb,D)\\r\\rTitle: Segment Anything Model Meets Image Harmonization\\rAuthors: Haoxing Chen and Yaohui Li and Zhangxuan Gu and Zhuoer Xu and Jun Lan\\r  and Huaxiong Li\\rCategories: cs.CV\\rComments: Accepted by ICASSP 2024\\r\\\\\\\\\\r  Image harmonization is a crucial technique in image composition that aims to\\rseamlessly match the background by adjusting the foreground of composite\\rimages. Current methods adopt either global-level or pixel-level feature\\rmatching. Global-level feature matching ignores the proximity prior, treating\\rforeground and background as separate entities. On the other hand, pixel-level\\rfeature matching loses contextual information. Therefore, it is necessary to\\ruse the information from semantic maps that describe different objects to guide\\rharmonization. In this paper, we propose Semantic-guided Region-aware Instance\\rNormalization (SRIN) that can utilize the semantic segmentation maps output by\\ra pre-trained Segment Anything Model (SAM) to guide the visual consistency\\rlearning of foreground and background features. Abundant experiments\\rdemonstrate the superiority of our method for image harmonization over\\rstate-of-the-art methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12729 ,  2500kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12730\\rDate: Wed, 20 Dec 2023 02:58:25 GMT   (3614kb,D)\\r\\rTitle: A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models\\rAuthors: Julio Silva-Rodriguez and Sina Hajimiri and Ismail Ben Ayed and Jose\\r  Dolz\\rCategories: cs.CV\\rComments: Code available at https://github.com/jusiro/CLAP\\r\\\\\\\\\\r  Efficient transfer learning (ETL) is receiving increasing attention to adapt\\rlarge pre-trained language-vision models on downstream tasks with a few labeled\\rsamples. While significant progress has been made, we reveal that\\rstate-of-the-art ETL approaches exhibit strong performance only in\\rnarrowly-defined experimental setups, and with a careful adjustment of\\rhyperparameters based on a large corpus of labeled samples. In particular, we\\rmake two interesting, and surprising empirical observations. First, to\\routperform a simple Linear Probing baseline, these methods require to optimize\\rtheir hyper-parameters on each target task. And second, they typically\\runderperform -- sometimes dramatically -- standard zero-shot predictions in the\\rpresence of distributional drifts. Motivated by the unrealistic assumptions\\rmade in the existing literature, i.e., access to a large validation set and\\rcase-specific grid-search for optimal hyperparameters, we propose a novel\\rapproach that meets the requirements of real-world scenarios. More concretely,\\rwe introduce a CLass-Adaptive linear Probe (CLAP) objective, whose balancing\\rterm is optimized via an adaptation of the general Augmented Lagrangian method\\rtailored to this context. We comprehensively evaluate CLAP on a broad span of\\rdatasets and scenarios, demonstrating that it consistently outperforms SoTA\\rapproaches, while yet being a much more efficient alternative.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12730 ,  3614kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12735\\rDate: Wed, 20 Dec 2023 03:16:34 GMT   (4917kb)\\r\\rTitle: MetaSegNet: Metadata-collaborative Vision-Language Representation\\r  Learning for Semantic Segmentation of Remote Sensing Images\\rAuthors: Libo Wang and Sijun Dong and Ying Chen and Xiaoliang Meng and Shenghui\\r  Fang\\rCategories: cs.CV\\r\\\\\\\\\\r  Semantic segmentation of remote sensing images plays a vital role in a wide\\rrange of Earth Observation (EO) applications, such as land use land cover\\rmapping, environment monitoring, and sustainable development. Driven by rapid\\rdevelopments in Artificial Intelligence (AI), deep learning (DL) has emerged as\\rthe mainstream tool for semantic segmentation and achieved many breakthroughs\\rin the field of remote sensing. However, the existing DL-based methods mainly\\rfocus on unimodal visual data while ignoring the rich multimodal information\\rinvolved in the real world, usually demonstrating weak reliability and\\rgenerlization. Inspired by the success of Vision Transformers and large\\rlanguage models, we propose a novel metadata-collaborative multimodal\\rsegmentation network (MetaSegNet) that applies vision-language representation\\rlearning for semantic segmentation of remote sensing images. Unlike the common\\rmodel structure that only uses unimodal visual data, we extract the key\\rcharacteristic (i.e. the climate zone) from freely available remote sensing\\rimage metadata and transfer it into knowledge-based text prompts via the\\rgeneric ChatGPT. Then, we construct an image encoder, a text encoder and a\\rcrossmodal attention fusion subnetwork to extract the image and text feature\\rand apply image-text interaction. Benefiting from such a design, the proposed\\rMetaSegNet demonstrates superior generalization and achieves competitive\\raccuracy with state-of-the-art semantic segmentation methods on the large-scale\\rOpenEarthMap dataset (68.6% mIoU) and Potsdam dataset (93.3% mean F1 score) as\\rwell as LoveDA dataset (52.2% mIoU).\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12735 ,  4917kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12742\\rDate: Wed, 20 Dec 2023 03:30:51 GMT   (2212kb,D)\\r\\rTitle: Cached Transformers: Improving Transformers with Differentiable Memory\\r  Cache\\rAuthors: Zhaoyang Zhang, Wenqi Shao, Yixiao Ge, Xiaogang Wang, Jinwei Gu, Ping\\r  Luo\\rCategories: cs.CV\\rComments: AAAI 2024\\r\\\\\\\\\\r  This work introduces a new Transformer model called Cached Transformer, which\\ruses Gated Recurrent Cached (GRC) attention to extend the self-attention\\rmechanism with a differentiable memory cache of tokens. GRC attention enables\\rattending to both past and current tokens, increasing the receptive field of\\rattention and allowing for exploring long-range dependencies. By utilizing a\\rrecurrent gating unit to continuously update the cache, our model achieves\\rsignificant advancements in \\\\textbf{six} language and vision tasks, including\\rlanguage modeling, machine translation, ListOPs, image classification, object\\rdetection, and instance segmentation. Furthermore, our approach surpasses\\rprevious memory-based techniques in tasks such as language modeling and\\rdisplays the ability to be applied to a broader range of situations.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12742 ,  2212kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12743\\rDate: Wed, 20 Dec 2023 03:34:48 GMT   (1155kb,D)\\r\\rTitle: PointeNet: A Lightweight Framework for Effective and Efficient Point\\r  Cloud Analysis\\rAuthors: Lipeng Gu, Xuefeng Yan, Liangliang Nan, Dingkun Zhu, Honghua Chen,\\r  Weiming Wang, Mingqiang Wei\\rCategories: cs.CV\\r\\\\\\\\\\r  Current methodologies in point cloud analysis predominantly explore 3D\\rgeometries, often achieved through the introduction of intricate learnable\\rgeometric extractors in the encoder or by deepening networks with repeated\\rblocks. However, these approaches inevitably lead to a significant number of\\rlearnable parameters, resulting in substantial computational costs and imposing\\rmemory burdens on CPU/GPU. Additionally, the existing strategies are primarily\\rtailored for object-level point cloud classification and segmentation tasks,\\rwith limited extensions to crucial scene-level applications, such as autonomous\\rdriving. In response to these limitations, we introduce PointeNet, an efficient\\rnetwork designed specifically for point cloud analysis. PointeNet distinguishes\\ritself with its lightweight architecture, low training cost, and plug-and-play\\rcapability, effectively capturing representative features. The network consists\\rof a Multivariate Geometric Encoding (MGE) module and an optional\\rDistance-aware Semantic Enhancement (DSE) module. The MGE module employs\\roperations of sampling, grouping, and multivariate geometric aggregation to\\rlightweightly capture and adaptively aggregate multivariate geometric features,\\rproviding a comprehensive depiction of 3D geometries. The DSE module, designed\\rfor real-world autonomous driving scenarios, enhances the semantic perception\\rof point clouds, particularly for distant points. Our method demonstrates\\rflexibility by seamlessly integrating with a classification/segmentation head\\ror embedding into off-the-shelf 3D object detection networks, achieving notable\\rperformance improvements at a minimal cost. Extensive experiments on\\robject-level datasets, including ModelNet40, ScanObjectNN, ShapeNetPart, and\\rthe scene-level dataset KITTI, demonstrate the superior performance of\\rPointeNet over state-of-the-art methods in point cloud analysis.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12743 ,  1155kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12754\\rDate: Wed, 20 Dec 2023 04:27:13 GMT   (546kb,D)\\r\\rTitle: Spectral Prompt Tuning:Unveiling Unseen Classes for Zero-Shot Semantic\\r  Segmentation\\rAuthors: Wenhao Xu, Rongtao Xu, Changwei Wang, Shibiao Xu, Li Guo, Man Zhang,\\r  Xiaopeng Zhang\\rCategories: cs.CV cs.CL\\rComments: AAAI2024 Accepted\\r\\\\\\\\\\r  Recently, CLIP has found practical utility in the domain of pixel-level\\rzero-shot segmentation tasks. The present landscape features two-stage\\rmethodologies beset by issues such as intricate pipelines and elevated\\rcomputational costs. While current one-stage approaches alleviate these\\rconcerns and incorporate Visual Prompt Training (VPT) to uphold CLIP's\\rgeneralization capacity, they still fall short in fully harnessing CLIP's\\rpotential for pixel-level unseen class demarcation and precise pixel\\rpredictions. To further stimulate CLIP's zero-shot dense prediction capability,\\rwe propose SPT-SEG, a one-stage approach that improves CLIP's adaptability from\\rimage to pixel. Specifically, we initially introduce Spectral Prompt Tuning\\r(SPT), incorporating spectral prompts into the CLIP visual encoder's shallow\\rlayers to capture structural intricacies of images, thereby enhancing\\rcomprehension of unseen classes. Subsequently, we introduce the Spectral Guided\\rDecoder (SGD), utilizing both high and low-frequency information to steer the\\rnetwork's spatial focus towards more prominent classification features,\\renabling precise pixel-level prediction outcomes. Through extensive experiments\\ron two public datasets, we demonstrate the superiority of our method over\\rstate-of-the-art approaches, performing well across all classes and\\rparticularly excelling in handling unseen classes. Code is available\\rat:https://github.com/clearxu/SPT.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12754 ,  546kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12763\\rDate: Wed, 20 Dec 2023 04:49:45 GMT   (2735kb,D)\\r\\rTitle: AMD:Anatomical Motion Diffusion with Interpretable Motion Decomposition\\r  and Fusion\\rAuthors: Beibei Jing, Youjia Zhang, Zikai Song, Junqing Yu, Wei Yang\\rCategories: cs.CV\\r\\\\\\\\\\r  Generating realistic human motion sequences from text descriptions is a\\rchallenging task that requires capturing the rich expressiveness of both\\rnatural language and human motion.Recent advances in diffusion models have\\renabled significant progress in human motion synthesis.However, existing\\rmethods struggle to handle text inputs that describe complex or long motions.In\\rthis paper, we propose the Adaptable Motion Diffusion (AMD) model, which\\rleverages a Large Language Model (LLM) to parse the input text into a sequence\\rof concise and interpretable anatomical scripts that correspond to the target\\rmotion.This process exploits the LLM's ability to provide anatomical guidance\\rfor complex motion synthesis.We then devise a two-branch fusion scheme that\\rbalances the influence of the input text and the anatomical scripts on the\\rinverse diffusion process, which adaptively ensures the semantic fidelity and\\rdiversity of the synthesized motion.Our method can effectively handle texts\\rwith complex or long motion descriptions, where existing methods often fail.\\rExperiments on datasets with relatively more complex motions, such as CLCD1 and\\rCLCD2, demonstrate that our AMD significantly outperforms existing\\rstate-of-the-art models.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12763 ,  2735kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12768\\rDate: Wed, 20 Dec 2023 05:06:01 GMT   (4635kb,D)\\r\\rTitle: Mutual-modality Adversarial Attack with Semantic Perturbation\\rAuthors: Jingwen Ye, Ruonan Yu, Songhua Liu, Xinchao Wang\\rCategories: cs.CV\\rComments: Accepted by AAAI2024\\r\\\\\\\\\\r  Adversarial attacks constitute a notable threat to machine learning systems,\\rgiven their potential to induce erroneous predictions and classifications.\\rHowever, within real-world contexts, the essential specifics of the deployed\\rmodel are frequently treated as a black box, consequently mitigating the\\rvulnerability to such attacks. Thus, enhancing the transferability of the\\radversarial samples has become a crucial area of research, which heavily relies\\ron selecting appropriate surrogate models. To address this challenge, we\\rpropose a novel approach that generates adversarial attacks in a\\rmutual-modality optimization scheme. Our approach is accomplished by leveraging\\rthe pre-trained CLIP model. Firstly, we conduct a visual attack on the clean\\rimage that causes semantic perturbations on the aligned embedding space with\\rthe other textual modality. Then, we apply the corresponding defense on the\\rtextual modality by updating the prompts, which forces the re-matching on the\\rperturbed embedding space. Finally, to enhance the attack transferability, we\\rutilize the iterative training strategy on the visual attack and the textual\\rdefense, where the two processes optimize from each other. We evaluate our\\rapproach on several benchmark datasets and demonstrate that our mutual-modal\\rattack strategy can effectively produce high-transferable attacks, which are\\rstable regardless of the target networks. Our approach outperforms\\rstate-of-the-art attack methods and can be readily deployed as a plug-and-play\\rsolution.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12768 ,  4635kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12773\\rDate: Wed, 20 Dec 2023 05:17:06 GMT   (4950kb)\\r\\rTitle: Segmenting Messy Text: Detecting Boundaries in Text Derived from\\r  Historical Newspaper Images\\rAuthors: Carol Anderson and Phil Crone (Ancestry.com)\\rCategories: cs.CV cs.CL cs.LG\\rComments: 8 pages, 4 figures\\rACM-class: I.2.7; I.7.5\\rJournal-ref: 2020 25th International Conference on Pattern Recognition (ICPR),\\r  Milan, Italy, 2021, pp. 5543-5550\\rDOI: 10.1109/ICPR48806.2021.9413279\\r\\\\\\\\\\r  Text segmentation, the task of dividing a document into sections, is often a\\rprerequisite for performing additional natural language processing tasks.\\rExisting text segmentation methods have typically been developed and tested\\rusing clean, narrative-style text with segments containing distinct topics.\\rHere we consider a challenging text segmentation task: dividing newspaper\\rmarriage announcement lists into units of one announcement each. In many cases\\rthe information is not structured into sentences, and adjacent segments are not\\rtopically distinct from each other. In addition, the text of the announcements,\\rwhich is derived from images of historical newspapers via optical character\\rrecognition, contains many typographical errors. As a result, these\\rannouncements are not amenable to segmentation with existing techniques. We\\rpresent a novel deep learning-based model for segmenting such text and show\\rthat it significantly outperforms an existing state-of-the-art method on our\\rtask.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12773 ,  4950kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12804\\rDate: Wed, 20 Dec 2023 06:52:38 GMT   (3245kb,D)\\r\\rTitle: Multi-stages attention Breast cancer classification based on nonlinear\\r  spiking neural P neurons with autapses\\rAuthors: Bo Yang, Hong Peng, Xiaohui Luo, Jun Wang, Xianzhong Long\\rCategories: cs.CV\\r\\\\\\\\\\r  Breast cancer(BC) is a prevalent type of malignant tumor in women. Early\\rdiagnosis and treatment are vital for enhancing the patients' survival rate.\\rDownsampling in deep networks may lead to loss of information, so for\\rcompensating the detail and edge information and allowing convolutional neural\\rnetworks to pay more attention to seek the lesion region, we propose a\\rmulti-stages attention architecture based on NSNP neurons with autapses. First,\\runlike the single-scale attention acquisition methods of existing methods, we\\rset up spatial attention acquisition at each feature map scale of the\\rconvolutional network to obtain an fusion global information on attention\\rguidance. Then we introduce a new type of NSNP variants called NSNP neurons\\rwith autapses. Specifically, NSNP systems are modularized as feature encoders,\\rrecoding the features extracted from convolutional neural network as well as\\rthe fusion of attention information and preserve the key characteristic\\relements in feature maps. This ensures the retention of valuable data while\\rgradually transforming high-dimensional complicated info into low-dimensional\\rones. The proposed method is evaluated on the public dataset BreakHis at\\rvarious magnifications and classification tasks. It achieves a classification\\raccuracy of 96.32% at all magnification cases, outperforming state-of-the-art\\rmethods. Ablation studies are also performed, verifying the proposed model's\\refficacy. The source code is available at\\rXhuBobYoung/Breast-cancer-Classification.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12804 ,  3245kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12807\\rDate: Wed, 20 Dec 2023 07:04:33 GMT   (7604kb,D)\\r\\rTitle: All but One: Surgical Concept Erasing with Model Preservation in\\r  Text-to-Image Diffusion Models\\rAuthors: Seunghoo Hong, Juhun Lee, Simon S. Woo\\rCategories: cs.CV cs.AI\\rComments: Main paper with supplementary materials\\r\\\\\\\\\\r  Text-to-Image models such as Stable Diffusion have shown impressive image\\rgeneration synthesis, thanks to the utilization of large-scale datasets.\\rHowever, these datasets may contain sexually explicit, copyrighted, or\\rundesirable content, which allows the model to directly generate them. Given\\rthat retraining these large models on individual concept deletion requests is\\rinfeasible, fine-tuning algorithms have been developed to tackle concept\\rerasing in diffusion models. While these algorithms yield good concept erasure,\\rthey all present one of the following issues: 1) the corrupted feature space\\ryields synthesis of disintegrated objects, 2) the initially synthesized content\\rundergoes a divergence in both spatial structure and semantics in the generated\\rimages, and 3) sub-optimal training updates heighten the model's susceptibility\\rto utility harm. These issues severely degrade the original utility of\\rgenerative models. In this work, we present a new approach that solves all of\\rthese challenges. We take inspiration from the concept of classifier guidance\\rand propose a surgical update on the classifier guidance term while\\rconstraining the drift of the unconditional score term. Furthermore, our\\ralgorithm empowers the user to select an alternative to the erasing concept,\\rallowing for more controllability. Our experimental results show that our\\ralgorithm not only erases the target concept effectively but also preserves the\\rmodel's generation capability.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12807 ,  7604kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12815\\rDate: Wed, 20 Dec 2023 07:34:20 GMT   (495kb,D)\\r\\rTitle: OCTOPUS: Open-vocabulary Content Tracking and Object Placement Using\\r  Semantic Understanding in Mixed Reality\\rAuthors: Luke Yoffe, Aditya Sharma, Tobias H\\\\ollerer\\rCategories: cs.CV cs.AI cs.CL\\rComments: IEEE International Symposium on Mixed and Augmented Reality (ISMAR)\\r  2023\\r\\\\\\\\\\r  One key challenge in augmented reality is the placement of virtual content in\\rnatural locations. Existing automated techniques are only able to work with a\\rclosed-vocabulary, fixed set of objects. In this paper, we introduce a new\\ropen-vocabulary method for object placement. Our eight-stage pipeline leverages\\rrecent advances in segmentation models, vision-language models, and LLMs to\\rplace any virtual object in any AR camera frame or scene. In a preliminary user\\rstudy, we show that our method performs at least as well as human experts 57%\\rof the time.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12815 ,  495kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12816\\rDate: Wed, 20 Dec 2023 07:36:38 GMT   (1166kb,D)\\r\\rTitle: Object-aware Adaptive-Positivity Learning for Audio-Visual Question\\r  Answering\\rAuthors: Zhangbin Li, Dan Guo, Jinxing Zhou, Jing Zhang, Meng Wang\\rCategories: cs.CV\\rComments: Accepted by AAAI-2024\\r\\\\\\\\\\r  This paper focuses on the Audio-Visual Question Answering (AVQA) task that\\raims to answer questions derived from untrimmed audible videos. To generate\\raccurate answers, an AVQA model is expected to find the most informative\\raudio-visual clues relevant to the given questions. In this paper, we propose\\rto explicitly consider fine-grained visual objects in video frames\\r(object-level clues) and explore the multi-modal relations(i.e., the object,\\raudio, and question) in terms of feature interaction and model optimization.\\rFor the former, we present an end-to-end object-oriented network that adopts a\\rquestion-conditioned clue discovery module to concentrate audio/visual\\rmodalities on respective keywords of the question and designs a\\rmodality-conditioned clue collection module to highlight closely associated\\raudio segments or visual objects. For model optimization, we propose an\\robject-aware adaptive-positivity learning strategy that selects the highly\\rsemantic-matched multi-modal pair as positivity. Specifically, we design two\\robject-aware contrastive loss functions to identify the highly relevant\\rquestion-object pairs and audio-object pairs, respectively. These selected\\rpairs are constrained to have larger similarity values than the mismatched\\rpairs. The positivity-selecting process is adaptive as the positivity pairs\\rselected in each video frame may be different. These two object-aware\\robjectives help the model understand which objects are exactly relevant to the\\rquestion and which are making sounds. Extensive experiments on the MUSIC-AVQA\\rdataset demonstrate the proposed method is effective in finding favorable\\raudio-visual clues and also achieves new state-of-the-art question-answering\\rperformance.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12816 ,  1166kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12826\\rDate: Wed, 20 Dec 2023 08:05:57 GMT   (45200kb,D)\\r\\rTitle: ReCo-Diff: Explore Retinex-Based Condition Strategy in Diffusion Model\\r  for Low-Light Image Enhancement\\rAuthors: Yuhui Wu, Guoqing Wang, Zhiwen Wang, Yang Yang, Tianyu Li, Peng Wang,\\r  Chongyi Li, Heng Tao Shen\\rCategories: cs.CV\\r\\\\\\\\\\r  Low-light image enhancement (LLIE) has achieved promising performance by\\remploying conditional diffusion models. In this study, we propose ReCo-Diff, a\\rnovel approach that incorporates Retinex-based prior as an additional\\rpre-processing condition to regulate the generating capabilities of the\\rdiffusion model. ReCo-Diff first leverages a pre-trained decomposition network\\rto produce initial reflectance and illumination maps of the low-light image.\\rThen, an adjustment network is introduced to suppress the noise in the\\rreflectance map and brighten the illumination map, thus forming the learned\\rRetinex-based condition. The condition is integrated into a refinement network,\\rimplementing Retinex-based conditional modules that offer sufficient guidance\\rat both feature- and image-levels. By treating Retinex theory as a condition,\\rReCo-Diff presents a unique perspective for establishing an LLIE-specific\\rdiffusion model. Extensive experiments validate the rationality and superiority\\rof our ReCo-Diff approach. The code will be made publicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12826 ,  45200kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12828\\rDate: Wed, 20 Dec 2023 08:15:40 GMT   (5100kb,D)\\r\\rTitle: TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary\\r  Multi-Label Classification of CLIP Without Training\\rAuthors: Yuqi Lin, Minghao Chen, Kaipeng Zhang, Hengjia Li, Mingming Li, Zheng\\r  Yang, Dongqin Lv, Binbin Lin, Haifeng Liu, Deng Cai\\rCategories: cs.CV cs.AI\\rComments: Accepted by AAAI2024\\r\\\\\\\\\\r  Contrastive Language-Image Pre-training (CLIP) has demonstrated impressive\\rcapabilities in open-vocabulary classification. The class token in the image\\rencoder is trained to capture the global features to distinguish different text\\rdescriptions supervised by contrastive loss, making it highly effective for\\rsingle-label classification. However, it shows poor performance on multi-label\\rdatasets because the global feature tends to be dominated by the most prominent\\rclass and the contrastive nature of softmax operation aggravates it. In this\\rstudy, we observe that the multi-label classification results heavily rely on\\rdiscriminative local features but are overlooked by CLIP. As a result, we\\rdissect the preservation of patch-wise spatial information in CLIP and proposed\\ra local-to-global framework to obtain image tags. It comprises three steps: (1)\\rpatch-level classification to obtain coarse scores; (2) dual-masking attention\\rrefinement (DMAR) module to refine the coarse scores; (3) class-wise\\rreidentification (CWR) module to remedy predictions from a global perspective.\\rThis framework is solely based on frozen CLIP and significantly enhances its\\rmulti-label classification performance on various benchmarks without\\rdataset-specific training. Besides, to comprehensively assess the quality and\\rpracticality of generated tags, we extend their application to the downstream\\rtask, i.e., weakly supervised semantic segmentation (WSSS) with generated tags\\ras image-level pseudo labels. Experiments demonstrate that this\\rclassify-then-segment paradigm dramatically outperforms other annotation-free\\rsegmentation methods and validates the effectiveness of generated tags. Our\\rcode is available at https://github.com/linyq2117/TagCLIP.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12828 ,  5100kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12856\\rDate: Wed, 20 Dec 2023 09:19:48 GMT   (4595kb,D)\\r\\rTitle: SkyScript: A Large and Semantically Diverse Vision-Language Dataset for\\r  Remote Sensing\\rAuthors: Zhecheng Wang, Rajanie Prabha, Tianyuan Huang, Jiajun Wu, Ram\\r  Rajagopal\\rCategories: cs.CV cs.AI cs.LG\\rComments: Accepted by AAAI 2024\\r\\\\\\\\\\r  Remote sensing imagery, despite its broad applications in helping achieve\\rSustainable Development Goals and tackle climate change, has not yet benefited\\rfrom the recent advancements of versatile, task-agnostic vision language models\\r(VLMs). A key reason is that the large-scale, semantically diverse image-text\\rdataset required for developing VLMs is still absent for remote sensing images.\\rUnlike natural images, remote sensing images and their associated text\\rdescriptions cannot be efficiently collected from the public Internet at scale.\\rIn this work, we bridge this gap by using geo-coordinates to automatically\\rconnect open, unlabeled remote sensing images with rich semantics covered in\\rOpenStreetMap, and thus construct SkyScript, a comprehensive vision-language\\rdataset for remote sensing images, comprising 2.6 million image-text pairs\\rcovering 29K distinct semantic tags. With continual pre-training on this\\rdataset, we obtain a VLM that surpasses baseline models with a 6.2% average\\raccuracy gain in zero-shot scene classification across seven benchmark\\rdatasets. It also demonstrates the ability of zero-shot transfer for\\rfine-grained object attribute classification and cross-modal retrieval. We hope\\rthis dataset can support the advancement of VLMs for various multi-modal tasks\\rin remote sensing, such as open-vocabulary classification, retrieval,\\rcaptioning, and text-to-image synthesis.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12856 ,  4595kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12865\\rDate: Wed, 20 Dec 2023 09:27:41 GMT   (12508kb,D)\\r\\rTitle: RadEdit: stress-testing biomedical vision models via diffusion image\\r  editing\\rAuthors: Fernando P\\\\'erez-Garc\\\\'ia and Sam Bond-Taylor and Pedro P. Sanchez and\\r  Boris van Breugel and Daniel C. Castro and Harshita Sharma and Valentina\\r  Salvatelli and Maria T. A. Wetscherek and Hannah Richardson and Matthew P.\\r  Lungren and Aditya Nori and Javier Alvarez-Valle and Ozan Oktay and\\r  Maximilian Ilse\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Biomedical imaging datasets are often small and biased, meaning that\\rreal-world performance of predictive models can be substantially lower than\\rexpected from internal testing. This work proposes using generative image\\rediting to simulate dataset shifts and diagnose failure modes of biomedical\\rvision models; this can be used in advance of deployment to assess readiness,\\rpotentially reducing cost and patient harm. Existing editing methods can\\rproduce undesirable changes, with spurious correlations learned due to the\\rco-occurrence of disease and treatment interventions, limiting practical\\rapplicability. To address this, we train a text-to-image diffusion model on\\rmultiple chest X-ray datasets and introduce a new editing method RadEdit that\\ruses multiple masks, if present, to constrain changes and ensure consistency in\\rthe edited images. We consider three types of dataset shifts: acquisition\\rshift, manifestation shift, and population shift, and demonstrate that our\\rapproach can diagnose failures and quantify model robustness without additional\\rdata collection, complementing more qualitative tools for explainable AI.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12865 ,  12508kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12870\\rDate: Wed, 20 Dec 2023 09:34:22 GMT   (38171kb,D)\\r\\rTitle: The Audio-Visual Conversational Graph: From an Egocentric-Exocentric\\r  Perspective\\rAuthors: Wenqi Jia, Miao Liu, Hao Jiang, Ishwarya Ananthabhotla, James M. Rehg,\\r  Vamsi Krishna Ithapu, Ruohan Gao\\rCategories: cs.CV\\r\\\\\\\\\\r  In recent years, the thriving development of research related to egocentric\\rvideos has provided a unique perspective for the study of conversational\\rinteractions, where both visual and audio signals play a crucial role. While\\rmost prior work focus on learning about behaviors that directly involve the\\rcamera wearer, we introduce the Ego-Exocentric Conversational Graph Prediction\\rproblem, marking the first attempt to infer exocentric conversational\\rinteractions from egocentric videos. We propose a unified multi-modal,\\rmulti-task framework -- Audio-Visual Conversational Attention (Av-CONV), for\\rthe joint prediction of conversation behaviors -- speaking and listening -- for\\rboth the camera wearer as well as all other social partners present in the\\regocentric video. Specifically, we customize the self-attention mechanism to\\rmodel the representations across-time, across-subjects, and across-modalities.\\rTo validate our method, we conduct experiments on a challenging egocentric\\rvideo dataset that includes first-person perspective, multi-speaker, and\\rmulti-conversation scenarios. Our results demonstrate the superior performance\\rof our method compared to a series of baselines. We also present detailed\\rablation studies to assess the contribution of each component in our model.\\rProject page: https://vjwq.github.io/AV-CONV/.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12870 ,  38171kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12872\\rDate: Wed, 20 Dec 2023 09:37:06 GMT   (284kb)\\r\\rTitle: Integration and Performance Analysis of Artificial Intelligence and\\r  Computer Vision Based on Deep Learning Algorithms\\rAuthors: Bo Liu, Liqiang Yu, Chang Che, Qunwei Lin, Hao Hu, Xinyu Zhao\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  This paper focuses on the analysis of the application effectiveness of the\\rintegration of deep learning and computer vision technologies. Deep learning\\rachieves a historic breakthrough by constructing hierarchical neural networks,\\renabling end-to-end feature learning and semantic understanding of images. The\\rsuccessful experiences in the field of computer vision provide strong support\\rfor training deep learning algorithms. The tight integration of these two\\rfields has given rise to a new generation of advanced computer vision systems,\\rsignificantly surpassing traditional methods in tasks such as machine vision\\rimage classification and object detection. In this paper, typical image\\rclassification cases are combined to analyze the superior performance of deep\\rneural network models while also pointing out their limitations in\\rgeneralization and interpretability, proposing directions for future\\rimprovements. Overall, the efficient integration and development trend of deep\\rlearning with massive visual data will continue to drive technological\\rbreakthroughs and application expansion in the field of computer vision, making\\rit possible to build truly intelligent machine vision systems. This deepening\\rfusion paradigm will powerfully promote unprecedented tasks and functions in\\rcomputer vision, providing stronger development momentum for related\\rdisciplines and industries.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12872 ,  284kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12877\\rDate: Wed, 20 Dec 2023 09:39:55 GMT   (8484kb,D)\\r\\rTitle: Relightable and Animatable Neural Avatars from Videos\\rAuthors: Wenbin Lin, Chengwei Zheng, Jun-Hai Yong, Feng Xu\\rCategories: cs.CV\\rComments: Accepted by AAAI 2024\\r\\\\\\\\\\r  Lightweight creation of 3D digital avatars is a highly desirable but\\rchallenging task. With only sparse videos of a person under unknown\\rillumination, we propose a method to create relightable and animatable neural\\ravatars, which can be used to synthesize photorealistic images of humans under\\rnovel viewpoints, body poses, and lighting. The key challenge here is to\\rdisentangle the geometry, material of the clothed body, and lighting, which\\rbecomes more difficult due to the complex geometry and shadow changes caused by\\rbody motions. To solve this ill-posed problem, we propose novel techniques to\\rbetter model the geometry and shadow changes. For geometry change modeling, we\\rpropose an invertible deformation field, which helps to solve the inverse\\rskinning problem and leads to better geometry quality. To model the spatial and\\rtemporal varying shading cues, we propose a pose-aware part-wise light\\rvisibility network to estimate light occlusion. Extensive experiments on\\rsynthetic and real datasets show that our approach reconstructs high-quality\\rgeometry and generates realistic shadows under different body poses. Code and\\rdata are available at\\r\\\\url{https://wenbin-lin.github.io/RelightableAvatar-page/}.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12877 ,  8484kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12908\\rDate: Wed, 20 Dec 2023 10:45:22 GMT   (807kb,D)\\r\\rTitle: The Common Optical Music Recognition Evaluation Framework\\rAuthors: Pau Torras and Sanket Biswas and Alicia Forn\\\\'es\\rCategories: cs.CV\\rComments: 18 pages, 4 figures, 3 tables, submitted (under review) for the\\r  International Journal in Document Analysis and Recognition\\rACM-class: I.4.9; J.5\\r\\\\\\\\\\r  The quality of Optical Music Recognition (OMR) systems is a rather difficult\\rmagnitude to measure. There is no lingua franca shared among OMR datasets that\\rallows to compare systems' performance on equal grounds, since most of them are\\rspecialised on certain approaches. As a result, most state-of-the-art works\\rcurrently report metrics that cannot be compared directly. In this paper we\\ridentify the need of a common music representation language and propose the\\rMusic Tree Notation (MTN) format, thanks to which the definition of standard\\rmetrics is possible. This format represents music as a set of primitives that\\rgroup together into higher-abstraction nodes, a compromise between the\\rexpression of fully graph-based and sequential notation formats. We have also\\rdeveloped a specific set of OMR metrics and a typeset score dataset as a proof\\rof concept of this idea.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12908 ,  807kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12913\\rDate: Wed, 20 Dec 2023 10:49:49 GMT   (2632kb,D)\\r\\rTitle: Produce Once, Utilize Twice for Anomaly Detection\\rAuthors: Shuyuan Wang, Qi Li, Huiyuan Luo, Chengkan Lv, Zhengtao Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  Visual anomaly detection aims at classifying and locating the regions that\\rdeviate from the normal appearance. Embedding-based methods and\\rreconstruction-based methods are two main approaches for this task. However,\\rthey are either not efficient or not precise enough for the industrial\\rdetection. To deal with this problem, we derive POUTA (Produce Once Utilize\\rTwice for Anomaly detection), which improves both the accuracy and efficiency\\rby reusing the discriminant information potential in the reconstructive\\rnetwork. We observe that the encoder and decoder representations of the\\rreconstructive network are able to stand for the features of the original and\\rreconstructed image respectively. And the discrepancies between the symmetric\\rreconstructive representations provides roughly accurate anomaly information.\\rTo refine this information, a coarse-to-fine process is proposed in POUTA,\\rwhich calibrates the semantics of each discriminative layer by the high-level\\rrepresentations and supervision loss. Equipped with the above modules, POUTA is\\rendowed with the ability to provide a more precise anomaly location than the\\rprior arts. Besides, the representation reusage also enables to exclude the\\rfeature extraction process in the discriminative network, which reduces the\\rparameters and improves the efficiency. Extensive experiments show that, POUTA\\ris superior or comparable to the prior methods with even less cost.\\rFurthermore, POUTA also achieves better performance than the state-of-the-art\\rfew-shot anomaly detection methods without any special design, showing that\\rPOUTA has strong ability to learn representations inherent in the training\\rdata.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12913 ,  2632kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12917\\rDate: Wed, 20 Dec 2023 10:53:06 GMT   (3046kb,D)\\r\\rTitle: Sign Language Production with Latent Motion Transformer\\rAuthors: Pan Xie, Taiyi Peng, Yao Du, Qipeng Zhang\\rCategories: cs.CV cs.AI\\rComments: Accepted by WACV2024\\r\\\\\\\\\\r  Sign Language Production (SLP) is the tough task of turning sign language\\rinto sign videos. The main goal of SLP is to create these videos using a sign\\rgloss. In this research, we've developed a new method to make high-quality sign\\rvideos without using human poses as a middle step. Our model works in two main\\rparts: first, it learns from a generator and the video's hidden features, and\\rnext, it uses another model to understand the order of these hidden features.\\rTo make this method even better for sign videos, we make several significant\\rimprovements. (i) In the first stage, we take an improved 3D VQ-GAN to learn\\rdownsampled latent representations. (ii) In the second stage, we introduce\\rsequence-to-sequence attention to better leverage conditional information.\\r(iii) The separated two-stage training discards the realistic visual semantic\\rof the latent codes in the second stage. To endow the latent sequences semantic\\rinformation, we extend the token-level autoregressive latent codes learning\\rwith perceptual loss and reconstruction loss for the prior model with visual\\rperception. Compared with previous state-of-the-art approaches, our model\\rperforms consistently better on two word-level sign language datasets, i.e.,\\rWLASL and NMFs-CSL.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12917 ,  3046kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12954\\rDate: Wed, 20 Dec 2023 11:51:49 GMT   (19450kb,D)\\r\\rTitle: TADAP: Trajectory-Aided Drivable area Auto-labeling with Pre-trained\\r  self-supervised features in winter driving conditions\\rAuthors: Eerik Alamikkotervo, Risto Ojala, Alvari Sepp\\\\anen, Kari Tammi\\rCategories: cs.CV\\r\\\\\\\\\\r  Detection of the drivable area in all conditions is crucial for autonomous\\rdriving and advanced driver assistance systems. However, the amount of labeled\\rdata in adverse driving conditions is limited, especially in winter, and\\rsupervised methods generalize poorly to conditions outside the training\\rdistribution. For easy adaption to all conditions, the need for human\\rannotation should be removed from the learning process. In this paper,\\rTrajectory-Aided Drivable area Auto-labeling with Pre-trained self-supervised\\rfeatures (TADAP) is presented for automated annotation of the drivable area in\\rwinter driving conditions. A sample of the drivable area is extracted based on\\rthe trajectory estimate from the global navigation satellite system. Similarity\\rwith the sample area is determined based on pre-trained self-supervised visual\\rfeatures. Image areas similar to the sample area are considered to be drivable.\\rThese TADAP labels were evaluated with a novel winter-driving dataset,\\rcollected in varying driving scenes. A prediction model trained with the TADAP\\rlabels achieved a +9.6 improvement in intersection over union compared to the\\rprevious state-of-the-art of self-supervised drivable area detection.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12954 ,  19450kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12961\\rDate: Wed, 20 Dec 2023 12:05:59 GMT   (5500kb,D)\\r\\rTitle: Radar Fields: An Extension of Radiance Fields to SAR\\rAuthors: Thibaud Ehret, Roger Mar\\\\'i, Dawa Derksen, Nicolas Gasnier, Gabriele\\r  Facciolo\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  Radiance fields have been a major breakthrough in the field of inverse\\rrendering, novel view synthesis and 3D modeling of complex scenes from\\rmulti-view image collections. Since their introduction, it was shown that they\\rcould be extended to other modalities such as LiDAR, radio frequencies, X-ray\\ror ultrasound. In this paper, we show that, despite the important difference\\rbetween optical and synthetic aperture radar (SAR) image formation models, it\\ris possible to extend radiance fields to radar images thus presenting the first\\rradar fields. This allows us to learn surface models using only collections\\rof radar images, similar to how regular radiance fields are learned and with\\rthe same computational complexity on average. Thanks to similarities in how\\rboth fields are defined, this work also shows a potential for hybrid methods\\rcombining both optical and SAR images.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12961 ,  5500kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12970\\rDate: Wed, 20 Dec 2023 12:19:17 GMT   (4490kb,D)\\r\\rTitle: D3Former: Jointly Learning Repeatable Dense Detectors and\\r  Feature-enhanced Descriptors via Saliency-guided Transformer\\rAuthors: Junjie Gao, Pengfei Wang, Qiujie Dong, Qiong Zeng, Shiqing Xin,\\r  Caiming Zhang\\rCategories: cs.CV\\rComments: 15 pages, 6 figures\\r\\\\\\\\\\r  Establishing accurate and representative matches is a crucial step in\\raddressing the point cloud registration problem. A commonly employed approach\\rinvolves detecting keypoints with salient geometric features and subsequently\\rmapping these keypoints from one frame of the point cloud to another. However,\\rmethods within this category are hampered by the repeatability of the sampled\\rkeypoints. In this paper, we introduce a saliency-guided trans\\\\textbf{former},\\rreferred to as \\\\textit{D3Former}, which entails the joint learning of\\rrepeatable \\\\textbf{D}ense \\\\textbf{D}etectors and feature-enhanced\\r\\\\textbf{D}escriptors. The model comprises a Feature Enhancement Descriptor\\rLearning (FEDL) module and a Repetitive Keypoints Detector Learning (RKDL)\\rmodule. The FEDL module utilizes a region attention mechanism to enhance\\rfeature distinctiveness, while the RKDL module focuses on detecting repeatable\\rkeypoints to enhance matching capabilities. Extensive experimental results on\\rchallenging indoor and outdoor benchmarks demonstrate that our proposed method\\rconsistently outperforms state-of-the-art point cloud matching methods.\\rNotably, tests on 3DLoMatch, even with a low overlap ratio, show that our\\rmethod consistently outperforms recently published approaches such as RoReg and\\rRoITr. For instance, with the number of extracted keypoints reduced to 250, the\\rregistration recall scores for RoReg, RoITr, and our method are 64.3\\\\%, 73.6\\\\%,\\rand 76.5\\\\%, respectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12970 ,  4490kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12995\\rDate: Wed, 20 Dec 2023 12:57:01 GMT   (34099kb,D)\\r\\rTitle: Aggregating Multiple Bio-Inspired Image Region Classifiers For Effective\\r  And Lightweight Visual Place Recognition\\rAuthors: Bruno Arcanjo, Bruno Ferrarini, Maria Fasli, Michael Milford, Klaus D.\\r  McDonald-Maier and Shoaib Ehsan\\rCategories: cs.CV\\r\\\\\\\\\\r  Visual place recognition (VPR) enables autonomous systems to localize\\rthemselves within an environment using image information. While VPR techniques\\rbuilt upon a Convolutional Neural Network (CNN) backbone dominate\\rstate-of-the-art VPR performance, their high computational requirements make\\rthem unsuitable for platforms equipped with low-end hardware. Recently, a\\rlightweight VPR system based on multiple bio-inspired classifiers, dubbed\\rDrosoNets, has been proposed, achieving great computational efficiency at the\\rcost of reduced absolute place retrieval performance. In this work, we propose\\ra novel multi-DrosoNet localization system, dubbed RegionDrosoNet, with\\rsignificantly improved VPR performance, while preserving a low-computational\\rprofile. Our approach relies on specializing distinct groups of DrosoNets on\\rdifferently sliced partitions of the original image, increasing extrinsic model\\rdifferentiation. Furthermore, we introduce a novel voting module to combine the\\routputs of all DrosoNets into the final place prediction which considers\\rmultiple top refence candidates from each DrosoNet. RegionDrosoNet outperforms\\rother lightweight VPR techniques when dealing with both appearance changes and\\rviewpoint variations. Moreover, it competes with computationally expensive\\rmethods on some benchmark datasets at a small fraction of their online\\rinference time.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12995 ,  34099kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13008\\rDate: Wed, 20 Dec 2023 13:20:31 GMT   (45799kb,D)\\r\\rTitle: No More Shortcuts: Realizing the Potential of Temporal Self-Supervision\\rAuthors: Ishan Rajendrakumar Dave, Simon Jenni, Mubarak Shah\\rCategories: cs.CV cs.AI cs.LG\\rComments: AAAI 2024 (Main Technical Track)\\r\\\\\\\\\\r  Self-supervised approaches for video have shown impressive results in video\\runderstanding tasks. However, unlike early works that leverage temporal\\rself-supervision, current state-of-the-art methods primarily rely on tasks from\\rthe image domain (e.g., contrastive learning) that do not explicitly promote\\rthe learning of temporal features. We identify two factors that limit existing\\rtemporal self-supervision: 1) tasks are too simple, resulting in saturated\\rtraining performance, and 2) we uncover shortcuts based on local appearance\\rstatistics that hinder the learning of high-level features. To address these\\rissues, we propose 1) a more challenging reformulation of temporal\\rself-supervision as frame-level (rather than clip-level) recognition tasks and\\r2) an effective augmentation strategy to mitigate shortcuts. Our model extends\\ra representation of single video frames, pre-trained through contrastive\\rlearning, with a transformer that we train through temporal self-supervision.\\rWe demonstrate experimentally that our more challenging frame-level task\\rformulations and the removal of shortcuts drastically improve the quality of\\rfeatures learned through temporal self-supervision. The generalization\\rcapability of our self-supervised video method is evidenced by its\\rstate-of-the-art performance in a wide range of high-level semantic tasks,\\rincluding video retrieval, action classification, and video attribute\\rrecognition (such as object and scene identification), as well as low-level\\rtemporal correspondence tasks like video object segmentation and pose tracking.\\rAdditionally, we show that the video representations learned through our method\\rexhibit increased robustness to the input perturbations.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13008 ,  45799kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13016\\rDate: Wed, 20 Dec 2023 13:31:11 GMT   (37710kb,D)\\r\\rTitle: DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View\\r  Synthesis\\rAuthors: Yuming Gu, Hongyi Xu, You Xie, Guoxian Song, Yichun Shi, Di Chang,\\r  Jing Yang, Lingjie Luo\\rCategories: cs.CV\\r\\\\\\\\\\r  We present DiffPortrait3D, a conditional diffusion model that is capable of\\rsynthesizing 3D-consistent photo-realistic novel views from as few as a single\\rin-the-wild portrait. Specifically, given a single RGB input, we aim to\\rsynthesize plausible but consistent facial details rendered from novel camera\\rviews with retained both identity and facial expression. In lieu of\\rtime-consuming optimization and fine-tuning, our zero-shot method generalizes\\rwell to arbitrary face portraits with unposed camera views, extreme facial\\rexpressions, and diverse artistic depictions. At its core, we leverage the\\rgenerative prior of 2D diffusion models pre-trained on large-scale image\\rdatasets as our rendering backbone, while the denoising is guided with\\rdisentangled attentive control of appearance and camera pose. To achieve this,\\rwe first inject the appearance context from the reference image into the\\rself-attention layers of the frozen UNets. The rendering view is then\\rmanipulated with a novel conditional control module that interprets the camera\\rpose by watching a condition image of a crossed subject from the same view.\\rFurthermore, we insert a trainable cross-view attention module to enhance view\\rconsistency, which is further strengthened with a novel 3D-aware noise\\rgeneration process during inference. We demonstrate state-of-the-art results\\rboth qualitatively and quantitatively on our challenging in-the-wild and\\rmulti-view benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13016 ,  37710kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13053\\rDate: Wed, 20 Dec 2023 14:26:54 GMT   (26733kb,D)\\r\\rTitle: Quantifying Bias in Text-to-Image Generative Models\\rAuthors: Jordan Vice, Naveed Akhtar, Richard Hartley, and Ajmal Mian\\rCategories: cs.CV cs.CR\\rComments: main manuscript = 9 pages, 6 tables, 4 figures. Supplementary\\r  material = 15 pages, 13 tables, 14 figures\\r\\\\\\\\\\r  Bias in text-to-image (T2I) models can propagate unfair social\\rrepresentations and may be used to aggressively market ideas or push\\rcontroversial agendas. Existing T2I model bias evaluation methods only focus on\\rsocial biases. We look beyond that and instead propose an evaluation\\rmethodology to quantify general biases in T2I generative models, without any\\rpreconceived notions. We assess four state-of-the-art T2I models and compare\\rtheir baseline bias characteristics to their respective variants (two for\\reach), where certain biases have been intentionally induced. We propose three\\revaluation metrics to assess model biases including: (i) Distribution bias,\\r(ii) Jaccard hallucination and (iii) Generative miss-rate. We conduct two\\revaluation studies, modelling biases under general, and task-oriented\\rconditions, using a marketing scenario as the domain for the latter. We also\\rquantify social biases to compare our findings to related works. Finally, our\\rmethodology is transferred to evaluate captioned-image datasets and measure\\rtheir bias. Our approach is objective, domain-agnostic and consistently\\rmeasures different forms of T2I model biases. We have developed a web\\rapplication and practical implementation of what has been proposed in this\\rwork, which is at https://huggingface.co/spaces/JVice/try-before-you-bias. A\\rvideo series with demonstrations is available at\\rhttps://www.youtube.com/channel/UCk-0xyUyT0MSd_hkp4jQt1Q\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13053 ,  26733kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13066\\rDate: Wed, 20 Dec 2023 14:45:57 GMT   (1555kb,D)\\r\\rTitle: PPEA-Depth: Progressive Parameter-Efficient Adaptation for\\r  Self-Supervised Monocular Depth Estimation\\rAuthors: Yue-Jiang Dong, Yuan-Chen Guo, Ying-Tian Liu, Fang-Lue Zhang, Song-Hai\\r  Zhang\\rCategories: cs.CV\\rComments: Accepted by AAAI 2024\\r\\\\\\\\\\r  Self-supervised monocular depth estimation is of significant importance with\\rapplications spanning across autonomous driving and robotics. However, the\\rreliance on self-supervision introduces a strong static-scene assumption,\\rthereby posing challenges in achieving optimal performance in dynamic scenes,\\rwhich are prevalent in most real-world situations. To address these issues, we\\rpropose PPEA-Depth, a Progressive Parameter-Efficient Adaptation approach to\\rtransfer a pre-trained image model for self-supervised depth estimation. The\\rtraining comprises two sequential stages: an initial phase trained on a dataset\\rprimarily composed of static scenes, succeeded by an expansion to more\\rintricate datasets involving dynamic scenes. To facilitate this process, we\\rdesign compact encoder and decoder adapters to enable parameter-efficient\\rtuning, allowing the network to adapt effectively. They not only uphold\\rgeneralized patterns from pre-trained image models but also retain knowledge\\rgained from the preceding phase into the subsequent one. Extensive experiments\\rdemonstrate that PPEA-Depth achieves state-of-the-art performance on KITTI,\\rCityScapes and DDAD datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13066 ,  1555kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13071\\rDate: Wed, 20 Dec 2023 14:52:07 GMT   (3049kb,D)\\r\\rTitle: Point Deformable Network with Enhanced Normal Embedding for Point Cloud\\r  Analysis\\rAuthors: Xingyilang Yin, Xi Yang, Liangchen Liu, Nannan Wang, Xinbo Gao\\rCategories: cs.CV\\r\\\\\\\\\\r  Recently MLP-based methods have shown strong performance in point cloud\\ranalysis. Simple MLP architectures are able to learn geometric features in\\rlocal point groups yet fail to model long-range dependencies directly. In this\\rpaper, we propose Point Deformable Network (PDNet), a concise MLP-based network\\rthat can capture long-range relations with strong representation ability.\\rSpecifically, we put forward Point Deformable Aggregation Module (PDAM) to\\rimprove representation capability in both long-range dependency and adaptive\\raggregation among points. For each query point, PDAM aggregates information\\rfrom deformable reference points rather than points in limited local areas. The\\rdeformable reference points are generated data-dependent, and we initialize\\rthem according to the input point positions. Additional offsets and modulation\\rscalars are learned on the whole point features, which shift the deformable\\rreference points to the regions of interest. We also suggest estimating the\\rnormal vector for point clouds and applying Enhanced Normal Embedding (ENE) to\\rthe geometric extractors to improve the representation ability of single-point.\\rExtensive experiments and ablation studies on various benchmarks demonstrate\\rthe effectiveness and superiority of our PDNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13071 ,  3049kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13081\\rDate: Wed, 20 Dec 2023 15:02:37 GMT   (7271kb,D)\\r\\rTitle: BEVSeg2TP: Surround View Camera Bird's-Eye-View Based Joint Vehicle\\r  Segmentation and Ego Vehicle Trajectory Prediction\\rAuthors: Sushil Sharma, Arindam Das, Ganesh Sistu, Mark Halton, Ciar\\\\'an Eising\\rCategories: cs.CV\\rComments: Accepted for publication in the International Conference on Computer\\r  Vision Theory and Applications (VISAPP) 2024\\r\\\\\\\\\\r  Trajectory prediction is, naturally, a key task for vehicle autonomy. While\\rthe number of traffic rules is limited, the combinations and uncertainties\\rassociated with each agent's behaviour in real-world scenarios are nearly\\rimpossible to encode. Consequently, there is a growing interest in\\rlearning-based trajectory prediction. The proposed method in this paper\\rpredicts trajectories by considering perception and trajectory prediction as a\\runified system. In considering them as unified tasks, we show that there is the\\rpotential to improve the performance of perception. To achieve these goals, we\\rpresent BEVSeg2TP - a surround-view camera bird's-eye-view-based joint vehicle\\rsegmentation and ego vehicle trajectory prediction system for autonomous\\rvehicles. The proposed system uses a network trained on multiple camera views.\\rThe images are transformed using several deep learning techniques to perform\\rsemantic segmentation of objects, including other vehicles, in the scene. The\\rsegmentation outputs are fused across the camera views to obtain a\\rcomprehensive representation of the surrounding vehicles from the\\rbird's-eye-view perspective. The system further predicts the future trajectory\\rof the ego vehicle using a spatiotemporal probabilistic network (STPN) to\\roptimize trajectory prediction. This network leverages information from\\rencoder-decoder transformers and joint vehicle segmentation.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13081 ,  7271kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13090\\rDate: Wed, 20 Dec 2023 15:12:27 GMT   (664kb,D)\\r\\rTitle: Perception Test 2023: A Summary of the First Challenge And Outcome\\rAuthors: Joseph Heyward, Jo\\\\~ao Carreira, Dima Damen, Andrew Zisserman, Viorica\\r  P\\\\u{a}tr\\\\u{a}ucean\\rCategories: cs.CV\\r\\\\\\\\\\r  The First Perception Test challenge was held as a half-day workshop alongside\\rthe IEEE/CVF International Conference on Computer Vision (ICCV) 2023, with the\\rgoal of benchmarking state-of-the-art video models on the recently proposed\\rPerception Test benchmark. The challenge had six tracks covering low-level and\\rhigh-level tasks, with both a language and non-language interface, across\\rvideo, audio, and text modalities, and covering: object tracking, point\\rtracking, temporal action localisation, temporal sound localisation,\\rmultiple-choice video question-answering, and grounded video\\rquestion-answering. We summarise in this report the task descriptions, metrics,\\rbaselines, and results.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13090 ,  664kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13091\\rDate: Wed, 20 Dec 2023 15:12:53 GMT   (41390kb,D)\\r\\rTitle: MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using\\r  Differentiable Shading\\rAuthors: Abdallah Dib, Luiz Gustavo Hafemann, Emeline Got, Trevor Anderson,\\r  Amin Fadaeinejad, Rafael M. O. Cruz, Marc-Andre Carbonneau\\rCategories: cs.CV cs.GR cs.LG\\rComments: https://ubisoft-laforge.github.io/character/mosar/\\rMSC-class: 68T45 (Primary) 68T07, 68T01 (Secondary)\\rACM-class: I.2.10; I.4; I.3.3; I.5\\r\\\\\\\\\\r  Reconstructing an avatar from a portrait image has many applications in\\rmultimedia, but remains a challenging research problem. Extracting reflectance\\rmaps and geometry from one image is ill-posed: recovering geometry is a\\rone-to-many mapping problem and reflectance and light are difficult to\\rdisentangle. Accurate geometry and reflectance can be captured under the\\rcontrolled conditions of a light stage, but it is costly to acquire large\\rdatasets in this fashion. Moreover, training solely with this type of data\\rleads to poor generalization with in-the-wild images. This motivates the\\rintroduction of MoSAR, a method for 3D avatar generation from monocular images.\\rWe propose a semi-supervised training scheme that improves generalization by\\rlearning from both light stage and in-the-wild datasets. This is achieved using\\ra novel differentiable shading formulation. We show that our approach\\reffectively disentangles the intrinsic face parameters, producing relightable\\ravatars. As a result, MoSAR estimates a richer set of skin reflectance maps,\\rand generates more realistic avatars than existing state-of-the-art methods. We\\ralso introduce a new dataset, named FFHQ-UV-Intrinsics, the first public\\rdataset providing intrisic face attributes at scale (diffuse, specular, ambient\\rocclusion and translucency maps) for a total of 10k subjects. The project\\rwebsite and the dataset are available on the following link:\\rhttps://ubisoftlaforge.github.io/character/mosar\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13091 ,  41390kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13100\\rDate: Wed, 20 Dec 2023 15:18:51 GMT   (1355kb,D)\\r\\rTitle: SEER-ZSL: Semantic Encoder-Enhanced Representations for Generalized\\r  Zero-Shot Learning\\rAuthors: William Heyden, Habib Ullah, M. Salman Siddiqui, Fadi Al Machot\\rCategories: cs.CV\\r\\\\\\\\\\r  Generalized Zero-Shot Learning (GZSL) recognizes unseen classes by\\rtransferring knowledge from the seen classes, depending on the inherent\\rinteractions between visual and semantic data. However, the discrepancy between\\rwell-prepared training data and unpredictable real-world test scenarios remains\\ra significant challenge. This paper introduces a dual strategy to address the\\rgeneralization gap. Firstly, we incorporate semantic information through an\\rinnovative encoder. This encoder effectively integrates class-specific semantic\\rinformation by targeting the performance disparity, enhancing the produced\\rfeatures to enrich the semantic space for class-specific attributes. Secondly,\\rwe refine our generative capabilities using a novel compositional loss\\rfunction. This approach generates discriminative classes, effectively\\rclassifying both seen and unseen classes. In addition, we extend the\\rexploitation of the learned latent space by utilizing controlled semantic\\rinputs, ensuring the robustness of the model in varying environments. This\\rapproach yields a model that outperforms the state-of-the-art models in terms\\rof both generalization and diverse settings, notably without requiring\\rhyperparameter tuning or domain-specific adaptations. We also propose a set of\\rnovel evaluation metrics to provide a more detailed assessment of the\\rreliability and reproducibility of the results. The complete code is made\\ravailable on https://github.com/william-heyden/SEER-ZeroShotLearning/.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13100 ,  1355kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13102\\rDate: Wed, 20 Dec 2023 15:20:25 GMT   (47330kb,D)\\r\\rTitle: SpecNeRF: Gaussian Directional Encoding for Specular Reflections\\rAuthors: Li Ma, Vasu Agrawal, Haithem Turki, Changil Kim, Chen Gao, Pedro\\r  Sander, Michael Zollh\\\\ofer, Christian Richardt\\rCategories: cs.CV\\rComments: Project page: https://limacv.github.io/SpecNeRF_web/\\r\\\\\\\\\\r  Neural radiance fields have achieved remarkable performance in modeling the\\rappearance of 3D scenes. However, existing approaches still struggle with the\\rview-dependent appearance of glossy surfaces, especially under complex lighting\\rof indoor environments. Unlike existing methods, which typically assume distant\\rlighting like an environment map, we propose a learnable Gaussian directional\\rencoding to better model the view-dependent effects under near-field lighting\\rconditions. Importantly, our new directional encoding captures the\\rspatially-varying nature of near-field lighting and emulates the behavior of\\rprefiltered environment maps. As a result, it enables the efficient evaluation\\rof preconvolved specular color at any 3D location with varying roughness\\rcoefficients. We further introduce a data-driven geometry prior that helps\\ralleviate the shape radiance ambiguity in reflection modeling. We show that our\\rGaussian directional encoding and geometry prior significantly improve the\\rmodeling of challenging specular reflections in neural radiance fields, which\\rhelps decompose appearance into more physically meaningful components.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13102 ,  47330kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13104\\rDate: Wed, 20 Dec 2023 15:22:34 GMT   (13970kb,D)\\r\\rTitle: Optimizing Ego Vehicle Trajectory Prediction: The Graph Enhancement\\r  Approach\\rAuthors: Sushil Sharma, Aryan Singh, Ganesh Sistu, Mark Halton, Ciar\\\\'an Eising\\rCategories: cs.CV\\rComments: Accepted for publication in the Electronic Imagine Autonomous\\r  Vehicles and Machines (EI-AVM) Conference\\r\\\\\\\\\\r  Predicting the trajectory of an ego vehicle is a critical component of\\rautonomous driving systems. Current state-of-the-art methods typically rely on\\rDeep Neural Networks (DNNs) and sequential models to process front-view images\\rfor future trajectory prediction. However, these approaches often struggle with\\rperspective issues affecting object features in the scene. To address this, we\\radvocate for the use of Bird's Eye View (BEV) perspectives, which offer unique\\radvantages in capturing spatial relationships and object homogeneity. In our\\rwork, we leverage Graph Neural Networks (GNNs) and positional encoding to\\rrepresent objects in a BEV, achieving competitive performance compared to\\rtraditional DNN-based methods. While the BEV-based approach loses some detailed\\rinformation inherent to front-view images, we balance this by enriching the BEV\\rdata by representing it as a graph where relationships between the objects in a\\rscene are captured effectively.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13104 ,  13970kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13108\\rDate: Wed, 20 Dec 2023 15:28:38 GMT   (3554kb,D)\\r\\rTitle: ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation\\rAuthors: Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao,\\r  Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, Hengxu Wang, Luowei Zhou,\\r  Mike Zheng Shou\\rCategories: cs.CV\\r\\\\\\\\\\r  Graphical User Interface (GUI) automation holds significant promise for\\rassisting users with complex tasks, thereby boosting human productivity.\\rExisting works leveraging Large Language Model (LLM) or LLM-based AI agents\\rhave shown capabilities in automating tasks on Android and Web platforms.\\rHowever, these tasks are primarily aimed at simple device usage and\\rentertainment operations. This paper presents a novel benchmark, AssistGUI, to\\revaluate whether models are capable of manipulating the mouse and keyboard on\\rthe Windows platform in response to user-requested tasks. We carefully\\rcollected a set of 100 tasks from nine widely-used software applications, such\\ras, After Effects and MS Word, each accompanied by the necessary project files\\rfor better evaluation. Moreover, we propose an advanced Actor-Critic Embodied\\rAgent framework, which incorporates a sophisticated GUI parser driven by an\\rLLM-agent and an enhanced reasoning mechanism adept at handling lengthy\\rprocedural tasks. Our experimental results reveal that our GUI Parser and\\rReasoning mechanism outshine existing methods in performance. Nevertheless, the\\rpotential remains substantial, with the best model attaining only a 46% success\\rrate on our benchmark. We conclude with a thorough analysis of the current\\rmethods' limitations, setting the stage for future breakthroughs in this\\rdomain.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13108 ,  3554kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13114\\rDate: Wed, 20 Dec 2023 15:34:15 GMT   (35113kb,D)\\r\\rTitle: Investigating Color Illusions from the Perspective of Computational\\r  Color Constancy\\rAuthors: Oguzhan Ulucan, Diclehan Ulucan, Marc Ebner\\rCategories: cs.CV\\rComments: This work is accepted at VISAPP 2024 as a long paper\\r\\\\\\\\\\r  Color constancy and color illusion perception are two phenomena occurring in\\rthe human visual system, which can help us reveal unknown mechanisms of human\\rperception. For decades computer vision scientists have developed numerous\\rcolor constancy methods, which estimate the reflectance of the surface by\\rdiscounting the illuminant. However, color illusions have not been analyzed in\\rdetail in the field of computational color constancy, which we find surprising\\rsince the relationship they share is significant and may let us design more\\rrobust systems. We argue that any model that can reproduce our sensation on\\rcolor illusions should also be able to provide pixel-wise estimates of the\\rlight source. In other words, we suggest that the analysis of color illusions\\rhelps us to improve the performance of the existing global color constancy\\rmethods, and enable them to provide pixel-wise estimates for scenes illuminated\\rby multiple light sources. In this study, we share the outcomes of our\\rinvestigation in which we take several color constancy methods and modify them\\rto reproduce the behavior of the human visual system on color illusions. Also,\\rwe show that parameters purely extracted from illusions are able to improve the\\rperformance of color constancy methods. A noteworthy outcome is that our\\rstrategy based on the investigation of color illusions outperforms the\\rstate-of-the-art methods that are specifically designed to transform global\\rcolor constancy algorithms into multi-illuminant algorithms.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13114 ,  35113kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13116\\rDate: Wed, 20 Dec 2023 15:36:30 GMT   (6121kb,D)\\r\\rTitle: VSR-Net: Vessel-like Structure Rehabilitation Network with Graph\\r  Clustering\\rAuthors: Haili Ye, Xiaoqing Zhang, Yan Hu, Huazhu Fu and Jiang Liu\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  The morphologies of vessel-like structures, such as blood vessels and nerve\\rfibres, play significant roles in disease diagnosis, e.g., Parkinson's disease.\\rDeep network-based refinement segmentation methods have recently achieved\\rpromising vessel-like structure segmentation results. There are still two\\rchallenges: (1) existing methods have limitations in rehabilitating subsection\\rruptures in segmented vessel-like structures; (2) they are often overconfident\\rin predicted segmentation results. To tackle these two challenges, this paper\\rattempts to leverage the potential of spatial interconnection relationships\\ramong subsection ruptures from the structure rehabilitation perspective. Based\\ron this, we propose a novel Vessel-like Structure Rehabilitation Network\\r(VSR-Net) to rehabilitate subsection ruptures and improve the model calibration\\rbased on coarse vessel-like structure segmentation results. VSR-Net first\\rconstructs subsection rupture clusters with Curvilinear Clustering Module\\r(CCM). Then, the well-designed Curvilinear Merging Module (CMM) is applied to\\rrehabilitate the subsection ruptures to obtain the refined vessel-like\\rstructures. Extensive experiments on five 2D/3D medical image datasets show\\rthat VSR-Net significantly outperforms state-of-the-art (SOTA) refinement\\rsegmentation methods with lower calibration error. Additionally, we provide\\rquantitative analysis to explain the morphological difference between the\\rrehabilitation results of VSR-Net and ground truth (GT), which is smaller than\\rSOTA methods and GT, demonstrating that our method better rehabilitates\\rvessel-like structures by restoring subsection ruptures.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13116 ,  6121kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13150\\rDate: Wed, 20 Dec 2023 16:14:58 GMT   (2946kb,D)\\r\\rTitle: Splatter Image: Ultra-Fast Single-View 3D Reconstruction\\rAuthors: Stanislaw Szymanowicz and Christian Rupprecht and Andrea Vedaldi\\rCategories: cs.CV\\rComments: Project page: https://szymanowiczs.github.io/splatter-image.html .\\r  Code: https://github.com/szymanowiczs/splatter-image\\r\\\\\\\\\\r  We introduce the Splatter Image, an ultra-fast approach for monocular 3D\\robject reconstruction which operates at 38 FPS. Splatter Image is based on\\rGaussian Splatting, which has recently brought real-time rendering, fast\\rtraining, and excellent scaling to multi-view reconstruction. For the first\\rtime, we apply Gaussian Splatting in a monocular reconstruction setting. Our\\rapproach is learning-based, and, at test time, reconstruction only requires the\\rfeed-forward evaluation of a neural network. The main innovation of Splatter\\rImage is the surprisingly straightforward design: it uses a 2D image-to-image\\rnetwork to map the input image to one 3D Gaussian per pixel. The resulting\\rGaussians thus have the form of an image, the Splatter Image. We further extend\\rthe method to incorporate more than one image as input, which we do by adding\\rcross-view attention. Owning to the speed of the renderer (588 FPS), we can use\\ra single GPU for training while generating entire images at each iteration in\\rorder to optimize perceptual metrics like LPIPS. On standard benchmarks, we\\rdemonstrate not only fast reconstruction but also better results than recent\\rand much more expensive baselines in terms of PSNR, LPIPS, and other metrics.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13150 ,  2946kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13216\\rDate: Wed, 20 Dec 2023 17:35:24 GMT   (16234kb,D)\\r\\rTitle: Improving Semantic Correspondence with Viewpoint-Guided Spherical Maps\\rAuthors: Octave Mariotti, Oisin Mac Aodha, Hakan Bilen\\rCategories: cs.CV\\r\\\\\\\\\\r  Recent progress in self-supervised representation learning has resulted in\\rmodels that are capable of extracting image features that are not only\\reffective at encoding image level, but also pixel-level, semantics. These\\rfeatures have been shown to be effective for dense visual semantic\\rcorrespondence estimation, even outperforming fully-supervised methods.\\rNevertheless, current self-supervised approaches still fail in the presence of\\rchallenging image characteristics such as symmetries and repeated parts. To\\raddress these limitations, we propose a new approach for semantic\\rcorrespondence estimation that supplements discriminative self-supervised\\rfeatures with 3D understanding via a weak geometric spherical prior. Compared\\rto more involved 3D pipelines, our model only requires weak viewpoint\\rinformation, and the simplicity of our spherical representation enables us to\\rinject informative geometric priors into the model during training. We propose\\ra new evaluation metric that better accounts for repeated part and\\rsymmetry-induced mistakes. We present results on the challenging SPair-71k\\rdataset, where we show that our approach demonstrates is capable of\\rdistinguishing between symmetric views and repeated parts across many object\\rcategories, and also demonstrate that we can generalize to unseen classes on\\rthe AwA dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13216 ,  16234kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13223\\rDate: Wed, 20 Dec 2023 17:46:48 GMT   (230kb,D)\\r\\rTitle: StableKD: Breaking Inter-block Optimization Entanglement for Stable\\r  Knowledge Distillation\\rAuthors: Shiu-hong Kao, Jierun Chen, S.H. Gary Chan\\rCategories: cs.CV\\r\\\\\\\\\\r  Knowledge distillation (KD) has been recognized as an effective tool to\\rcompress and accelerate models. However, current KD approaches generally suffer\\rfrom an accuracy drop and/or an excruciatingly long distillation process. In\\rthis paper, we tackle the issue by first providing a new insight into a\\rphenomenon that we call the Inter-Block Optimization Entanglement (IBOE), which\\rmakes the conventional end-to-end KD approaches unstable with noisy gradients.\\rWe then propose StableKD, a novel KD framework that breaks the IBOE and\\rachieves more stable optimization. StableKD distinguishes itself through two\\roperations: Decomposition and Recomposition, where the former divides a pair of\\rteacher and student networks into several blocks for separate distillation, and\\rthe latter progressively merges them back, evolving towards end-to-end\\rdistillation. We conduct extensive experiments on CIFAR100, Imagewoof, and\\rImageNet datasets with various teacher-student pairs. Compared to other KD\\rapproaches, our simple yet effective StableKD greatly boosts the model accuracy\\rby 1% ~ 18%, speeds up the convergence up to 10 times, and outperforms them\\rwith only 40% of the training data.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13223 ,  230kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13240\\rDate: Wed, 20 Dec 2023 18:08:02 GMT   (6405kb,D)\\r\\rTitle: Efficient Verification-Based Face Identification\\rAuthors: Amit Rozner, Barak Battash, Ofir Lindenbaum, Lior Wolf\\rCategories: cs.CV\\rComments: 10 pages, 5 figures\\rACM-class: I.4\\r\\\\\\\\\\r  We study the problem of performing face verification with an efficient neural\\rmodel $f$. The efficiency of $f$ stems from simplifying the face verification\\rproblem from an embedding nearest neighbor search into a binary problem; each\\ruser has its own neural network $f$. To allow information sharing between\\rdifferent individuals in the training set, we do not train $f$ directly but\\rinstead generate the model weights using a hypernetwork $h$. This leads to the\\rgeneration of a compact personalized model for face identification that can be\\rdeployed on edge devices. Key to the method's success is a novel way of\\rgenerating hard negatives and carefully scheduling the training objectives. Our\\rmodel leads to a substantially small $f$ requiring only 23k parameters and 5M\\rfloating point operations (FLOPS). We use six face verification datasets to\\rdemonstrate that our method is on par or better than state-of-the-art models,\\rwith a significantly reduced number of parameters and computational burden.\\rFurthermore, we perform an extensive ablation study to demonstrate the\\rimportance of each element in our method.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13240 ,  6405kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13252\\rDate: Wed, 20 Dec 2023 18:27:47 GMT   (21371kb,D)\\r\\rTitle: Zero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model\\rAuthors: Saurabh Saxena, Junhwa Hur, Charles Herrmann, Deqing Sun, David J.\\r  Fleet\\rCategories: cs.CV\\r\\\\\\\\\\r  While methods for monocular depth estimation have made significant strides on\\rstandard benchmarks, zero-shot metric depth estimation remains unsolved.\\rChallenges include the joint modeling of indoor and outdoor scenes, which often\\rexhibit significantly different distributions of RGB and depth, and the\\rdepth-scale ambiguity due to unknown camera intrinsics. Recent work has\\rproposed specialized multi-head architectures for jointly modeling indoor and\\routdoor scenes. In contrast, we advocate a generic, task-agnostic diffusion\\rmodel, with several advancements such as log-scale depth parameterization to\\renable joint modeling of indoor and outdoor scenes, conditioning on the\\rfield-of-view (FOV) to handle scale ambiguity and synthetically augmenting FOV\\rduring training to generalize beyond the limited camera intrinsics in training\\rdatasets. Furthermore, by employing a more diverse training mixture than is\\rcommon, and an efficient diffusion parameterization, our method, DMD (Diffusion\\rfor Metric Depth) achieves a 25\\\\% reduction in relative error (REL) on\\rzero-shot indoor and 33\\\\% reduction on zero-shot outdoor datasets over the\\rcurrent SOTA using only a small number of denoising steps. For an overview see\\rhttps://diffusion-vision.github.io/dmd\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13252 ,  21371kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13253\\rDate: Wed, 20 Dec 2023 18:27:53 GMT   (7823kb,D)\\r\\rTitle: Conditional Image Generation with Pretrained Generative Model\\rAuthors: Rajesh Shrestha, Bowen Xie\\rCategories: cs.CV cs.AI cs.LG\\r\\\\\\\\\\r  In recent years, diffusion models have gained popularity for their ability to\\rgenerate higher-quality images in comparison to GAN models. However, like any\\rother large generative models, these models require a huge amount of data,\\rcomputational resources, and meticulous tuning for successful training. This\\rposes a significant challenge, rendering it infeasible for most individuals. As\\ra result, the research community has devised methods to leverage pre-trained\\runconditional diffusion models with additional guidance for the purpose of\\rconditional image generative. These methods enable conditional image\\rgenerations on diverse inputs and, most importantly, circumvent the need for\\rtraining the diffusion model. In this paper, our objective is to reduce the\\rtime-required and computational overhead introduced by the addition of guidance\\rin diffusion models -- while maintaining comparable image quality. We propose a\\rset of methods based on our empirical analysis, demonstrating a reduction in\\rcomputation time by approximately threefold.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13253 ,  7823kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13265\\rDate: Wed, 20 Dec 2023 18:43:20 GMT   (42926kb,D)\\r\\rTitle: ClassLIE: Structure- and Illumination-Adaptive Classification for\\r  Low-Light Image Enhancement\\rAuthors: Zixiang Wei, Yiting Wang, Lichao Sun, Athanasios V. Vasilakos, Lin\\r  Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  Low-light images often suffer from limited visibility and multiple types of\\rdegradation, rendering low-light image enhancement (LIE) a non-trivial task.\\rSome endeavors have been recently made to enhance low-light images using\\rconvolutional neural networks (CNNs). However, they have low efficiency in\\rlearning the structural information and diverse illumination levels at the\\rlocal regions of an image. Consequently, the enhanced results are affected by\\runexpected artifacts, such as unbalanced exposure, blur, and color bias. To\\rthis end, this paper proposes a novel framework, called ClassLIE, that combines\\rthe potential of CNNs and transformers. It classifies and adaptively learns the\\rstructural and illumination information from the low-light images in a holistic\\rand regional manner, thus showing better enhancement performance. Our framework\\rfirst employs a structure and illumination classification (SIC) module to learn\\rthe degradation information adaptively. In SIC, we decompose an input image\\rinto an illumination map and a reflectance map. A class prediction block is\\rthen designed to classify the degradation information by calculating the\\rstructure similarity scores on the reflectance map and mean square error on the\\rillumination map. As such, each input image can be divided into patches with\\rthree enhancement difficulty levels. Then, a feature learning and fusion (FLF)\\rmodule is proposed to adaptively learn the feature information with CNNs for\\rdifferent enhancement difficulty levels while learning the long-range\\rdependencies for the patches in a holistic manner. Experiments on five\\rbenchmark datasets consistently show our ClassLIE achieves new state-of-the-art\\rperformance, with 25.74 PSNR and 0.92 SSIM on the LOL dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13265 ,  42926kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13271\\rDate: Wed, 20 Dec 2023 18:51:02 GMT   (32244kb,D)\\r\\rTitle: Repaint123: Fast and High-quality One Image to 3D Generation with\\r  Progressive Controllable 2D Repainting\\rAuthors: Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida\\r  Wei, Wangbo Yu, Munan Ning, Li Yuan\\rCategories: cs.CV\\rComments: Code: https://github.com/junwuzhang19/repaint123\\r\\\\\\\\\\r  Recent one image to 3D generation methods commonly adopt Score Distillation\\rSampling (SDS). Despite the impressive results, there are multiple deficiencies\\rincluding multi-view inconsistency, over-saturated and over-smoothed textures,\\ras well as the slow generation speed. To address these deficiencies, we present\\rRepaint123 to alleviate multi-view bias as well as texture degradation and\\rspeed up the generation process. The core idea is to combine the powerful image\\rgeneration capability of the 2D diffusion model and the texture alignment\\rability of the repainting strategy for generating high-quality multi-view\\rimages with consistency. We further propose visibility-aware adaptive\\rrepainting strength for overlap regions to enhance the generated image quality\\rin the repainting process. The generated high-quality and multi-view consistent\\rimages enable the use of simple Mean Square Error (MSE) loss for fast 3D\\rcontent generation. We conduct extensive experiments and show that our method\\rhas a superior ability to generate high-quality 3D content with multi-view\\rconsistency and fine textures in 2 minutes from scratch. Code is at\\rhttps://github.com/junwuzhang19/repaint123.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13271 ,  32244kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13277\\rDate: Wed, 20 Dec 2023 18:56:45 GMT   (22507kb,D)\\r\\rTitle: Deep Learning on 3D Neural Fields\\rAuthors: Pierluigi Zama Ramirez, Luca De Luigi, Daniele Sirocchi, Adriano\\r  Cardace, Riccardo Spezialetti, Francesco Ballerini, Samuele Salti, Luigi Di\\r  Stefano\\rCategories: cs.CV\\rComments: Extended version of the paper Deep Learning on Implicit Neural\\r  Representations of Shapes that was presented at ICLR 2023. arXiv admin note:\\r  text overlap with arXiv:2302.05438\\r\\\\\\\\\\r  In recent years, Neural Fields (NFs) have emerged as an effective tool for\\rencoding diverse continuous signals such as images, videos, audio, and 3D\\rshapes. When applied to 3D data, NFs offer a solution to the fragmentation and\\rlimitations associated with prevalent discrete representations. However, given\\rthat NFs are essentially neural networks, it remains unclear whether and how\\rthey can be seamlessly integrated into deep learning pipelines for solving\\rdownstream tasks. This paper addresses this research problem and introduces\\rnf2vec, a framework capable of generating a compact latent representation for\\ran input NF in a single inference pass. We demonstrate that nf2vec effectively\\rembeds 3D objects represented by the input NFs and showcase how the resulting\\rembeddings can be employed in deep learning pipelines to successfully address\\rvarious tasks, all while processing exclusively NFs. We test this framework on\\rseveral NFs used to represent 3D surfaces, such as unsigned/signed distance and\\roccupancy fields. Moreover, we demonstrate the effectiveness of our approach\\rwith more complex NFs that encompass both geometry and appearance of 3D objects\\rsuch as neural radiance fields.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13277 ,  22507kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13285\\rDate: Wed, 20 Dec 2023 18:59:42 GMT   (33452kb,D)\\r\\rTitle: UniSDF: Unifying Neural Representations for High-Fidelity 3D\\r  Reconstruction of Complex Scenes with Reflections\\rAuthors: Fangjinhua Wang, Marie-Julie Rakotosaona, Michael Niemeyer, Richard\\r  Szeliski, Marc Pollefeys, Federico Tombari\\rCategories: cs.CV\\rComments: Project page: https://fangjinhuawang.github.io/UniSDF\\r\\\\\\\\\\r  Neural 3D scene representations have shown great potential for 3D\\rreconstruction from 2D images. However, reconstructing real-world captures of\\rcomplex scenes still remains a challenge. Existing generic 3D reconstruction\\rmethods often struggle to represent fine geometric details and do not\\radequately model reflective surfaces of large-scale scenes. Techniques that\\rexplicitly focus on reflective surfaces can model complex and detailed\\rreflections by exploiting better reflection parameterizations. However, we\\robserve that these methods are often not robust in real unbounded scenarios\\rwhere non-reflective as well as reflective components are present. In this\\rwork, we propose UniSDF, a general purpose 3D reconstruction method that can\\rreconstruct large complex scenes with reflections. We investigate both\\rview-based as well as reflection-based color prediction parameterization\\rtechniques and find that explicitly blending these representations in 3D space\\renables reconstruction of surfaces that are more geometrically accurate,\\respecially for reflective surfaces. We further combine this representation with\\ra multi-resolution grid backbone that is trained in a coarse-to-fine manner,\\renabling faster reconstructions than prior methods. Extensive experiments on\\robject-level datasets DTU, Shiny Blender as well as unbounded datasets Mip-NeRF\\r360 and Ref-NeRF real demonstrate that our method is able to robustly\\rreconstruct complex large-scale scenes with fine details and reflective\\rsurfaces. Please see our project page at\\rhttps://fangjinhuawang.github.io/UniSDF.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13285 ,  33452kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13286\\rDate: Wed, 20 Dec 2023 18:59:58 GMT   (22548kb,D)\\r\\rTitle: Generative Multimodal Models are In-Context Learners\\rAuthors: Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong\\r  Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang\\rCategories: cs.CV\\rComments: Project page: https://baaivision.github.io/emu2\\r\\\\\\\\\\r  The human ability to easily solve multimodal tasks in context (i.e., with\\ronly a few demonstrations or simple instructions), is what current multimodal\\rsystems have largely struggled to imitate. In this work, we demonstrate that\\rthe task-agnostic in-context learning capabilities of large multimodal models\\rcan be significantly enhanced by effective scaling-up. We introduce Emu2, a\\rgenerative multimodal model with 37 billion parameters, trained on large-scale\\rmultimodal sequences with a unified autoregressive objective. Emu2 exhibits\\rstrong multimodal in-context learning abilities, even emerging to solve tasks\\rthat require on-the-fly reasoning, such as visual prompting and object-grounded\\rgeneration. The model sets a new record on multiple multimodal understanding\\rtasks in few-shot settings. When instruction-tuned to follow specific\\rinstructions, Emu2 further achieves new state-of-the-art on challenging tasks\\rsuch as question answering benchmarks for large multimodal models and\\ropen-ended subject-driven generation. These achievements demonstrate that Emu2\\rcan serve as a base model and general-purpose interface for a wide range of\\rmultimodal tasks. Code and models are publicly available to facilitate future\\rresearch.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13286 ,  22548kb)\\r%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12464 (*cross-listing*)\\rDate: Mon, 18 Dec 2023 21:11:17 GMT   (7848kb,D)\\r\\rTitle: Towards Better Serialization of Tabular Data for Few-shot Classification\\rAuthors: Sukriti Jaitly, Tanay Shah, Ashish Shugani, Razik Singh Grewal\\rCategories: cs.LG cs.AI cs.CL\\rComments: 4 pages, 2 figures\\r\\\\\\\\\\r  We present a study on the integration of Large Language Models (LLMs) in\\rtabular data classification, emphasizing an efficient framework. Building upon\\rexisting work done in TabLLM (arXiv:2210.10723), we introduce three novel\\rserialization techniques, including the standout LaTeX serialization method.\\rThis method significantly boosts the performance of LLMs in processing\\rdomain-specific datasets, Our method stands out for its memory efficiency and\\rability to fully utilize complex data structures. Through extensive\\rexperimentation, including various serialization approaches like feature\\rcombination and importance, we demonstrate our work's superiority in accuracy\\rand efficiency over traditional models.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12464 ,  7848kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12466 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 03:18:12 GMT   (493kb)\\r\\rTitle: Users Approach on Providing Feedback for Smart Home Devices\\rAuthors: Santhosh Pogaku\\rCategories: cs.HC cs.CL\\rComments: arXiv admin note: text overlap with arXiv:2312.11817\\r\\\\\\\\\\r  Smart Home technology has accomplished extraordinary interest in making\\rindividuals' lives more straightforward and more relaxing as of late.\\rTechnology as of late brought about delivering numerous savvy and refined\\rframeworks which advanced clever living innovation. In this paper, we will be\\rinvestigating the behavioural intention of user's approach on providing\\rfeedback for smart home devices. We will be conducting an online survey for\\rsample of three to five students selected by simple random sampling to study\\rthe user's motto for giving feedback on smart home devices and their\\rexpectations. We have observed that most users are ready to share their\\rfeedback on smart home devices actively to improvise the service and quality of\\rthe product to fulfill the user needs and make their lives easier.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12466 ,  493kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12655 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 22:57:13 GMT   (2512kb,D)\\r\\rTitle: Can Transformers Learn Sequential Function Classes In Context?\\rAuthors: Ryan Campbell, Emma Guo, Evan Hu, Reya Vir, Ethan Hsiao\\rCategories: cs.LG cs.AI cs.CL\\rComments: 8 pages, 8 figures\\r\\\\\\\\\\r  In-context learning (ICL) has revolutionized the capabilities of transformer\\rmodels in NLP. In our project, we extend the understanding of the mechanisms\\runderpinning ICL by exploring whether transformers can learn from sequential,\\rnon-textual function class data distributions. We introduce a novel sliding\\rwindow sequential function class and employ toy-sized transformers with a GPT-2\\rarchitecture to conduct our experiments. Our analysis indicates that these\\rmodels can indeed leverage ICL when trained on non-textual sequential function\\rclasses. Additionally, our experiments with randomized y-label sequences\\rhighlights that transformers retain some ICL capabilities even when the label\\rassociations are obfuscated. We provide evidence that transformers can reason\\rwith and understand sequentiality encoded within function classes, as reflected\\rby the effective learning of our proposed tasks. Our results also show that the\\rperformance deteriorated with increasing randomness in the labels, though not\\rto the extent one might expect, implying a potential robustness of learned\\rsequentiality against label noise. Future research may want to look into how\\rprevious explanations of transformers, such as induction heads and task\\rvectors, relate to sequentiality in ICL in these toy examples. Our\\rinvestigation lays the groundwork for further research into how transformers\\rprocess and perceive sequential data.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12655 ,  2512kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12747 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 03:44:18 GMT   (840kb,D)\\r\\rTitle: ALMANACS: A Simulatability Benchmark for Language Model Explainability\\rAuthors: Edmund Mills, Shiye Su, Stuart Russell, Scott Emmons\\rCategories: cs.LG cs.AI cs.CL stat.ML\\rComments: Code is available at\\r  https://github.com/edmundmills/ALMANACS}{https://github.com/edmundmills/ALMANACS\\r\\\\\\\\\\r  How do we measure the efficacy of language model explainability methods?\\rWhile many explainability methods have been developed, they are typically\\revaluated on bespoke tasks, preventing an apples-to-apples comparison. To help\\rfill this gap, we present ALMANACS, a language model explainability benchmark.\\rALMANACS scores explainability methods on simulatability, i.e., how well the\\rexplanations improve behavior prediction on new inputs. The ALMANACS scenarios\\rspan twelve safety-relevant topics such as ethical reasoning and advanced AI\\rbehaviors; they have idiosyncratic premises to invoke model-specific behavior;\\rand they have a train-test distributional shift to encourage faithful\\rexplanations. By using another language model to predict behavior based on the\\rexplanations, ALMANACS is a fully automated benchmark. We use ALMANACS to\\revaluate counterfactuals, rationalizations, attention, and Integrated Gradients\\rexplanations. Our results are sobering: when averaged across all topics, no\\rexplanation method outperforms the explanation-free control. We conclude that\\rdespite modest successes in prior work, developing an explanation method that\\raids simulatability in ALMANACS remains an open challenge.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12747 ,  840kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12764 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 04:52:24 GMT   (116kb)\\r\\rTitle: Lattice Rescoring Based on Large Ensemble of Complementary Neural\\r  Language Models\\rAuthors: Atsunori Ogawa, Naohiro Tawara, Marc Delcroix, Shoko Araki\\rCategories: eess.AS cs.CL cs.SD\\rComments: Accepted to ICASSP 2022\\r\\\\\\\\\\r  We investigate the effectiveness of using a large ensemble of advanced neural\\rlanguage models (NLMs) for lattice rescoring on automatic speech recognition\\r(ASR) hypotheses. Previous studies have reported the effectiveness of combining\\ra small number of NLMs. In contrast, in this study, we combine up to eight\\rNLMs, i.e., forward/backward long short-term memory/Transformer-LMs that are\\rtrained with two different random initialization seeds. We combine these NLMs\\rthrough iterative lattice generation. Since these NLMs work complementarily\\rwith each other, by combining them one by one at each rescoring iteration,\\rlanguage scores attached to given lattice arcs can be gradually refined.\\rConsequently, errors of the ASR hypotheses can be gradually reduced. We also\\rinvestigate the effectiveness of carrying over contextual information (previous\\rrescoring results) across a lattice sequence of a long speech such as a lecture\\rspeech. In experiments using a lecture speech corpus, by combining the eight\\rNLMs and using context carry-over, we obtained a 24.4% relative word error rate\\rreduction from the ASR 1-best baseline. For further comparison, we performed\\rsimultaneous (i.e., non-iterative) NLM combination and 100-best rescoring using\\rthe large ensemble of NLMs, which confirmed the advantage of lattice rescoring\\rwith iterative NLM combination.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12764 ,  116kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12783 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 06:02:12 GMT   (4052kb,D)\\r\\rTitle: Stable Distillation: Regularizing Continued Pre-training for\\r  Low-Resource Automatic Speech Recognition\\rAuthors: Ashish Seth and Sreyan Ghosh and S. Umesh and Dinesh Manocha\\rCategories: eess.AS cs.AI cs.CL cs.SD\\rComments: Accepted to ICASSP 2024. Code:\\r  https://github.com/cs20s030/stable_distillation\\r\\\\\\\\\\r  Continued self-supervised (SSL) pre-training for adapting existing SSL models\\rto the target domain has shown to be extremely effective for low-resource\\rAutomatic Speech Recognition (ASR). This paper proposes Stable Distillation, a\\rsimple and novel approach for SSL-based continued pre-training that boosts ASR\\rperformance in the target domain where both labeled and unlabeled data are\\rlimited. Stable Distillation employs self-distillation as regularization for\\rcontinued pre-training, alleviating the over-fitting issue, a common problem\\rcontinued pre-training faces when the source and target domains differ.\\rSpecifically, first, we perform vanilla continued pre-training on an initial\\rSSL pre-trained model on the target domain ASR dataset and call it the teacher.\\rNext, we take the same initial pre-trained model as a student to perform\\rcontinued pre-training while enforcing its hidden representations to be close\\rto that of the teacher (via MSE loss). This student is then used for downstream\\rASR fine-tuning on the target dataset. In practice, Stable Distillation\\routperforms all our baselines by 0.8 - 7 WER when evaluated in various\\rexperimental settings.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12783 ,  4052kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12881 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 09:45:44 GMT   (4261kb,D)\\r\\rTitle: Big Tech influence over AI research revisited: memetic analysis of\\r  attribution of ideas to affiliation\\rAuthors: Stanis{\\\\l}aw Gizi\\\\'nski, Paulina Kaczy\\\\'nska, Hubert Ruczy\\\\'nski,\\r  Emilia Wi\\\\'snios, Bartosz Pieli\\\\'nski, Przemys{\\\\l}aw Biecek, Julian\\r  Sienkiewicz\\rCategories: physics.soc-ph cs.CL cs.SI\\r\\\\\\\\\\r  There exists a growing discourse around the domination of Big Tech on the\\rlandscape of artificial intelligence (AI) research, yet our comprehension of\\rthis phenomenon remains cursory. This paper aims to broaden and deepen our\\runderstanding of Big Tech's reach and power within AI research. It highlights\\rthe dominance not merely in terms of sheer publication volume but rather in the\\rpropagation of new ideas or \\\\textit{memes}. Current studies often oversimplify\\rthe concept of influence to the share of affiliations in academic papers,\\rtypically sourced from limited databases such as arXiv or specific academic\\rconferences.\\r  The main goal of this paper is to unravel the specific nuances of such\\rinfluence, determining which AI ideas are predominantly driven by Big Tech\\rentities. By employing network and memetic analysis on AI-oriented paper\\rabstracts and their citation network, we are able to grasp a deeper insight\\rinto this phenomenon. By utilizing two databases: OpenAlex and S2ORC, we are\\rable to perform such analysis on a much bigger scale than previous attempts.\\r  Our findings suggest, that while Big Tech-affiliated papers are\\rdisproportionately more cited in some areas, the most cited papers are those\\raffiliated with both Big Tech and Academia. Focusing on the most contagious\\rmemes, their attribution to specific affiliation groups (Big Tech, Academia,\\rmixed affiliation) seems to be equally distributed between those three groups.\\rThis suggests that the notion of Big Tech domination over AI research is\\roversimplified in the discourse.\\r  Ultimately, this more nuanced understanding of Big Tech's and Academia's\\rinfluence could inform a more symbiotic alliance between these stakeholders\\rwhich would better serve the dual goals of societal welfare and the scientific\\rintegrity of AI research.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12881 ,  4261kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12989 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 12:46:44 GMT   (2326kb,D)\\r\\rTitle: Benchmarking and Analyzing In-context Learning, Fine-tuning and\\r  Supervised Learning for Biomedical Knowledge Curation: a focused study on\\r  chemical entities of biological interest\\rAuthors: Emily Groves, Minhong Wang, Yusuf Abdulle, Holger Kunz, Jason\\r  Hoelscher-Obermaier, Ronin Wu, Honghan Wu\\rCategories: cs.LG cs.CL q-bio.QM\\rComments: 26 pages, 5 figures, 14 tables\\r\\\\\\\\\\r  Automated knowledge curation for biomedical ontologies is key to ensure that\\rthey remain comprehensive, high-quality and up-to-date. In the era of\\rfoundational language models, this study compares and analyzes three NLP\\rparadigms for curation tasks: in-context learning (ICL), fine-tuning (FT), and\\rsupervised learning (ML). Using the Chemical Entities of Biological Interest\\r(ChEBI) database as a model ontology, three curation tasks were devised. For\\rICL, three prompting strategies were employed with GPT-4, GPT-3.5, BioGPT.\\rPubmedBERT was chosen for the FT paradigm. For ML, six embedding models were\\rutilized for training Random Forest and Long-Short Term Memory models. Five\\rsetups were designed to assess ML and FT model performance across different\\rdata availability scenarios.Datasets for curation tasks included: task 1\\r(620,386), task 2 (611,430), and task 3 (617,381), maintaining a 50:50 positive\\rversus negative ratio. For ICL models, GPT-4 achieved best accuracy scores of\\r0.916, 0.766 and 0.874 for tasks 1-3 respectively. In a direct comparison, ML\\r(trained on ~260,000 triples) outperformed ICL in accuracy across all tasks.\\r(accuracy differences: +.11, +.22 and +.17). Fine-tuned PubmedBERT performed\\rsimilarly to leading ML models in tasks 1 & 2 (F1 differences: -.014 and\\r+.002), but worse in task 3 (-.048). Simulations revealed performance declines\\rin both ML and FT models with smaller and higher imbalanced training data.\\rwhere ICL (particularly GPT-4) excelled in tasks 1 & 3. GPT-4 excelled in tasks\\r1 and 3 with less than 6,000 triples, surpassing ML/FT. ICL underperformed\\rML/FT in task 2.ICL-augmented foundation models can be good assistants for\\rknowledge curation with correct prompting, however, not making ML and FT\\rparadigms obsolete. The latter two require task-specific data to beat ICL. In\\rsuch cases, ML relies on small pretrained embeddings, minimizing computational\\rdemands.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12989 ,  2326kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13026 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 13:50:05 GMT   (5228kb,D)\\r\\rTitle: FusDom: Combining In-Domain and Out-of-Domain Knowledge for Continuous\\r  Self-Supervised Learning\\rAuthors: Ashish Seth and Sreyan Ghosh and S. Umesh and Dinesh Manocha\\rCategories: eess.AS cs.AI cs.CL cs.SD\\rComments: Accepted at ICASSP 2024. Code: https://github.com/cs20s030/fusdom\\r\\\\\\\\\\r  Continued pre-training (CP) offers multiple advantages, like target domain\\radaptation and the potential to exploit the continuous stream of unlabeled data\\ravailable online. However, continued pre-training on out-of-domain\\rdistributions often leads to catastrophic forgetting of previously acquired\\rknowledge, leading to sub-optimal ASR performance. This paper presents FusDom,\\ra simple and novel methodology for SSL-based continued pre-training. FusDom\\rlearns speech representations that are robust and adaptive yet not forgetful of\\rconcepts seen in the past. Instead of solving the SSL pre-text task on the\\routput representations of a single model, FusDom leverages two identical\\rpre-trained SSL models, a teacher and a student, with a modified pre-training\\rhead to solve the CP SSL pre-text task. This head employs a cross-attention\\rmechanism between the representations of both models while only the student\\rreceives gradient updates and the teacher does not. Finally, the student is\\rfine-tuned for ASR. In practice, FusDom outperforms all our baselines across\\rsettings significantly, with WER improvements in the range of 0.2 WER - 7.3 WER\\rin the target domain while retaining the performance in the earlier domain.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13026 ,  5228kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13119 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 15:38:59 GMT   (1406kb,D)\\r\\rTitle: Prometheus: Infrastructure Security Posture Analysis with AI-generated\\r  Attack Graphs\\rAuthors: Xin Jin, Charalampos Katsis, Fan Sang, Jiahao Sun, Elisa Bertino,\\r  Ramana Rao Kompella, Ashish Kundu\\rCategories: cs.CR cs.CL cs.LG\\r\\\\\\\\\\r  The rampant occurrence of cybersecurity breaches imposes substantial\\rlimitations on the progress of network infrastructures, leading to compromised\\rdata, financial losses, potential harm to individuals, and disruptions in\\ressential services. The current security landscape demands the urgent\\rdevelopment of a holistic security assessment solution that encompasses\\rvulnerability analysis and investigates the potential exploitation of these\\rvulnerabilities as attack paths. In this paper, we propose Prometheus, an\\radvanced system designed to provide a detailed analysis of the security posture\\rof computing infrastructures. Using user-provided information, such as device\\rdetails and software versions, Prometheus performs a comprehensive security\\rassessment. This assessment includes identifying associated vulnerabilities and\\rconstructing potential attack graphs that adversaries can exploit. Furthermore,\\rPrometheus evaluates the exploitability of these attack paths and quantifies\\rthe overall security posture through a scoring mechanism. The system takes a\\rholistic approach by analyzing security layers encompassing hardware, system,\\rnetwork, and cryptography. Furthermore, Prometheus delves into the\\rinterconnections between these layers, exploring how vulnerabilities in one\\rlayer can be leveraged to exploit vulnerabilities in others. In this paper, we\\rpresent the end-to-end pipeline implemented in Prometheus, showcasing the\\rsystematic approach adopted for conducting this thorough security analysis.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13119 ,  1406kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13219 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 17:38:04 GMT   (5016kb,D)\\r\\rTitle: Interactive Visual Task Learning for Robots\\rAuthors: Weiwei Gu, Anant Sah, Nakul Gopalan\\rCategories: cs.RO cs.CL cs.CV\\rComments: In Proceedings of The 38th Annual AAAI Conference on Artificial\\r  Intelligence\\r\\\\\\\\\\r  We present a framework for robots to learn novel visual concepts and tasks\\rvia in-situ linguistic interactions with human users. Previous approaches have\\reither used large pre-trained visual models to infer novel objects zero-shot,\\ror added novel concepts along with their attributes and representations to a\\rconcept hierarchy. We extend the approaches that focus on learning visual\\rconcept hierarchies by enabling them to learn novel concepts and solve unseen\\rrobotics tasks with them. To enable a visual concept learner to solve robotics\\rtasks one-shot, we developed two distinct techniques. Firstly, we propose a\\rnovel approach, Hi-Viscont(HIerarchical VISual CONcept learner for Task), which\\raugments information of a novel concept to its parent nodes within a concept\\rhierarchy. This information propagation allows all concepts in a hierarchy to\\rupdate as novel concepts are taught in a continual learning setting. Secondly,\\rwe represent a visual task as a scene graph with language annotations, allowing\\rus to create novel permutations of a demonstrated task zero-shot in-situ. We\\rpresent two sets of results. Firstly, we compare Hi-Viscont with the baseline\\rmodel (FALCON) on visual question answering(VQA) in three domains. While being\\rcomparable to the baseline model on leaf level concepts, Hi-Viscont achieves an\\rimprovement of over 9% on non-leaf concepts on average. We compare our model's\\rperformance against the baseline FALCON model. Our framework achieves 33%\\rimprovements in success rate metric, and 19% improvements in the object level\\raccuracy compared to the baseline model. With both of these results we\\rdemonstrate the ability of our model to learn tasks and concepts in a continual\\rlearning setting on the robot.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13219 ,  5016kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12483 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 16:19:33 GMT   (3057kb,D)\\r\\rTitle: SCoTTi: Save Computation at Training Time with an adaptive framework\\rAuthors: Ziyu Lin, Enzo Tartaglione, Van-Tam Nguyen\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\\\r  On-device training is an emerging approach in machine learning where models\\rare trained on edge devices, aiming to enhance privacy protection and real-time\\rperformance. However, edge devices typically possess restricted computational\\rpower and resources, making it challenging to perform computationally intensive\\rmodel training tasks. Consequently, reducing resource consumption during\\rtraining has become a pressing concern in this field. To this end, we propose\\rSCoTTi (Save Computation at Training Time), an adaptive framework that\\raddresses the aforementioned challenge. It leverages an optimizable threshold\\rparameter to effectively reduce the number of neuron updates during training\\rwhich corresponds to a decrease in memory and computation footprint. Our\\rproposed approach demonstrates superior performance compared to the\\rstate-of-the-art methods regarding computational resource savings on various\\rcommonly employed benchmarks and popular architectures, including ResNets,\\rMobileNet, and Swin-T.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12483 ,  3057kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12488 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 17:17:25 GMT   (14882kb,D)\\r\\rTitle: Foreseeing Reconstruction Quality of Gradient Inversion: An Optimization\\r  Perspective\\rAuthors: HyeongGwon Hong, Yooshin Cho, Hanbyel Cho, Jaesung Ahn, Junmo Kim\\rCategories: cs.LG cs.CR cs.CV\\rComments: To appear in AAAI 2024\\r\\\\\\\\\\r  Gradient inversion attacks can leak data privacy when clients share weight\\rupdates with the server in federated learning (FL). Existing studies mainly use\\rL2 or cosine distance as the loss function for gradient matching in the attack.\\rOur empirical investigation shows that the vulnerability ranking varies with\\rthe loss function used. Gradient norm, which is commonly used as a\\rvulnerability proxy for gradient inversion attack, cannot explain this as it\\rremains constant regardless of the loss function for gradient matching. In this\\rpaper, we propose a loss-aware vulnerability proxy (LAVP) for the first time.\\rLAVP refers to either the maximum or minimum eigenvalue of the Hessian with\\rrespect to gradient matching loss at ground truth. This suggestion is based on\\rour theoretical findings regarding the local optimization of the gradient\\rinversion in proximity to the ground truth, which corresponds to the worst case\\rattack scenario. We demonstrate the effectiveness of LAVP on various\\rarchitectures and datasets, showing its consistent superiority over the\\rgradient norm in capturing sample vulnerabilities. The performance of each\\rproxy is measured in terms of Spearman's rank correlation with respect to\\rseveral similarity scores. This work will contribute to enhancing FL security\\ragainst any potential loss functions beyond L2 or cosine distance in the\\rfuture.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12488 ,  14882kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12599 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 20:59:19 GMT   (8495kb,D)\\r\\rTitle: Unsupervised Segmentation of Colonoscopy Images\\rAuthors: Heming Yao, J\\\\'er\\\\^ome L\\\\uscher, Benjamin Gutierrez Becker, Josep\\r  Ar\\\\'us-Pous, Tommaso Biancalani, Amelie Bigorgne, David Richmond\\rCategories: eess.IV cs.AI cs.CV cs.LG\\r\\\\\\\\\\r  Colonoscopy plays a crucial role in the diagnosis and prognosis of various\\rgastrointestinal diseases. Due to the challenges of collecting large-scale\\rhigh-quality ground truth annotations for colonoscopy images, and more\\rgenerally medical images, we explore using self-supervised features from vision\\rtransformers in three challenging tasks for colonoscopy images. Our results\\rindicate that image-level features learned from DINO models achieve image\\rclassification performance comparable to fully supervised models, and\\rpatch-level features contain rich semantic information for object detection.\\rFurthermore, we demonstrate that self-supervised features combined with\\runsupervised segmentation can be used to discover multiple clinically relevant\\rstructures in a fully unsupervised manner, demonstrating the tremendous\\rpotential of applying these methods in medical image analysis.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12599 ,  8495kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12606 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 21:21:25 GMT   (702kb,D)\\r\\rTitle: Optimizing Neural Networks with Gradient Lexicase Selection\\rAuthors: Li Ding, Lee Spector\\rCategories: cs.LG cs.CV cs.NE\\rComments: ICLR 2022\\rJournal-ref: International Conference on Learning Representations (2022)\\r\\\\\\\\\\r  One potential drawback of using aggregated performance measurement in machine\\rlearning is that models may learn to accept higher errors on some training\\rcases as compromises for lower errors on others, with the lower errors actually\\rbeing instances of overfitting. This can lead to both stagnation at local\\roptima and poor generalization. Lexicase selection is an uncompromising method\\rdeveloped in evolutionary computation, which selects models on the basis of\\rsequences of individual training case errors instead of using aggregated\\rmetrics such as loss and accuracy. In this paper, we investigate how lexicase\\rselection, in its general form, can be integrated into the context of deep\\rlearning to enhance generalization. We propose Gradient Lexicase Selection, an\\roptimization framework that combines gradient descent and lexicase selection in\\ran evolutionary fashion. Our experimental results demonstrate that the proposed\\rmethod improves the generalization performance of various widely-used deep\\rneural network architectures across three image classification benchmarks.\\rAdditionally, qualitative analysis suggests that our method assists networks in\\rlearning more diverse representations. Our source code is available on GitHub:\\rhttps://github.com/ld-ing/gradient-lexicase.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12606 ,  702kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12644 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 22:40:51 GMT   (16240kb,D)\\r\\rTitle: Rotational Augmented Noise2Inverse for Low-dose Computed Tomography\\r  Reconstruction\\rAuthors: Hang Xu, Alessandro Perelli\\rCategories: eess.IV cs.CV physics.med-ph\\rComments: 14 pages, 12 figures, accepted manuscript in IEEE Transactions on\\r  Radiation and Plasma Medical Sciences\\rMSC-class: 92C55, 94A08\\rACM-class: I.4.5; J.3\\rDOI: 10.1109/TRPMS.2023.3340955\\r\\\\\\\\\\r  In this work, we present a novel self-supervised method for Low Dose Computed\\rTomography (LDCT) reconstruction. Reducing the radiation dose to patients\\rduring a CT scan is a crucial challenge since the quality of the reconstruction\\rhighly degrades because of low photons or limited measurements. Supervised deep\\rlearning methods have shown the ability to remove noise in images but require\\raccurate ground truth which can be obtained only by performing additional\\rhigh-radiation CT scans. Therefore, we propose a novel self-supervised\\rframework for LDCT, in which ground truth is not required for training the\\rconvolutional neural network (CNN). Based on the Noise2Inverse (N2I) method, we\\renforce in the training loss the equivariant property of rotation\\rtransformation, which is induced by the CT imaging system, to improve the\\rquality of the CT image in a lower dose. Numerical and experimental results\\rshow that the reconstruction accuracy of N2I with sparse views is degrading\\rwhile the proposed rotational augmented Noise2Inverse (RAN2I) method keeps\\rbetter image quality over a different range of sampling angles. Finally, the\\rquantitative results demonstrate that RAN2I achieves higher image quality\\rcompared to N2I, and experimental results of RAN2I on real projection data show\\rcomparable performance to supervised learning.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12644 ,  16240kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12648 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 22:45:57 GMT   (316kb,D)\\r\\rTitle: IS-DARTS: Stabilizing DARTS through Precise Measurement on Candidate\\r  Importance\\rAuthors: Hongyi He, Longjun Liu, Haonan Zhang and Nanning Zheng\\rCategories: cs.LG cs.CV\\rComments: accepted by AAAI2024, paper + supplementary, 11 pages\\r\\\\\\\\\\r  Among existing Neural Architecture Search methods, DARTS is known for its\\refficiency and simplicity. This approach applies continuous relaxation of\\rnetwork representation to construct a weight-sharing supernet and enables the\\ridentification of excellent subnets in just a few GPU days. However,\\rperformance collapse in DARTS results in deteriorating architectures filled\\rwith parameter-free operations and remains a great challenge to the robustness.\\rTo resolve this problem, we reveal that the fundamental reason is the biased\\restimation of the candidate importance in the search space through theoretical\\rand experimental analysis, and more precisely select operations via\\rinformation-based measurements. Furthermore, we demonstrate that the excessive\\rconcern over the supernet and inefficient utilization of data in bi-level\\roptimization also account for suboptimal results. We adopt a more realistic\\robjective focusing on the performance of subnets and simplify it with the help\\rof the information-based measurements. Finally, we explain theoretically why\\rprogressively shrinking the width of the supernet is necessary and reduce the\\rapproximation error of optimal weights in DARTS. Our proposed method, named\\rIS-DARTS, comprehensively improves DARTS and resolves the aforementioned\\rproblems. Extensive experiments on NAS-Bench-201 and DARTS-based search space\\rdemonstrate the effectiveness of IS-DARTS.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12648 ,  316kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12649 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 22:50:02 GMT   (732kb,D)\\r\\rTitle: Surf-CDM: Score-Based Surface Cold-Diffusion Model For Medical Image\\r  Segmentation\\rAuthors: Fahim Ahmed Zaman, Mathews Jacob, Amanda Chang, Kan Liu, Milan Sonka\\r  and Xiaodong Wu\\rCategories: eess.IV cs.CV\\rComments: 5 pages, 5 figures, conference\\r\\\\\\\\\\r  Diffusion models have shown impressive performance for image generation,\\roften times outperforming other generative models. Since their introduction,\\rresearchers have extended the powerful noise-to-image denoising pipeline to\\rdiscriminative tasks, including image segmentation. In this work we propose a\\rconditional score-based generative modeling framework for medical image\\rsegmentation which relies on a parametric surface representation for the\\rsegmentation masks. The surface re-parameterization allows the direct\\rapplication of standard diffusion theory, as opposed to when the mask is\\rrepresented as a binary mask. Moreover, we adapted an extended variant of the\\rdiffusion technique known as the cold-diffusion where the diffusion model can\\rbe constructed with deterministic perturbations instead of Gaussian noise,\\rwhich facilitates significantly faster convergence in the reverse diffusion. We\\revaluated our method on the segmentation of the left ventricle from 65\\rtransthoracic echocardiogram videos (2230 echo image frames) and compared its\\rperformance to the most popular and widely used image segmentation models. Our\\rproposed model not only outperformed the compared methods in terms of\\rsegmentation accuracy, but also showed potential in estimating segmentation\\runcertainties for further downstream analyses due to its inherent generative\\rnature.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12649 ,  732kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12653 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 22:53:32 GMT   (432kb,D)\\r\\rTitle: Diagnosis Of Takotsubo Syndrome By Robust Feature Selection From The\\r  Complex Latent Space Of DL-based Segmentation Network\\rAuthors: Fahim Ahmed Zaman, Wahidul Alam, Tarun Kanti Roy, Amanda Chang, Kan\\r  Liu and Xiaodong Wu\\rCategories: eess.IV cs.CV\\rComments: 5 pages, 3 figures, conference\\r\\\\\\\\\\r  Researchers have shown significant correlations among segmented objects in\\rvarious medical imaging modalities and disease related pathologies. Several\\rstudies showed that using hand crafted features for disease prediction neglects\\rthe immense possibility to use latent features from deep learning (DL) models\\rwhich may reduce the overall accuracy of differential diagnosis. However,\\rdirectly using classification or segmentation models on medical to learn latent\\rfeatures opt out robust feature selection and may lead to overfitting. To fill\\rthis gap, we propose a novel feature selection technique using the latent space\\rof a segmentation model that can aid diagnosis. We evaluated our method in\\rdifferentiating a rare cardiac disease: Takotsubo Syndrome (TTS) from the ST\\relevation myocardial infarction (STEMI) using echocardiogram videos (echo). TTS\\rcan mimic clinical features of STEMI in echo and extremely hard to distinguish.\\rOur approach shows promising results in differential diagnosis of TTS with 82%\\rdiagnosis accuracy beating the previous state-of-the-art (SOTA) approach.\\rMoreover, the robust feature selection technique using LASSO algorithm shows\\rgreat potential in reducing the redundant features and creates a robust\\rpipeline for short- and long-term disease prognoses in the downstream analysis.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12653 ,  432kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12668 (*cross-listing*)\\rDate: Tue, 19 Dec 2023 23:48:43 GMT   (5968kb,D)\\r\\rTitle: Convolutional Channel-wise Competitive Learning for the Forward-Forward\\r  Algorithm\\rAuthors: Andreas Papachristodoulou, Christos Kyrkou, Stelios Timotheou,\\r  Theocharis Theocharides\\rCategories: cs.LG cs.AI cs.CV\\rComments: To be published in AAAI 2024, 11 pages, 7 figures\\r\\\\\\\\\\r  The Forward-Forward (FF) Algorithm has been recently proposed to alleviate\\rthe issues of backpropagation (BP) commonly used to train deep neural networks.\\rHowever, its current formulation exhibits limitations such as the generation of\\rnegative data, slower convergence, and inadequate performance on complex tasks.\\rIn this paper, we take the main ideas of FF and improve them by leveraging\\rchannel-wise competitive learning in the context of convolutional neural\\rnetworks for image classification tasks. A layer-wise loss function is\\rintroduced that promotes competitive learning and eliminates the need for\\rnegative data construction. To enhance both the learning of compositional\\rfeatures and feature space partitioning, a channel-wise feature separator and\\rextractor block is proposed that complements the competitive learning process.\\rOur method outperforms recent FF-based models on image classification tasks,\\rachieving testing errors of 0.58%, 7.69%, 21.89%, and 48.77% on MNIST,\\rFashion-MNIST, CIFAR-10 and CIFAR-100 respectively. Our approach bridges the\\rperformance gap between FF learning and BP methods, indicating the potential of\\rour proposed approach to learn useful representations in a layer-wise modular\\rfashion, enabling more efficient and flexible learning.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12668 ,  5968kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12691 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 01:29:11 GMT   (8405kb,D)\\r\\rTitle: How Good Are Deep Generative Models for Solving Inverse Problems?\\rAuthors: Shichong Peng, Alireza Moazeni, Ke Li\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Deep generative models, such as diffusion models, GANs, and IMLE, have shown\\rimpressive capability in tackling inverse problems. However, the validity of\\rmodel-generated solutions w.r.t. the forward problem and the reliability of\\rassociated uncertainty estimates remain understudied. This study evaluates\\rrecent diffusion-based, GAN-based, and IMLE-based methods on three inverse\\rproblems, i.e., $16\\\\times$ super-resolution, colourization, and image\\rdecompression. We assess the validity of these models' outputs as solutions to\\rthe inverse problems and conduct a thorough analysis of the reliability of the\\rmodels' estimates of uncertainty over the solution. Overall, we find that the\\rIMLE-based CHIMLE method outperforms other methods in terms of producing valid\\rsolutions and reliable uncertainty estimates.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12691 ,  8405kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12789 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 06:22:21 GMT   (3127kb,D)\\r\\rTitle: SLP-Net:An efficient lightweight network for segmentation of skin\\r  lesions\\rAuthors: Bo Yang, Hong Peng, Chenggang Guo, Xiaohui Luo, Jun Wang, Xianzhong\\r  Long\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  Prompt treatment for melanoma is crucial. To assist physicians in identifying\\rlesion areas precisely in a quick manner, we propose a novel skin lesion\\rsegmentation technique namely SLP-Net, an ultra-lightweight segmentation\\rnetwork based on the spiking neural P(SNP) systems type mechanism. Most\\rexisting convolutional neural networks achieve high segmentation accuracy while\\rneglecting the high hardware cost. SLP-Net, on the contrary, has a very small\\rnumber of parameters and a high computation speed. We design a lightweight\\rmulti-scale feature extractor without the usual encoder-decoder structure.\\rRather than a decoder, a feature adaptation module is designed to replace it\\rand implement multi-scale information decoding. Experiments at the ISIC2018\\rchallenge demonstrate that the proposed model has the highest Acc and DSC among\\rthe state-of-the-art methods, while experiments on the PH2 dataset also\\rdemonstrate a favorable generalization ability. Finally, we compare the\\rcomputational complexity as well as the computational speed of the models in\\rexperiments, where SLP-Net has the highest overall superiority\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12789 ,  3127kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12824 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 07:58:41 GMT   (16071kb,D)\\r\\rTitle: FedSODA: Federated Cross-assessment and Dynamic Aggregation for\\r  Histopathology Segmentation\\rAuthors: Yuan Zhang, Yaolei Qi, Xiaoming Qi, Lotfi Senhadji, Yongyue Wei, Feng\\r  Chen, Guanyu Yang\\rCategories: eess.IV cs.CV\\rComments: Accepted by ICASSP2024\\r\\\\\\\\\\r  Federated learning (FL) for histopathology image segmentation involving\\rmultiple medical sites plays a crucial role in advancing the field of accurate\\rdisease diagnosis and treatment. However, it is still a task of great\\rchallenges due to the sample imbalance across clients and large data\\rheterogeneity from disparate organs, variable segmentation tasks, and diverse\\rdistribution. Thus, we propose a novel FL approach for histopathology nuclei\\rand tissue segmentation, FedSODA, via synthetic-driven cross-assessment\\roperation (SO) and dynamic stratified-layer aggregation (DA). Our SO constructs\\ra cross-assessment strategy to connect clients and mitigate the representation\\rbias under sample imbalance. Our DA utilizes layer-wise interaction and dynamic\\raggregation to diminish heterogeneity and enhance generalization. The\\reffectiveness of our FedSODA has been evaluated on the most extensive\\rhistopathology image segmentation dataset from 7 independent datasets. The code\\ris available at https://github.com/yuanzhang7/FedSODA.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12824 ,  16071kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12833 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 08:30:07 GMT   (8854kb,D)\\r\\rTitle: Learning Exhaustive Correlation for Spectral Super-Resolution: Where\\r  Unified Spatial-Spectral Attention Meets Mutual Linear Dependence\\rAuthors: Hongyuan Wang, Lizhi Wang, Jiang Xu, Chang Chen, Xue Hu, Fenglong Song\\r  and Youliang Yan\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Spectral super-resolution from the easily obtainable RGB image to\\rhyperspectral image (HSI) has drawn increasing interest in the field of\\rcomputational photography. The crucial aspect of spectral super-resolution lies\\rin exploiting the correlation within HSIs. However, two types of bottlenecks in\\rexisting Transformers limit performance improvement and practical applications.\\rFirst, existing Transformers often separately emphasize either spatial-wise or\\rspectral-wise correlation, disrupting the 3D features of HSI and hindering the\\rexploitation of unified spatial-spectral correlation. Second, the existing\\rself-attention mechanism learns the correlation between pairs of tokens and\\rcaptures the full-rank correlation matrix, leading to its inability to\\restablish mutual linear dependence among multiple tokens. To address these\\rissues, we propose a novel Exhaustive Correlation Transformer (ECT) for\\rspectral super-resolution. First, we propose a Spectral-wise Discontinuous 3D\\r(SD3D) splitting strategy, which models unified spatial-spectral correlation by\\rsimultaneously utilizing spatial-wise continuous splitting and spectral-wise\\rdiscontinuous splitting. Second, we propose a Dynamic Low-Rank Mapping (DLRM)\\rmodel, which captures mutual linear dependence among multiple tokens through a\\rdynamically calculated low-rank dependence map. By integrating unified\\rspatial-spectral attention with mutual linear dependence, our ECT can establish\\rexhaustive correlation within HSI. The experimental results on both simulated\\rand real data indicate that our method achieves state-of-the-art performance.\\rCodes and pretrained models will be available later.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12833 ,  8854kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12838 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 08:42:57 GMT   (3749kb,D)\\r\\rTitle: FedA3I: Annotation Quality-Aware Aggregation for Federated Medical Image\\r  Segmentation Against Heterogeneous Annotation Noise\\rAuthors: Nannan Wu, Zhaobin Sun, Zengqiang Yan, Li Yu\\rCategories: cs.LG cs.CV\\rComments: Accepted at AAAI'24\\r\\\\\\\\\\r  Federated learning (FL) has emerged as a promising paradigm for training\\rsegmentation models on decentralized medical data, owing to its\\rprivacy-preserving property. However, existing research overlooks the prevalent\\rannotation noise encountered in real-world medical datasets, which limits the\\rperformance ceilings of FL. In this paper, we, for the first time, identify and\\rtackle this problem. For problem formulation, we propose a contour evolution\\rfor modeling non-independent and identically distributed (Non-IID) noise across\\rpixels within each client and then extend it to the case of multi-source data\\rto form a heterogeneous noise model (\\\\textit{i.e.}, Non-IID annotation noise\\racross clients). For robust learning from annotations with such two-level\\rNon-IID noise, we emphasize the importance of data quality in model\\raggregation, allowing high-quality clients to have a greater impact on FL. To\\rachieve this, we propose \\\\textbf{Fed}erated learning with \\\\textbf{A}nnotation\\rqu\\\\textbf{A}lity-aware \\\\textbf{A}ggregat\\\\textbf{I}on, named \\\\textbf{FedA$^3$I},\\rby introducing a quality factor based on client-wise noise estimation.\\rSpecifically, noise estimation at each client is accomplished through the\\rGaussian mixture model and then incorporated into model aggregation in a\\rlayer-wise manner to up-weight high-quality clients. Extensive experiments on\\rtwo real-world medical image segmentation datasets demonstrate the superior\\rperformance of FedA$^3$I against the state-of-the-art approaches in dealing\\rwith cross-client annotation noise. The code is available at\\r\\\\color{blue}{https://github.com/wnn2000/FedAAAI}.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12838 ,  3749kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12848 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 08:56:35 GMT   (10365kb,D)\\r\\rTitle: Quantum Annealing for Computer Vision Minimization Problems\\rAuthors: Shahrokh Heidari, Michael J. Dinneen, Patrice Delmas\\rCategories: quant-ph cs.CV\\r\\\\\\\\\\r  Computer Vision (CV) labelling algorithms play a pivotal role in the domain\\rof low-level vision. For decades, it has been known that these problems can be\\relegantly formulated as discrete energy minimization problems derived from\\rprobabilistic graphical models (such as Markov Random Fields). Despite recent\\radvances in inference algorithms (such as graph-cut and message-passing\\ralgorithms), the resulting energy minimization problems are generally viewed as\\rintractable. The emergence of quantum computations, which offer the potential\\rfor faster solutions to certain problems than classical methods, has led to an\\rincreased interest in utilizing quantum properties to overcome intractable\\rproblems. Recently, there has also been a growing interest in Quantum Computer\\rVision (QCV), with the hope of providing a credible alternative or assistant to\\rdeep learning solutions in the field. This study investigates a new Quantum\\rAnnealing based inference algorithm for CV discrete energy minimization\\rproblems. Our contribution is focused on Stereo Matching as a significant CV\\rlabeling problem. As a proof of concept, we also use a hybrid quantum-classical\\rsolver provided by D-Wave System to compare our results with the best classical\\rinference algorithms in the literature.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12848 ,  10365kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12876 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 09:39:53 GMT   (858kb)\\r\\rTitle: COVID-19 Diagnosis: ULGFBP-ResNet51 approach on the CT and the Chest\\r  X-ray Images Classification\\rAuthors: Vida Esmaeili and Mahmood Mohassel Feghhi and Seyed Omid Shahdi\\rCategories: eess.IV cs.CV\\rComments: 16 pages, 8 figures, submitted for possible journal publication\\r\\\\\\\\\\r  The contagious and pandemic COVID-19 disease is currently considered as the\\rmain health concern and posed widespread panic across human-beings. It affects\\rthe human respiratory tract and lungs intensely. So that it has imposed\\rsignificant threats for premature death. Although, its early diagnosis can play\\ra vital role in revival phase, the radiography tests with the manual\\rintervention are a time-consuming process. Time is also limited for such manual\\rinspecting of numerous patients in the hospitals. Thus, the necessity of\\rautomatic diagnosis on the chest X-ray or the CT images with a high efficient\\rperformance is urgent. Toward this end, we propose a novel method, named as the\\rULGFBP-ResNet51 to tackle with the COVID-19 diagnosis in the images. In fact,\\rthis method includes Uniform Local Binary Pattern (ULBP), Gabor Filter (GF),\\rand ResNet51. According to our results, this method could offer superior\\rperformance in comparison with the other methods, and attain maximum accuracy.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12876 ,  858kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12880 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 09:45:21 GMT   (10123kb,D)\\r\\rTitle: Testing the Segment Anything Model on radiology data\\rAuthors: Jos\\\\'e Guilherme de Almeida and Nuno M. Rodrigues and Sara Silva and\\r  Nickolas Papanikolaou\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  Deep learning models trained with large amounts of data have become a recent\\rand effective approach to predictive problem solving -- these have become known\\ras foundation models as they can be used as fundamental tools for other\\rapplications. While the paramount examples of image classification (earlier)\\rand large language models (more recently) led the way, the Segment Anything\\rModel (SAM) was recently proposed and stands as the first foundation model for\\rimage segmentation, trained on over 10 million images and with recourse to over\\r1 billion masks. However, the question remains -- what are the limits of this\\rfoundation? Given that magnetic resonance imaging (MRI) stands as an important\\rmethod of diagnosis, we sought to understand whether SAM could be used for a\\rfew tasks of zero-shot segmentation using MRI data. Particularly, we wanted to\\rknow if selecting masks from the pool of SAM predictions could lead to good\\rsegmentations.\\r  Here, we provide a critical assessment of the performance of SAM on magnetic\\rresonance imaging data. We show that, while acceptable in a very limited set of\\rcases, the overall trend implies that these models are insufficient for MRI\\rsegmentation across the whole volume, but can provide good segmentations in a\\rfew, specific slices. More importantly, we note that while foundation models\\rtrained on natural images are set to become key aspects of predictive\\rmodelling, they may prove ineffective when used on other imaging modalities.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12880 ,  10123kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12990 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 12:48:18 GMT   (1418kb,D)\\r\\rTitle: Multi-task Learning To Improve Semantic Segmentation Of CBCT Scans Using\\r  Image Reconstruction\\rAuthors: Maximilian Ernst Tschuchnig, Julia Coste-Marin, Philipp Steininger,\\r  Michael Gadermayr\\rCategories: eess.IV cs.CV\\rComments: Accepted at German Conference on Medical Image Computing (BVM) 2024\\r\\\\\\\\\\r  Semantic segmentation is a crucial task in medical image processing,\\ressential for segmenting organs or lesions such as tumors. In this study we aim\\rto improve automated segmentation in CBCTs through multi-task learning. To\\revaluate effects on different volume qualities, a CBCT dataset is synthesised\\rfrom the CT Liver Tumor Segmentation Benchmark (LiTS) dataset. To improve\\rsegmentation, two approaches are investigated. First, we perform multi-task\\rlearning to add morphology based regularization through a volume reconstruction\\rtask. Second, we use this reconstruction task to reconstruct the best quality\\rCBCT (most similar to the original CT), facilitating denoising effects. We\\rexplore both holistic and patch-based approaches. Our findings reveal that,\\respecially using a patch-based approach, multi-task learning improves\\rsegmentation in most cases and that these results can further be improved by\\rour denoising approach.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12990 ,  1418kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13027 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 13:50:26 GMT   (3085kb,D)\\r\\rTitle: Doubly Perturbed Task-Free Continual Learning\\rAuthors: Byung Hyun Lee, Min-hwan Oh, Se Young Chun\\rCategories: cs.LG cs.CV\\rComments: Accepted to AAAI 2024\\r\\\\\\\\\\r  Task-free online continual learning (TF-CL) is a challenging problem where\\rthe model incrementally learns tasks without explicit task information.\\rAlthough training with entire data from the past, present as well as future is\\rconsidered as the gold standard, naive approaches in TF-CL with the current\\rsamples may be conflicted with learning with samples in the future, leading to\\rcatastrophic forgetting and poor plasticity. Thus, a proactive consideration of\\ran unseen future sample in TF-CL becomes imperative. Motivated by this\\rintuition, we propose a novel TF-CL framework considering future samples and\\rshow that injecting adversarial perturbations on both input data and\\rdecision-making is effective. Then, we propose a novel method named Doubly\\rPerturbed Continual Learning (DPCL) to efficiently implement these input and\\rdecision-making perturbations. Specifically, for input perturbation, we propose\\ran approximate perturbation method that injects noise into the input data as\\rwell as the feature vector and then interpolates the two perturbed samples. For\\rdecision-making process perturbation, we devise multiple stochastic\\rclassifiers. We also investigate a memory management scheme and learning rate\\rscheduling reflecting our proposed double perturbations. We demonstrate that\\rour proposed method outperforms the state-of-the-art baseline methods by large\\rmargins on various TF-CL benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13027 ,  3085kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13127 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 15:47:21 GMT   (6831kb,D)\\r\\rTitle: Pixel-to-Abundance Translation: Conditional Generative Adversarial\\r  Networks Based on Patch Transformer for Hyperspectral Unmixing\\rAuthors: Li Wang, Xiaohua Zhang, Longfei Li, Hongyun Meng and Xianghai Cao\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Spectral unmixing is a significant challenge in hyperspectral image\\rprocessing. Existing unmixing methods utilize prior knowledge about the\\rabundance distribution to solve the regularization optimization problem, where\\rthe difficulty lies in choosing appropriate prior knowledge and solving the\\rcomplex regularization optimization problem. To solve these problems, we\\rpropose a hyperspectral conditional generative adversarial network (HyperGAN)\\rmethod as a generic unmixing framework, based on the following assumption: the\\runmixing process from pixel to abundance can be regarded as a transformation of\\rtwo modalities with an internal specific relationship. The proposed HyperGAN is\\rcomposed of a generator and discriminator, the former completes the modal\\rconversion from mixed hyperspectral pixel patch to the abundance of\\rcorresponding endmember of the central pixel and the latter is used to\\rdistinguish whether the distribution and structure of generated abundance are\\rthe same as the true ones. We propose hyperspectral image (HSI) Patch\\rTransformer as the main component of the generator, which utilize adaptive\\rattention score to capture the internal pixels correlation of the HSI patch and\\rleverage the spatial-spectral information in a fine-grained way to achieve\\roptimization of the unmixing process. Experiments on synthetic data and real\\rhyperspectral data achieve impressive results compared to state-of-the-art\\rcompetitors.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13127 ,  6831kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13139 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 16:00:43 GMT   (8668kb,D)\\r\\rTitle: Unleashing Large-Scale Video Generative Pre-training for Visual Robot\\r  Manipulation\\rAuthors: Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu,\\r  Xinghang Li, Minghuan Liu, Hang Li, Tao Kong\\rCategories: cs.RO cs.CV\\rComments: Project page: https://GR1-Manipulation.github.io\\r\\\\\\\\\\r  Generative pre-trained models have demonstrated remarkable effectiveness in\\rlanguage and vision domains by learning useful representations. In this paper,\\rwe extend the scope of this effectiveness by showing that visual robot\\rmanipulation can significantly benefit from large-scale video generative\\rpre-training. We introduce GR-1, a straightforward GPT-style model designed for\\rmulti-task language-conditioned visual robot manipulation. GR-1 takes as inputs\\ra language instruction, a sequence of observation images, and a sequence of\\rrobot states. It predicts robot actions as well as future images in an\\rend-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly\\rfinetuned on robot data after pre-trained on a large-scale video dataset. We\\rperform extensive experiments on the challenging CALVIN benchmark and a real\\rrobot. On CALVIN benchmark, our method outperforms state-of-the-art baseline\\rmethods and improves the success rate from 88.9% to 94.9%. In the setting of\\rzero-shot unseen scene generalization, GR-1 improves the success rate from\\r53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline\\rmethods and shows strong potentials in generalization to unseen scenes and\\robjects. We provide inaugural evidence that a unified GPT-style transformer,\\raugmented with large-scale video generative pre-training, exhibits remarkable\\rgeneralization to multi-task visual robot manipulation. Project page:\\rhttps://GR1-Manipulation.github.io\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13139 ,  8668kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13162 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 16:23:48 GMT   (921kb,D)\\r\\rTitle: Brain-Inspired Visual Odometry: Balancing Speed and Interpretability\\r  through a System of Systems Approach\\rAuthors: Habib Boloorchi Tabrizi, Christopher Crick\\rCategories: cs.RO cs.CV\\rComments: https://www.american-cse.org/csci2023 is website of conference and\\r  conference name is CSCI2023\\r\\\\\\\\\\r  In this study, we address the critical challenge of balancing speed and\\raccuracy while maintaining interpretablity in visual odometry (VO) systems, a\\rpivotal aspect in the field of autonomous navigation and robotics. Traditional\\rVO systems often face a trade-off between computational speed and the precision\\rof pose estimation. To tackle this issue, we introduce an innovative system\\rthat synergistically combines traditional VO methods with a specifically\\rtailored fully connected network (FCN). Our system is unique in its approach to\\rhandle each degree of freedom independently within the FCN, placing a strong\\remphasis on causal inference to enhance interpretability. This allows for a\\rdetailed and accurate assessment of relative pose error (RPE) across various\\rdegrees of freedom, providing a more comprehensive understanding of parameter\\rvariations and movement dynamics in different environments. Notably, our system\\rdemonstrates a remarkable improvement in processing speed without compromising\\raccuracy. In certain scenarios, it achieves up to a 5% reduction in Root Mean\\rSquare Error (RMSE), showcasing its ability to effectively bridge the gap\\rbetween speed and accuracy that has long been a limitation in VO research. This\\radvancement represents a significant step forward in developing more efficient\\rand reliable VO systems, with wide-ranging applications in real-time navigation\\rand robotic systems.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13162 ,  921kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13220 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 17:38:56 GMT   (7700kb,D)\\r\\rTitle: SISMIK for brain MRI: Deep-learning-based motion estimation and\\r  model-based motion correction in k-space\\rAuthors: Oscar Dabrowski (1 and 2), Jean-Luc Falcone (1), Antoine Klauser (2\\r  and 3), Julien Songeon (2 and 3), Michel Kocher (4), Bastien Chopard (1),\\r  Fran\\\\c{c}ois Lazeyras (2 and 3), S\\\\'ebastien Courvoisier (2 and 3) ((1)\\r  Computer Science Department, Faculty of Science, University of Geneva,\\r  Switzerland, (2) Department of Radiology and Medical Informatics, Faculty of\\r  Medicine, University of Geneva, Switzerland, (3) CIBM Center for Biomedical\\r  Imaging, MRI HUG-UNIGE, Geneva, Switzerland, (4) EPFL Biomedical Imaging\\r  Group (BIG), Lausanne, Switzerland)\\rCategories: eess.IV cs.AI cs.CV\\r\\\\\\\\\\r  MRI, a widespread non-invasive medical imaging modality, is highly sensitive\\rto patient motion. Despite many attempts over the years, motion correction\\rremains a difficult problem and there is no general method applicable to all\\rsituations. We propose a retrospective method for motion quantification and\\rcorrection to tackle the problem of in-plane rigid-body motion, apt for\\rclassical 2D Spin-Echo scans of the brain, which are regularly used in clinical\\rpractice. Due to the sequential acquisition of k-space, motion artifacts are\\rwell localized. The method leverages the power of deep neural networks to\\restimate motion parameters in k-space and uses a model-based approach to\\rrestore degraded images to avoid ''hallucinations''. Notable advantages are its\\rability to estimate motion occurring in high spatial frequencies without the\\rneed of a motion-free reference. The proposed method operates on the whole\\rk-space dynamic range and is moderately affected by the lower SNR of higher\\rharmonics. As a proof of concept, we provide models trained using supervised\\rlearning on 600k motion simulations based on motion-free scans of 43 different\\rsubjects. Generalization performance was tested with simulations as well as\\rin-vivo. Qualitative and quantitative evaluations are presented for motion\\rparameter estimations and image reconstruction. Experimental results show that\\rour approach is able to obtain good generalization performance on simulated\\rdata and in-vivo acquisitions.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13220 ,  7700kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13236 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 18:00:16 GMT   (6317kb,D)\\r\\rTitle: Diffusion Models With Learned Adaptive Noise\\rAuthors: Subham Sekhar Sahoo, Aaron Gokaslan, Chris De Sa, Volodymyr Kuleshov\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Diffusion models have gained traction as powerful algorithms for synthesizing\\rhigh-quality images. Central to these algorithms is the diffusion process,\\rwhich maps data to noise according to equations inspired by thermodynamics and\\rcan significantly impact performance. A widely held assumption is that the ELBO\\robjective of a diffusion model is invariant to the noise process (Kingma et\\ral.,2021). In this work, we dispel this assumption -- we propose multivariate\\rlearned adaptive noise (MuLAN), a learned diffusion process that applies\\rGaussian noise at different rates across an image. Our method consists of three\\rcomponents -- a multivariate noise schedule, instance-conditional diffusion,\\rand auxiliary variables -- which ensure that the learning objective is no\\rlonger invariant to the choice of the noise schedule as in previous works. Our\\rwork is grounded in Bayesian inference and casts the learned diffusion process\\ras an approximate variational posterior that yields a tighter lower bound on\\rmarginal likelihood. Empirically, MuLAN sets a new state-of-the-art in density\\restimation on CIFAR-10 and ImageNet compared to classical diffusion. Code is\\ravailable at https://github.com/s-sahoo/MuLAN\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13236 ,  6317kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.13250 (*cross-listing*)\\rDate: Wed, 20 Dec 2023 18:25:15 GMT   (2699kb,D)\\r\\rTitle: The role of data embedding in equivariant quantum convolutional neural\\r  networks\\rAuthors: Sreetama Das, Stefano Martina, Filippo Caruso\\rCategories: quant-ph cs.CV cs.ET cs.LG\\rComments: 9 pages, 7 figures\\r\\\\\\\\\\r  Geometric deep learning refers to the scenario in which the symmetries of a\\rdataset are used to constrain the parameter space of a neural network and thus,\\rimprove their trainability and generalization. Recently this idea has been\\rincorporated into the field of quantum machine learning, which has given rise\\rto equivariant quantum neural networks (EQNNs). In this work, we investigate\\rthe role of classical-to-quantum embedding on the performance of equivariant\\rquantum convolutional neural networks (EQCNNs) for the classification of\\rimages. We discuss the connection between the data embedding method and the\\rresulting representation of a symmetry group and analyze how changing\\rrepresentation affects the expressibility of an EQCNN. We numerically compare\\rthe classification accuracy of EQCNNs with three different basis-permuted\\ramplitude embeddings to the one obtained from a non-equivariant quantum\\rconvolutional neural network (QCNN). Our results show that all the EQCNNs\\rachieve higher classification accuracy than the non-equivariant QCNN for small\\rnumbers of training iterations, while for large iterations this improvement\\rcrucially depends on the used embedding. It is expected that the results of\\rthis work can be useful to the community for a better understanding of the\\rimportance of data embedding choice in the context of geometric quantum machine\\rlearning.\\r\\\\\\\\ ( https://arxiv.org/abs/2312.13250 ,  2699kb)\\r%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.01039\\rreplaced with revised version Wed, 20 Dec 2023 15:00:43 GMT   (292kb,D)\\r\\rTitle: SoftCorrect: Error Correction with Soft Detection for Automatic Speech\\r  Recognition\\rAuthors: Yichong Leng, Xu Tan, Wenjie Liu, Kaitao Song, Rui Wang, Xiang-Yang\\r  Li, Tao Qin, Edward Lin, Tie-Yan Liu\\rCategories: cs.CL cs.LG eess.AS\\rComments: AAAI 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2212.01039 ,  292kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.01246\\rreplaced with revised version Wed, 20 Dec 2023 09:19:15 GMT   (1021kb,D)\\r\\rTitle: Safety Analysis in the Era of Large Language Models: A Case Study of\\r  STPA using ChatGPT\\rAuthors: Yi Qi, Xingyu Zhao, Siddartha Khastgir, Xiaowei Huang\\rCategories: cs.CL cs.AI cs.CY cs.SE\\rComments: Under Review\\r\\\\\\\\ ( https://arxiv.org/abs/2304.01246 ,  1021kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.03898\\rreplaced with revised version Wed, 20 Dec 2023 02:43:39 GMT   (2724kb,D)\\r\\rTitle: The Short Text Matching Model Enhanced with Knowledge via Contrastive\\r  Learning\\rAuthors: Ruiqiang Liu, Qiqiang Zhong, Mengmeng Cui, Hanjie Mai, Qiang Zhang,\\r  Shaohua Xu, Xiangzheng Liu, Yanlong Du\\rCategories: cs.CL cs.AI\\rComments: 11 pages,2 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2304.03898 ,  2724kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.11662\\rreplaced with revised version Wed, 20 Dec 2023 12:57:34 GMT   (266kb,D)\\r\\rTitle: Separating form and meaning: Using self-consistency to quantify task\\r  understanding across multiple senses\\rAuthors: Xenia Ohmer, Elia Bruni, Dieuwke Hupkes\\rCategories: cs.CL cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2305.11662 ,  266kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.15685\\rreplaced with revised version Tue, 19 Dec 2023 23:57:01 GMT   (506kb,D)\\r\\rTitle: RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting\\rAuthors: Lei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Yinxiao Liu, Simon\\r  Tong, Jindong Chen, Lei Meng\\rCategories: cs.CL cs.AI\\rJournal-ref: AAAI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2305.15685 ,  506kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.16307\\rreplaced with revised version Wed, 20 Dec 2023 17:08:28 GMT   (1213kb)\\r\\rTitle: IndicTrans2: Towards High-Quality and Accessible Machine Translation\\r  Models for all 22 Scheduled Indian Languages\\rAuthors: Jay Gala and Pranjal A. Chitale and Raghavan AK and Varun Gumma and\\r  Sumanth Doddapaneni and Aswanth Kumar and Janki Nawale and Anupama Sujatha\\r  and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M.\\r  Khapra and Raj Dabre and Anoop Kunchukuttan\\rCategories: cs.CL\\rComments: Accepted at TMLR\\r\\\\\\\\ ( https://arxiv.org/abs/2305.16307 ,  1213kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.11698\\rreplaced with revised version Tue, 19 Dec 2023 19:38:39 GMT   (115590kb,D)\\r\\rTitle: DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT\\r  Models\\rAuthors: Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang,\\r  Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T.\\r  Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng,\\r  Sanmi Koyejo, Dawn Song, Bo Li\\rCategories: cs.CL cs.AI cs.CR\\rComments: NeurIPS 2023 Outstanding Paper (Datasets and Benchmarks Track)\\r\\\\\\\\ ( https://arxiv.org/abs/2306.11698 ,  115590kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.12976\\rreplaced with revised version Wed, 20 Dec 2023 11:52:41 GMT   (1033kb,D)\\r\\rTitle: Evaluating the Ripple Effects of Knowledge Editing in Language Models\\rAuthors: Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, Mor Geva\\rCategories: cs.CL\\rComments: Accepted for publication in Transactions of the Association for\\r  Computational Linguistics (TACL), 2024. Author's final version\\r\\\\\\\\ ( https://arxiv.org/abs/2307.12976 ,  1033kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.08742\\rreplaced with revised version Wed, 20 Dec 2023 03:16:09 GMT   (534kb,D)\\r\\rTitle: PMET: Precise Model Editing in a Transformer\\rAuthors: Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, and Jie Yu\\rCategories: cs.CL cs.AI cs.LG\\rComments: Accepted in AAAI24\\r\\\\\\\\ ( https://arxiv.org/abs/2308.08742 ,  534kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.09156\\rreplaced with revised version Tue, 19 Dec 2023 22:03:48 GMT   (733kb,D)\\r\\rTitle: Characterizing Information Seeking Events in Health-Related Social\\r  Discourse\\rAuthors: Omar Sharif, Madhusudan Basak, Tanzia Parvin, Ava Scharfstein,\\r  Alphonso Bradham, Jacob T. Borodovsky, Sarah E. Lord, Sarah M. Preum\\rCategories: cs.CL\\rComments: Accepted at AAAI-2024. 9 pages, 6 tables, 2 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2308.09156 ,  733kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.13198\\rreplaced with revised version Wed, 20 Dec 2023 11:05:17 GMT   (2650kb,D)\\r\\rTitle: Journey to the Center of the Knowledge Neurons: Discoveries of\\r  Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons\\rAuthors: Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao\\rCategories: cs.CL\\rComments: Accepted in the 38th AAAI Conference on Artificial Intelligence (AAAI\\r  2024)\\r\\\\\\\\ ( https://arxiv.org/abs/2308.13198 ,  2650kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.01431\\rreplaced with revised version Wed, 20 Dec 2023 11:54:11 GMT   (1827kb,D)\\r\\rTitle: Benchmarking Large Language Models in Retrieval-Augmented Generation\\rAuthors: Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun\\rCategories: cs.CL\\rComments: Accepted to AAAI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2309.01431 ,  1827kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.03560\\rreplaced with revised version Wed, 20 Dec 2023 01:14:42 GMT   (1636kb,D)\\r\\rTitle: Redefining Digital Health Interfaces with Large Language Models\\rAuthors: Fergus Imrie, Paulius Rauba, Mihaela van der Schaar\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2310.03560 ,  1636kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.14747\\rreplaced with revised version Wed, 20 Dec 2023 06:50:20 GMT   (7729kb,D)\\r\\rTitle: MCC-KD: Multi-CoT Consistent Knowledge Distillation\\rAuthors: Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, Ji Zhang\\rCategories: cs.CL\\rComments: Accepted to ENMLP 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2310.14747 ,  7729kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.15494\\rreplaced with revised version Wed, 20 Dec 2023 08:46:01 GMT   (130kb,D)\\r\\rTitle: TRAMS: Training-free Memory Selection for Long-range Language Modeling\\rAuthors: Haofei Yu, Cunxiang Wang, Yue Zhang, Wei Bi\\rCategories: cs.CL\\rComments: Findings of EMNLP 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2310.15494 ,  130kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.08206\\rreplaced with revised version Tue, 19 Dec 2023 23:03:56 GMT   (37kb)\\r\\rTitle: Human-Centric Autonomous Systems With LLMs for User Command Reasoning\\rAuthors: Yi Yang and Qingwen Zhang and Ci Li and Daniel Sim\\\\~oes Marta and\\r  Nazre Batool and John Folkesson\\rCategories: cs.CL cs.AI cs.RO\\rComments: In Proceedings of the IEEE/CVF Winter Conference on Applications of\\r  Computer Vision (WACV) Workshops, 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2311.08206 ,  37kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.03719\\rreplaced with revised version Wed, 20 Dec 2023 06:40:30 GMT   (460kb)\\r\\rTitle: Assessing AI Chatbots Performance in Comprehensive Standardized Test\\r  Preparation; A Case Study with GRE\\rAuthors: Mohammad Abu-Haifa, Bara'a Etawi, Huthaifa Alkhatatbeh, and Ayman\\r  Ababneh\\rCategories: cs.CL cs.AI\\rComments: 19 Pages, 6 figures, and 6 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2312.03719 ,  460kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.06022\\rreplaced with revised version Wed, 20 Dec 2023 15:07:59 GMT   (607kb,D)\\r\\rTitle: Exploiting Representation Bias for Data Distillation in Abstractive Text\\r  Summarization\\rAuthors: Yash Kumar Atri, Vikram Goyal, Tanmoy Chakraborty\\rCategories: cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2312.06022 ,  607kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.09085\\rreplaced with revised version Wed, 20 Dec 2023 08:03:12 GMT   (6951kb,D)\\r\\rTitle: The Earth is Flat because...: Investigating LLMs' Belief towards\\r  Misinformation via Persuasive Conversation\\rAuthors: Rongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi,\\r  Tianwei Zhang, Zhixuan Fang, Wei Xu, Han Qiu\\rCategories: cs.CL cs.AI cs.CR cs.CY\\rComments: 45 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2312.09085 ,  6951kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11193\\rreplaced with revised version Wed, 20 Dec 2023 14:57:11 GMT   (1121kb)\\r\\rTitle: Paraphrasing The Original Text Makes High Accuracy Long-Context QA\\rAuthors: Yijiong Yu\\rCategories: cs.CL cs.AI\\rComments: Chinese version of this paper can be downloaded from\\r  (https://cloud.tsinghua.edu.cn/d/5894ec4442e54a6aac96/)\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11193 ,  1121kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11276\\rreplaced with revised version Wed, 20 Dec 2023 09:43:01 GMT   (5834kb,D)\\r\\rTitle: Compositional Generalization for Multi-label Text Classification: A\\r  Data-Augmentation Approach\\rAuthors: Yuyang Chai, Zhuang Li, Jiahui Liu, Lei Chen, Fei Li, Donghong Ji and\\r  Chong Teng\\rCategories: cs.CL\\rComments: Accepted by AAAI'24\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11276 ,  5834kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11517\\rreplaced with revised version Wed, 20 Dec 2023 16:43:54 GMT   (925kb,D)\\r\\rTitle: Unlocking Musculoskeletal Disorder Risk Factors: NLP-Based\\r  Classification and Mode-Based Ranking\\rAuthors: Md Abrar Jahin and Subrata Talapatra\\rCategories: cs.CL cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11517 ,  925kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11985\\rreplaced with revised version Wed, 20 Dec 2023 05:27:30 GMT   (809kb,D)\\r\\rTitle: Climate Change from Large Language Models\\rAuthors: Hongyin Zhu, Prayag Tiwari\\rCategories: cs.CL cs.CY\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11985 ,  809kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12037\\rreplaced with revised version Wed, 20 Dec 2023 17:42:18 GMT   (30kb,D)\\r\\rTitle: Founder-GPT: Self-play to evaluate the Founder-Idea fit\\rAuthors: Sichao Xiong and Yigit Ihlamur\\rCategories: cs.CL cs.AI cs.CE\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12037 ,  30kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.09169\\rreplaced with revised version Wed, 20 Dec 2023 08:58:03 GMT   (17789kb,D)\\r\\rTitle: Rich Action-semantic Consistent Knowledge for Early Action Prediction\\rAuthors: Xiaoli Liu, Jianqin Yin, Di Guo, and Huaping Liu\\rCategories: cs.CV\\rComments: Accepted by IEEE TIP,15pages\\r\\\\\\\\ ( https://arxiv.org/abs/2201.09169 ,  17789kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.02980\\rreplaced with revised version Wed, 20 Dec 2023 14:16:19 GMT   (3773kb,D)\\r\\rTitle: 3D Object Detection from Images for Autonomous Driving: A Survey\\rAuthors: Xinzhu Ma, Wanli Ouyang, Andrea Simonelli, Elisa Ricci\\rCategories: cs.CV\\rComments: Accepted by T-PAMI\\r\\\\\\\\ ( https://arxiv.org/abs/2202.02980 ,  3773kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2206.07207\\rreplaced with revised version Wed, 20 Dec 2023 03:22:02 GMT   (23230kb,D)\\r\\rTitle: Beyond Grounding: Extracting Fine-Grained Event Hierarchies Across\\r  Modalities\\rAuthors: Hammad A. Ayyubi, Christopher Thomas, Lovish Chum, Rahul Lokesh, Long\\r  Chen, Yulei Niu, Xudong Lin, Xuande Feng, Jaywon Koo, Sounak Ray and Shih-Fu\\r  Chang\\rCategories: cs.CV cs.CL\\rComments: AAAI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2206.07207 ,  23230kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2209.14719\\rreplaced with revised version Wed, 20 Dec 2023 16:08:32 GMT   (924kb,D)\\r\\rTitle: In Search of Projectively Equivariant Networks\\rAuthors: Georg B\\\\okman, Axel Flinth, Fredrik Kahl\\rCategories: cs.CV\\rComments: v3: Another significant rewrite. Accepted for publication in TMLR.\\r  v2: Significant rewrite. The title has been changed: neural network ->\\r  network. More general description of projectively equivariant linear\\r  layers, with new proposed architectures, and a completely new accompanying\\r  experiment section, as a result\\rMSC-class: 68T07 (Primary) 20C35 (Secondary)\\r\\\\\\\\ ( https://arxiv.org/abs/2209.14719 ,  924kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.03087\\rreplaced with revised version Wed, 20 Dec 2023 17:24:33 GMT   (10159kb,D)\\r\\rTitle: Iterative Vision-and-Language Navigation\\rAuthors: Jacob Krantz, Shurjo Banerjee, Wang Zhu, Jason Corso, Peter Anderson,\\r  Stefan Lee and Jesse Thomason\\rCategories: cs.CV cs.CL cs.RO\\rComments: Accepted by CVPR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2210.03087 ,  10159kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.15563\\rreplaced with revised version Wed, 20 Dec 2023 06:18:19 GMT   (315kb,D)\\r\\rTitle: Multimodal Transformer Distillation for Audio-Visual Synchronization\\rAuthors: Xuanjun Chen, Haibin Wu, Chung-Che Wang, Hung-yi Lee, Jyh-Shing Roger\\r  Jang\\rCategories: cs.CV cs.IR cs.SD eess.AS\\rComments: Accepted by ICASSP 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2210.15563 ,  315kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.04155\\rreplaced with revised version Tue, 19 Dec 2023 19:47:15 GMT   (12186kb,D)\\r\\rTitle: Latent Graph Representations for Critical View of Safety Assessment\\rAuthors: Aditya Murali, Deepak Alapatt, Pietro Mascagni, Armine Vardazaryan,\\r  Alain Garcia, Nariaki Okamoto, Didier Mutter, Nicolas Padoy\\rCategories: cs.CV\\rComments: 12 pages, 4 figures\\rReport-no: 10.1109/TMI.2023.3333034\\r\\\\\\\\ ( https://arxiv.org/abs/2212.04155 ,  12186kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.05122\\rreplaced with revised version Wed, 20 Dec 2023 01:08:15 GMT   (968kb,D)\\r\\rTitle: M-Tuning: Prompt Tuning with Mitigated Label Bias in Open-Set Scenarios\\rAuthors: Ning Liao, Xiaopeng Zhang, Min Cao, Junchi Yan, Qi Tian\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2303.05122 ,  968kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.06854\\rreplaced with revised version Tue, 19 Dec 2023 19:12:53 GMT   (4727kb,D)\\r\\rTitle: Robust Contrastive Language-Image Pre-training against Data Poisoning\\r  and Backdoor Attacks\\rAuthors: Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman\\rCategories: cs.CV cs.CL cs.CR cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2303.06854 ,  4727kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.09429\\rreplaced with revised version Wed, 20 Dec 2023 11:07:57 GMT   (44983kb,D)\\r\\rTitle: Data Roaming and Quality Assessment for Composed Image Retrieval\\rAuthors: Matan Levy, Rami Ben-Ari, Nir Darshan, Dani Lischinski\\rCategories: cs.CV\\rComments: Camera Ready version for AAAI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2303.09429 ,  44983kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.11048\\rreplaced with revised version Wed, 20 Dec 2023 14:11:26 GMT   (1177kb,D)\\r\\rTitle: SGFormer: Semantic Graph Transformer for Point Cloud-based 3D Scene\\r  Graph Generation\\rAuthors: Changsheng Lv, Mengshi Qi, Xia Li, Zhengyuan Yang, Huadong Ma\\rCategories: cs.CV\\rComments: To be published in Thirty-Eighth AAAI Conference on Artificial\\r  Intelligence\\r\\\\\\\\ ( https://arxiv.org/abs/2303.11048 ,  1177kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.11938\\rreplaced with revised version Wed, 20 Dec 2023 07:12:06 GMT   (17510kb,D)\\r\\rTitle: 3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion\\rAuthors: Yu-Jhe Li, Tao Xu, Ji Hou, Bichen Wu, Xiaoliang Dai, Albert Pumarola,\\r  Peizhao Zhang, Peter Vajda, Kris Kitani\\rCategories: cs.CV\\rComments: 15 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2303.11938 ,  17510kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.12332\\rreplaced with revised version Wed, 20 Dec 2023 14:08:37 GMT   (621kb,D)\\r\\rTitle: Weakly-Supervised Temporal Action Localization by Inferring Salient\\r  Snippet-Feature\\rAuthors: Wulian Yun, Mengshi Qi, Chuanming Wang, Huadong Ma\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2303.12332 ,  621kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.12484\\rreplaced with revised version Wed, 20 Dec 2023 02:14:25 GMT   (15990kb,D)\\r\\rTitle: Label-Efficient Deep Learning in Medical Image Analysis: Challenges and\\r  Future Directions\\rAuthors: Cheng Jin, Zhengrui Guo, Yi Lin, Luyang Luo, Hao Chen\\rCategories: cs.CV cs.AI\\rComments: Update Few-shot Methods\\r\\\\\\\\ ( https://arxiv.org/abs/2303.12484 ,  15990kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.15413\\rreplaced with revised version Tue, 19 Dec 2023 22:03:12 GMT   (39913kb,D)\\r\\rTitle: Debiasing Scores and Prompts of 2D Diffusion for View-consistent\\r  Text-to-3D Generation\\rAuthors: Susung Hong, Donghoon Ahn, Seungryong Kim\\rCategories: cs.CV cs.CL cs.GR cs.LG\\rComments: Accepted to NeurIPS 2023. Project Page:\\r  https://susunghong.github.io/Debiased-Score-Distillation-Sampling/\\r\\\\\\\\ ( https://arxiv.org/abs/2303.15413 ,  39913kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.17908\\rreplaced with revised version Tue, 19 Dec 2023 19:12:39 GMT   (3339kb,D)\\r\\rTitle: Trade-offs in Fine-tuned Diffusion Models Between Accuracy and\\r  Interpretability\\rAuthors: Mischa Dombrowski, Hadrien Reynaud, Johanna P. M\\\\uller, Matthew\\r  Baugh, Bernhard Kainz\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2303.17908 ,  3339kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.02150\\rreplaced with revised version Wed, 20 Dec 2023 16:15:43 GMT   (20932kb,D)\\r\\rTitle: Re-Evaluating LiDAR Scene Flow for Autonomous Driving\\rAuthors: Nathaniel Chodosh, Deva Ramanan, Simon Lucey\\rCategories: cs.CV\\rComments: WACV 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2304.02150 ,  20932kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.03693\\rreplaced with revised version Wed, 20 Dec 2023 08:19:09 GMT   (4528kb,D)\\r\\rTitle: Model-Agnostic Gender Debiased Image Captioning\\rAuthors: Yusuke Hirota, Yuta Nakashima, Noa Garcia\\rCategories: cs.CV\\rComments: CVPR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2304.03693 ,  4528kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06385\\rreplaced with revised version Wed, 20 Dec 2023 01:28:57 GMT   (4712kb,D)\\r\\rTitle: TransHP: Image Classification with Hierarchical Prompting\\rAuthors: Wenhao Wang, Yifan Sun, Wei Li, Yi Yang\\rCategories: cs.CV\\rComments: Accepted to NeurIPS 2023; Released code\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06385 ,  4712kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.10701\\rreplaced with revised version Wed, 20 Dec 2023 05:52:41 GMT   (4813kb,D)\\r\\rTitle: Personalization as a Shortcut for Few-Shot Backdoor Attack against\\r  Text-to-Image Diffusion Models\\rAuthors: Yihao Huang, Felix Juefei-Xu, Qing Guo, Jie Zhang, Yutong Wu, Ming Hu,\\r  Tianlin Li, Geguang Pu, Yang Liu\\rCategories: cs.CV\\rComments: 16 pages, accepted by AAAI 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2305.10701 ,  4813kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.12554\\rreplaced with revised version Tue, 19 Dec 2023 23:52:51 GMT   (25914kb,D)\\r\\rTitle: Towards Consistent Stochastic Human Motion Prediction via Motion\\r  Diffusion\\rAuthors: Jiarui Sun, Girish Chowdhary\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2305.12554 ,  25914kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.15296\\rreplaced with revised version Wed, 20 Dec 2023 18:52:00 GMT   (36223kb,D)\\r\\rTitle: MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal\\r  Image Generation\\rAuthors: Marco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich,\\r  Bj\\\\orn Deiseroth, Constantin Eichenberg, Andrew Dai, Robert Baldock,\\r  Souradeep Nanda, Koen Oostermeijer, Andres Felipe Cruz-Salinas, Patrick\\r  Schramowski, Kristian Kersting, Samuel Weinbach\\rCategories: cs.CV cs.AI cs.LG\\rComments: Proceedings of Advances in Neural Information Processing Systems:\\r  Annual Conference on Neural Information Processing Systems (NeurIPS)\\r\\\\\\\\ ( https://arxiv.org/abs/2305.15296 ,  36223kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.16172\\rreplaced with revised version Wed, 20 Dec 2023 07:10:27 GMT   (431kb,D)\\r\\rTitle: Masked and Permuted Implicit Context Learning for Scene Text Recognition\\rAuthors: Xiaomeng Yang, Zhi Qiao, Jin Wei, Dongbao Yang, Yu Zhou\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2305.16172 ,  431kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02273\\rreplaced with revised version Wed, 20 Dec 2023 12:10:09 GMT   (6999kb,D)\\r\\rTitle: Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient\\r  Neural Image Compression\\rAuthors: Ahmed Ghorbel, Wassim Hamidouche and Luce Morin\\rCategories: cs.CV eess.IV\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02273 ,  6999kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.07063\\rreplaced with revised version Tue, 19 Dec 2023 22:13:30 GMT   (1414kb,D)\\r\\rTitle: Bootstrapping Vision-Language Learning with Decoupled Language\\r  Pre-training\\rAuthors: Yiren Jian, Chongyang Gao, Soroush Vosoughi\\rCategories: cs.CV cs.LG\\rComments: Accepted to NeurIPS 2023 (spotlight). The code is available at\\r  https://github.com/yiren-jian/BLIText\\r\\\\\\\\ ( https://arxiv.org/abs/2307.07063 ,  1414kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.15409\\rreplaced with revised version Wed, 20 Dec 2023 15:08:11 GMT   (5848kb,D)\\r\\rTitle: Uncertainty-aware Unsupervised Multi-Object Tracking\\rAuthors: Kai Liu, Sheng Jin, Zhihang Fu, Ze Chen, Rongxin Jiang, Jieping Ye\\rCategories: cs.CV\\rComments: Accepted by International Conference on Computer Vision (ICCV) 2023.\\r  Code is available at https://github.com/alibaba/u2mot/\\r\\\\\\\\ ( https://arxiv.org/abs/2307.15409 ,  5848kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.03108\\rreplaced with revised version Wed, 20 Dec 2023 07:32:44 GMT   (25832kb,D)\\r\\rTitle: SAAM: Stealthy Adversarial Attack on Monocular Depth Estimation\\rAuthors: Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammad Shafique\\rCategories: cs.CV cs.CR\\r\\\\\\\\ ( https://arxiv.org/abs/2308.03108 ,  25832kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.08658\\rreplaced with revised version Tue, 19 Dec 2023 21:05:22 GMT   (775kb)\\r\\rTitle: A Data-Theoretic Approach to Identifying Violent Facial Expressions in\\r  Social Crime Contexts\\rAuthors: Arindam Kumar Paul\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2308.08658 ,  775kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.10079\\rreplaced with revised version Wed, 20 Dec 2023 08:49:59 GMT   (23480kb,D)\\r\\rTitle: MeDM: Mediating Image Diffusion Models for Video-to-Video Translation\\r  with Temporal Correspondence Guidance\\rAuthors: Ernie Chu, Tzuhsuan Huang, Shuo-Yen Lin, Jun-Cheng Chen\\rCategories: cs.CV\\rComments: Accepted as a conference paper in AAAI 2024. Project page:\\r  https://medm2023.github.io\\r\\\\\\\\ ( https://arxiv.org/abs/2308.10079 ,  23480kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.12535\\rreplaced with revised version Wed, 20 Dec 2023 03:46:13 GMT   (7115kb,D)\\r\\rTitle: SCP: Spherical-Coordinate-based Learned Point Cloud Compression\\rAuthors: Ao Luo, Linxin Song, Keisuke Nonaka, Kyohei Unno, Heming Sun, Masayuki\\r  Goto, Jiro Katto\\rCategories: cs.CV eess.IV\\r\\\\\\\\ ( https://arxiv.org/abs/2308.12535 ,  7115kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.13739\\rreplaced with revised version Wed, 20 Dec 2023 05:55:10 GMT   (13648kb,D)\\r\\rTitle: Devignet: High-Resolution Vignetting Removal via a Dual Aggregated\\r  Fusion Transformer With Adaptive Channel Expansion\\rAuthors: Shenghong Luo, Xuhang Chen, Weiwen Chen, Zinuo Li, Shuqiang Wang,\\r  Chi-Man Pun\\rCategories: cs.CV eess.IV\\rComments: Accepted by AAAI Conference on Artificial Intelligence 2024 (AAAI\\r  2024)\\r\\\\\\\\ ( https://arxiv.org/abs/2308.13739 ,  13648kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.14078\\rreplaced with revised version Wed, 20 Dec 2023 09:04:05 GMT   (16611kb,D)\\r\\rTitle: Sparse3D: Distilling Multiview-Consistent Diffusion for Object\\r  Reconstruction from Sparse Views\\rAuthors: Zi-Xin Zou, Weihao Cheng, Yan-Pei Cao, Shi-Sheng Huang, Ying Shan,\\r  Song-Hai Zhang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2308.14078 ,  16611kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.10689\\rreplaced with revised version Wed, 20 Dec 2023 03:06:10 GMT   (17075kb,D)\\r\\rTitle: ReShader: View-Dependent Highlights for Single Image View-Synthesis\\rAuthors: Avinash Paliwal, Brandon Nguyen, Andrii Tsarov, Nima Khademi Kalantari\\rCategories: cs.CV cs.GR\\rComments: SIGGRAPH Asia 2023. Project page at\\r  https://people.engr.tamu.edu/nimak/Papers/SIGAsia2023_Reshader/index.html and\\r  video at https://www.youtube.com/watch?v=XW-tl48D3Ok\\rJournal-ref: ACM Transactions on Graphics (ToG) 42,6 (2023) 1-9\\rDOI: 10.1145/3618393\\r\\\\\\\\ ( https://arxiv.org/abs/2309.10689 ,  17075kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.06958\\rreplaced with revised version Tue, 19 Dec 2023 20:34:57 GMT   (565kb,D)\\r\\rTitle: Comparing the robustness of modern no-reference image- and video-quality\\r  metrics to adversarial attacks\\rAuthors: Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Ekaterina\\r  Shumitskaya, Sergey Lavrushkin, Dmitriy Vatolin\\rCategories: cs.CV cs.LG cs.MM eess.IV\\r\\\\\\\\ ( https://arxiv.org/abs/2310.06958 ,  565kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.14958\\rreplaced with revised version Wed, 20 Dec 2023 09:10:00 GMT   (2935kb,D)\\r\\rTitle: Learning Real-World Image De-Weathering with Imperfect Supervision\\rAuthors: Xiaohui Liu and Zhilu Zhang and Xiaohe Wu and Chaoyu Feng and Xiaotao\\r  Wang and LEI LEI and Wangmeng Zuo\\rCategories: cs.CV\\rComments: 17 pages, 14 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2310.14958 ,  2935kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2310.16999\\rreplaced with revised version Tue, 19 Dec 2023 22:29:46 GMT   (1746kb,D)\\r\\rTitle: Trust, but Verify: Robust Image Segmentation using Deep Learning\\rAuthors: Fahim Ahmed Zaman, Xiaodong Wu, Weiyu Xu, Milan Sonka and Raghuraman\\r  Mudumbai\\rCategories: cs.CV cs.LG eess.IV\\rComments: 5 Pages, 8 Figures, conference\\r\\\\\\\\ ( https://arxiv.org/abs/2310.16999 ,  1746kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.04207\\rreplaced with revised version Tue, 19 Dec 2023 22:01:10 GMT   (3793kb,D)\\r\\rTitle: Deep Hashing via Householder Quantization\\rAuthors: Lucas R. Schwengber, Lucas Resende, Paulo Orenstein, Roberto I.\\r  Oliveira\\rCategories: cs.CV cs.IR\\r\\\\\\\\ ( https://arxiv.org/abs/2311.04207 ,  3793kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.11383\\rreplaced with revised version Tue, 19 Dec 2023 19:55:31 GMT   (1483kb,D)\\r\\rTitle: A Survey of Emerging Applications of Diffusion Probabilistic Models in\\r  MRI\\rAuthors: Yuheng Fan, Hanxi Liao, Shiqi Huang, Yimin Luo, Huazhu Fu, Haikun Qi\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2311.11383 ,  1483kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.13073\\rreplaced with revised version Wed, 20 Dec 2023 15:58:26 GMT   (3262kb,D)\\r\\rTitle: FusionFrames: Efficient Architectural Aspects for Text-to-Video\\r  Generation Pipeline\\rAuthors: Vladimir Arkhipkin, Zein Shaheen, Viacheslav Vasilev, Elizaveta\\r  Dakhova, Andrey Kuznetsov, Denis Dimitrov\\rCategories: cs.CV cs.LG cs.MM\\rComments: Project page: https://ai-forever.github.io/kandinsky-video/\\r\\\\\\\\ ( https://arxiv.org/abs/2311.13073 ,  3262kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.14521\\rreplaced with revised version Wed, 20 Dec 2023 14:35:27 GMT   (36445kb,D)\\r\\rTitle: GaussianEditor: Swift and Controllable 3D Editing with Gaussian\\r  Splatting\\rAuthors: Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai\\r  Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin\\rCategories: cs.CV\\rComments: Project Page: https://buaacyw.github.io/gaussian-editor/ Code:\\r  https://github.com/buaacyw/GaussianEditor\\r\\\\\\\\ ( https://arxiv.org/abs/2311.14521 ,  36445kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.15803\\rreplaced with revised version Wed, 20 Dec 2023 09:15:57 GMT   (32828kb,D)\\r\\rTitle: SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using\\r  Neural Radiance Fields\\rAuthors: Quentin Herau, Nathan Piasco, Moussab Bennehar, Luis Rold\\\\~ao, Dzmitry\\r  Tsishkou, Cyrille Migniot, Pascal Vasseur, C\\\\'edric Demonceaux\\rCategories: cs.CV cs.RO\\rComments: Paper + Supplementary, under review. Project page:\\r  https://qherau.github.io/SOAC/\\r\\\\\\\\ ( https://arxiv.org/abs/2311.15803 ,  32828kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.02464\\rreplaced with revised version Wed, 20 Dec 2023 15:26:34 GMT   (3846kb,D)\\r\\rTitle: SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object\\r  and Boundary Constraints\\rAuthors: Xianping Ma, Qianqian Wu, Xingyu Zhao, Xiaokang Zhang, Man-On Pun, and\\r  Bo Huang\\rCategories: cs.CV\\rComments: 10 pages, 4 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2312.02464 ,  3846kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.02916\\rreplaced with revised version Wed, 20 Dec 2023 11:42:46 GMT   (3655kb,D)\\r\\rTitle: MIND: Multi-Task Incremental Network Distillation\\rAuthors: Jacopo Bonato, Francesco Pelosin, Luigi Sabetta, Alessandro Nicolosi\\rCategories: cs.CV cs.LG\\rComments: Accepted at the 38th AAAI Conference on Artificial Intelligence\\r\\\\\\\\ ( https://arxiv.org/abs/2312.02916 ,  3655kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.03795\\rreplaced with revised version Wed, 20 Dec 2023 07:52:24 GMT   (23273kb,D)\\r\\rTitle: AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and\\r  Reconstruction with Canonical Score Distillation\\rAuthors: Xinzhou Wang, Yikai Wang, Junliang Ye, Zhengyi Wang, Fuchun Sun,\\r  Pengkun Liu, Ling Wang, Kai Sun, Xintong Wang, Bin He\\rCategories: cs.CV\\rComments: Project page: https://animatabledreamer.github.io/\\rACM-class: I.4.5\\r\\\\\\\\ ( https://arxiv.org/abs/2312.03795 ,  23273kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.04810\\rreplaced with revised version Wed, 20 Dec 2023 11:17:20 GMT   (8281kb,D)\\r\\rTitle: RS-Corrector: Correcting the Racial Stereotypes in Latent Diffusion\\r  Models\\rAuthors: Yue Jiang, Yueming Lyu, Tianxiang Ma, Bo Peng, Jing Dong\\rCategories: cs.CV\\rComments: 16 pages, 15 figures, conference\\r\\\\\\\\ ( https://arxiv.org/abs/2312.04810 ,  8281kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.04875\\rreplaced with revised version Tue, 19 Dec 2023 19:35:25 GMT   (40461kb,D)\\r\\rTitle: MVDD: Multi-View Depth Diffusion Models\\rAuthors: Zhen Wang, Qiangeng Xu, Feitong Tan, Menglei Chai, Shichen Liu, Rohit\\r  Pandey, Sean Fanello, Achuta Kadambi, Yinda Zhang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2312.04875 ,  40461kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.06106\\rreplaced with revised version Tue, 19 Dec 2023 19:08:15 GMT   (47081kb,D)\\r\\rTitle: AUGCAL: Improving Sim2Real Adaptation by Uncertainty Calibration on\\r  Augmented Synthetic Images\\rAuthors: Prithvijit Chattopadhyay, Bharat Goyal, Boglarka Ecsedi, Viraj Prabhu,\\r  Judy Hoffman\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2312.06106 ,  47081kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.07879\\rreplaced with revised version Wed, 20 Dec 2023 08:53:40 GMT   (13334kb,D)\\r\\rTitle: CoIE: Chain-of-Instruct Editing for Multi-Attribute Face Manipulation\\rAuthors: Zhenduo Zhang, Bo-Wen Zhang, Guang Liu\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2312.07879 ,  13334kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.07937\\rreplaced with revised version Wed, 20 Dec 2023 04:50:14 GMT   (19287kb,D)\\r\\rTitle: BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics\\rAuthors: Wenqian Zhang, Molin Huang, Yuxuan Zhou, Juze Zhang, Jingyi Yu, Jingya\\r  Wang, Lan Xu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2312.07937 ,  19287kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.08288\\rreplaced with revised version Wed, 20 Dec 2023 10:46:33 GMT   (328kb,D)\\r\\rTitle: Hybrid Sample Synthesis-based Debiasing of Classifier in Limited Data\\r  Setting\\rAuthors: Piyush Arora, Pratik Mazumder\\rCategories: cs.CV cs.LG\\rComments: Accepted in WACV 2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.08288 ,  328kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.10461\\rreplaced with revised version Wed, 20 Dec 2023 07:27:27 GMT   (3373kb,D)\\r\\rTitle: Rethinking the Up-Sampling Operations in CNN-based Generative Network\\r  for Generalizable Deepfake Detection\\rAuthors: Chuangchuang Tan, Huan Liu, Yao Zhao, Shikui Wei, Guanghua Gu, Ping\\r  Liu, Yunchao Wei\\rCategories: cs.CV\\rComments: 10 pages, 4 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2312.10461 ,  3373kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11841\\rreplaced with revised version Wed, 20 Dec 2023 03:14:40 GMT   (10066kb,D)\\r\\rTitle: MixRT: Mixed Neural Representations For Real-Time NeRF Rendering\\rAuthors: Chaojian Li, Bichen Wu, Peter Vajda, Yingyan (Celine) Lin\\rCategories: cs.CV\\rComments: Accepted by 3DV'24. Project Page: https://licj15.github.io/MixRT/\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11841 ,  10066kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12096\\rreplaced with revised version Wed, 20 Dec 2023 05:21:26 GMT   (5176kb,D)\\r\\rTitle: DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular\\r  Videos\\rAuthors: Chunjie Luo, Fei Luo, Yusen Wang, Enxu Zhao, Chunxia Xiao\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12096 ,  5176kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12143\\rreplaced with revised version Wed, 20 Dec 2023 18:58:17 GMT   (1034kb,D)\\r\\rTitle: Integrating Human Vision Perception in Vision Transformers for\\r  Classifying Waste Items\\rAuthors: Akshat Kishore Shrivastava, Tapan Kumar Gandhi\\rCategories: cs.CV eess.IV\\rComments: 16 pages, 4 figures\\rMSC-class: 68T45\\rACM-class: I.2; I.4\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12143 ,  1034kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12263\\rreplaced with revised version Wed, 20 Dec 2023 03:59:45 GMT   (442kb,D)\\r\\rTitle: FedDiv: Collaborative Noise Filtering for Federated Learning with Noisy\\r  Labels\\rAuthors: Jichang Li, Guanbin Li, Hui Cheng, Zicheng Liao, Yizhou Yu\\rCategories: cs.CV\\rComments: To appear in AAAI-2024; correct minor typos\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12263 ,  442kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12340\\rreplaced with revised version Wed, 20 Dec 2023 08:27:37 GMT   (20096kb,D)\\r\\rTitle: Scalable Geometric Fracture Assembly via Co-creation Space among\\r  Assemblers\\rAuthors: Ruiyuan Zhang and Jiaxiang Liu and Zexi Li and Hao Dong and Jie Fu and\\r  Chao Wu\\rCategories: cs.CV\\rComments: AAAI2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12340 ,  20096kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12436\\rreplaced with revised version Wed, 20 Dec 2023 12:40:47 GMT   (28758kb,D)\\r\\rTitle: A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise\\rAuthors: Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang, Zhengye Zhang,\\r  Longtian Qiu, Gaoxiang Ye, Yunhang Shen, Mengdan Zhang, Peixian Chen, Sirui\\r  Zhao, Shaohui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hongsheng Li, Xing\\r  Sun\\rCategories: cs.CV cs.AI cs.CL cs.MM\\rComments: Total 120 pages. See our project at\\r  https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12436 ,  28758kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.05221 (*cross-listing*)\\rreplaced with revised version Wed, 20 Dec 2023 08:47:39 GMT   (729kb,D)\\r\\rTitle: SEAM: An Integrated Activation-Coupled Model of Sentence Processing and\\r  Eye Movements in Reading\\rAuthors: Maximilian M. Rabe, Dario Paape, Daniela Mertzen, Shravan Vasishth,\\r  Ralf Engbert\\rCategories: q-bio.NC cs.CL\\rDOI: 10.1016/j.jml.2023.104496\\r\\\\\\\\ ( https://arxiv.org/abs/2303.05221 ,  729kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2309.17255\\rreplaced with revised version Wed, 20 Dec 2023 13:34:31 GMT   (771kb,D)\\r\\rTitle: Knowledge Graphs for the Life Sciences: Recent Developments, Challenges\\r  and Opportunities\\rAuthors: Jiaoyan Chen, Hang Dong, Janna Hastings, Ernesto Jim\\\\'enez-Ruiz,\\r  Vanessa L\\\\'opez, Pierre Monnin, Catia Pesquita, Petr \\\\v{S}koda, Valentina\\r  Tamma\\rCategories: cs.AI cs.CL\\rComments: 33 pages, 1 figure, camera-ready version, accepted for Transactions\\r  on Graph Data and Knowledge (TGDK)\\rACM-class: I.2.4; J.3\\rDOI: 10.4230/TGDK.1.1.5\\r\\\\\\\\ ( https://arxiv.org/abs/2309.17255 ,  771kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2311.12420\\rreplaced with revised version Wed, 20 Dec 2023 15:48:15 GMT   (2469kb,D)\\r\\rTitle: How Far Have We Gone in Vulnerability Detection Using Large Language\\r  Models\\rAuthors: Zeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, Chao Zhang\\rCategories: cs.AI cs.CL cs.CR\\r\\\\\\\\ ( https://arxiv.org/abs/2311.12420 ,  2469kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11562\\rreplaced with revised version Wed, 20 Dec 2023 07:25:58 GMT   (3867kb,D)\\r\\rTitle: A Survey of Reasoning with Foundation Models: Concepts, Methodologies,\\r  and Outlook\\rAuthors: Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu,\\r  Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai\\r  Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan,\\r  Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, Pheng Ann Heng,\\r  Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui\\r  Xiong, Qun Liu, Zhenguo Li\\rCategories: cs.AI cs.CL cs.CV cs.LG\\rComments: 20 Figures, 159 Pages, 740 References, Project Page\\r  https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11562 ,  3867kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11681\\rreplaced with revised version Wed, 20 Dec 2023 03:01:36 GMT   (443kb,D)\\r\\rTitle: Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows\\rAuthors: Madeleine Grunde-McLaughlin, Michelle S. Lam, Ranjay Krishna, Daniel\\r  S. Weld, Jeffrey Heer\\rCategories: cs.HC cs.AI cs.CL\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11681 ,  443kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.12430\\rreplaced with revised version Wed, 20 Dec 2023 03:33:54 GMT   (817kb,D)\\r\\rTitle: Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP\\rAuthors: Ziyi Chen, Heyi Tao, Daqian Zuo, Jize Jiang, Jun Yang, Yuxiang Wei\\rCategories: cs.IR cs.AI cs.CL cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2312.12430 ,  817kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.02515\\rreplaced with revised version Tue, 19 Dec 2023 23:30:52 GMT   (1922kb,D)\\r\\rTitle: Deep Learning for Time Series Classification and Extrinsic Regression: A\\r  Current Survey\\rAuthors: Navid Mohammadi Foumani, Lynn Miller, Chang Wei Tan, Geoffrey I. Webb,\\r  Germain Forestier, Mahsa Salehi\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2302.02515 ,  1922kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.03483 (*cross-listing*)\\rreplaced with revised version Wed, 20 Dec 2023 08:18:10 GMT   (3288kb,D)\\r\\rTitle: RED-PSM: Regularization by Denoising of Partially Separable Models for\\r  Dynamic Imaging\\rAuthors: Berk Iskender, Marc L. Klasky, Yoram Bresler\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2304.03483 ,  3288kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.03373 (*cross-listing*)\\rreplaced with revised version Wed, 20 Dec 2023 02:42:13 GMT   (3309kb,D)\\r\\rTitle: CiT-Net: Convolutional Neural Networks Hand in Hand with Vision\\r  Transformers for Medical Image Segmentation\\rAuthors: Tao Lei, Rui Sun, Xuan Wang, Yingbo Wang, Xi He, Asoke Nandi\\rCategories: eess.IV cs.CV\\rComments: 9 pages, 3 figures, 3 tables\\rJournal-ref: The 32nd International Joint Conference on Artificial\\r  Intelligence, IJCAI2023, MACAO\\rDOI: 10.24963/ijcai.2023/113\\r\\\\\\\\ ( https://arxiv.org/abs/2306.03373 ,  3309kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.04086 (*cross-listing*)\\rreplaced with revised version Wed, 20 Dec 2023 02:34:49 GMT   (4735kb,D)\\r\\rTitle: TEC-Net: Vision Transformer Embrace Convolutional Neural Networks for\\r  Medical Image Segmentation\\rAuthors: Rui Sun, Tao Lei, Weichuan Zhang, Yong Wan, Yong Xia, Asoke K. Nandi\\rCategories: eess.IV cs.CV\\rComments: arXiv admin note: substantial text overlap with arXiv:2306.03373\\r\\\\\\\\ ( https://arxiv.org/abs/2306.04086 ,  4735kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.13986 (*cross-listing*)\\rreplaced with revised version Wed, 20 Dec 2023 14:24:17 GMT   (2125kb,D)\\r\\rTitle: Hybrid Representation-Enhanced Sampling for Bayesian Active Learning in\\r  Musculoskeletal Segmentation of Lower Extremities\\rAuthors: Ganping Li, Yoshito Otake, Mazen Soufi, Masashi Taniguchi, Masahide\\r  Yagi, Noriaki Ichihashi, Keisuke Uemura, Masaki Takao, Nobuhiko Sugano,\\r  Yoshinobu Sato\\rCategories: eess.IV cs.CV\\rComments: 15 pages, 5 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2307.13986 ,  2125kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.08511 (*cross-listing*)\\rreplaced with revised version Wed, 20 Dec 2023 02:21:20 GMT   (40100kb,D)\\r\\rTitle: Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse\\r  Problems\\rAuthors: Zirong Li, Yanyang Wang, Jianjia Zhang and Weiwen Wu, Hengyong Yu\\rCategories: eess.IV cs.CV cs.LG\\rComments: 10 pages, 13 figures\\rJournal-ref: Computers in Biology and Medicine Volume 168, January 2024, 107819\\rDOI: 10.1016/j.compbiomed.2023.107819\\r\\\\\\\\ ( https://arxiv.org/abs/2308.08511 ,  40100kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2308.10542 (*cross-listing*)\\rreplaced with revised version Wed, 20 Dec 2023 11:17:24 GMT   (925kb,D)\\r\\rTitle: Learning Weakly Convex Regularizers for Convergent Image-Reconstruction\\r  Algorithms\\rAuthors: Alexis Goujon, Sebastian Neumayer, Michael Unser\\rCategories: eess.IV cs.CV cs.LG\\rMSC-class: 26B25, 47A52, 49N45, 68U10, 65D07, 68T05, 90C26\\r\\\\\\\\ ( https://arxiv.org/abs/2308.10542 ,  925kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.06914\\rreplaced with revised version Tue, 19 Dec 2023 22:16:34 GMT   (1524kb,D)\\r\\rTitle: Exploring Novel Object Recognition and Spontaneous Location Recognition\\r  Machine Learning Analysis Techniques in Alzheimer's Mice\\rAuthors: Soham Bafana\\rCategories: cs.LG cs.CV\\rComments: 10 Pages. All code used in this research can be found at\\r  https://github.com/bafanaS/DLC-Object-Recognition-Analysis.git\\r\\\\\\\\ ( https://arxiv.org/abs/2312.06914 ,  1524kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.08488\\rreplaced with revised version Wed, 20 Dec 2023 18:53:23 GMT   (131kb,D)\\r\\rTitle: PnP for Two-Dimensional Pose Estimation\\rAuthors: Joshua Wang\\rCategories: cs.RO cs.CV\\rComments: 4 pages, 3 figures. Improved testing figures from version 1\\r\\\\\\\\ ( https://arxiv.org/abs/2312.08488 ,  131kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.08866 (*cross-listing*)\\rreplaced with revised version Wed, 20 Dec 2023 04:31:00 GMT   (7641kb,D)\\r\\rTitle: MCANet: Medical Image Segmentation with Multi-Scale Cross-Axis Attention\\rAuthors: Hao Shao, Quansheng Zeng, Qibin Hou, Jufeng Yang\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2312.08866 ,  7641kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2312.11057\\rreplaced with revised version Wed, 20 Dec 2023 01:40:15 GMT   (3792kb,D)\\r\\rTitle: DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via\\r  Diffusion Models\\rAuthors: Jiachen Zhou, Peizhuo Lv, Yibing Lan, Guozhu Meng, Kai Chen, Hualong\\r  Ma\\rCategories: cs.CR cs.AI cs.CV\\rComments: Accepted by AAAI2024\\r\\\\\\\\ ( https://arxiv.org/abs/2312.11057 ,  3792kb)\\r%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%--- ------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\rSend any comments regarding submissions directly to submitter.\\r------------------------------------------------------------------------------\\rArchives at http://arxiv.org/\\rTo unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel\\r------------------------------------------------------------------------------\\r Submissions to:\\rComputer Vision and Pattern Recognition\\r received from  Tue 31 Jan 23 19:00:00 GMT  to  Wed  1 Feb 23 19:00:00 GMT\\r------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00059\\rDate: Tue, 31 Jan 2023 19:48:37 GMT   (11413kb,D)\\r\\rTitle: NASiam: Efficient Representation Learning using Neural Architecture\\r  Search for Siamese Networks\\rAuthors: Alexandre Heuillet, Hedi Tabia, Hichem Arioui\\rCategories: cs.CV cs.AI\\rComments: 8 pages, 6 figures\\r\\\\\\\\\\r  Siamese networks are one of the most trending methods to achieve\\rself-supervised visual representation learning (SSL). Since hand labeling is\\rcostly, SSL can play a crucial part by allowing deep learning to train on large\\runlabeled datasets. Meanwhile, Neural Architecture Search (NAS) is becoming\\rincreasingly important as a technique to discover novel deep learning\\rarchitectures. However, early NAS methods based on reinforcement learning or\\revolutionary algorithms suffered from ludicrous computational and memory costs.\\rIn contrast, differentiable NAS, a gradient-based approach, has the advantage\\rof being much more efficient and has thus retained most of the attention in the\\rpast few years. In this article, we present NASiam, a novel approach that uses\\rfor the first time differentiable NAS to improve the multilayer perceptron\\rprojector and predictor (encoder/predictor pair) architectures inside\\rsiamese-networks-based contrastive learning frameworks (e.g., SimCLR, SimSiam,\\rand MoCo) while preserving the simplicity of previous baselines. We crafted a\\rsearch space designed explicitly for multilayer perceptrons, inside which we\\rexplored several alternatives to the standard ReLU activation function. We show\\rthat these new architectures allow ResNet backbone convolutional models to\\rlearn strong representations efficiently. NASiam reaches competitive\\rperformance in both small-scale (i.e., CIFAR-10/CIFAR-100) and large-scale\\r(i.e., ImageNet) image classification datasets while costing only a few GPU\\rhours. We discuss the composition of the NAS-discovered architectures and emit\\rhypotheses on why they manage to prevent collapsing behavior. Our code is\\ravailable at https://github.com/aheuillet/NASiam.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00059 ,  11413kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00117\\rDate: Tue, 31 Jan 2023 21:54:15 GMT   (16712kb,D)\\r\\rTitle: Real Estate Property Valuation using Self-Supervised Vision Transformers\\rAuthors: Mahdieh Yazdani and Maziar Raissi\\rCategories: cs.CV cs.AI cs.LG econ.EM\\r\\\\\\\\\\r  The use of Artificial Intelligence (AI) in the real estate market has been\\rgrowing in recent years. In this paper, we propose a new method for property\\rvaluation that utilizes self-supervised vision transformers, a recent\\rbreakthrough in computer vision and deep learning. Our proposed algorithm uses\\ra combination of machine learning, computer vision and hedonic pricing models\\rtrained on real estate data to estimate the value of a given property. We\\rcollected and pre-processed a data set of real estate properties in the city of\\rBoulder, Colorado and used it to train, validate and test our algorithm. Our\\rdata set consisted of qualitative images (including house interiors, exteriors,\\rand street views) as well as quantitative features such as the number of\\rbedrooms, bathrooms, square footage, lot square footage, property age, crime\\rrates, and proximity to amenities. We evaluated the performance of our model\\rusing metrics such as Root Mean Squared Error (RMSE). Our findings indicate\\rthat these techniques are able to accurately predict the value of properties,\\rwith a low RMSE. The proposed algorithm outperforms traditional appraisal\\rmethods that do not leverage property images and has the potential to be used\\rin real-world applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00117 ,  16712kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00123\\rDate: Tue, 31 Jan 2023 22:04:53 GMT   (18803kb,D)\\r\\rTitle: Design and Implementation of A Soccer Ball Detection System with\\r  Multiple Cameras\\rAuthors: Lei Li, Tianfang Zhang, Zhongfeng Kang, Wenhan Zhang\\rCategories: cs.CV cs.AI\\rComments: 89 pages\\r\\\\\\\\\\r  The detection of small and medium-sized objects in three dimensions has\\ralways been a frontier exploration problem. This technology has a very wide\\rapplication in sports analysis, games, virtual reality, human animation and\\rother fields. The traditional three-dimensional small target detection\\rtechnology has the disadvantages of high cost, low precision and inconvenience,\\rso it is difficult to apply in practice. With the development of machine\\rlearning and deep learning, the technology of computer vision algorithms is\\rbecoming more mature. Creating an immersive media experience is considered to\\rbe a very important research work in sports.\\r  The main work is to explore and solve the problem of football detection under\\rthe multiple cameras, aiming at the research and implementation of the live\\rbroadcast system of football matches. Using multi cameras detects a target ball\\rand determines its position in three dimension with the occlusion, motion, low\\rillumination of the target object.\\r  This paper designed and implemented football detection system under multiple\\rcameras for the detection and capture of targets in real-time matches. The main\\rwork mainly consists of three parts, football detector, single camera\\rdetection, and multi-cameras detection. The system used bundle adjustment to\\robtain the three-dimensional position of the target, and the GPU to accelerates\\rdata pre-processing and achieve accurate real-time capture of the target. By\\rtesting the system, it shows that the system can accurately detect and capture\\rthe moving targets in 3D.\\r  In addition, the solution in this paper is reusable for large-scale\\rcompetitions, like basketball and soccer. The system framework can be well\\rtransplanted into other similar engineering project systems. It has been put\\rinto the market.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00123 ,  18803kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00162\\rDate: Wed, 1 Feb 2023 00:49:21 GMT   (45202kb,D)\\r\\rTitle: Continual Segment: Towards a Single, Unified and Accessible Continual\\r  Segmentation Model of 143 Whole-body Organs in CT Scans\\rAuthors: Zhanghexuan Ji, Dazhou Guo, Puyang Wang, Ke Yan, Jia Ge, Xianghua Ye,\\r  Minfeng Xu, Jingren Zhou, Le Lu, Mingchen Gao, Dakai Jin\\rCategories: cs.CV\\r\\\\\\\\\\r  Deep learning empowers the mainstream medical image segmentation methods.\\rNevertheless current deep segmentation approaches are not capable of\\refficiently and effectively adapting and updating the trained models when new\\rincremental segmentation classes (along with new training datasets or not) are\\rrequired to be added. In real clinical environment, it can be preferred that\\rsegmentation models could be dynamically extended to segment new organs/tumors\\rwithout the (re-)access to previous training datasets due to obstacles of\\rpatient privacy and data storage. This process can be viewed as a continual\\rsemantic segmentation (CSS) problem, being understudied for multi-organ\\rsegmentation. In this work, we propose a new architectural CSS learning\\rframework to learn a single deep segmentation model for segmenting a total of\\r143 whole-body organs. Using the encoder/decoder network structure, we\\rdemonstrate that a continually-trained then frozen encoder coupled with\\rincrementally-added decoders can extract and preserve sufficiently\\rrepresentative image features for new classes to be subsequently and validly\\rsegmented. To maintain a single network model complexity, we trim each decoder\\rprogressively using neural architecture search and teacher-student based\\rknowledge distillation. To incorporate with both healthy and pathological\\rorgans appearing in different datasets, a novel anomaly-aware and confidence\\rlearning module is proposed to merge the overlapped organ predictions,\\roriginated from different decoders. Trained and validated on 3D CT scans of\\r2500+ patients from four datasets, our single network can segment total 143\\rwhole-body organs with very high accuracy, closely reaching the upper bound\\rperformance level by training four separate segmentation models (i.e., one\\rmodel per dataset/task).\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00162 ,  45202kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00164\\rDate: Wed, 1 Feb 2023 00:57:58 GMT   (1005kb)\\r\\rTitle: Detection of Tomato Ripening Stages using Yolov3-tiny\\rAuthors: Gerardo Antonio Alvarez Hern\\\\'andez, Juan Carlos Olguin, Juan Irving\\r  Vasquez, Abril Valeria Uriarte, Maria Claudia Villica\\\\~na Torres\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  One of the most important agricultural products in Mexico is the tomato\\r(Solanum lycopersicum), which occupies the 4th place national most produced\\rproduct . Therefore, it is necessary to improve its production, building\\rautomatic detection system that detect, classify an keep tacks of the fruits is\\rone way to archieve it. So, in this paper, we address the design of a computer\\rvision system to detect tomatoes at different ripening stages. To solve the\\rproblem, we use a neural network-based model for tomato classification and\\rdetection. Specifically, we use the YOLOv3-tiny model because it is one of the\\rlightest current deep neural networks. To train it, we perform two grid\\rsearches testing several combinations of hyperparameters. Our experiments\\rshowed an f1-score of 90.0% in the localization and classification of ripening\\rstages in a custom dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00164 ,  1005kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00178\\rDate: Wed, 1 Feb 2023 01:51:45 GMT   (566kb,D)\\r\\rTitle: Program Generation from Diverse Video Demonstrations\\rAuthors: Anthony Manchin, Jamie Sherrah, Qi Wu, Anton van den Hengel\\rCategories: cs.CV cs.CL cs.LG\\r\\\\\\\\\\r  The ability to use inductive reasoning to extract general rules from multiple\\robservations is a vital indicator of intelligence. As humans, we use this\\rability to not only interpret the world around us, but also to predict the\\routcomes of the various interactions we experience. Generalising over multiple\\robservations is a task that has historically presented difficulties for\\rmachines to grasp, especially when requiring computer vision. In this paper, we\\rpropose a model that can extract general rules from video demonstrations by\\rsimultaneously performing summarisation and translation. Our approach differs\\rfrom prior works by framing the problem as a multi-sequence-to-sequence task,\\rwherein summarisation is learnt by the model. This allows our model to utilise\\redge cases that would otherwise be suppressed or discarded by traditional\\rsummarisation techniques. Additionally, we show that our approach can handle\\rnoisy specifications without the need for additional filtering methods. We\\revaluate our model by synthesising programs from video demonstrations in the\\rVizdoom environment achieving state-of-the-art results with a relative increase\\rof 11.75% program accuracy on prior works\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00178 ,  566kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00179\\rDate: Wed, 1 Feb 2023 01:51:47 GMT   (37858kb,D)\\r\\rTitle: Stable Attribute Group Editing for Reliable Few-shot Image Generation\\rAuthors: Guanqi Ding, Xinzhe Han, Shuhui Wang, Xin Jin, Dandan Tu, Qingming\\r  Huang\\rCategories: cs.CV\\r\\\\\\\\\\r  Few-shot image generation aims to generate data of an unseen category based\\ron only a few samples. Apart from basic content generation, a bunch of\\rdownstream applications hopefully benefit from this task, such as low-data\\rdetection and few-shot classification. To achieve this goal, the generated\\rimages should guarantee category retention for classification beyond the visual\\rquality and diversity. In our preliminary work, we present an ``editing-based''\\rframework Attribute Group Editing (AGE) for reliable few-shot image generation,\\rwhich largely improves the generation performance. Nevertheless, AGE's\\rperformance on downstream classification is not as satisfactory as expected.\\rThis paper investigates the class inconsistency problem and proposes Stable\\rAttribute Group Editing (SAGE) for more stable class-relevant image generation.\\rSAGE takes use of all given few-shot images and estimates a class center\\rembedding based on the category-relevant attribute dictionary. Meanwhile,\\raccording to the projection weights on the category-relevant attribute\\rdictionary, we can select category-irrelevant attributes from the similar seen\\rcategories. Consequently, SAGE injects the whole distribution of the novel\\rclass into StyleGAN's latent space, thus largely remains the category retention\\rand stability of the generated images. Going one step further, we find that\\rclass inconsistency is a common problem in GAN-generated images for downstream\\rclassification. Even though the generated images look photo-realistic and\\rrequires no category-relevant editing, they are usually of limited help for\\rdownstream classification. We systematically discuss this issue from both the\\rgenerative model and classification model perspectives, and propose to boost\\rthe downstream classification performance of SAGE by enhancing the pixel and\\rfrequency components.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00179 ,  37858kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00190\\rDate: Wed, 1 Feb 2023 02:47:53 GMT   (32763kb,D)\\r\\rTitle: Neural Wavelet-domain Diffusion for 3D Shape Generation, Inversion, and\\r  Manipulation\\rAuthors: Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Ruihui Li and Chi-Wing Fu\\rCategories: cs.CV cs.GR\\rComments: arXiv admin note: substantial text overlap with arXiv:2209.08725\\r\\\\\\\\\\r  This paper presents a new approach for 3D shape generation, inversion, and\\rmanipulation, through a direct generative modeling on a continuous implicit\\rrepresentation in wavelet domain. Specifically, we propose a compact wavelet\\rrepresentation with a pair of coarse and detail coefficient volumes to\\rimplicitly represent 3D shapes via truncated signed distance functions and\\rmulti-scale biorthogonal wavelets. Then, we design a pair of neural networks: a\\rdiffusion-based generator to produce diverse shapes in the form of the coarse\\rcoefficient volumes and a detail predictor to produce compatible detail\\rcoefficient volumes for introducing fine structures and details. Further, we\\rmay jointly train an encoder network to learn a latent space for inverting\\rshapes, allowing us to enable a rich variety of whole-shape and region-aware\\rshape manipulations. Both quantitative and qualitative experimental results\\rmanifest the compelling shape generation, inversion, and manipulation\\rcapabilities of our approach over the state-of-the-art methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00190 ,  32763kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00220\\rDate: Wed, 1 Feb 2023 03:51:27 GMT   (5784kb,D)\\r\\rTitle: Efficient Scopeformer: Towards Scalable and Rich Feature Extraction for\\r  Intracranial Hemorrhage Detection\\rAuthors: Yassine Barhoumi, Nidhal C. Bouaynaya, Ghulam Rasool\\rCategories: cs.CV cs.LG q-bio.QM\\r\\\\\\\\\\r  The quality and richness of feature maps extracted by convolution neural\\rnetworks (CNNs) and vision Transformers (ViTs) directly relate to the robust\\rmodel performance. In medical computer vision, these information-rich features\\rare crucial for detecting rare cases within large datasets. This work presents\\rthe Scopeformer, a novel multi-CNN-ViT model for intracranial hemorrhage\\rclassification in computed tomography (CT) images. The Scopeformer architecture\\ris scalable and modular, which allows utilizing various CNN architectures as\\rthe backbone with diversified output features and pre-training strategies. We\\rpropose effective feature projection methods to reduce redundancies among\\rCNN-generated features and to control the input size of ViTs. Extensive\\rexperiments with various Scopeformer models show that the model performance is\\rproportional to the number of convolutional blocks employed in the feature\\rextractor. Using multiple strategies, including diversifying the pre-training\\rparadigms for CNNs, different pre-training datasets, and style transfer\\rtechniques, we demonstrate an overall improvement in the model performance at\\rvarious computational budgets. Later, we propose smaller compute-efficient\\rScopeformer versions with three different types of input and output ViT\\rconfigurations. Efficient Scopeformers use four different pre-trained CNN\\rarchitectures as feature extractors to increase feature richness. Our best\\rEfficient Scopeformer model achieved an accuracy of 96.94\\\\% and a weighted\\rlogarithmic loss of 0.083 with an eight times reduction in the number of\\rtrainable parameters compared to the base Scopeformer. Another version of the\\rEfficient Scopeformer model further reduced the parameter space by almost 17\\rtimes with negligible performance reduction. Hybrid CNNs and ViTs might provide\\rthe desired feature richness for developing accurate medical computer vision\\rmodels\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00220 ,  5784kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00224\\rDate: Wed, 1 Feb 2023 04:05:14 GMT   (1447kb,D)\\r\\rTitle: Human Fall Detection- Multimodality Approach\\rAuthors: Xi Wang, Ramya Penta, Bhavya Sehgal, Dale Chen-Song\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Falls have become more frequent in recent years, which has been harmful for\\rsenior citizens.Therefore detecting falls have become important and several\\rdata sets and machine learning model have been introduced related to fall\\rdetection. In this project report, a human fall detection method is proposed\\rusing a multi modality approach. We used the UP-FALL detection data set which\\ris collected by dozens of volunteers using different sensors and two cameras.\\rWe use wrist sensor with acclerometer data keeping labels to binary\\rclassification, namely fall and no fall from the data set.We used fusion of\\rcamera and sensor data to increase performance. The experimental results shows\\rthat using only wrist data as compared to multi sensor for binary\\rclassification did not impact the model prediction performance for fall\\rdetection.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00224 ,  1447kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00225\\rDate: Wed, 1 Feb 2023 04:07:11 GMT   (1572kb)\\r\\rTitle: The Past, Current, and Future of Neonatal Intensive Care Units with\\r  Artificial Intelligence\\rAuthors: Elif Keles and Ulas Bagci\\rCategories: cs.CV cs.AI\\rComments: 58 pages, review article\\r\\\\\\\\\\r  Artificial intelligence (AI), specifically a branch of AI called deep\\rlearning (DL), has proven revolutionary developments in almost all fields, from\\rcomputer vision to health sciences, and its effects in medicine have changed\\rclinical applications significantly. Although some sub-fields of medicine such\\ras pediatrics have been relatively slow in receiving critical benefits of AI,\\rrelated research in pediatrics started to be accumulated to a significant level\\rtoo. Hence, in this paper, we review recently developed machine learning and\\rdeep learning based systems for neonatology applications. We systematically\\revaluate the role of AI in neonatology applications, define the methodologies,\\rincluding algorithmic developments, and describe the remaining challenges in\\rneonatal diseases. To date, survival analysis, neuroimaging, EEG, pattern\\ranalysis of vital parameters, and retinopathy of prematurity diagnosis with AI\\rhave been the main focus in neonatology. We have categorically summarized 96\\rresearch articles, from 1996 to 2022, and discussed their pros and cons,\\rrespectively. We also discuss possible directions for new AI models and the\\rfuture of neonatology with the rising power of AI, suggesting roadmaps for\\rintegration of AI into neonatal intensive care units.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00225 ,  1572kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00268\\rDate: Wed, 1 Feb 2023 06:20:54 GMT   (1433kb,D)\\r\\rTitle: Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video\\r  Relation Detection\\rAuthors: Kaifeng Gao, Long Chen, Hanwang Zhang, Jun Xiao, Qianru Sun\\rCategories: cs.CV\\rComments: accepted by ICLR 2023\\r\\\\\\\\\\r  Prompt tuning with large-scale pretrained vision-language models empowers\\ropen-vocabulary predictions trained on limited base categories, e.g., object\\rclassification and detection. In this paper, we propose compositional prompt\\rtuning with motion cues: an extended prompt tuning paradigm for compositional\\rpredictions of video data. In particular, we present Relation Prompt (RePro)\\rfor Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where\\rconventional prompt tuning is easily biased to certain subject-object\\rcombinations and motion patterns. To this end, RePro addresses the two\\rtechnical challenges of Open-VidVRD: 1) the prompt tokens should respect the\\rtwo different semantic roles of subject and object, and 2) the tuning should\\raccount for the diverse spatio-temporal motion patterns of the subject-object\\rcompositions. Without bells and whistles, our RePro achieves a new\\rstate-of-the-art performance on two VidVRD benchmarks of not only the base\\rtraining object and predicate categories, but also the unseen ones. Extensive\\rablations also demonstrate the effectiveness of the proposed compositional and\\rmulti-mode design of prompts. Code is available at\\rhttps://github.com/Dawn-LX/OpenVoc-VidVRD.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00268 ,  1433kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00275\\rDate: Wed, 1 Feb 2023 06:44:07 GMT   (2078kb,D)\\r\\rTitle: Learning Generalized Zero-Shot Learners for Open-Domain Image\\r  Geolocalization\\rAuthors: Lukas Haas, Silas Alberti, Michal Skreta\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Image geolocalization is the challenging task of predicting the geographic\\rcoordinates of origin for a given photo. It is an unsolved problem relying on\\rthe ability to combine visual clues with general knowledge about the world to\\rmake accurate predictions across geographies. We present\\r$\\\\href{https://huggingface.co/geolocal/StreetCLIP}{\\\\text{StreetCLIP}}$, a\\rrobust, publicly available foundation model not only achieving state-of-the-art\\rperformance on multiple open-domain image geolocalization benchmarks but also\\rdoing so in a zero-shot setting, outperforming supervised models trained on\\rmore than 4 million images. Our method introduces a meta-learning approach for\\rgeneralized zero-shot learning by pretraining CLIP from synthetic captions,\\rgrounding CLIP in a domain of choice. We show that our method effectively\\rtransfers CLIP's generalized zero-shot capabilities to the domain of image\\rgeolocalization, improving in-domain generalized zero-shot performance without\\rfinetuning StreetCLIP on a fixed set of classes.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00275 ,  2078kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00290\\rDate: Wed, 1 Feb 2023 07:45:10 GMT   (6872kb,D)\\r\\rTitle: Multispectral Pedestrian Detection via Reference Box Constrained Cross\\r  Attention and Modality Balanced Optimization\\rAuthors: Yinghui Xing, Song Wang, Guoqiang Liang, Qingyi Li, Xiuwei Zhang,\\r  Shizhou Zhang, Yanning Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  Multispectral pedestrian detection is an important task for many\\raround-the-clock applications, since the visible and thermal modalities can\\rprovide complementary information especially under low light conditions. To\\rreduce the influence of hand-designed components in available multispectral\\rpedestrian detectors, we propose a MultiSpectral pedestrian DEtection\\rTRansformer (MS-DETR), which extends deformable DETR to multi-modal paradigm.\\rIn order to facilitate the multi-modal learning process, a Reference box\\rConstrained Cross-Attention (RCCA) module is firstly introduced to the\\rmulti-modal Transformer decoder, which takes fusion branch together with the\\rreference boxes as intermediaries to enable the interaction of visible and\\rthermal modalities. To further balance the contribution of different\\rmodalities, we design a modality-balanced optimization strategy, which aligns\\rthe slots of decoders by adaptively adjusting the instance-level weight of\\rthree branches. Our end-to-end MS-DETR shows superior performance on the\\rchallenging KAIST and CVC-14 benchmark datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00290 ,  6872kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00368\\rDate: Wed, 1 Feb 2023 10:55:27 GMT   (2316kb,D)\\r\\rTitle: Test-Time Amendment with a Coarse Classifier for Fine-Grained\\r  Classification\\rAuthors: Kanishk Jain, Shyamgopal Karthik, Vineet Gandhi\\rCategories: cs.CV cs.LG\\rComments: 8 pages, 2 figures, 3 tables\\r\\\\\\\\\\r  We investigate the problem of reducing mistake severity for fine-grained\\rclassification. Fine-grained classification can be challenging, mainly due to\\rthe requirement of knowledge or domain expertise for accurate annotation.\\rHowever, humans are particularly adept at performing coarse classification as\\rit requires relatively low levels of expertise. To this end, we present a novel\\rapproach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that\\rutilizes label hierarchy to improve the performance of fine-grained\\rclassification at test-time using the coarse-grained predictions. By only\\rrequiring the parents of leaf nodes, our method significantly reduces avg.\\rmistake severity while improving top-1 accuracy on the iNaturalist-19 and\\rtieredImageNet-H datasets, achieving a new state-of-the-art on both benchmarks.\\rWe also investigate the efficacy of our approach in the semi-supervised\\rsetting. Our approach brings notable gains in top-1 accuracy while\\rsignificantly decreasing the severity of mistakes as training data decreases\\rfor the fine-grained classes. The simplicity and post-hoc nature of HiE render\\rit practical to be used with any off-the-shelf trained model to improve its\\rpredictions further.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00368 ,  2316kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00384\\rDate: Wed, 1 Feb 2023 11:41:21 GMT   (5881kb,D)\\r\\rTitle: Alphazzle: Jigsaw Puzzle Solver with Deep Monte-Carlo Tree Search\\rAuthors: Marie-Morgane Paumard, Hedi Tabia, David Picard\\rCategories: cs.CV\\r\\\\\\\\\\r  Solving jigsaw puzzles requires to grasp the visual features of a sequence of\\rpatches and to explore efficiently a solution space that grows exponentially\\rwith the sequence length. Therefore, visual deep reinforcement learning (DRL)\\rshould answer this problem more efficiently than optimization solvers coupled\\rwith neural networks. Based on this assumption, we introduce Alphazzle, a\\rreassembly algorithm based on single-player Monte Carlo Tree Search (MCTS). A\\rmajor difference with DRL algorithms lies in the unavailability of game reward\\rfor MCTS, and we show how to estimate it from the visual input with neural\\rnetworks. This constraint is induced by the puzzle-solving task and\\rdramatically adds to the task complexity (and interest!). We perform an in-deep\\rablation study that shows the importance of MCTS and the neural networks\\rworking together. We achieve excellent results and get exciting insights into\\rthe combination of DRL and visual feature learning.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00384 ,  5881kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00386\\rDate: Wed, 1 Feb 2023 11:46:04 GMT   (4777kb,D)\\r\\rTitle: EfficientRep:An Efficient Repvgg-style ConvNets with Hardware-aware\\r  Neural Network Design\\rAuthors: Kaiheng Weng, Xiangxiang Chu, Xiaoming Xu, Junshi Huang and Xiaoming\\r  Wei\\rCategories: cs.CV\\r\\\\\\\\\\r  We present a hardware-efficient architecture of convolutional neural network,\\rwhich has a repvgg-like architecture. Flops or parameters are traditional\\rmetrics to evaluate the efficiency of networks which are not sensitive to\\rhardware including computing ability and memory bandwidth. Thus, how to design\\ra neural network to efficiently use the computing ability and memory bandwidth\\rof hardware is a critical problem. This paper proposes a method how to design\\rhardware-aware neural network. Based on this method, we designed EfficientRep\\rseries convolutional networks, which are high-computation hardware(e.g. GPU)\\rfriendly and applied in YOLOv6 object detection framework. YOLOv6 has published\\rYOLOv6N/YOLOv6S/YOLOv6M/YOLOv6L models in v1 and v2 versions.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00386 ,  4777kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00391\\rDate: Wed, 1 Feb 2023 12:02:04 GMT   (4507kb,D)\\r\\rTitle: PresSim: An End-to-end Framework for Dynamic Ground Pressure Profile\\r  Generation from Monocular Videos Using Physics-based 3D Simulation\\rAuthors: Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Paul Lukowicz\\rCategories: cs.CV cs.AI cs.GR\\rComments: Percom2023 workshop(UMUM2023)\\r\\\\\\\\\\r  Ground pressure exerted by the human body is a valuable source of information\\rfor human activity recognition (HAR) in unobtrusive pervasive sensing. While\\rdata collection from pressure sensors to develop HAR solutions requires\\rsignificant resources and effort, we present a novel end-to-end framework,\\rPresSim, to synthesize sensor data from videos of human activities to reduce\\rsuch effort significantly. PresSim adopts a 3-stage process: first, extract the\\r3D activity information from videos with computer vision architectures; then\\rsimulate the floor mesh deformation profiles based on the 3D activity\\rinformation and gravity-included physics simulation; lastly, generate the\\rsimulated pressure sensor data with deep learning models. We explored two\\rapproaches for the 3D activity information: inverse kinematics with mesh\\rre-targeting, and volumetric pose and shape estimation. We validated PresSim\\rwith an experimental setup with a monocular camera to provide input and a\\rpressure-sensing fitness mat (80x28 spatial resolution) to provide the sensor\\rground truth, where nine participants performed a set of predefined yoga\\rsequences.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00391 ,  4507kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00402\\rDate: Wed, 1 Feb 2023 12:40:03 GMT   (4814kb,D)\\r\\rTitle: mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image\\r  and Video\\rAuthors: Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu,\\r  Chenliang Li, Bin Bi, Qi Qian, Wei Wang, Guohai Xu, Ji Zhang, Songfang Huang,\\r  Fei Huang, Jingren Zhou\\rCategories: cs.CV cs.CL cs.MM\\r\\\\\\\\\\r  Recent years have witnessed a big convergence of language, vision, and\\rmulti-modal pretraining. In this work, we present mPLUG-2, a new unified\\rparadigm with modularized design for multi-modal pretraining, which can benefit\\rfrom modality collaboration while addressing the problem of modality\\rentanglement. In contrast to predominant paradigms of solely relying on\\rsequence-to-sequence generation or encoder-based instance discrimination,\\rmPLUG-2 introduces a multi-module composition network by sharing common\\runiversal modules for modality collaboration and disentangling different\\rmodality modules to deal with modality entanglement. It is flexible to select\\rdifferent modules for different understanding and generation tasks across all\\rmodalities including text, image, and video. Empirical study shows that mPLUG-2\\rachieves state-of-the-art or competitive results on a broad range of over 30\\rdownstream tasks, spanning multi-modal tasks of image-text and video-text\\runderstanding and generation, and uni-modal tasks of text-only, image-only, and\\rvideo-only understanding. Notably, mPLUG-2 shows new state-of-the-art results\\rof 48.0 top-1 accuracy and 80.3 CIDEr on the challenging MSRVTT video QA and\\rvideo caption tasks with a far smaller model size and data scale. It also\\rdemonstrates strong zero-shot transferability on vision-language and\\rvideo-language tasks. Code and models will be released in\\rhttps://github.com/alibaba/AliceMind.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00402 ,  4814kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00431\\rDate: Wed, 1 Feb 2023 13:25:54 GMT   (3534kb,D)\\r\\rTitle: Do I Have Your Attention: A Large Scale Engagement Prediction Dataset\\r  and Baselines\\rAuthors: Monisha Singh, Ximi Hoque, Donghuo Zeng, Yanan Wang, Kazushi Ikeda,\\r  Abhinav Dhall\\rCategories: cs.CV cs.HC\\r\\\\\\\\\\r  The degree of concentration, enthusiasm, optimism, and passion displayed by\\rindividual(s) while interacting with a machine is referred to as `user\\rengagement'. Engagement comprises of behavioural, cognitive, and affect related\\rcues. To create engagement predictions systems, which can work in real-world\\rconditions it is quintessential to learn from rich diverse datasets. To this\\rend, a large scale multi-faceted engagement in the wild dataset is proposed. 31\\rhours duration data of 127 participants representing different illumination\\rconditions is recorded. Thorough experiments are performed exploring\\rapplicability of different features action units, eye gaze and head pose and\\rtransformers. To further validate the rich nature of the dataset, evaluation is\\ralso performed on the EngageWild dataset. The experiments show the usefulness\\rof the proposed dataset. The code, models and dataset will be made publicly\\ravailable.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00431 ,  3534kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00491\\rDate: Wed, 1 Feb 2023 15:02:58 GMT   (7117kb,D)\\r\\rTitle: Learning Prototype Classifiers for Long-Tailed Recognition\\rAuthors: Saurabh Sharma, Yongqin Xian, Ning Yu, Ambuj Singh\\rCategories: cs.CV cs.LG\\rComments: 11 pages, 4 figures and 9 tables\\r\\\\\\\\\\r  The problem of long-tailed recognition (LTR) has received attention in recent\\ryears due to the fundamental power-law distribution of objects in the\\rreal-world. Most recent works in LTR use softmax classifiers that have a\\rtendency to correlate classifier norm with the amount of training data for a\\rgiven class. On the other hand, Prototype classifiers do not suffer from this\\rshortcoming and can deliver promising results simply using Nearest-Class-Mean\\r(NCM), a special case where prototypes are empirical centroids. However, the\\rpotential of Prototype classifiers as an alternative to softmax in LTR is\\rrelatively underexplored. In this work, we propose Prototype classifiers, which\\rjointly learn prototypes that minimize average cross-entropy loss based on\\rprobability scores from distances to prototypes. We theoretically analyze the\\rproperties of Euclidean distance based prototype classifiers that leads to\\rstable gradient-based optimization which is robust to outliers. We further\\renhance Prototype classifiers by learning channel-dependent temperature\\rparameters to enable independent distance scales along each channel. Our\\ranalysis shows that prototypes learned by Prototype classifiers are better\\rseparated than empirical centroids. Results on four long-tailed recognition\\rbenchmarks show that Prototype classifier outperforms or is comparable to the\\rstate-of-the-art methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00491 ,  7117kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00503\\rDate: Wed, 1 Feb 2023 15:17:12 GMT   (20174kb,D)\\r\\rTitle: Tracking People in Highly Dynamic Industrial Environments\\rAuthors: Savvas Papaioannou, Andrew Markham, and Niki Trigoni\\rCategories: cs.CV\\rJournal-ref: IEEE Transactions on Mobile Computing, vol. 16, no. 8, pp.\\r  2351-2365, 1 Aug. 2017\\rDOI: 10.1109/TMC.2016.2613523\\r\\\\\\\\\\r  To date, the majority of positioning systems have been designed to operate\\rwithin environments that have long-term stable macro-structure with potential\\rsmall-scale dynamics. These assumptions allow the existing positioning systems\\rto produce and utilize stable maps. However, in highly dynamic industrial\\rsettings these assumptions are no longer valid and the task of tracking people\\ris more challenging due to the rapid large-scale changes in structure. In this\\rpaper we propose a novel positioning system for tracking people in highly\\rdynamic industrial environments, such as construction sites. The proposed\\rsystem leverages the existing CCTV camera infrastructure found in many\\rindustrial settings along with radio and inertial sensors within each worker's\\rmobile phone to accurately track multiple people. This multi-target\\rmulti-sensor tracking framework also allows our system to use cross-modality\\rtraining in order to deal with the environment dynamics. In particular, we show\\rhow our system uses cross-modality training in order to automatically keep\\rtrack environmental changes (i.e. new walls) by utilizing occlusion maps. In\\raddition, we show how these maps can be used in conjunction with social forces\\rto accurately predict human motion and increase the tracking accuracy. We have\\rconducted extensive real-world experiments in a construction site showing\\rsignificant accuracy improvement via cross-modality training and the use of\\rsocial forces.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00503 ,  20174kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00517\\rDate: Wed, 1 Feb 2023 15:37:35 GMT   (4467kb,D)\\r\\rTitle: Synthesis-based Imaging-Differentiation Representation Learning for\\r  Multi-Sequence 3D/4D MRI\\rAuthors: Luyi Han, Tao Tan, Tianyu Zhang, Yunzhi Huang, Xin Wang, Yuan Gao,\\r  Jonas Teuwen, Ritse Mann\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  Multi-sequence MRIs can be necessary for reliable diagnosis in clinical\\rpractice due to the complimentary information within sequences. However,\\rredundant information exists across sequences, which interferes with mining\\refficient representations by modern machine learning or deep learning models.\\rTo handle various clinical scenarios, we propose a sequence-to-sequence\\rgeneration framework (Seq2Seq) for imaging-differentiation representation\\rlearning. In this study, not only do we propose arbitrary 3D/4D sequence\\rgeneration within one model to generate any specified target sequence, but also\\rwe are able to rank the importance of each sequence based on a new metric\\restimating the difficulty of a sequence being generated. Furthermore, we also\\rexploit the generation inability of the model to extract regions that contain\\runique information for each sequence. We conduct extensive experiments using\\rthree datasets including a toy dataset of 20,000 simulated subjects, a brain\\rMRI dataset of 1,251 subjects, and a breast MRI dataset of 2,101 subjects, to\\rdemonstrate that (1) our proposed Seq2Seq is efficient and lightweight for\\rcomplex clinical datasets and can achieve excellent image quality; (2)\\rtop-ranking sequences can be used to replace complete sequences with\\rnon-inferior performance; (3) combining MRI with our imaging-differentiation\\rmap leads to better performance in clinical tasks such as glioblastoma MGMT\\rpromoter methylation status prediction and breast cancer pathological complete\\rresponse status prediction. Our code is available at\\rhttps://github.com/fiy2W/mri_seq2seq.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00517 ,  4467kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00523\\rDate: Wed, 1 Feb 2023 15:52:24 GMT   (11256kb,D)\\r\\rTitle: Uncertainty-Driven Dense Two-View Structure from Motion\\rAuthors: Weirong Chen, Suryansh Kumar, Fisher Yu\\rCategories: cs.CV cs.RO\\rComments: Accepted for publication at IEEE Robotics and Automation Letters\\r  (RA-L) 2023\\r\\\\\\\\\\r  This work introduces an effective and practical solution to the dense\\rtwo-view structure from motion (SfM) problem. One vital question addressed is\\rhow to mindfully use per-pixel optical flow correspondence between two frames\\rfor accurate pose estimation -- as perfect per-pixel correspondence between two\\rimages is difficult, if not impossible, to establish. With the carefully\\restimated camera pose and predicted per-pixel optical flow correspondences, a\\rdense depth of the scene is computed. Later, an iterative refinement procedure\\ris introduced to further improve optical flow matching confidence, camera pose,\\rand depth, exploiting their inherent dependency in rigid SfM. The fundamental\\ridea presented is to benefit from per-pixel uncertainty in the optical flow\\restimation and provide robustness to the dense SfM system via an online\\rrefinement. Concretely, we introduce a pipeline consisting of (i) an\\runcertainty-aware dense optical flow estimation approach that provides\\rper-pixel correspondence with their confidence score of matching; (ii) a\\rweighted dense bundle adjustment formulation that depends on optical flow\\runcertainty and bidirectional optical flow consistency to refine both pose and\\rdepth; (iii) a depth estimation network that considers its consistency with the\\restimated poses and optical flow respecting epipolar constraint. Extensive\\rexperiments show that the proposed approach achieves remarkable depth accuracy\\rand state-of-the-art camera pose results superseding SuperPoint and SuperGlue\\raccuracy when tested on benchmark datasets such as DeMoN, YFCC100M, and\\rScanNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00523 ,  11256kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00545\\rDate: Wed, 1 Feb 2023 16:08:58 GMT   (2497kb,D)\\r\\rTitle: An Out-of-Domain Synapse Detection Challenge for Microwasp Brain\\r  Connectomes\\rAuthors: Jingpeng Wu, Yicong Li, Nishika Gupta, Kazunori Shinomiya, Pat Gunn,\\r  Alexey Polilov, Hanspeter Pfister, Dmitri Chklovskii, Donglai Wei\\rCategories: cs.CV q-bio.NC\\r\\\\\\\\\\r  The size of image stacks in connectomics studies now reaches the terabyte and\\roften petabyte scales with a great diversity of appearance across brain regions\\rand samples. However, manual annotation of neural structures, e.g., synapses,\\ris time-consuming, which leads to limited training data often smaller than\\r0.001\\\\% of the test data in size. Domain adaptation and generalization\\rapproaches were proposed to address similar issues for natural images, which\\rwere less evaluated on connectomics data due to a lack of out-of-domain\\rbenchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00545 ,  2497kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00556\\rDate: Wed, 1 Feb 2023 16:23:21 GMT   (6109kb,D)\\r\\rTitle: Correspondence-free online human motion retargeting\\rAuthors: Mathieu Marsot, Rim Rekik, Stefanie Wuhrer, Jean-S\\\\'ebastien Franco\\r  and Anne-H\\\\'el\\\\`ene Olivier\\rCategories: cs.CV\\r\\\\\\\\\\r  We present a novel data-driven framework for unsupervised human motion\\rretargeting which animates a target body shape with a source motion. This\\rallows to retarget motions between different characters by animating a target\\rsubject with a motion of a source subject. Our method is\\rcorrespondence-free,~\\\\ie neither spatial correspondences between the source and\\rtarget shapes nor temporal correspondences between different frames of the\\rsource motion are required. Our proposed method directly animates a target\\rshape with arbitrary sequences of humans in motion, possibly captured using 4D\\racquisition platforms or consumer devices. Our framework takes into account\\rlong-term temporal context of $1$ second during retargeting while accounting\\rfor surface details. To achieve this, we take inspiration from two lines of\\rexisting work: skeletal motion retargeting, which leverages long-term temporal\\rcontext at the cost of surface detail, and surface-based retargeting, which\\rpreserves surface details without considering long-term temporal context. We\\runify the advantages of these works by combining a learnt skinning field with a\\rskeletal retargeting approach. During inference, our method runs online,~\\\\ie\\rthe input can be processed in a serial way, and retargeting is performed in a\\rsingle forward pass per frame. Experiments show that including long-term\\rtemporal context during training improves the method's accuracy both in terms\\rof the retargeted skeletal motion and the detail preservation. Furthermore, our\\rmethod generalizes well on unobserved motions and body shapes. We demonstrate\\rthat the proposed framework achieves state-of-the-art results on two test\\rdatasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00556 ,  6109kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00573\\rDate: Wed, 1 Feb 2023 16:46:46 GMT   (8473kb,D)\\r\\rTitle: An automated, geometry-based method for the analysis of hippocampal\\r  thickness\\rAuthors: Kersten Diers and Hannah Baumeister and Frank Jessen and Emrah D\\\\uzel\\r  and David Berron and Martin Reuter\\rCategories: cs.CV cs.CG\\r\\\\\\\\\\r  The hippocampus is one of the most studied neuroanatomical structures due to\\rits involvement in attention, learning, and memory as well as its atrophy in\\rageing, neurological, and psychiatric diseases. Hippocampal shape changes,\\rhowever, are complex and cannot be fully characterized by a single summary\\rmetric such as hippocampal volume as determined from MR images. In this work,\\rwe propose an automated, geometry-based approach for the unfolding, point-wise\\rcorrespondence, and local analysis of hippocampal shape features such as\\rthickness and curvature. Starting from an automated segmentation of hippocampal\\rsubfields, we create a 3D tetrahedral mesh model as well as a 3D intrinsic\\rcoordinate system of the hippocampal body. From this coordinate system, we\\rderive local curvature and thickness estimates as well as a 2D sheet for\\rhippocampal unfolding. We evaluate the performance of our algorithm with a\\rseries of experiments to quantify neurodegenerative changes in Mild Cognitive\\rImpairment and Alzheimer's disease dementia. We find that hippocampal thickness\\restimates detect known differences between clinical groups and can determine\\rthe location of these effects on the hippocampal sheet. Further, thickness\\restimates improve classification of clinical groups and cognitively unimpaired\\rcontrols when added as an additional predictor. Comparable results are obtained\\rwith different datasets and segmentation algorithms. Taken together, we\\rreplicate canonical findings on hippocampal volume/shape changes in dementia,\\rextend them by gaining insight into their spatial localization on the\\rhippocampal sheet, and provide additional, complementary information beyond\\rtraditional measures. We provide a new set of sensitive processing and analysis\\rtools for the analysis of hippocampal geometry that allows comparisons across\\rstudies without relying on image registration or requiring manual intervention.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00573 ,  8473kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00592\\rDate: Wed, 28 Dec 2022 09:58:04 GMT   (768kb,D)\\r\\rTitle: Comparative Study of Parameter Selection for Enhanced Edge Inference for\\r  a Multi-Output Regression model for Head Pose Estimation\\rAuthors: Asiri Lindamulage, Nuwan Kodagoda, Shyam Reyal, Pradeepa Samarasinghe\\r  and Pratheepan Yogarajah\\rCategories: cs.CV cs.AI\\rComments: Conference:- in TENCON 2022 - 2022 IEEE Region 10 Conference (TENCON)\\rJournal-ref: TENCON 2022 - 2022 IEEE Region 10 Conference (TENCON), Nov. 2022\\rDOI: 10.1109/TENCON55691.2022.9977637\\r\\\\\\\\\\r  Magnitude-based pruning is a technique used to optimise deep learning models\\rfor edge inference. We have achieved over 75% model size reduction with a\\rhigher accuracy than the original multi-output regression model for head-pose\\restimation.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00592 ,  768kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00594\\rDate: Thu, 29 Dec 2022 23:34:19 GMT   (862kb)\\r\\rTitle: Inching Towards Automated Understanding of the Meaning of Art: An\\r  Application to Computational Analysis of Mondrian's Artwork\\rAuthors: Alex Doboli, Mahan Agha Zahedi, Niloofar Gholamrezaei\\rCategories: cs.CV cs.AI\\rComments: 40 pages, 5 figures\\r\\\\\\\\\\r  Deep Neural Networks (DNNs) have been successfully used in classifying\\rdigital images but have been less successful in classifying images with\\rmeanings that are not linear combinations of their visualized features, like\\rimages of artwork. Moreover, it is unknown what additional features must be\\rincluded into DNNs, so that they can possibly classify using features beyond\\rvisually displayed features, like color, size, and form. Non-displayed features\\rare important in abstract representations, reasoning, and understanding\\rambiguous expressions, which are arguably topics less studied by current AI\\rmethods. This paper attempts to identify capabilities that are related to\\rsemantic processing, a current limitation of DNNs. The proposed methodology\\ridentifies the missing capabilities by comparing the process of understanding\\rMondrian's paintings with the process of understanding electronic circuit\\rdesigns, another creative problem solving instance. The compared entities are\\rcognitive architectures that attempt to loosely mimic cognitive activities. The\\rpaper offers a detailed presentation of the characteristics of the\\rarchitectural components, like goals, concepts, ideas, rules, procedures,\\rbeliefs, expectations, and outcomes. To explain the usefulness of the\\rmethodology, the paper discusses a new, three-step computational method to\\rdistinguish Mondrian's paintings from other artwork. The method includes in a\\rbackward order the cognitive architecture's components that operate only with\\rthe characteristics of the available data.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00594 ,  862kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00595\\rDate: Fri, 30 Dec 2022 05:34:54 GMT   (11294kb,D)\\r\\rTitle: Stroke-based Rendering: From Heuristics to Deep Learning\\rAuthors: Florian Nolte, Andrew Melnik, Helge Ritter\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  In the last few years, artistic image-making with deep learning models has\\rgained a considerable amount of traction. A large number of these models\\roperate directly in the pixel space and generate raster images. This is however\\rnot how most humans would produce artworks, for example, by planning a sequence\\rof shapes and strokes to draw. Recent developments in deep learning methods\\rhelp to bridge the gap between stroke-based paintings and pixel photo\\rgeneration. With this survey, we aim to provide a structured introduction and\\runderstanding of common challenges and approaches in stroke-based rendering\\ralgorithms. These algorithms range from simple rule-based heuristics to stroke\\roptimization and deep reinforcement agents, trained to paint images with\\rdifferentiable vector graphics and neural rendering.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00595 ,  11294kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00624\\rDate: Wed, 1 Feb 2023 17:44:17 GMT   (1247kb,D)\\r\\rTitle: Transforming CLIP to an Open-vocabulary Video Model via Interpolated\\r  Weight Optimization\\rAuthors: Zejia Weng, Xitong Yang, Ang Li, Zuxuan Wu, Yu-Gang Jiang\\rCategories: cs.CV\\r\\\\\\\\\\r  Contrastive Language-Image Pretraining (CLIP) has demonstrated impressive\\rzero-shot learning abilities for image understanding, yet limited effort has\\rbeen made to investigate CLIP for zero-shot video recognition. We introduce\\rOpen-VCLIP, a simple yet effective approach that transforms CLIP into strong\\rzero-shot video classifiers that can recognize unseen actions and events at\\rtest time. Our framework extends CLIP with minimal modifications to model\\rspatial-temporal relationships in videos, making it a specialized video\\rclassifier, while striving for generalization. We formally show that training\\ran Open-VCLIP is equivalent to continual learning with zero historical data. To\\raddress this problem, we propose Interpolated Weight Optimization, which\\rutilizes the benefit of weight interpolation in both training and test time. We\\revaluate our method on three popular and challenging action recognition\\rdatasets following various zero-shot evaluation protocols and we demonstrate\\rour approach outperforms state-of-the-art methods by clear margins. In\\rparticular, we achieve 87.9%, 58.3%, 81.1% zero-shot accuracy on UCF, HMDB and\\rKinetics-600 respectively, outperforming state-of-the-art methods by 8.3%, 7.8%\\rand 12.2%.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00624 ,  1247kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00626\\rDate: Wed, 1 Feb 2023 17:46:00 GMT   (2845kb,D)\\r\\rTitle: Continuous U-Net: Faster, Greater and Noiseless\\rAuthors: Chun-Wun Cheng, Christina Runkel, Lihao Liu, Raymond H Chan,\\r  Carola-Bibiane Sch\\\\onlieb, Angelica I Aviles-Rivero\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  Image segmentation is a fundamental task in image analysis and clinical\\rpractice. The current state-of-the-art techniques are based on U-shape type\\rencoder-decoder networks with skip connections, called U-Net. Despite the\\rpowerful performance reported by existing U-Net type networks, they suffer from\\rseveral major limitations. Issues include the hard coding of the receptive\\rfield size, compromising the performance and computational cost, as well as the\\rfact that they do not account for inherent noise in the data. They have\\rproblems associated with discrete layers, and do not offer any theoretical\\runderpinning. In this work we introduce continuous U-Net, a novel family of\\rnetworks for image segmentation. Firstly, continuous U-Net is a continuous deep\\rneural network that introduces new dynamic blocks modelled by second order\\rordinary differential equations. Secondly, we provide theoretical guarantees\\rfor our network demonstrating faster convergence, higher robustness and less\\rsensitivity to noise. Thirdly, we derive qualitative measures to tailor-made\\rsegmentation tasks. We demonstrate, through extensive numerical and visual\\rresults, that our model outperforms existing U-Net blocks for several medical\\rimage segmentation benchmarking datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00626 ,  2845kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00648\\rDate: Wed, 1 Feb 2023 18:22:23 GMT   (550kb)\\r\\rTitle: Image-Based Vehicle Classification by Synergizing Features from\\r  Supervised and Self-Supervised Learning Paradigms\\rAuthors: Shihan Ma and Jidong J. Yang\\rCategories: cs.CV eess.IV\\rComments: 15 pages, 7 figures, 7 tables\\rJournal-ref: Eng. 2023; 4(1):444-456\\rDOI: 10.3390/eng4010027\\r\\\\\\\\\\r  This paper introduces a novel approach to leverage features learned from both\\rsupervised and self-supervised paradigms, to improve image classification\\rtasks, specifically for vehicle classification. Two state-of-the-art\\rself-supervised learning methods, DINO and data2vec, were evaluated and\\rcompared for their representation learning of vehicle images. The former\\rcontrasts local and global views while the latter uses masked prediction on\\rmulti-layered representations. In the latter case, supervised learning is\\remployed to finetune a pretrained YOLOR object detector for detecting vehicle\\rwheels, from which definitive wheel positional features are retrieved. The\\rrepresentations learned from these self-supervised learning methods were\\rcombined with the wheel positional features for the vehicle classification\\rtask. Particularly, a random wheel masking strategy was utilized to finetune\\rthe previously learned representations in harmony with the wheel positional\\rfeatures during the training of the classifier. Our experiments show that the\\rdata2vec-distilled representations, which are consistent with our wheel masking\\rstrategy, outperformed the DINO counterpart, resulting in a celebrated Top-1\\rclassification accuracy of 97.2% for classifying the 13 vehicle classes defined\\rby the Federal Highway Administration.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00648 ,  550kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00673\\rDate: Wed, 1 Feb 2023 18:59:19 GMT   (2681kb,D)\\r\\rTitle: ADAPT: Action-aware Driving Caption Transformer\\rAuthors: Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang,\\r  Yuhang Zheng, Guyue Zhou and Jingjing Liu\\rCategories: cs.CV cs.HC\\rComments: Accepted to ICRA2023. Code: https://github.com/jxbbb/ADAPT\\r\\\\\\\\\\r  End-to-end autonomous driving has great potential in the transportation\\rindustry. However, the lack of transparency and interpretability of the\\rautomatic decision-making process hinders its industrial adoption in practice.\\rThere have been some early attempts to use attention maps or cost volume for\\rbetter model explainability which is difficult for ordinary passengers to\\runderstand. To bridge the gap, we propose an end-to-end transformer-based\\rarchitecture, ADAPT (Action-aware Driving cAPtion Transformer), which provides\\ruser-friendly natural language narrations and reasoning for each decision\\rmaking step of autonomous vehicular control and action. ADAPT jointly trains\\rboth the driving caption task and the vehicular control prediction task,\\rthrough a shared video representation. Experiments on BDD-X (Berkeley DeepDrive\\reXplanation) dataset demonstrate state-of-the-art performance of the ADAPT\\rframework on both automatic metrics and human evaluation. To illustrate the\\rfeasibility of the proposed framework in real-world applications, we build a\\rnovel deployable system that takes raw car videos as input and outputs the\\raction narrations and reasoning in real time. The code, models and data are\\ravailable at https://github.com/jxbbb/ADAPT.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00673 ,  2681kb)\\r%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00070 (*cross-listing*)\\rDate: Tue, 31 Jan 2023 20:09:33 GMT   (10298kb,D)\\r\\rTitle: Debiasing Vision-Language Models via Biased Prompts\\rAuthors: Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba,\\r  Stefanie Jegelka\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Machine learning models have been shown to inherit biases from their training\\rdatasets, which can be particularly problematic for vision-language foundation\\rmodels trained on uncurated datasets scraped from the internet. The biases can\\rbe amplified and propagated to downstream applications like zero-shot\\rclassifiers and text-to-image generative models. In this study, we propose a\\rgeneral approach for debiasing vision-language foundation models by projecting\\rout biased directions in the text embedding. In particular, we show that\\rdebiasing only the text embedding with a calibrated projection matrix suffices\\rto yield robust classifiers and fair generative models. The closed-form\\rsolution enables easy integration into large-scale pipelines, and empirical\\rresults demonstrate that our approach effectively reduces social bias and\\rspurious correlation in both discriminative and generative vision-language\\rmodels without the need for additional data or training.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00070 ,  10298kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00192 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 02:53:34 GMT   (286kb,D)\\r\\rTitle: Density peak clustering using tensor network\\rAuthors: Xiao Shi, Yun Shang\\rCategories: cs.LG cs.CV quant-ph\\r\\\\\\\\\\r  Tensor networks, which have been traditionally used to simulate many-body\\rphysics, have recently gained significant attention in the field of machine\\rlearning due to their powerful representation capabilities. In this work, we\\rpropose a density-based clustering algorithm inspired by tensor networks. We\\rencode classical data into tensor network states on an extended Hilbert space\\rand train the tensor network states to capture the features of the clusters.\\rHere, we define density and related concepts in terms of fidelity, rather than\\rusing a classical distance measure. We evaluate the performance of our\\ralgorithm on six synthetic data sets, four real world data sets, and three\\rcommonly used computer vision data sets. The results demonstrate that our\\rmethod provides state-of-the-art performance on several synthetic data sets and\\rreal world data sets, even when the number of clusters is unknown.\\rAdditionally, our algorithm performs competitively with state-of-the-art\\ralgorithms on the MNIST, USPS, and Fashion-MNIST image data sets. These\\rfindings reveal the great potential of tensor networks for machine learning\\rapplications.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00192 ,  286kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00209 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 03:25:43 GMT   (3230kb,D)\\r\\rTitle: QCRS: Improve Randomized Smoothing using Quasi-Concave Optimization\\rAuthors: Bo-Han Kung and Shang-Tse Chen\\rCategories: cs.LG cs.CV math.OC\\rComments: 10 pages\\r\\\\\\\\\\r  Randomized smoothing is currently the state-of-the-art method that provides\\rcertified robustness for deep neural networks. However, it often cannot achieve\\ran adequate certified region on real-world datasets. One way to obtain a larger\\rcertified region is to use an input-specific algorithm instead of using a fixed\\rGaussian filter for all data points. Several methods based on this idea have\\rbeen proposed, but they either suffer from high computational costs or gain\\rmarginal improvement in certified radius. In this work, we show that by\\rexploiting the quasiconvex problem structure, we can find the optimal certified\\rradii for most data points with slight computational overhead. This observation\\rleads to an efficient and effective input-specific randomized smoothing\\ralgorithm. We conduct extensive experiments and empirical analysis on Cifar10\\rand ImageNet. The results show that the proposed method significantly enhances\\rthe certified radii with low computational overhead.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00209 ,  3230kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00232 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 04:22:59 GMT   (1527kb,D)\\r\\rTitle: SPIDE: A Purely Spike-based Method for Training Feedback Spiking Neural\\r  Networks\\rAuthors: Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Yisen Wang, Zhouchen Lin\\rCategories: cs.NE cs.CV cs.LG\\rComments: Accepted by Neural Networks\\rDOI: 10.1016/j.neunet.2023.01.026\\r\\\\\\\\\\r  Spiking neural networks (SNNs) with event-based computation are promising\\rbrain-inspired models for energy-efficient applications on neuromorphic\\rhardware. However, most supervised SNN training methods, such as conversion\\rfrom artificial neural networks or direct training with surrogate gradients,\\rrequire complex computation rather than spike-based operations of spiking\\rneurons during training. In this paper, we study spike-based implicit\\rdifferentiation on the equilibrium state (SPIDE) that extends the recently\\rproposed training method, implicit differentiation on the equilibrium state\\r(IDE), for supervised learning with purely spike-based computation, which\\rdemonstrates the potential for energy-efficient training of SNNs. Specifically,\\rwe introduce ternary spiking neuron couples and prove that implicit\\rdifferentiation can be solved by spikes based on this design, so the whole\\rtraining procedure, including both forward and backward passes, is made as\\revent-driven spike computation, and weights are updated locally with two-stage\\raverage firing rates. Then we propose to modify the reset membrane potential to\\rreduce the approximation error of spikes. With these key components, we can\\rtrain SNNs with flexible structures in a small number of time steps and with\\rfiring sparsity during training, and the theoretical estimation of energy costs\\rdemonstrates the potential for high efficiency. Meanwhile, experiments show\\rthat even with these constraints, our trained models can still achieve\\rcompetitive results on MNIST, CIFAR-10, CIFAR-100, and CIFAR10-DVS. Our code is\\ravailable at https://github.com/pkuxmq/SPIDE-FSNN.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00232 ,  1527kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00291 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 07:45:54 GMT   (38299kb,D)\\r\\rTitle: Development of Real-time Rendering Technology for High-Precision Models\\r  in Autonomous Driving\\rAuthors: Zhang Whencheng and Wang Chengyi\\rCategories: cs.RO cs.CV\\rComments: 3 pages, 6 figures\\r\\\\\\\\\\r  Our autonomous driving simulation lab produces a high-precision 3D model\\rsimulating the parking lot. However, the current model still has poor rendering\\rquality in some aspects. In this work, we develop a system to improve the\\rrendering of the model and evaluate the quality of the rendered model.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00291 ,  38299kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00332 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 09:29:20 GMT   (15733kb,D)\\r\\rTitle: iPAL: A Machine Learning Based Smart Healthcare Framework For Automatic\\r  Diagnosis Of Attention Deficit/Hyperactivity Disorder (ADHD)\\rAuthors: Abhishek Sharma, Arpit Jain, Shubhangi Sharma, Ashutosh Gupta, Prateek\\r  Jain, Saraju P. Mohanty\\rCategories: cs.AI cs.CV\\r\\\\\\\\\\r  ADHD is a prevalent disorder among the younger population. Standard\\revaluation techniques currently use evaluation forms, interviews with the\\rpatient, and more. However, its symptoms are similar to those of many other\\rdisorders like depression, conduct disorder, and oppositional defiant disorder,\\rand these current diagnosis techniques are not very effective. Thus, a\\rsophisticated computing model holds the potential to provide a promising\\rdiagnosis solution to this problem. This work attempts to explore methods to\\rdiagnose ADHD using combinations of multiple established machine learning\\rtechniques like neural networks and SVM models on the ADHD200 dataset and\\rexplore the field of neuroscience. In this work, multiclass classification is\\rperformed on phenotypic data using an SVM model. The better results have been\\ranalyzed on the phenotypic data compared to other supervised learning\\rtechniques like Logistic regression, KNN, AdaBoost, etc. In addition, neural\\rnetworks have been implemented on functional connectivity from the MRI data of\\ra sample of 40 subjects provided to achieve high accuracy without prior\\rknowledge of neuroscience. It is combined with the phenotypic classifier using\\rthe ensemble technique to get a binary classifier. It is further trained and\\rtested on 400 out of 824 subjects from the ADHD200 data set and achieved an\\raccuracy of 92.5% for binary classification The training and testing accuracy\\rhas been achieved upto 99% using ensemble classifier.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00332 ,  15733kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00353 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 10:24:55 GMT   (973kb,D)\\r\\rTitle: Towards Label-Efficient Incremental Learning: A Survey\\rAuthors: Mert Kilickaya, Joost van de Weijer and Yuki M. Asano\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  The current dominant paradigm when building a machine learning model is to\\riterate over a dataset over and over until convergence. Such an approach is\\rnon-incremental, as it assumes access to all images of all categories at once.\\rHowever, for many applications, non-incremental learning is unrealistic. To\\rthat end, researchers study incremental learning, where a learner is required\\rto adapt to an incoming stream of data with a varying distribution while\\rpreventing forgetting of past knowledge. Significant progress has been made,\\rhowever, the vast majority of works focus on the fully supervised setting,\\rmaking these algorithms label-hungry thus limiting their real-life deployment.\\rTo that end, in this paper, we make the first attempt to survey recently\\rgrowing interest in label-efficient incremental learning. We identify three\\rsubdivisions, namely semi-, few-shot- and self-supervised learning to reduce\\rlabeling efforts. Finally, we identify novel directions that can further\\renhance label-efficiency and improve incremental learning scalability. Project\\rwebsite: {https://github.com/kilickaya/label-efficient-il.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00353 ,  973kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00362 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 10:40:05 GMT   (5322kb,D)\\r\\rTitle: A Flexible Framework for Virtual Omnidirectional Vision to Improve\\r  Operator Situation Awareness\\rAuthors: Martin Oehler and Oskar von Stryk\\rCategories: cs.RO cs.CV cs.HC\\rComments: Accepted to European Conference on Mobile Robots (ECMR) 2021. Video\\r  link: https://youtu.be/7pocpdsMxOM Project page:\\r  https://tu-darmstadt-ros-pkg.github.io/omnidirectional_vision\\rJournal-ref: European Conference on Mobile Robots (ECMR) 2021\\rDOI: 10.1109/ECMR50962.2021.9568840\\r\\\\\\\\\\r  During teleoperation of a mobile robot, providing good operator situation\\rawareness is a major concern as a single mistake can lead to mission failure.\\rCamera streams are widely used for teleoperation but offer limited\\rfield-of-view. In this paper, we present a flexible framework for virtual\\rprojections to increase situation awareness based on a novel method to fuse\\rmultiple cameras mounted anywhere on the robot. Moreover, we propose a\\rcomplementary approach to improve scene understanding by fusing camera images\\rand geometric 3D Lidar data to obtain a colorized point cloud. The\\rimplementation on a compact omnidirectional camera reduces system complexity\\rconsiderably and solves multiple use-cases on a much smaller footprint compared\\rto traditional approaches such as actuated pan-tilt units. Finally, we\\rdemonstrate the generality of the approach by application to the multi-camera\\rsystem of the Boston Dynamics Spot. The software implementation is available as\\ropen-source ROS packages on the project page\\rhttps://tu-darmstadt-ros-pkg.github.io/omnidirectional_vision.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00362 ,  5322kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00487 (*cross-listing*)\\rDate: Tue, 31 Jan 2023 11:34:56 GMT   (4312kb,D)\\r\\rTitle: A Comprehensive Survey of Continual Learning: Theory, Method and\\r  Application\\rAuthors: Liyuan Wang, Xingxing Zhang, Hang Su, Jun Zhu\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\\\r  To cope with real-world dynamics, an intelligent agent needs to incrementally\\racquire, update, accumulate, and exploit knowledge throughout its lifetime.\\rThis ability, known as continual learning, provides a foundation for AI systems\\rto develop themselves adaptively. In a general sense, continual learning is\\rexplicitly limited by catastrophic forgetting, where learning a new task\\rusually results in a dramatic performance drop of the old tasks. Beyond this,\\rincreasingly numerous advances have emerged in recent years that largely extend\\rthe understanding and application of continual learning. The growing and\\rwidespread interest in this direction demonstrates its realistic significance\\ras well as complexity. In this work, we present a comprehensive survey of\\rcontinual learning, seeking to bridge the basic settings, theoretical\\rfoundations, representative methods, and practical applications. Based on\\rexisting theoretical and empirical results, we summarize the general objectives\\rof continual learning as ensuring a proper stability-plasticity trade-off and\\ran adequate intra/inter-task generalizability in the context of resource\\refficiency. Then we provide a state-of-the-art and elaborated taxonomy,\\rextensively analyzing how representative strategies address continual learning,\\rand how they are adapted to particular challenges in various applications.\\rThrough an in-depth discussion of continual learning in terms of the current\\rtrends, cross-directional prospects and interdisciplinary connections with\\rneuroscience, we believe that such a holistic perspective can greatly\\rfacilitate subsequent exploration in this field and beyond.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00487 ,  4312kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00509 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 15:28:55 GMT   (1410kb,D)\\r\\rTitle: Exploring Semantic Perturbations on Grover\\rAuthors: Pranav Kulkarni, Ziqing Ji, Yan Xu, Marko Neskovic, Kevin Nolan\\rCategories: cs.LG cs.CL cs.CV\\rComments: 15 pages, 12 figures, 1 table, capstone research in machine learning\\r\\\\\\\\\\r  With news and information being as easy to access as they currently are, it\\ris more important than ever to ensure that people are not mislead by what they\\rread. Recently, the rise of neural fake news (AI-generated fake news) and its\\rdemonstrated effectiveness at fooling humans has prompted the development of\\rmodels to detect it. One such model is the Grover model, which can both detect\\rneural fake news to prevent it, and generate it to demonstrate how a model\\rcould be misused to fool human readers. In this work we explore the Grover\\rmodel's fake news detection capabilities by performing targeted attacks through\\rperturbations on input news articles. Through this we test Grover's resilience\\rto these adversarial attacks and expose some potential vulnerabilities which\\rshould be addressed in further iterations to ensure it can detect all types of\\rfake news accurately.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00509 ,  1410kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00514 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 15:34:24 GMT   (4080kb,D)\\r\\rTitle: Towards Implementing Energy-aware Data-driven Intelligence for Smart\\r  Health Applications on Mobile Platforms\\rAuthors: G. Dumindu Samaraweera, Hung Nguyen, Hadi Zanddizari, Behnam Zeinali,\\r  and J. Morris Chang\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Recent breakthrough technological progressions of powerful mobile computing\\rresources such as low-cost mobile GPUs along with cutting-edge, open-source\\rsoftware architectures have enabled high-performance deep learning on mobile\\rplatforms. These advancements have revolutionized the capabilities of today's\\rmobile applications in different dimensions to perform data-driven intelligence\\rlocally, particularly for smart health applications. Unlike traditional machine\\rlearning (ML) architectures, modern on-device deep learning frameworks are\\rproficient in utilizing computing resources in mobile platforms seamlessly, in\\rterms of producing highly accurate results in less inference time. However, on\\rthe flip side, energy resources in a mobile device are typically limited.\\rHence, whenever a complex Deep Neural Network (DNN) architecture is fed into\\rthe on-device deep learning framework, while it achieves high prediction\\raccuracy (and performance), it also urges huge energy demands during the\\rruntime. Therefore, managing these resources efficiently within the spectrum of\\rperformance and energy efficiency is the newest challenge for any mobile\\rapplication featuring data-driven intelligence beyond experimental evaluations.\\rIn this paper, first, we provide a timely review of recent advancements in\\ron-device deep learning while empirically evaluating the performance metrics of\\rcurrent state-of-the-art ML architectures and conventional ML approaches with\\rthe emphasis given on energy characteristics by deploying them on a smart\\rhealth application. With that, we are introducing a new framework through an\\renergy-aware, adaptive model comprehension and realization (EAMCR) approach\\rthat can be utilized to make more robust and efficient inference decisions\\rbased on the available computing/energy resources in the mobile device during\\rthe runtime.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00514 ,  4080kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00528 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 15:55:34 GMT   (7931kb,D)\\r\\rTitle: A latent space for unsupervised MR image quality control via artifact\\r  assessment\\rAuthors: Lianrui Zuo, Yuan Xue, Blake E. Dewey, Yihao Liu, Jerry L. Prince,\\r  Aaron Carass\\rCategories: eess.IV cs.CV\\rComments: Accepted at the International Society for Optics and Photonics -\\r  Medical Imaging (SPIE-MI) 2023\\r\\\\\\\\\\r  Image quality control (IQC) can be used in automated magnetic resonance (MR)\\rimage analysis to exclude erroneous results caused by poorly acquired or\\rartifact-laden images. Existing IQC methods for MR imaging generally require\\rhuman effort to craft meaningful features or label large datasets for\\rsupervised training. The involvement of human labor can be burdensome and\\rbiased, as labeling MR images based on their quality is a subjective task. In\\rthis paper, we propose an automatic IQC method that evaluates the extent of\\rartifacts in MR images without supervision. In particular, we design an\\rartifact encoding network that learns representations of artifacts based on\\rcontrastive learning. We then use a normalizing flow to estimate the density of\\rlearned representations for unsupervised classification. Our experiments on\\rlarge-scale multi-cohort MR datasets show that the proposed method accurately\\rdetects images with high levels of artifacts, which can inform downstream\\ranalysis tasks about potentially flawed data.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00528 ,  7931kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00596 (*cross-listing*)\\rDate: Fri, 30 Dec 2022 17:07:26 GMT   (33059kb,D)\\r\\rTitle: Accelerated and Improved Stabilization for High Order Moments of Racah\\r  Polynomials\\rAuthors: Basheera M. Mahmmod and Sadiq H. Abdulhussain and Tom\\\\'a\\\\v{s} Suk\\rCategories: math.NA cs.CV cs.NA eess.IV\\r\\\\\\\\\\r  One of the most effective orthogonal moments, discrete Racah polynomials\\r(DRPs) and their moments are used in many disciplines of sciences, including\\rimage processing, and computer vision. Moments are the projections of a signal\\ron the polynomial basis functions. Racah polynomials were introduced by Wilson\\rand modified by Zhu for image processing and they are orthogonal on a discrete\\rset of samples. However, when the moment order is high, they experience the\\rissue of numerical instability. In this paper, we propose a new algorithm for\\rthe computation of DRPs coefficients called Improved Stabilization (ImSt). In\\rthe proposed algorithm, {the DRP plane is partitioned into four parts, which\\rare asymmetric because they rely on the values of the polynomial size and the\\rDRP parameters.} The logarithmic gamma function is utilized to compute the\\rinitial values, which empower the computation of the initial value for a wide\\rrange of DRP parameter values as well as large size of the polynomials. In\\raddition, a new formula is used to compute the values of the initial sets based\\ron the initial value. Moreover, we optimized the use of the stabilizing\\rcondition in specific parts of the algorithm. ImSt works for wider range of\\rparameters until higher degree than the current algorithms. We compare it with\\rthe other methods in a number of experiments.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00596 ,  33059kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00633 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 17:52:40 GMT   (7754kb,D)\\r\\rTitle: Deep Dependency Networks for Multi-Label Classification\\rAuthors: Shivvrat Arya, Yu Xiang and Vibhav Gogate\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  We propose a simple approach which combines the strengths of probabilistic\\rgraphical models and deep learning architectures for solving the multi-label\\rclassification task, focusing specifically on image and video data. First, we\\rshow that the performance of previous approaches that combine Markov Random\\rFields with neural networks can be modestly improved by leveraging more\\rpowerful methods such as iterative join graph propagation, integer linear\\rprogramming, and $\\\\ell_1$ regularization-based structure learning. Then we\\rpropose a new modeling framework called deep dependency networks, which\\raugments a dependency network, a model that is easy to train and learns more\\raccurate dependencies but is limited to Gibbs sampling for inference, to the\\routput layer of a neural network. We show that despite its simplicity, jointly\\rlearning this new architecture yields significant improvements in performance\\rover the baseline neural network. In particular, our experimental evaluation on\\rthree video activity classification datasets: Charades, Textually Annotated\\rCooking Scenes (TACoS), and Wetlab, and three multi-label image classification\\rdatasets: MS-COCO, PASCAL VOC, and NUS-WIDE show that deep dependency networks\\rare almost always superior to pure neural architectures that do not use\\rdependency networks.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00633 ,  7754kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00669 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 18:56:09 GMT   (8751kb,D)\\r\\rTitle: Detecting Histologic Glioblastoma Regions of Prognostic Relevance\\rAuthors: Bhakti Baheti, Shubham Innani, Garv Mehdiratta, MacLean P. Nasrallah,\\r  and Spyridon Bakas\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Glioblastoma is the most common and aggressive malignant adult tumor of the\\rcentral nervous system, with grim prognosis and heterogeneous morphologic and\\rmolecular profiles. Since the adoption of the current standard of care\\rtreatment, 18 years ago, there are no substantial prognostic improvements\\rnoticed. Accurate prediction of patient overall survival (OS) from clinical\\rhistopathology whole slide images (WSI) using advanced computational methods\\rcould contribute to optimization of clinical decision making and patient\\rmanagement. Here, we focus on identifying prognostically relevant glioblastoma\\rmorphologic patterns on H&E stained WSI. The exact approach capitalizes on the\\rcomprehensive WSI curation of apparent artifactual content and on an\\rinterpretability mechanism via a weakly supervised attention based multiple\\rinstance learning algorithm that further utilizes clustering to constrain the\\rsearch space. The automatically identified patterns of high diagnostic value\\rare used to classify the WSI as representative of a short or a long survivor.\\rIdentifying tumor morphologic patterns associated with short and long OS will\\rallow the clinical neuropathologist to provide additional prognostic\\rinformation gleaned during microscopic assessment to the treating team, as well\\ras suggest avenues of biological investigation for understanding and\\rpotentially treating glioblastoma.\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00669 ,  8751kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.00670 (*cross-listing*)\\rDate: Wed, 1 Feb 2023 18:57:01 GMT   (37071kb,D)\\r\\rTitle: Stable Target Field for Reduced Variance Score Estimation in Diffusion\\r  Models\\rAuthors: Yilun Xu, Shangyuan Tong, Tommi Jaakkola\\rCategories: cs.LG cs.CV\\rComments: Accepted by ICLR 2023. Code available at:\\r  https://github.com/Newbeeer/stf\\r\\\\\\\\\\r  Diffusion models generate samples by reversing a fixed forward diffusion\\rprocess. Despite already providing impressive empirical results, these\\rdiffusion models algorithms can be further improved by reducing the variance of\\rthe training targets in their denoising score-matching objective. We argue that\\rthe source of such variance lies in the handling of intermediate noise-variance\\rscales, where multiple modes in the data affect the direction of reverse paths.\\rWe propose to remedy the problem by incorporating a reference batch which we\\ruse to calculate weighted conditional scores as more stable training targets.\\rWe show that the procedure indeed helps in the challenging intermediate regime\\rby reducing (the trace of) the covariance of training targets. The new stable\\rtargets can be seen as trading bias for reduced variance, where the bias\\rvanishes with increasing reference batch size. Empirically, we show that the\\rnew objective improves the image quality, stability, and training speed of\\rvarious popular diffusion models across datasets with both general ODE and SDE\\rsolvers. When used in combination with EDM, our method yields a current SOTA\\rFID of 1.90 with 35 network evaluations on the unconditional CIFAR-10\\rgeneration task. The code is available at https://github.com/Newbeeer/stf\\r\\\\\\\\ ( https://arxiv.org/abs/2302.00670 ,  37071kb)\\r%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2103.09151\\rreplaced with revised version Wed, 1 Feb 2023 10:12:11 GMT   (3432kb,D)\\r\\rTitle: Adversarial Driving: Attacking End-to-End Autonomous Driving\\rAuthors: Han Wu, Syed Yunas, Sareh Rowlands, Wenjie Ruan, and Johan Wahlstrom\\rCategories: cs.CV\\rComments: 7 pages, 5 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2103.09151 ,  3432kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2106.10689\\rreplaced with revised version Wed, 1 Feb 2023 06:00:21 GMT   (25750kb,D)\\r\\rTitle: NeuS: Learning Neural Implicit Surfaces by Volume Rendering for\\r  Multi-view Reconstruction\\rAuthors: Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura,\\r  Wenping Wang\\rCategories: cs.CV cs.GR\\rComments: 23 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2106.10689 ,  25750kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2108.03702\\rreplaced with revised version Wed, 1 Feb 2023 11:07:15 GMT   (22092kb,D)\\r\\rTitle: BIGRoC: Boosting Image Generation via a Robust Classifier\\rAuthors: Roy Ganz and Michael Elad\\rCategories: cs.CV\\rJournal-ref: Transactions on machine learning research, 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2108.03702 ,  22092kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2108.06663\\rreplaced with revised version Tue, 31 Jan 2023 19:47:50 GMT   (322kb,D)\\r\\rTitle: HCR-Net: A deep learning based script independent handwritten character\\r  recognition network\\rAuthors: Vinod Kumar Chauhan, Sukhdeep Singh and Anuj Sharma\\rCategories: cs.CV cs.AI cs.LG\\rComments: 23 pages (double-column), 6 figures, 16 tables (under review) --\\r  revised version\\r\\\\\\\\ ( https://arxiv.org/abs/2108.06663 ,  322kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2111.06812\\rreplaced with revised version Wed, 1 Feb 2023 13:54:51 GMT   (48343kb,D)\\r\\rTitle: Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial\\r  Imagery\\rAuthors: Hasan Nasrallah, Mustafa Shukor and Ali J. Ghandour\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2111.06812 ,  48343kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2206.00771\\rreplaced with revised version Wed, 1 Feb 2023 17:58:54 GMT   (16805kb,D)\\r\\rTitle: Dynamic Linear Transformer for 3D Biomedical Image Segmentation\\rAuthors: Zheyuan Zhang, Ulas Bagci\\rCategories: cs.CV\\rComments: 8 Pages\\r\\\\\\\\ ( https://arxiv.org/abs/2206.00771 ,  16805kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2206.08182\\rreplaced with revised version Wed, 1 Feb 2023 16:53:32 GMT   (14302kb,D)\\r\\rTitle: Nucleus Segmentation and Analysis in Breast Cancer with the MIScnn\\r  Framework\\rAuthors: Adrian Pfleiderer, Dominik M\\\\uller, Frank Kramer\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2206.08182 ,  14302kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2207.01887\\rreplaced with revised version Wed, 1 Feb 2023 10:59:03 GMT   (1807kb,D)\\r\\rTitle: Open-Vocabulary Multi-Label Classification via Multi-Modal Knowledge\\r  Transfer\\rAuthors: Sunan He, Taian Guo, Tao Dai, Ruizhi Qiao, Bo Ren, Shu-Tao Xia\\rCategories: cs.CV\\rComments: AAAI 2023 (Oral presentation paper). Updated version\\r\\\\\\\\ ( https://arxiv.org/abs/2207.01887 ,  1807kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2207.11378\\rreplaced with revised version Wed, 1 Feb 2023 12:24:57 GMT   (2124kb,D)\\r\\rTitle: Do Perceptually Aligned Gradients Imply Adversarial Robustness?\\rAuthors: Roy Ganz, Bahjat Kawar and Michael Elad\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2207.11378 ,  2124kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.12055\\rreplaced with revised version Wed, 1 Feb 2023 09:56:45 GMT   (2976kb,D)\\r\\rTitle: Combating Mode Collapse in GANs via Manifold Entropy Estimation\\rAuthors: Haozhe Liu, Bing Li, Haoqian Wu, Hanbang Liang, Yawen Huang, Yuexiang\\r  Li, Bernard Ghanem, Yefeng Zheng\\rCategories: cs.CV cs.AI\\rComments: Accepted by AAAI'2023 (Oral); Code is released at\\r  https://github.com/HaozheLiu-ST/MEE\\r\\\\\\\\ ( https://arxiv.org/abs/2208.12055 ,  2976kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2209.03139\\rreplaced with revised version Wed, 1 Feb 2023 05:44:42 GMT   (4370kb,D)\\r\\rTitle: Pixel-Level Equalized Matching for Video Object Segmentation\\rAuthors: Suhwan Cho, Woo Jin Kim, MyeongAh Cho, Seunghoon Lee, Minhyeok Lee,\\r  Chaewon Park, Sangyoun Lee\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2209.03139 ,  4370kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2209.15264\\rreplaced with revised version Wed, 1 Feb 2023 10:27:16 GMT   (32754kb,D)\\r\\rTitle: Diffusion-based Image Translation using Disentangled Style and Content\\r  Representation\\rAuthors: Gihyun Kwon, Jong Chul Ye\\rCategories: cs.CV cs.AI cs.LG stat.ML\\rComments: ICLR 2023 camera ready\\r\\\\\\\\ ( https://arxiv.org/abs/2209.15264 ,  32754kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.01112\\rreplaced with revised version Wed, 1 Feb 2023 06:12:26 GMT   (7174kb,D)\\r\\rTitle: Generative Category-Level Shape and Pose Estimation with Semantic\\r  Primitives\\rAuthors: Guanglin Li, Yifeng Li, Zhichao Ye, Qihang Zhang, Tao Kong, Zhaopeng\\r  Cui, Guofeng Zhang\\rCategories: cs.CV cs.RO\\rComments: CoRL 2022, 18 pages, 13 figures, typos corrected, references added\\r\\\\\\\\ ( https://arxiv.org/abs/2210.01112 ,  7174kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.01987\\rreplaced with revised version Tue, 31 Jan 2023 19:52:37 GMT   (4587kb,D)\\r\\rTitle: ImpressLearn: Continual Learning via Combined Task Impressions\\rAuthors: Dhrupad Bhardwaj, Julia Kempe, Artem Vysogorets, Angela M. Teng, and\\r  Evaristus C. Ezekwem\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2210.01987 ,  4587kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.02357\\rreplaced with revised version Wed, 1 Feb 2023 13:51:07 GMT   (24740kb,D)\\r\\rTitle: Image Masking for Robust Self-Supervised Monocular Depth Estimation\\rAuthors: Hemang Chawla, Kishaan Jeeveswaran, Elahe Arani, Bahram Zonooz\\rCategories: cs.CV\\rComments: Accepted at 2023 IEEE International Conference on Robotics and\\r  Automation (ICRA)\\r\\\\\\\\ ( https://arxiv.org/abs/2210.02357 ,  24740kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.05616\\rreplaced with revised version Wed, 1 Feb 2023 11:06:18 GMT   (5755kb,D)\\r\\rTitle: Neural Shape Deformation Priors\\rAuthors: Jiapeng Tang, Lev Markhasin, Bi Wang, Justus Thies, Matthias\\r  Nie{\\\\ss}ner\\rCategories: cs.CV\\rComments: NeurIPS 2022 Spotlight\\r\\\\\\\\ ( https://arxiv.org/abs/2210.05616 ,  5755kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.06642\\rreplaced with revised version Wed, 1 Feb 2023 04:41:44 GMT   (7424kb,D)\\r\\rTitle: What's in a Decade? Transforming Faces Through Time\\rAuthors: Eric Ming Chen, Jin Sun, Apoorv Khandelwal, Dani Lischinski, Noah\\r  Snavely, Hadar Averbuch-Elor\\rCategories: cs.CV cs.GR\\rComments: Project Page: https://facesthroughtime.github.io\\r\\\\\\\\ ( https://arxiv.org/abs/2210.06642 ,  7424kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.15948\\rreplaced with revised version Wed, 1 Feb 2023 13:16:22 GMT   (9758kb,D)\\r\\rTitle: Matching entropy based disparity estimation from light field\\rAuthors: Ligen Shi (1), Chang Liu (2), Di He (2), Xing Zhao (1), and Jun Qiu\\r  (2)\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2210.15948 ,  9758kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.16762\\rreplaced with revised version Wed, 1 Feb 2023 08:10:13 GMT   (42419kb,D)\\r\\rTitle: GeoUDF: Surface Reconstruction from 3D Point Clouds via Geometry-guided\\r  Distance Representation\\rAuthors: Siyu Ren, Junhui Hou, Xiaodong Chen, Ying He, Wenping Wang\\rCategories: cs.CV\\rComments: 1 correct some unclear claim 2 add the results of DOG 3 redraw some\\r  figures\\r\\\\\\\\ ( https://arxiv.org/abs/2211.16762 ,  42419kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.06384\\rreplaced with revised version Wed, 1 Feb 2023 02:57:14 GMT   (22370kb,D)\\r\\rTitle: PV3D: A 3D Generative Model for Portrait Video Generation\\rAuthors: Eric Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Wenqing Zhang, Song\\r  Bai, Jiashi Feng, Mike Zheng Shou\\rCategories: cs.CV\\rComments: Accepted to ICLR2023, Project Page https://showlab.github.io/pv3d\\r\\\\\\\\ ( https://arxiv.org/abs/2212.06384 ,  22370kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.11696\\rreplaced with revised version Wed, 1 Feb 2023 07:16:40 GMT   (1508kb,D)\\r\\rTitle: Reversible Column Networks\\rAuthors: Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiangwen Kong, Jun\\r  Li, Xiangyu Zhang\\rCategories: cs.CV\\rComments: Accepted by ICLR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2212.11696 ,  1508kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.11445\\rreplaced with revised version Wed, 1 Feb 2023 17:37:49 GMT   (26664kb,D)\\r\\rTitle: 3DShape2VecSet: A 3D Shape Representation for Neural Fields and\\r  Generative Diffusion Models\\rAuthors: Biao Zhang, Jiapeng Tang, Matthias Niessner, Peter Wonka\\rCategories: cs.CV cs.GR\\rComments: Project demo: https://youtu.be/KKQsQccpBFk\\r\\\\\\\\ ( https://arxiv.org/abs/2301.11445 ,  26664kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.13082\\rreplaced with revised version Wed, 1 Feb 2023 12:19:42 GMT   (25743kb,D)\\r\\rTitle: PaCaNet: A Study on CycleGAN with Transfer Learning for Diversifying\\r  Fused Chinese Painting and Calligraphy\\rAuthors: Zuhao Yang, Huajun Bai, Zhang Luo, Yang Xu, Wei Pang, Yue Wang,\\r  Yisheng Yuan, Yingfang Yuan\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2301.13082 ,  25743kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.13384\\rreplaced with revised version Wed, 1 Feb 2023 01:59:23 GMT   (13627kb,D)\\r\\rTitle: GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition\\rAuthors: Ekkasit Pinyoanuntapong, Ayman Ali, Kalvik Jakkala, Pu Wang, Minwoo\\r  Lee, Qucheng Peng, Chen Chen, Zhi Sun\\rCategories: cs.CV\\rComments: Submitted to ACM Transactions on Sensor Networks (TOSN)\\rACM-class: I.2; I.5\\r\\\\\\\\ ( https://arxiv.org/abs/2301.13384 ,  13627kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.13670\\rreplaced with revised version Wed, 1 Feb 2023 08:38:14 GMT   (6894kb,D)\\r\\rTitle: What Makes Good Examples for Visual In-Context Learning?\\rAuthors: Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu\\rCategories: cs.CV\\rComments: code and\\r  models:https://github.com/ZhangYuanhan-AI/visual_prompt_retrieval\\r\\\\\\\\ ( https://arxiv.org/abs/2301.13670 ,  6894kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.13721\\rreplaced with revised version Wed, 1 Feb 2023 03:44:24 GMT   (1436kb,D)\\r\\rTitle: DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models\\rAuthors: Tao Yang, Yuwang Wang, Yan Lv, Nanning Zheng\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2301.13721 ,  1436kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2109.02342 (*cross-listing*)\\rreplaced with revised version Wed, 1 Feb 2023 00:15:36 GMT   (15689kb,D)\\r\\rTitle: Automated Cardiac Resting Phase Detection Targeted on the Right Coronary\\r  Artery\\rAuthors: Seung Su Yoon, Elisabeth Preuhs, Michaela Schmidt, Christoph Forman,\\r  Teodora Chitiboi, Puneet Sharma, Juliano Lara Fernandes, Christoph Tillmanns,\\r  Jens Wetzl, Andreas Maier\\rCategories: eess.IV cs.CV physics.med-ph\\rComments: Accepted for publication at the Journal of Machine Learning for\\r  Biomedical Imaging (MELBA) https://melba-journal.org/2023:001\\rJournal-ref: Machine.Learning.for.Biomedical.Imaging. 2 (2023)\\r\\\\\\\\ ( https://arxiv.org/abs/2109.02342 ,  15689kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.08741 (*cross-listing*)\\rreplaced with revised version Tue, 31 Jan 2023 20:49:39 GMT   (1172kb)\\r\\rTitle: Improving Across-Dataset Brain Tissue Segmentation Using Transformer\\rAuthors: Vishwanatha M. Rao, Zihan Wan, Soroush Arabshahi, David J. Ma, Pin-Yu\\r  Lee, Ye Tian, Xuzhe Zhang, Andrew F. Laine, Jia Guo\\rCategories: eess.IV cs.CV\\rACM-class: I.4.6\\rDOI: 10.3389/fnimg.2022.1023481\\r\\\\\\\\ ( https://arxiv.org/abs/2201.08741 ,  1172kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.05516\\rreplaced with revised version Wed, 1 Feb 2023 03:18:41 GMT   (2224kb,D)\\r\\rTitle: Quality Not Quantity: On the Interaction between Dataset Design and\\r  Robustness of CLIP\\rAuthors: Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, Ludwig\\r  Schmidt\\rCategories: cs.LG cs.CV\\rComments: Oral paper at NeurIPS 2022\\r\\\\\\\\ ( https://arxiv.org/abs/2208.05516 ,  2224kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2209.01962\\rreplaced with revised version Wed, 1 Feb 2023 10:10:02 GMT   (5228kb,D)\\r\\rTitle: Adversarial Detection: Attacking Object Detection in Real Time\\rAuthors: Han Wu, Syed Yunas, Sareh Rowlands, Wenjie Ruan, and Johan Wahlstrom\\rCategories: cs.AI cs.CV cs.RO\\rComments: 7 pages, 9 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2209.01962 ,  5228kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2209.11820\\rreplaced with revised version Wed, 1 Feb 2023 03:25:21 GMT   (1205kb,D)\\r\\rTitle: Expanding the Deployment Envelope of Behavior Prediction via Adaptive\\r  Meta-Learning\\rAuthors: Boris Ivanovic, James Harrison, Marco Pavone\\rCategories: cs.LG cs.CV cs.RO\\rComments: 12 pages, 13 figures, 2 tables. To appear at ICRA 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2209.11820 ,  1205kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.13449\\rreplaced with revised version Tue, 31 Jan 2023 22:45:41 GMT   (9693kb,D)\\r\\rTitle: Fast Sampling of Diffusion Models via Operator Learning\\rAuthors: Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, Anima\\r  Anandkumar\\rCategories: cs.LG cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2211.13449 ,  9693kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.11494\\rreplaced with revised version Wed, 1 Feb 2023 18:56:20 GMT   (16300kb,D)\\r\\rTitle: Learning Vortex Dynamics for Fluid Inference and Prediction\\rAuthors: Yitong Deng, Hong-Xing Yu, Jiajun Wu, Bo Zhu\\rCategories: cs.LG cs.CV cs.GR\\rComments: ICLR 2023, project webpage:\\r  https://yitongdeng.github.io/vortex_learning_webpage/\\r\\\\\\\\ ( https://arxiv.org/abs/2301.11494 ,  16300kb)\\r%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%--- ------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\rSend any comments regarding submissions directly to submitter.\\r------------------------------------------------------------------------------\\rArchives at http://arxiv.org/\\rTo unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel\\r------------------------------------------------------------------------------\\r Submissions to:\\rComputer Vision and Pattern Recognition\\r received from  Tue 11 Apr 23 18:00:00 GMT  to  Wed 12 Apr 23 18:00:00 GMT\\r------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05395\\rDate: Mon, 10 Apr 2023 18:00:02 GMT   (3882kb,D)\\r\\rTitle: SE-ORNet: Self-Ensembling Orientation-aware Network for Unsupervised\\r  Point Cloud Shape Correspondence\\rAuthors: Jiacheng Deng, Chuxin Wang, Jiahao Lu, Jianfeng He, Tianzhu Zhang,\\r  Jiyang Yu, Zhe Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  Unsupervised point cloud shape correspondence aims to obtain dense\\rpoint-to-point correspondences between point clouds without manually annotated\\rpairs. However, humans and some animals have bilateral symmetry and various\\rorientations, which lead to severe mispredictions of symmetrical parts.\\rBesides, point cloud noise disrupts consistent representations for point cloud\\rand thus degrades the shape correspondence accuracy. To address the above\\rissues, we propose a Self-Ensembling ORientation-aware Network termed SE-ORNet.\\rThe key of our approach is to exploit an orientation estimation module with a\\rdomain adaptive discriminator to align the orientations of point cloud pairs,\\rwhich significantly alleviates the mispredictions of symmetrical parts.\\rAdditionally, we design a selfensembling framework for unsupervised point cloud\\rshape correspondence. In this framework, the disturbances of point cloud noise\\rare overcome by perturbing the inputs of the student and teacher networks with\\rdifferent data augmentations and constraining the consistency of predictions.\\rExtensive experiments on both human and animal datasets show that our SE-ORNet\\rcan surpass state-of-the-art unsupervised point cloud shape correspondence\\rmethods.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05395 ,  3882kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05402\\rDate: Tue, 11 Apr 2023 11:43:57 GMT   (413kb,D)\\r\\rTitle: Boosting Cross-task Transferability of Adversarial Patches with Visual\\r  Relations\\rAuthors: Tony Ma, Songze Li, Yisong Xiao, Shunchang Liu\\rCategories: cs.CV cs.CR cs.LG cs.MM\\r\\\\\\\\\\r  The transferability of adversarial examples is a crucial aspect of evaluating\\rthe robustness of deep learning systems, particularly in black-box scenarios.\\rAlthough several methods have been proposed to enhance cross-model\\rtransferability, little attention has been paid to the transferability of\\radversarial examples across different tasks. This issue has become increasingly\\rrelevant with the emergence of foundational multi-task AI systems such as\\rVisual ChatGPT, rendering the utility of adversarial samples generated by a\\rsingle task relatively limited. Furthermore, these systems often entail\\rinferential functions beyond mere recognition-like tasks. To address this gap,\\rwe propose a novel Visual Relation-based cross-task Adversarial Patch\\rgeneration method called VRAP, which aims to evaluate the robustness of various\\rvisual tasks, especially those involving visual reasoning, such as Visual\\rQuestion Answering and Image Captioning. VRAP employs scene graphs to combine\\robject recognition-based deception with predicate-based relations elimination,\\rthereby disrupting the visual reasoning information shared among inferential\\rtasks. Our extensive experiments demonstrate that VRAP significantly surpasses\\rprevious methods in terms of black-box transferability across diverse visual\\rreasoning tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05402 ,  413kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05403\\rDate: Mon, 10 Apr 2023 01:58:50 GMT   (4275kb,D)\\r\\rTitle: Isolated Sign Language Recognition based on Tree Structure Skeleton\\r  Images\\rAuthors: David Laines, Gissella Bejarano, Miguel Gonzalez-Mendoza, Gilberto\\r  Ochoa-Ruiz\\rCategories: cs.CV\\rComments: This paper has been accepted the the LatinX in Computer Vision\\r  Research Workshop at CVPR 2023\\r\\\\\\\\\\r  Sign Language Recognition (SLR) systems aim to be embedded in video stream\\rplatforms to recognize the sign performed in front of a camera. SLR research\\rhas taken advantage of recent advances in pose estimation models to use\\rskeleton sequences estimated from videos instead of RGB information to predict\\rsigns. This approach can make HAR-related tasks less complex and more robust to\\rdiverse backgrounds, lightning conditions, and physical appearances. In this\\rwork, we explore the use of a spatio-temporal skeleton representation such as\\rTree Structure Skeleton Image (TSSI) as an alternative input to improve the\\raccuracy of skeleton-based models for SLR. TSSI converts a skeleton sequence\\rinto an RGB image where the columns represent the joints of the skeleton in a\\rdepth-first tree traversal order, the rows represent the temporal evolution of\\rthe joints, and the three channels represent the (x, y, z) coordinates of the\\rjoints. We trained a DenseNet-121 using this type of input and compared it with\\rother skeleton-based deep learning methods using a large-scale American Sign\\rLanguage (ASL) dataset, WLASL. Our model (SL-TSSI-DenseNet) overcomes the\\rstate-of-the-art of other skeleton-based models. Moreover, when including data\\raugmentation our proposal achieves better results than both skeleton-based and\\rRGB-based models. We evaluated the effectiveness of our model on the Ankara\\rUniversity Turkish Sign Language (TSL) dataset, AUTSL, and a Mexican Sign\\rLanguage (LSM) dataset. On the AUTSL dataset, the model achieves similar\\rresults to the state-of-the-art of other skeleton-based models. On the LSM\\rdataset, the model achieves higher results than the baseline. Code has been\\rmade available at: https://github.com/davidlainesv/SL-TSSI-DenseNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05403 ,  4275kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05417\\rDate: Tue, 11 Apr 2023 18:00:02 GMT   (26465kb,D)\\r\\rTitle: The MONET dataset: Multimodal drone thermal dataset recorded in rural\\r  scenarios\\rAuthors: Luigi Riz, Andrea Caraffa, Matteo Bortolon, Mohamed Lamine Mekhalfi,\\r  Davide Boscani, Andr\\\\'e Moura, Jos\\\\'e Antunes, Andr\\\\'e Dias, Hugo Silva,\\r  Andreas Leonidou, Christos Constantinides, Christos Keleshis, Dante Abate,\\r  Fabio Poiesi\\rCategories: cs.CV\\rComments: Accepted in Computer Vision and Pattern Recognition (CVPR) Workshops\\r  2023 - 6th Multimodal Learning and Applications Workshop\\r\\\\\\\\\\r  We present MONET, a new multimodal dataset captured using a thermal camera\\rmounted on a drone that flew over rural areas, and recorded human and vehicle\\ractivities. We captured MONET to study the problem of object localisation and\\rbehaviour understanding of targets undergoing large-scale variations and being\\rrecorded from different and moving viewpoints. Target activities occur in two\\rdifferent land sites, each with unique scene structures and cluttered\\rbackgrounds. MONET consists of approximately 53K images featuring 162K manually\\rannotated bounding boxes. Each image is timestamp-aligned with drone metadata\\rthat includes information about attitudes, speed, altitude, and GPS\\rcoordinates. MONET is different from previous thermal drone datasets because it\\rfeatures multimodal data, including rural scenes captured with thermal cameras\\rcontaining both person and vehicle targets, along with trajectory information\\rand metadata. We assessed the difficulty of the dataset in terms of transfer\\rlearning between the two sites and evaluated nine object detection algorithms\\rto identify the open challenges associated with this type of data. Project\\rpage: https://github.com/fabiopoiesi/monet_dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05417 ,  26465kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05440\\rDate: Tue, 11 Apr 2023 18:16:47 GMT   (830kb,D)\\r\\rTitle: PixelRNN: In-pixel Recurrent Neural Networks for End-to-end-optimized\\r  Perception with Neural Sensors\\rAuthors: Haley M. So, Laurie Bose, Piotr Dudek, and Gordon Wetzstein\\rCategories: cs.CV\\r\\\\\\\\\\r  Conventional image sensors digitize high-resolution images at fast frame\\rrates, producing a large amount of data that needs to be transmitted off the\\rsensor for further processing. This is challenging for perception systems\\roperating on edge devices, because communication is power inefficient and\\rinduces latency. Fueled by innovations in stacked image sensor fabrication,\\remerging sensor-processors offer programmability and minimal processing\\rcapabilities directly on the sensor. We exploit these capabilities by\\rdeveloping an efficient recurrent neural network architecture, PixelRNN, that\\rencodes spatio-temporal features on the sensor using purely binary operations.\\rPixelRNN reduces the amount of data to be transmitted off the sensor by a\\rfactor of 64x compared to conventional systems while offering competitive\\raccuracy for hand gesture recognition and lip reading tasks. We experimentally\\rvalidate PixelRNN using a prototype implementation on the SCAMP-5\\rsensor-processor platform.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05440 ,  830kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05448\\rDate: Tue, 11 Apr 2023 18:38:57 GMT   (3263kb,D)\\r\\rTitle: Amortized Learning of Dynamic Feature Scaling for Image Segmentation\\rAuthors: Jose Javier Gonzalez Ortiz, John Guttag, Adrian Dalca\\rCategories: cs.CV cs.LG\\rComments: Code available at https://github.com/JJGO/amortized-feature-scaling\\r\\\\\\\\\\r  Convolutional neural networks (CNN) have become the predominant model for\\rimage segmentation tasks. Most CNN segmentation architectures resize spatial\\rdimensions by a fixed factor of two to aggregate spatial context. Recent work\\rhas explored using other resizing factors to improve model accuracy for\\rspecific applications. However, finding the appropriate rescaling factor most\\roften involves training a separate network for many different factors and\\rcomparing the performance of each model. The computational burden of these\\rmodels means that in practice it is rarely done, and when done only a few\\rdifferent scaling factors are considered.\\r  In this work, we present a hypernetwork strategy that can be used to easily\\rand rapidly generate the Pareto frontier for the trade-off between accuracy and\\refficiency as the rescaling factor varies. We show how to train a single\\rhypernetwork that generates CNN parameters conditioned on a rescaling factor.\\rThis enables a user to quickly choose a rescaling factor that appropriately\\rbalances accuracy and computational efficiency for their particular needs. We\\rfocus on image segmentation tasks, and demonstrate the value of this approach\\racross various domains. We also find that, for a given rescaling factor, our\\rsingle hypernetwork outperforms CNNs trained with fixed rescaling factors.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05448 ,  3263kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05464\\rDate: Tue, 11 Apr 2023 19:27:18 GMT   (3604kb,D)\\r\\rTitle: UnCRtainTS: Uncertainty Quantification for Cloud Removal in Optical\\r  Satellite Time Series\\rAuthors: Patrick Ebel, Vivien Sainte Fare Garnot, Michael Schmitt, Jan Dirk\\r  Wegner, Xiao Xiang Zhu\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  Clouds and haze often occlude optical satellite images, hindering continuous,\\rdense monitoring of the Earth's surface. Although modern deep learning methods\\rcan implicitly learn to ignore such occlusions, explicit cloud removal as\\rpre-processing enables manual interpretation and allows training models when\\ronly few annotations are available. Cloud removal is challenging due to the\\rwide range of occlusion scenarios -- from scenes partially visible through\\rhaze, to completely opaque cloud coverage. Furthermore, integrating\\rreconstructed images in downstream applications would greatly benefit from\\rtrustworthy quality assessment. In this paper, we introduce UnCRtainTS, a\\rmethod for multi-temporal cloud removal combining a novel attention-based\\rarchitecture, and a formulation for multivariate uncertainty prediction. These\\rtwo components combined set a new state-of-the-art performance in terms of\\rimage reconstruction on two public cloud removal datasets. Additionally, we\\rshow how the well-calibrated predicted uncertainties enable a precise control\\rof the reconstruction quality.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05464 ,  3604kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05469\\rDate: Tue, 11 Apr 2023 19:37:47 GMT   (44793kb,D)\\r\\rTitle: CamDiff: Camouflage Image Augmentation via Diffusion Model\\rAuthors: Xue-Jing Luo, Shuo Wang, Zongwei Wu, Christos Sakaridis, Yun Cheng,\\r  Deng-Ping Fan, Luc Van Gool\\rCategories: cs.CV\\r\\\\\\\\\\r  The burgeoning field of camouflaged object detection (COD) seeks to identify\\robjects that blend into their surroundings. Despite the impressive performance\\rof recent models, we have identified a limitation in their robustness, where\\rexisting methods may misclassify salient objects as camouflaged ones, despite\\rthese two characteristics being contradictory. This limitation may stem from\\rlacking multi-pattern training images, leading to less saliency robustness. To\\raddress this issue, we introduce CamDiff, a novel approach inspired by\\rAI-Generated Content (AIGC) that overcomes the scarcity of multi-pattern\\rtraining images. Specifically, we leverage the latent diffusion model to\\rsynthesize salient objects in camouflaged scenes, while using the zero-shot\\rimage classification ability of the Contrastive Language-Image Pre-training\\r(CLIP) model to prevent synthesis failures and ensure the synthesized object\\raligns with the input prompt. Consequently, the synthesized image retains its\\roriginal camouflage label while incorporating salient objects, yielding\\rcamouflage samples with richer characteristics. The results of user studies\\rshow that the salient objects in the scenes synthesized by our framework\\rattract the user's attention more; thus, such samples pose a greater challenge\\rto the existing COD models. Our approach enables flexible editing and efficient\\rlarge-scale dataset generation at a low cost. It significantly enhances COD\\rbaselines' training and testing phases, emphasizing robustness across diverse\\rdomains. Our newly-generated datasets and source code are available at\\rhttps://github.com/drlxj/CamDiff.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05469 ,  44793kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05472\\rDate: Tue, 11 Apr 2023 19:54:50 GMT   (23357kb,D)\\r\\rTitle: Light Sampling Field and BRDF Representation for Physically-based Neural\\r  Rendering\\rAuthors: Jing Yang, Hanyuan Xiao, Wenbin Teng, Yunxuan Cai, Yajie Zhao\\rCategories: cs.CV cs.AI cs.GR\\rComments: ICLR 2023 Poster\\r\\\\\\\\\\r  Physically-based rendering (PBR) is key for immersive rendering effects used\\rwidely in the industry to showcase detailed realistic scenes from computer\\rgraphics assets. A well-known caveat is that producing the same is\\rcomputationally heavy and relies on complex capture devices. Inspired by the\\rsuccess in quality and efficiency of recent volumetric neural rendering, we\\rwant to develop a physically-based neural shader to eliminate device dependency\\rand significantly boost performance. However, no existing lighting and material\\rmodels in the current neural rendering approaches can accurately represent the\\rcomprehensive lighting models and BRDFs properties required by the PBR process.\\rThus, this paper proposes a novel lighting representation that models direct\\rand indirect light locally through a light sampling strategy in a learned light\\rsampling field. We also propose BRDF models to separately represent\\rsurface/subsurface scattering details to enable complex objects such as\\rtranslucent material (i.e., skin, jade). We then implement our proposed\\rrepresentations with an end-to-end physically-based neural face skin shader,\\rwhich takes a standard face asset (i.e., geometry, albedo map, and normal map)\\rand an HDRI for illumination as inputs and generates a photo-realistic\\rrendering as output. Extensive experiments showcase the quality and efficiency\\rof our PBR face skin shader, indicating the effectiveness of our proposed\\rlighting and material representations.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05472 ,  23357kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05497\\rDate: Tue, 11 Apr 2023 21:07:59 GMT   (6997kb,D)\\r\\rTitle: Revisiting Single-gated Mixtures of Experts\\rAuthors: Amelie Royer, Ilia Karmanov, Andrii Skliar, Babak Ehteshami Bejnordi,\\r  Tijmen Blankevoort\\rCategories: cs.CV cs.LG\\rComments: BMVC 2022\\r\\\\\\\\\\r  Mixture of Experts (MoE) are rising in popularity as a means to train\\rextremely large-scale models, yet allowing for a reasonable computational cost\\rat inference time. Recent state-of-the-art approaches usually assume a large\\rnumber of experts, and require training all experts jointly, which often lead\\rto training instabilities such as the router collapsing In contrast, in this\\rwork, we propose to revisit the simple single-gate MoE, which allows for more\\rpractical training. Key to our work are (i) a base model branch acting both as\\ran early-exit and an ensembling regularization scheme, (ii) a simple and\\refficient asynchronous training pipeline without router collapse issues, and\\rfinally (iii) a per-sample clustering-based initialization. We show\\rexperimentally that the proposed model obtains efficiency-to-accuracy\\rtrade-offs comparable with other more complex MoE, and outperforms non-mixture\\rbaselines. This showcases the merits of even a simple single-gate MoE, and\\rmotivates further exploration in this area.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05497 ,  6997kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05523\\rDate: Tue, 11 Apr 2023 22:26:10 GMT   (1156kb,D)\\r\\rTitle: MoMo: A shared encoder Model for text, image and multi-Modal\\r  representations\\rAuthors: Rakesh Chada, Zhaoheng Zheng, Pradeep Natarajan\\rCategories: cs.CV cs.AI cs.CL\\r\\\\\\\\\\r  We propose a self-supervised shared encoder model that achieves strong\\rresults on several visual, language and multimodal benchmarks while being data,\\rmemory and run-time efficient. We make three key contributions. First, in\\rcontrast to most existing works, we use a single transformer with all the\\rencoder layers processing both the text and the image modalities. Second, we\\rpropose a stage-wise training strategy where the model is first trained on\\rimages, then jointly with unimodal text and image datasets and finally jointly\\rwith text and text-image datasets. Third, to preserve information across both\\rthe modalities, we propose a training pipeline that learns simultaneously from\\rgradient updates of different modalities at each training update step. The\\rresults on downstream text-only, image-only and multimodal tasks show that our\\rmodel is competitive with several strong models while using fewer parameters\\rand lesser pre-training data. For example, MoMo performs competitively with\\rFLAVA on multimodal (+3.1), image-only (+1.1) and text-only (-0.1) tasks\\rdespite having 2/5th the number of parameters and using 1/3rd the image-text\\rtraining pairs. Finally, we ablate various design choices and further show that\\rincreasing model size produces significant performance gains indicating\\rpotential for substantial improvements with larger models using our approach.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05523 ,  1156kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05530\\rDate: Tue, 11 Apr 2023 23:02:16 GMT   (4262kb)\\r\\rTitle: SceneCalib: Automatic Targetless Calibration of Cameras and Lidars in\\r  Autonomous Driving\\rAuthors: Ayon Sen, Gang Pan, Anton Mitrokhin, Ashraful Islam\\rCategories: cs.CV\\rComments: 7 pages, 6 figures, ICRA 2023 conference\\r\\\\\\\\\\r  Accurate camera-to-lidar calibration is a requirement for sensor data fusion\\rin many 3D perception tasks. In this paper, we present SceneCalib, a novel\\rmethod for simultaneous self-calibration of extrinsic and intrinsic parameters\\rin a system containing multiple cameras and a lidar sensor. Existing methods\\rtypically require specially designed calibration targets and human operators,\\ror they only attempt to solve for a subset of calibration parameters. We\\rresolve these issues with a fully automatic method that requires no explicit\\rcorrespondences between camera images and lidar point clouds, allowing for\\rrobustness to many outdoor environments. Furthermore, the full system is\\rjointly calibrated with explicit cross-camera constraints to ensure that\\rcamera-to-camera and camera-to-lidar extrinsic parameters are consistent.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05530 ,  4262kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05538\\rDate: Tue, 11 Apr 2023 23:55:50 GMT   (37672kb,D)\\r\\rTitle: Zoom is what you need: An empirical study of the power of zoom and\\r  spatial biases in image classification\\rAuthors: Mohammad Reza Taesiri, Giang Nguyen, Sarra Habchi, Cor-Paul Bezemer,\\r  Anh Nguyen\\rCategories: cs.CV\\r\\\\\\\\\\r  Image classifiers are information-discarding machines, by design. Yet, how\\rthese models discard information remains mysterious. We hypothesize that one\\rway for image classifiers to reach high accuracy is to first zoom to the most\\rdiscriminative region in the image and then extract features from there to\\rpredict image labels. We study six popular networks ranging from AlexNet to\\rCLIP and find that proper framing of the input image can lead to the correct\\rclassification of 98.91% of ImageNet images. Furthermore, we explore the\\rpotential and limits of zoom transforms in image classification and uncover\\rpositional biases in various datasets, especially a strong center bias in two\\rpopular datasets: ImageNet-A and ObjectNet. Finally, leveraging our insights\\rinto the potential of zoom, we propose a state-of-the-art test-time\\raugmentation (TTA) technique that improves classification accuracy by forcing\\rmodels to explicitly perform zoom-in operations before making predictions. Our\\rmethod is more interpretable, accurate, and faster than MEMO, a\\rstate-of-the-art TTA method. Additionally, we propose ImageNet-Hard, a new\\rbenchmark where zooming in alone often does not help state-of-the-art models\\rbetter label images.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05538 ,  37672kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05548\\rDate: Wed, 12 Apr 2023 00:46:41 GMT   (704kb,D)\\r\\rTitle: Distilling Token-Pruned Pose Transformer for 2D Human Pose Estimation\\rAuthors: Feixiang Ren\\rCategories: cs.CV\\r\\\\\\\\\\r  Human pose estimation has seen widespread use of transformer models in recent\\ryears. Pose transformers benefit from the self-attention map, which captures\\rthe correlation between human joint tokens and the image. However, training\\rsuch models is computationally expensive. The recent token-Pruned Pose\\rTransformer (PPT) solves this problem by pruning the background tokens of the\\rimage, which are usually less informative. However, although it improves\\refficiency, PPT inevitably leads to worse performance than TokenPose due to the\\rpruning of tokens. To overcome this problem, we present a novel method called\\rDistilling Pruned-Token Transformer for human pose estimation (DPPT). Our\\rmethod leverages the output of a pre-trained TokenPose to supervise the\\rlearning process of PPT. We also establish connections between the internal\\rstructure of pose transformers and PPT, such as attention maps and joint\\rfeatures. Our experimental results on the MPII datasets show that our DPPT can\\rsignificantly improve PCK compared to previous PPT models while still reducing\\rcomputational complexity.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05548 ,  704kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05552\\rDate: Wed, 12 Apr 2023 01:16:53 GMT   (3421kb,D)\\r\\rTitle: DynamicDet: A Unified Dynamic Architecture for Object Detection\\rAuthors: Zhihao Lin, Yongtao Wang, Jinhe Zhang, Xiaojie Chu\\rCategories: cs.CV cs.AI\\rComments: Accepted by CVPR 2023\\r\\\\\\\\\\r  Dynamic neural network is an emerging research topic in deep learning. With\\radaptive inference, dynamic models can achieve remarkable accuracy and\\rcomputational efficiency. However, it is challenging to design a powerful\\rdynamic detector, because of no suitable dynamic architecture and exiting\\rcriterion for object detection. To tackle these difficulties, we propose a\\rdynamic framework for object detection, named DynamicDet. Firstly, we carefully\\rdesign a dynamic architecture based on the nature of the object detection task.\\rThen, we propose an adaptive router to analyze the multi-scale information and\\rto decide the inference route automatically. We also present a novel\\roptimization strategy with an exiting criterion based on the detection losses\\rfor our dynamic detectors. Last, we present a variable-speed inference\\rstrategy, which helps to realize a wide range of accuracy-speed trade-offs with\\ronly one dynamic detector. Extensive experiments conducted on the COCO\\rbenchmark demonstrate that the proposed DynamicDet achieves new\\rstate-of-the-art accuracy-speed trade-offs. For instance, with comparable\\raccuracy, the inference speed of our dynamic detector Dy-YOLOv7-W6 surpasses\\rYOLOv7-E6 by 12%, YOLOv7-D6 by 17%, and YOLOv7-E6E by 39%. The code is\\ravailable at https://github.com/VDIGPKU/DynamicDet.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05552 ,  3421kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05554\\rDate: Wed, 12 Apr 2023 01:20:58 GMT   (499kb,D)\\r\\rTitle: Learning Transferable Pedestrian Representation from Multimodal\\r  Information Supervision\\rAuthors: Liping Bao, Longhui Wei, Xiaoyu Qiu, Wengang Zhou, Houqiang Li, Qi\\r  Tian\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Recent researches on unsupervised person re-identification~(reID) have\\rdemonstrated that pre-training on unlabeled person images achieves superior\\rperformance on downstream reID tasks than pre-training on ImageNet. However,\\rthose pre-trained methods are specifically designed for reID and suffer\\rflexible adaption to other pedestrian analysis tasks. In this paper, we propose\\rVAL-PAT, a novel framework that learns transferable representations to enhance\\rvarious pedestrian analysis tasks with multimodal information. To train our\\rframework, we introduce three learning objectives, \\\\emph{i.e.,} self-supervised\\rcontrastive learning, image-text contrastive learning and multi-attribute\\rclassification. The self-supervised contrastive learning facilitates the\\rlearning of the intrinsic pedestrian properties, while the image-text\\rcontrastive learning guides the model to focus on the appearance information of\\rpedestrians.Meanwhile, multi-attribute classification encourages the model to\\rrecognize attributes to excavate fine-grained pedestrian information. We first\\rperform pre-training on LUPerson-TA dataset, where each image contains text and\\rattribute annotations, and then transfer the learned representations to various\\rdownstream tasks, including person reID, person attribute recognition and\\rtext-based person search. Extensive experiments demonstrate that our framework\\rfacilitates the learning of general pedestrian representations and thus leads\\rto promising results on various pedestrian analysis tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05554 ,  499kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05556\\rDate: Wed, 12 Apr 2023 01:31:46 GMT   (42540kb,D)\\r\\rTitle: An End-to-End Network for Upright Adjustment of Panoramic Images\\rAuthors: Heyu Chen, Jianfeng Li and Shigang Li\\rCategories: cs.CV\\r\\\\\\\\\\r  Nowadays, panoramic images can be easily obtained by panoramic cameras.\\rHowever, when the panoramic camera orientation is tilted, a non-upright\\rpanoramic image will be captured. Existing upright adjustment models focus on\\rhow to estimate more accurate camera orientation, and attribute image\\rreconstruction to offline or post-processing tasks. To this end, we propose an\\ronline end-to-end network for upright adjustment. Our network is designed to\\rreconstruct the image while finding the angle. Our network consists of three\\rmodules: orientation estimation, LUT online generation, and upright\\rreconstruction. Direction estimation estimates the tilt angle of the panoramic\\rimage. Then, a converter block with upsampling function is designed to generate\\rangle to LUT. This module can output corresponding online LUT for different\\rinput angles. Finally, a lightweight generative adversarial network (GAN) aims\\rto generate upright images from shallow features. The experimental results show\\rthat in terms of angles, we have improved the accuracy of small angle errors.\\rIn terms of image reconstruction, In image reconstruction, we have achieved the\\rfirst real-time online upright reconstruction of panoramic images using deep\\rlearning networks.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05556 ,  42540kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05561\\rDate: Wed, 12 Apr 2023 01:47:11 GMT   (20350kb,D)\\r\\rTitle: On the Adversarial Inversion of Deep Biometric Representations\\rAuthors: Gioacchino Tangari and Shreesh Keskar and Hassan Jameel Asghar and\\r  Dali Kaafar\\rCategories: cs.CV cs.CR cs.LG\\r\\\\\\\\\\r  Biometric authentication service providers often claim that it is not\\rpossible to reverse-engineer a user's raw biometric sample, such as a\\rfingerprint or a face image, from its mathematical (feature-space)\\rrepresentation. In this paper, we investigate this claim on the specific\\rexample of deep neural network (DNN) embeddings. Inversion of DNN embeddings\\rhas been investigated for explaining deep image representations or synthesizing\\rnormalized images. Existing studies leverage full access to all layers of the\\roriginal model, as well as all possible information on the original dataset.\\rFor the biometric authentication use case, we need to investigate this under\\radversarial settings where an attacker has access to a feature-space\\rrepresentation but no direct access to the exact original dataset nor the\\roriginal learned model. Instead, we assume varying degree of attacker's\\rbackground knowledge about the distribution of the dataset as well as the\\roriginal learned model (architecture and training process). In these cases, we\\rshow that the attacker can exploit off-the-shelf DNN models and public\\rdatasets, to mimic the behaviour of the original learned model to varying\\rdegrees of success, based only on the obtained representation and attacker's\\rprior knowledge. We propose a two-pronged attack that first infers the original\\rDNN by exploiting the model footprint on the embedding, and then reconstructs\\rthe raw data by using the inferred model. We show the practicality of the\\rattack on popular DNNs trained for two prominent biometric modalities, face and\\rfingerprint recognition. The attack can effectively infer the original\\rrecognition model (mean accuracy 83\\\\% for faces, 86\\\\% for fingerprints), and\\rcan craft effective biometric reconstructions that are successfully\\rauthenticated with 1-vs-1 authentication accuracy of up to 92\\\\% for some\\rmodels.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05561 ,  20350kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05564\\rDate: Wed, 12 Apr 2023 01:56:42 GMT   (5719kb,D)\\r\\rTitle: Neural Invertible Variable-degree Optical Aberrations Correction\\rAuthors: Shuang Cui, Bingnan Wang, Quan Zheng\\rCategories: cs.CV eess.IV\\rJournal-ref: Optics Express 2023\\rDOI: 10.1364/OE.485258\\r\\\\\\\\\\r  Optical aberrations of optical systems cause significant degradation of\\rimaging quality. Aberration correction by sophisticated lens designs and\\rspecial glass materials generally incurs high cost of manufacturing and the\\rincrease in the weight of optical systems, thus recent work has shifted to\\raberration correction with deep learning-based post-processing. Though\\rreal-world optical aberrations vary in degree, existing methods cannot\\reliminate variable-degree aberrations well, especially for the severe degrees\\rof degradation. Also, previous methods use a single feed-forward neural network\\rand suffer from information loss in the output. To address the issues, we\\rpropose a novel aberration correction method with an invertible architecture by\\rleveraging its information-lossless property. Within the architecture, we\\rdevelop conditional invertible blocks to allow the processing of aberrations\\rwith variable degrees. Our method is evaluated on both a synthetic dataset from\\rphysics-based imaging simulation and a real captured dataset. Quantitative and\\rqualitative experimental results demonstrate that our method outperforms\\rcompared methods in correcting variable-degree optical aberrations.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05564 ,  5719kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05568\\rDate: Wed, 12 Apr 2023 02:08:34 GMT   (34619kb,D)\\r\\rTitle: Improving Diffusion Models for Scene Text Editing with Dual Encoders\\rAuthors: Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian\\r  Price, Shiyu Chang\\rCategories: cs.CV cs.AI\\rComments: 22 pages, 19 figures\\r\\\\\\\\\\r  Scene text editing is a challenging task that involves modifying or inserting\\rspecified texts in an image while maintaining its natural and realistic\\rappearance. Most previous approaches to this task rely on style-transfer models\\rthat crop out text regions and feed them into image transfer models, such as\\rGANs. However, these methods are limited in their ability to change text style\\rand are unable to insert texts into images. Recent advances in diffusion models\\rhave shown promise in overcoming these limitations with text-conditional image\\rediting. However, our empirical analysis reveals that state-of-the-art\\rdiffusion models struggle with rendering correct text and controlling text\\rstyle. To address these problems, we propose DIFFSTE to improve pre-trained\\rdiffusion models with a dual encoder design, which includes a character encoder\\rfor better text legibility and an instruction encoder for better style control.\\rAn instruction tuning framework is introduced to train our model to learn the\\rmapping from the text instruction to the corresponding image with either the\\rspecified style or the style of the surrounding texts in the background. Such a\\rtraining method further brings our method the zero-shot generalization ability\\rto the following three scenarios: generating text with unseen font variation,\\re.g., italic and bold, mixing different fonts to construct a new font, and\\rusing more relaxed forms of natural language as the instructions to guide the\\rgeneration task. We evaluate our approach on five datasets and demonstrate its\\rsuperior performance in terms of text correctness, image naturalness, and style\\rcontrollability. Our code is publicly available.\\rhttps://github.com/UCSB-NLP-Chang/DiffSTE\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05568 ,  34619kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05571\\rDate: Wed, 12 Apr 2023 02:20:29 GMT   (4844kb,D)\\r\\rTitle: SGL: Structure Guidance Learning for Camera Localization\\rAuthors: Xudong Zhang, Shuang Gao, Xiaohu Nan, Haikuan Ning, Yuchen Yang,\\r  Yishan Ping, Jixiang Wan, Shuzhou Dong, Jijunnan Li, Yandong Guo\\rCategories: cs.CV\\r\\\\\\\\\\r  Camera localization is a classical computer vision task that serves various\\rArtificial Intelligence and Robotics applications. With the rapid developments\\rof Deep Neural Networks (DNNs), end-to-end visual localization methods are\\rprosperous in recent years. In this work, we focus on the scene coordinate\\rprediction ones and propose a network architecture named as Structure Guidance\\rLearning (SGL) which utilizes the receptive branch and the structure branch to\\rextract both high-level and low-level features to estimate the 3D coordinates.\\rWe design a confidence strategy to refine and filter the predicted 3D\\robservations, which enables us to estimate the camera poses by employing the\\rPerspective-n-Point (PnP) with RANSAC. In the training part, we design the\\rBundle Adjustment trainer to help the network fit the scenes better.\\rComparisons with some state-of-the-art (SOTA) methods and sufficient ablation\\rexperiments confirm the validity of our proposed architecture.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05571 ,  4844kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05619\\rDate: Wed, 12 Apr 2023 05:27:30 GMT   (44533kb,D)\\r\\rTitle: NutritionVerse-3D: A 3D Food Model Dataset for Nutritional Intake\\r  Estimation\\rAuthors: Chi-en Amy Tai, Matthew Keller, Mattie Kerrigan, Yuhao Chen, Saeejith\\r  Nair, Pengcheng Xi, Alexander Wong\\rCategories: cs.CV\\r\\\\\\\\\\r  77% of adults over 50 want to age in place today, presenting a major\\rchallenge to ensuring adequate nutritional intake. It has been reported that\\rone in four older adults that are 65 years or older are malnourished and given\\rthe direct link between malnutrition and decreased quality of life, there have\\rbeen numerous studies conducted on how to efficiently track nutritional intake\\rof food. Recent advancements in machine learning and computer vision show\\rpromise of automated nutrition tracking methods of food, but require a large\\rhigh-quality dataset in order to accurately identify the nutrients from the\\rfood on the plate. Unlike existing datasets, a collection of 3D models with\\rnutritional information allow for view synthesis to create an infinite number\\rof 2D images for any given viewpoint/camera angle along with the associated\\rnutritional information. In this paper, we develop a methodology for collecting\\rhigh-quality 3D models for food items with a particular focus on speed and\\rconsistency, and introduce NutritionVerse-3D, a large-scale high-quality\\rhigh-resolution dataset of 105 3D food models, in conjunction with their\\rassociated weight, food name, and nutritional value. These models allow for\\rlarge quantity food intake scenes, diverse and customizable scene layout, and\\ran infinite number of camera settings and lighting conditions.\\rNutritionVerse-3D is publicly available as a part of an open initiative to\\raccelerate machine learning for nutrition sensing.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05619 ,  44533kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05620\\rDate: Wed, 12 Apr 2023 05:34:32 GMT   (1656kb,D)\\r\\rTitle: NutritionVerse-Thin: An Optimized Strategy for Enabling Improved\\r  Rendering of 3D Thin Food Models\\rAuthors: Chi-en Amy Tai, Jason Li, Sriram Kumar, Saeejith Nair, Yuhao Chen,\\r  Pengcheng Xi, Alexander Wong\\rCategories: cs.CV\\r\\\\\\\\\\r  With the growth in capabilities of generative models, there has been growing\\rinterest in using photo-realistic renders of common 3D food items to improve\\rdownstream tasks such as food printing, nutrition prediction, or management of\\rfood wastage. Despite 3D modelling capabilities being more accessible than ever\\rdue to the success of NeRF based view-synthesis, such rendering methods still\\rstruggle to correctly capture thin food objects, often generating meshes with\\rsignificant holes. In this study, we present an optimized strategy for enabling\\rimproved rendering of thin 3D food models, and demonstrate qualitative\\rimprovements in rendering quality. Our method generates the 3D model mesh via a\\rproposed thin-object-optimized differentiable reconstruction method and tailors\\rthe strategy at both the data collection and training stages to better handle\\rthin objects. While simple, we find that this technique can be employed for\\rquick and highly consistent capturing of thin 3D objects.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05620 ,  1656kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05634\\rDate: Wed, 12 Apr 2023 06:31:14 GMT   (3788kb,D)\\r\\rTitle: How you feelin'? Learning Emotions and Mental States in Movie Scenes\\rAuthors: Dhruv Srivastava and Aditya Kumar Singh and Makarand Tapaswi\\rCategories: cs.CV\\rComments: CVPR 2023. Project Page: https://katha-ai.github.io/projects/emotx/\\r\\\\\\\\\\r  Movie story analysis requires understanding characters' emotions and mental\\rstates. Towards this goal, we formulate emotion understanding as predicting a\\rdiverse and multi-label set of emotions at the level of a movie scene and for\\reach character. We propose EmoTx, a multimodal Transformer-based architecture\\rthat ingests videos, multiple characters, and dialog utterances to make joint\\rpredictions. By leveraging annotations from the MovieGraphs dataset, we aim to\\rpredict classic emotions (e.g. happy, angry) and other mental states (e.g.\\rhonest, helpful). We conduct experiments on the most frequently occurring 10\\rand 25 labels, and a mapping that clusters 181 labels to 26. Ablation studies\\rand comparison against adapted state-of-the-art emotion recognition approaches\\rshows the effectiveness of EmoTx. Analyzing EmoTx's self-attention scores\\rreveals that expressive emotions often look at character tokens while other\\rmental states rely on video and dialog cues.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05634 ,  3788kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05640\\rDate: Wed, 12 Apr 2023 06:41:16 GMT   (8474kb,D)\\r\\rTitle: Instance-Aware Domain Generalization for Face Anti-Spoofing\\rAuthors: Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Xuequan Lu, Ran Yi, Shouhong\\r  Ding, Lizhuang Ma\\rCategories: cs.CV\\rComments: Accepted to IEEE/CVF Conference on Computer Vision and Pattern\\r  Recognition (CVPR), 2023\\r\\\\\\\\\\r  Face anti-spoofing (FAS) based on domain generalization (DG) has been\\rrecently studied to improve the generalization on unseen scenarios. Previous\\rmethods typically rely on domain labels to align the distribution of each\\rdomain for learning domain-invariant representations. However, artificial\\rdomain labels are coarse-grained and subjective, which cannot reflect real\\rdomain distributions accurately. Besides, such domain-aware methods focus on\\rdomain-level alignment, which is not fine-grained enough to ensure that learned\\rrepresentations are insensitive to domain styles. To address these issues, we\\rpropose a novel perspective for DG FAS that aligns features on the instance\\rlevel without the need for domain labels. Specifically, Instance-Aware Domain\\rGeneralization framework is proposed to learn the generalizable feature by\\rweakening the features' sensitivity to instance-specific styles. Concretely, we\\rpropose Asymmetric Instance Adaptive Whitening to adaptively eliminate the\\rstyle-sensitive feature correlation, boosting the generalization. Moreover,\\rDynamic Kernel Generator and Categorical Style Assembly are proposed to first\\rextract the instance-specific features and then generate the style-diversified\\rfeatures with large style shifts, respectively, further facilitating the\\rlearning of style-insensitive features. Extensive experiments and analysis\\rdemonstrate the superiority of our method over state-of-the-art competitors.\\rCode will be publicly available at https://github.com/qianyuzqy/IADG.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05640 ,  8474kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05645\\rDate: Wed, 12 Apr 2023 06:48:26 GMT   (17027kb,D)\\r\\rTitle: WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with\\r  Multi-modal Visual Data and Natural Language\\rAuthors: Zhenxiang Lin, Xidong Peng, Peishan Cong, Yuenan Hou, Xinge Zhu, Sibei\\r  Yang, Yuexin Ma\\rCategories: cs.CV\\r\\\\\\\\\\r  We introduce the task of 3D visual grounding in large-scale dynamic scenes\\rbased on natural linguistic descriptions and online captured multi-modal visual\\rdata, including 2D images and 3D LiDAR point clouds. We present a novel method,\\rWildRefer, for this task by fully utilizing the appearance features in images,\\rthe location and geometry features in point clouds, and the dynamic features in\\rconsecutive input frames to match the semantic features in language. In\\rparticular, we propose two novel datasets, STRefer and LifeRefer, which focus\\ron large-scale human-centric daily-life scenarios with abundant 3D object and\\rnatural language annotations. Our datasets are significant for the research of\\r3D visual grounding in the wild and has huge potential to boost the development\\rof autonomous driving and service robots. Extensive comparisons and ablation\\rstudies illustrate that our method achieves state-of-the-art performance on two\\rproposed datasets. Code and dataset will be released when the paper is\\rpublished.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05645 ,  17027kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05646\\rDate: Wed, 12 Apr 2023 06:49:56 GMT   (3837kb,D)\\r\\rTitle: Modality-Invariant Representation for Infrared and Visible Image\\r  Registration\\rAuthors: Zhiying Jiang, Zengxi Zhang, Jinyuan Liu, Xin Fan, Risheng Liu\\rCategories: cs.CV\\rComments: 10 pages, 11 figures\\r\\\\\\\\\\r  Since the differences in viewing range, resolution and relative position, the\\rmulti-modality sensing module composed of infrared and visible cameras needs to\\rbe registered so as to have more accurate scene perception. In practice, manual\\rcalibration-based registration is the most widely used process, and it is\\rregularly calibrated to maintain accuracy, which is time-consuming and\\rlabor-intensive. To cope with these problems, we propose a scene-adaptive\\rinfrared and visible image registration. Specifically, in regard of the\\rdiscrepancy between multi-modality images, an invertible translation process is\\rdeveloped to establish a modality-invariant domain, which comprehensively\\rembraces the feature intensity and distribution of both infrared and visible\\rmodalities. We employ homography to simulate the deformation between different\\rplanes and develop a hierarchical framework to rectify the deformation inferred\\rfrom the proposed latent representation in a coarse-to-fine manner. For that,\\rthe advanced perception ability coupled with the residual estimation conducive\\rto the regression of sparse offsets, and the alternate correlation search\\rfacilitates a more accurate correspondence matching. Moreover, we propose the\\rfirst ground truth available misaligned infrared and visible image dataset,\\rinvolving three synthetic sets and one real-world set. Extensive experiments\\rvalidate the effectiveness of the proposed method against the\\rstate-of-the-arts, advancing the subsequent applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05646 ,  3837kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05653\\rDate: Wed, 12 Apr 2023 07:16:55 GMT   (9298kb,D)\\r\\rTitle: CLIP Surgery for Better Explainability with Enhancement in\\r  Open-Vocabulary Tasks\\rAuthors: Yi Li, Hualiang Wang, Yiqun Duan, Xiaomeng Li\\rCategories: cs.CV\\rComments: 21 pages, 13 figures, under review\\r\\\\\\\\\\r  Contrastive Language-Image Pre-training (CLIP) is a powerful multimodal large\\rvision model that has demonstrated significant benefits for downstream tasks,\\rincluding many zero-shot learning and text-guided vision tasks. However, we\\rnotice some severe problems regarding the model's explainability, which\\rundermines its credibility and impedes related tasks. Specifically, we find\\rCLIP prefers the background regions than the foregrounds according to the\\rpredicted similarity map, which contradicts human understanding. Besides, there\\rare obvious noisy activations on the visualization results at irrelevant\\rpositions. To address these two issues, we conduct in-depth analyses and reveal\\rthe reasons with new findings and evidences. Based on these insights, we\\rpropose the CLIP Surgery, a method that enables surgery-like modifications for\\rthe inference architecture and features, for better explainability and\\renhancement in multiple open-vocabulary tasks. The proposed method has\\rsignificantly improved the explainability of CLIP for both convolutional\\rnetworks and vision transformers, surpassing existing methods by large margins.\\rBesides, our approach also demonstrates remarkable improvements in\\ropen-vocabulary segmentation and multi-label recognition tasks. For examples,\\rthe mAP improvement on NUS-Wide multi-label recognition is 4.41% without any\\radditional training, and our CLIP Surgery surpasses the state-of-the-art method\\rby 8.74% at mIoU on Cityscapes open-vocabulary semantic segmentation.\\rFurthermore, our method benefits other tasks including multimodal visualization\\rand interactive segmentation like Segment Anything Model (SAM). The code is\\ravailable at https://github.com/xmed-lab/CLIP_Surgery\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05653 ,  9298kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05659\\rDate: Wed, 12 Apr 2023 07:34:13 GMT   (799kb,D)\\r\\rTitle: RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer\\rAuthors: Jiahao Wang, Songyang Zhang, Yong Liu, Taiqiang Wu, Yujiu Yang, Xihui\\r  Liu, Kai Chen, Ping Luo, Dahua Lin\\rCategories: cs.CV cs.AI\\rComments: 8 pages, accepted by CVPR2023\\r\\\\\\\\\\r  This paper studies how to keep a vision backbone effective while removing\\rtoken mixers in its basic building blocks. Token mixers, as self-attention for\\rvision transformers (ViTs), are intended to perform information communication\\rbetween different spatial tokens but suffer from considerable computational\\rcost and latency. However, directly removing them will lead to an incomplete\\rmodel structure prior, and thus brings a significant accuracy drop. To this\\rend, we first develop an RepIdentityFormer base on the re-parameterizing idea,\\rto study the token mixer free model architecture. And we then explore the\\rimproved learning paradigm to break the limitation of simple token mixer free\\rbackbone, and summarize the empirical practice into 5 guidelines. Equipped with\\rthe proposed optimization strategy, we are able to build an extremely simple\\rvision backbone with encouraging performance, while enjoying the high\\refficiency during inference. Extensive experiments and ablative analysis also\\rdemonstrate that the inductive bias of network architecture, can be\\rincorporated into simple network structure with appropriate optimization\\rstrategy. We hope this work can serve as a starting point for the exploration\\rof optimization-driven efficient network design. Project page:\\rhttps://techmonsterwang.github.io/RIFormer/.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05659 ,  799kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05661\\rDate: Wed, 12 Apr 2023 07:39:20 GMT   (27967kb,D)\\r\\rTitle: SuperpixelGraph: Semi-automatic generation of building footprint through\\r  semantic-sensitive superpixel and neural graph networks\\rAuthors: Haojia Yu, Han Hu, Bo Xu, Qisen Shang, Zhendong Wang and Qing Zhu\\rCategories: cs.CV\\r\\\\\\\\\\r  Most urban applications necessitate building footprints in the form of\\rconcise vector graphics with sharp boundaries rather than pixel-wise raster\\rimages. This need contrasts with the majority of existing methods, which\\rtypically generate over-smoothed footprint polygons. Editing these\\rautomatically produced polygons can be inefficient, if not more time-consuming\\rthan manual digitization. This paper introduces a semi-automatic approach for\\rbuilding footprint extraction through semantically-sensitive superpixels and\\rneural graph networks. Drawing inspiration from object-based classification\\rtechniques, we first learn to generate superpixels that are not only\\rboundary-preserving but also semantically-sensitive. The superpixels respond\\rexclusively to building boundaries rather than other natural objects, while\\rsimultaneously producing semantic segmentation of the buildings. These\\rintermediate superpixel representations can be naturally considered as nodes\\rwithin a graph. Consequently, graph neural networks are employed to model the\\rglobal interactions among all superpixels and enhance the representativeness of\\rnode features for building segmentation. Classical approaches are utilized to\\rextract and regularize boundaries for the vectorized building footprints.\\rUtilizing minimal clicks and straightforward strokes, we efficiently accomplish\\raccurate segmentation outcomes, eliminating the necessity for editing polygon\\rvertices. Our proposed approach demonstrates superior precision and efficacy,\\ras validated by experimental assessments on various public benchmark datasets.\\rWe observe a 10\\\\% enhancement in the metric for superpixel clustering and an\\r8\\\\% increment in vector graphics evaluation, when compared with established\\rtechniques. Additionally, we have devised an optimized and sophisticated\\rpipeline for interactive editing, poised to further augment the overall quality\\rof the results.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05661 ,  27967kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05667\\rDate: Wed, 12 Apr 2023 07:44:50 GMT   (5075kb,D)\\r\\rTitle: Rail Detection: An Efficient Row-based Network and A New Benchmark\\rAuthors: Xinpeng Li and Xiaojiang Peng\\rCategories: cs.CV\\rComments: Accepted by ACMMM 2022\\r\\\\\\\\\\r  Rail detection, essential for railroad anomaly detection, aims to identify\\rthe railroad region in video frames. Although various studies on rail detection\\rexist, neither an open benchmark nor a high-speed network is available in the\\rcommunity, making algorithm comparison and development difficult. Inspired by\\rthe growth of lane detection, we propose a rail database and a row-based rail\\rdetection method. In detail, we make several contributions: (i) We present a\\rreal-world railway dataset, Rail-DB, with 7432 pairs of images and annotations.\\rThe images are collected from different situations in lighting, road\\rstructures, and views. The rails are labeled with polylines, and the images are\\rcategorized into nine scenes. The Rail-DB is expected to facilitate the\\rimprovement of rail detection algorithms. (ii) We present an efficient\\rrow-based rail detection method, Rail-Net, containing a lightweight\\rconvolutional backbone and an anchor classifier. Specifically, we formulate the\\rprocess of rail detection as a row-based selecting problem. This strategy\\rreduces the computational cost compared to alternative segmentation methods.\\r(iii) We evaluate the Rail-Net on Rail-DB with extensive experiments, including\\rcross-scene settings and network backbones ranging from ResNet to Vision\\rTransformers. Our method achieves promising performance in terms of both speed\\rand accuracy. Notably, a lightweight version could achieve 92.77% accuracy and\\r312 frames per second. The Rail-Net outperforms the traditional method by\\r50.65% and the segmentation one by 5.86%. The database and code are available\\rat: https://github.com/Sampson-Lee/Rail-Detection.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05667 ,  5075kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05669\\rDate: Wed, 12 Apr 2023 07:46:05 GMT   (38505kb,D)\\r\\rTitle: Factorized Inverse Path Tracing for Efficient and Accurate\\r  Material-Lighting Estimation\\rAuthors: Liwen Wu, Rui Zhu, Mustafa B. Yaldiz, Yinhao Zhu, Hong Cai, Janarbek\\r  Matai, Fatih Porikli, Tzu-Mao Li, Manmohan Chandraker, Ravi Ramamoorthi\\rCategories: cs.CV cs.GR\\rComments: Updated results from MILO on Apr 10, 2023\\r\\\\\\\\\\r  Inverse path tracing has recently been applied to joint material and lighting\\restimation, given geometry and multi-view HDR observations of an indoor scene.\\rHowever, it has two major limitations: path tracing is expensive to compute,\\rand ambiguities exist between reflection and emission. We propose a novel\\rFactorized Inverse Path Tracing (FIPT) method which utilizes a factored light\\rtransport formulation and finds emitters driven by rendering errors. Our\\ralgorithm enables accurate material and lighting optimization faster than\\rprevious work, and is more effective at resolving ambiguities. The exhaustive\\rexperiments on synthetic scenes show that our method (1) outperforms\\rstate-of-the-art indoor inverse rendering and relighting methods particularly\\rin the presence of complex illumination effects; (2) speeds up inverse path\\rtracing optimization to less than an hour. We further demonstrate robustness to\\rnoisy inputs through material and lighting estimates that allow plausible\\rrelighting in a real scene. The source code is available at:\\rhttps://github.com/lwwu2/fipt\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05669 ,  38505kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05673\\rDate: Wed, 12 Apr 2023 07:49:21 GMT   (1890kb,D)\\r\\rTitle: Precise localization of corneal reflections in eye images using deep\\r  learning trained on synthetic data\\rAuthors: Sean Anthony Byrne, Marcus Nystr\\\\om, Virmarie Maquiling, Enkelejda\\r  Kasneci, Diederick C. Niehorster\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  We present a deep learning method for accurately localizing the center of a\\rsingle corneal reflection (CR) in an eye image. Unlike previous approaches, we\\ruse a convolutional neural network (CNN) that was trained solely using\\rsimulated data. Using only simulated data has the benefit of completely\\rsidestepping the time-consuming process of manual annotation that is required\\rfor supervised training on real eye images. To systematically evaluate the\\raccuracy of our method, we first tested it on images with simulated CRs placed\\ron different backgrounds and embedded in varying levels of noise. Second, we\\rtested the method on high-quality videos captured from real eyes. Our method\\routperformed state-of-the-art algorithmic methods on real eye images with a 35%\\rreduction in terms of spatial precision, and performed on par with\\rstate-of-the-art on simulated images in terms of spatial accuracy.We conclude\\rthat our method provides a precise method for CR center localization and\\rprovides a solution to the data availability problem which is one of the\\rimportant common roadblocks in the development of deep learning models for gaze\\restimation. Due to the superior CR center localization and ease of application,\\rour method has the potential to improve the accuracy and precision of CR-based\\reye trackers\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05673 ,  1890kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05675\\rDate: Wed, 12 Apr 2023 07:49:52 GMT   (2729kb,D)\\r\\rTitle: Semantic-Aware Mixup for Domain Generalization\\rAuthors: Chengchao Xu and Xinmei Tian\\rCategories: cs.CV\\rComments: Accepted by IJCNN\\r\\\\\\\\\\r  Deep neural networks (DNNs) have shown exciting performance in various tasks,\\ryet suffer generalization failures when meeting unknown target domains. One of\\rthe most promising approaches to achieve domain generalization (DG) is\\rgenerating unseen data, e.g., mixup, to cover the unknown target data. However,\\rexisting works overlook the challenges induced by the simultaneous appearance\\rof changes in both the semantic and distribution space. Accordingly, such a\\rchallenge makes source distributions hard to fit for DNNs. To mitigate the\\rhard-fitting issue, we propose to perform a semantic-aware mixup (SAM) for\\rdomain generalization, where whether to perform mixup depends on the semantic\\rand domain information. The feasibility of SAM shares the same spirits with the\\rFourier-based mixup. Namely, the Fourier phase spectrum is expected to contain\\rsemantics information (relating to labels), while the Fourier amplitude retains\\rother information (relating to style information). Built upon the insight, SAM\\rapplies different mixup strategies to the Fourier phase spectrum and amplitude\\rinformation. For instance, SAM merely performs mixup on the amplitude spectrum\\rwhen both the semantic and domain information changes. Consequently, the\\roverwhelmingly large change can be avoided. We validate the effectiveness of\\rSAM using image classification tasks on several DG benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05675 ,  2729kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05678\\rDate: Wed, 12 Apr 2023 08:01:43 GMT   (4976kb,D)\\r\\rTitle: Real-time Trajectory-based Social Group Detection\\rAuthors: Simindokht Jahangard, Munawar Hayat and Hamid Rezatofighi\\rCategories: cs.CV\\r\\\\\\\\\\r  Social group detection is a crucial aspect of various robotic applications,\\rincluding robot navigation and human-robot interactions. To date, a range of\\rmodel-based techniques have been employed to address this challenge, such as\\rthe F-formation and trajectory similarity frameworks. However, these approaches\\roften fail to provide reliable results in crowded and dynamic scenarios. Recent\\radvancements in this area have mainly focused on learning-based methods, such\\ras deep neural networks that use visual content or human pose. Although visual\\rcontent-based methods have demonstrated promising performance on large-scale\\rdatasets, their computational complexity poses a significant barrier to their\\rpractical use in real-time applications. To address these issues, we propose a\\rsimple and efficient framework for social group detection. Our approach\\rexplores the impact of motion trajectory on social grouping and utilizes a\\rnovel, reliable, and fast data-driven method. We formulate the individuals in a\\rscene as a graph, where the nodes are represented by LSTM-encoded trajectories\\rand the edges are defined by the distances between each pair of tracks. Our\\rframework employs a modified graph transformer module and graph clustering\\rlosses to detect social groups. Our experiments on the popular JRDBAct dataset\\rreveal noticeable improvements in performance, with relative improvements\\rranging from 2% to 11%. Furthermore, our framework is significantly faster,\\rwith up to 12x faster inference times compared to state-of-the-art methods\\runder the same computation resources. These results demonstrate that our\\rproposed method is suitable for real-time robotic applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05678 ,  4976kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05684\\rDate: Wed, 12 Apr 2023 08:12:29 GMT   (5161kb,D)\\r\\rTitle: InterGen: Diffusion-based Multi-human Motion Generation under Complex\\r  Interactions\\rAuthors: Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, Lan Xu\\rCategories: cs.CV\\r\\\\\\\\\\r  We have recently seen tremendous progress in diffusion advances for\\rgenerating realistic human motions. Yet, they largely disregard the rich\\rmulti-human interactions. In this paper, we present InterGen, an effective\\rdiffusion-based approach that incorporates human-to-human interactions into the\\rmotion diffusion process, which enables layman users to customize high-quality\\rtwo-person interaction motions, with only text guidance. We first contribute a\\rmultimodal dataset, named InterHuman. It consists of about 107M frames for\\rdiverse two-person interactions, with accurate skeletal motions and 16,756\\rnatural language descriptions. For the algorithm side, we carefully tailor the\\rmotion diffusion model to our two-person interaction setting. To handle the\\rsymmetry of human identities during interactions, we propose two cooperative\\rtransformer-based denoisers that explicitly share weights, with a mutual\\rattention mechanism to further connect the two denoising processes. Then, we\\rpropose a novel representation for motion input in our interaction diffusion\\rmodel, which explicitly formulates the global relations between the two\\rperformers in the world frame. We further introduce two novel regularization\\rterms to encode spatial relations, equipped with a corresponding damping scheme\\rduring the training of our interaction diffusion model. Extensive experiments\\rvalidate the effectiveness and generalizability of InterGen. Notably, it can\\rgenerate more diverse and compelling two-person motions than previous methods\\rand enables various downstream applications for human interactions.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05684 ,  5161kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05690\\rDate: Wed, 12 Apr 2023 08:29:31 GMT   (16557kb,D)\\r\\rTitle: HybrIK-X: Hybrid Analytical-Neural Inverse Kinematics for Whole-body\\r  Mesh Recovery\\rAuthors: Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin Yang, Cewu Lu\\rCategories: cs.CV\\rComments: An eXpressive extension of HybrIK [arXiv:2011.14672], supports\\r  SMPL-X. arXiv admin note: substantial text overlap with arXiv:2011.14672\\r\\\\\\\\\\r  Recovering whole-body mesh by inferring the abstract pose and shape\\rparameters from visual content can obtain 3D bodies with realistic structures.\\rHowever, the inferring process is highly non-linear and suffers from image-mesh\\rmisalignment, resulting in inaccurate reconstruction. In contrast, 3D keypoint\\restimation methods utilize the volumetric representation to achieve pixel-level\\raccuracy but may predict unrealistic body structures. To address these issues,\\rthis paper presents a novel hybrid inverse kinematics solution, HybrIK, that\\rintegrates the merits of 3D keypoint estimation and body mesh recovery in a\\runified framework. HybrIK directly transforms accurate 3D joints to body-part\\rrotations via twist-and-swing decomposition. The swing rotations are\\ranalytically solved with 3D joints, while the twist rotations are derived from\\rvisual cues through neural networks. To capture comprehensive whole-body\\rdetails, we further develop a holistic framework, HybrIK-X, which enhances\\rHybrIK with articulated hands and an expressive face. HybrIK-X is fast and\\raccurate by solving the whole-body pose with a one-stage model. Experiments\\rdemonstrate that HybrIK and HybrIK-X preserve both the accuracy of 3D joints\\rand the realistic structure of the parametric human model, leading to\\rpixel-aligned whole-body mesh recovery. The proposed method significantly\\rsurpasses the state-of-the-art methods on various benchmarks for body-only,\\rhand-only, and whole-body scenarios. Code and results can be found at\\rhttps://jeffli.site/HybrIK-X/\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05690 ,  16557kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05694\\rDate: Wed, 12 Apr 2023 08:34:56 GMT   (1949kb,D)\\r\\rTitle: Multi-scale Geometry-aware Transformer for 3D Point Cloud Classification\\rAuthors: Xian Wei, Muyu Wang, Shing-Ho Jonathan Lin, Zhengyu Li, Jian Yang,\\r  Arafat Al-Jawari, Xuan Tang\\rCategories: cs.CV\\r\\\\\\\\\\r  Self-attention modules have demonstrated remarkable capabilities in capturing\\rlong-range relationships and improving the performance of point cloud tasks.\\rHowever, point cloud objects are typically characterized by complex,\\rdisordered, and non-Euclidean spatial structures with multiple scales, and\\rtheir behavior is often dynamic and unpredictable. The current self-attention\\rmodules mostly rely on dot product multiplication and dimension alignment among\\rquery-key-value features, which cannot adequately capture the multi-scale\\rnon-Euclidean structures of point cloud objects. To address these problems,\\rthis paper proposes a self-attention plug-in module with its variants,\\rMulti-scale Geometry-aware Transformer (MGT). MGT processes point cloud data\\rwith multi-scale local and global geometric information in the following three\\raspects. At first, the MGT divides point cloud data into patches with multiple\\rscales. Secondly, a local feature extractor based on sphere mapping is proposed\\rto explore the geometry inner each patch and generate a fixed-length\\rrepresentation for each patch. Thirdly, the fixed-length representations are\\rfed into a novel geodesic-based self-attention to capture the global\\rnon-Euclidean geometry between patches. Finally, all the modules are integrated\\rinto the framework of MGT with an end-to-end training scheme. Experimental\\rresults demonstrate that the MGT vastly increases the capability of capturing\\rmulti-scale geometry using the self-attention mechanism and achieves strong\\rcompetitive performance on mainstream point cloud benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05694 ,  1949kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05716\\rDate: Wed, 12 Apr 2023 09:18:38 GMT   (6058kb,D)\\r\\rTitle: Impact of Pseudo Depth on Open World Object Segmentation with Minimal\\r  User Guidance\\rAuthors: Robin Sch\\\\on, Katja Ludwig, Rainer Lienhart\\rCategories: cs.CV\\rComments: Accepted to L3D-IVU Workshop at CVPR 2023\\r\\\\\\\\\\r  Pseudo depth maps are depth map predicitions which are used as ground truth\\rduring training. In this paper we leverage pseudo depth maps in order to\\rsegment objects of classes that have never been seen during training. This\\rrenders our object segmentation task an open world task. The pseudo depth maps\\rare generated using pretrained networks, which have either been trained with\\rthe full intention to generalize to downstream tasks (LeRes and MiDaS), or\\rwhich have been trained in an unsupervised fashion on video sequences\\r(MonodepthV2). In order to tell our network which object to segment, we provide\\rthe network with a single click on the object's surface on the pseudo depth map\\rof the image as input. We test our approach on two different scenarios: One\\rwithout the RGB image and one where the RGB image is part of the input. Our\\rresults demonstrate a considerably better generalization performance from seen\\rto unseen object types when depth is used. On the Semantic Boundaries Dataset\\rwe achieve an improvement from $61.57$ to $69.79$ IoU score on unseen classes,\\rwhen only using half of the training classes during training and performing the\\rsegmentation on depth maps only.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05716 ,  6058kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05731\\rDate: Wed, 12 Apr 2023 09:40:38 GMT   (5524kb,D)\\r\\rTitle: SketchANIMAR: Sketch-based 3D Animal Fine-Grained Retrieval\\rAuthors: Trung-Nghia Le, Tam V. Nguyen, Minh-Quan Le, Trong-Thuan Nguyen,\\r  Viet-Tham Huynh, Trong-Le Do, Khanh-Duy Le, Mai-Khiem Tran, Nhat Hoang-Xuan,\\r  Thang-Long Nguyen-Ho, Vinh-Tiep Nguyen, Nhat-Quynh Le-Pham, Huu-Phuc Pham,\\r  Trong-Vu Hoang, Quang-Binh Nguyen, Trong-Hieu Nguyen-Mau, Tuan-Luc Huynh,\\r  Thanh-Danh Le, Ngoc-Linh Nguyen-Ha, Tuong-Vy Truong-Thuy, Truong Hoai Phong,\\r  Tuong-Nghiem Diep, Khanh-Duy Ho, Xuan-Hieu Nguyen, Thien-Phuc Tran, Tuan-Anh\\r  Yang, Kim-Phat Tran, Nhu-Vinh Hoang, Minh-Quang Nguyen, Hoai-Danh Vo,\\r  Minh-Hoa Doan, Hai-Dang Nguyen, Akihiro Sugimoto, Minh-Triet Tran\\rCategories: cs.CV\\r\\\\\\\\\\r  The retrieval of 3D objects has gained significant importance in recent years\\rdue to its broad range of applications in computer vision, computer graphics,\\rvirtual reality, and augmented reality. However, the retrieval of 3D objects\\rpresents significant challenges due to the intricate nature of 3D models, which\\rcan vary in shape, size, and texture, and have numerous polygons and vertices.\\rTo this end, we introduce a novel SHREC challenge track that focuses on\\rretrieving relevant 3D animal models from a dataset using sketch queries and\\rexpedites accessing 3D models through available sketches. Furthermore, a new\\rdataset named ANIMAR was constructed in this study, comprising a collection of\\r711 unique 3D animal models and 140 corresponding sketch queries. Our contest\\rrequires participants to retrieve 3D models based on complex and detailed\\rsketches. We receive satisfactory results from eight teams and 204 runs.\\rAlthough further improvement is necessary, the proposed task has the potential\\rto incentivize additional research in the domain of 3D object retrieval,\\rpotentially yielding benefits for a wide range of applications. We also provide\\rinsights into potential areas of future research, such as improving techniques\\rfor feature extraction and matching, and creating more diverse datasets to\\revaluate retrieval performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05731 ,  5524kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05734\\rDate: Wed, 12 Apr 2023 09:43:39 GMT   (407kb,D)\\r\\rTitle: Few-shot Class-incremental Learning for Cross-domain Disease\\r  Classification\\rAuthors: Hao Yang, Weijian Huang, Jiarun Liu, Cheng Li, Shanshan Wang\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  The ability to incrementally learn new classes from limited samples is\\rcrucial to the development of artificial intelligence systems for real clinical\\rapplication. Although existing incremental learning techniques have attempted\\rto address this issue, they still struggle with only few labeled data,\\rparticularly when the samples are from varied domains. In this paper, we\\rexplore the cross-domain few-shot incremental learning (CDFSCIL) problem.\\rCDFSCIL requires models to learn new classes from very few labeled samples\\rincrementally, and the new classes may be vastly different from the target\\rspace. To counteract this difficulty, we propose a cross-domain enhancement\\rconstraint and cross-domain data augmentation method. Experiments on MedMNIST\\rshow that the classification performance of this method is better than other\\rsimilar incremental learning methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05734 ,  407kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05741\\rDate: Wed, 12 Apr 2023 09:50:25 GMT   (1209kb,D)\\r\\rTitle: Learning to search for and detect objects in foveal images using deep\\r  learning\\rAuthors: Beatriz Paula and Plinio Moreno\\rCategories: cs.CV\\r\\\\\\\\\\r  The human visual system processes images with varied degrees of resolution,\\rwith the fovea, a small portion of the retina, capturing the highest acuity\\rregion, which gradually declines toward the field of view's periphery. However,\\rthe majority of existing object localization methods rely on images acquired by\\rimage sensors with space-invariant resolution, ignoring biological attention\\rmechanisms.\\r  As a region of interest pooling, this study employs a fixation prediction\\rmodel that emulates human objective-guided attention of searching for a given\\rclass in an image. The foveated pictures at each fixation point are then\\rclassified to determine whether the target is present or absent in the scene.\\rThroughout this two-stage pipeline method, we investigate the varying results\\robtained by utilizing high-level or panoptic features and provide a\\rground-truth label function for fixation sequences that is smoother,\\rconsidering in a better way the spatial structure of the problem.\\r  Finally, we present a novel dual task model capable of performing fixation\\rprediction and detection simultaneously, allowing knowledge transfer between\\rthe two tasks. We conclude that, due to the complementary nature of both tasks,\\rthe training process benefited from the sharing of knowledge, resulting in an\\rimprovement in performance when compared to the previous approach's baseline\\rscores.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05741 ,  1209kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05750\\rDate: Wed, 12 Apr 2023 10:10:03 GMT   (28354kb,D)\\r\\rTitle: Segment Anything Is Not Always Perfect: An Investigation of SAM on\\r  Different Real-world Applications\\rAuthors: Wei Ji, Jingjing Li, Qi Bi, Wenbo Li, Li Cheng\\rCategories: cs.CV\\rComments: Tech Report\\r\\\\\\\\\\r  Recently, Meta AI Research approaches a general, promptable Segment Anything\\rModel (SAM) pre-trained on an unprecedentedly large segmentation dataset\\r(SA-1B). Without a double, the emergence of SAM will yield significant benefits\\rfor a wide array of practical image segmentation applications. In this study,\\rwe conduct a series of intriguing investigations into the performance of SAM\\racross various applications, particularly in the fields of natural images,\\ragriculture, manufacturing, remote sensing, and healthcare. We analyze and\\rdiscuss the benefits and limitations of SAM and provide an outlook on future\\rdevelopment of segmentation tasks. Note that our work does not intend to\\rpropose new algorithms or theories, but rather provide a comprehensive view of\\rSAM in practice. This work is expected to provide insights that facilitate\\rfuture research activities toward generic segmentation.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05750 ,  28354kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05753\\rDate: Wed, 12 Apr 2023 10:29:42 GMT   (3475kb,D)\\r\\rTitle: Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results\\rAuthors: Dong Wang, Jia Guo, Qiqi Shao, Haochi He, Zhian Chen, Chuanbao Xiao,\\r  Ajian Liu, Sergio Escalera, Hugo Jair Escalante, Lei Zhen, Jun Wan, Jiankang\\r  Deng\\rCategories: cs.CV cs.AI\\rComments: CVPRW2023\\r\\\\\\\\\\r  Face anti-spoofing (FAS) is an essential mechanism for safeguarding the\\rintegrity of automated face recognition systems. Despite substantial\\radvancements, the generalization of existing approaches to real-world\\rapplications remains challenging. This limitation can be attributed to the\\rscarcity and lack of diversity in publicly available FAS datasets, which often\\rleads to overfitting during training or saturation during testing. In terms of\\rquantity, the number of spoof subjects is a critical determinant. Most datasets\\rcomprise fewer than 2,000 subjects. With regard to diversity, the majority of\\rdatasets consist of spoof samples collected in controlled environments using\\rrepetitive, mechanical processes. This data collection methodology results in\\rhomogenized samples and a dearth of scenario diversity. To address these\\rshortcomings, we introduce the Wild Face Anti-Spoofing (WFAS) dataset, a\\rlarge-scale, diverse FAS dataset collected in unconstrained settings. Our\\rdataset encompasses 853,729 images of 321,751 spoof subjects and 529,571 images\\rof 148,169 live subjects, representing a substantial increase in quantity.\\rMoreover, our dataset incorporates spoof data obtained from the internet,\\rspanning a wide array of scenarios and various commercial sensors, including 17\\rpresentation attacks (PAs) that encompass both 2D and 3D forms. This novel data\\rcollection strategy markedly enhances FAS data diversity. Leveraging the WFAS\\rdataset and Protocol 1 (Known-Type), we host the Wild Face Anti-Spoofing\\rChallenge at the CVPR2023 workshop. Additionally, we meticulously evaluate\\rrepresentative methods using Protocol 1 and Protocol 2 (Unknown-Type). Through\\ran in-depth examination of the challenge outcomes and benchmark baselines, we\\rprovide insightful analyses and propose potential avenues for future research.\\rThe dataset is released under Insightface.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05753 ,  3475kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05755\\rDate: Wed, 12 Apr 2023 10:33:18 GMT   (3361kb,D)\\r\\rTitle: ALADIN-NST: Self-supervised disentangled representation learning of\\r  artistic style through Neural Style Transfer\\rAuthors: Dan Ruta, Gemma Canet Tarres, Alex Black, Andrew Gilbert, John\\r  Collomosse\\rCategories: cs.CV\\r\\\\\\\\\\r  Representation learning aims to discover individual salient features of a\\rdomain in a compact and descriptive form that strongly identifies the unique\\rcharacteristics of a given sample respective to its domain. Existing works in\\rvisual style representation literature have tried to disentangle style from\\rcontent during training explicitly. A complete separation between these has yet\\rto be fully achieved. Our paper aims to learn a representation of visual\\rartistic style more strongly disentangled from the semantic content depicted in\\ran image. We use Neural Style Transfer (NST) to measure and drive the learning\\rsignal and achieve state-of-the-art representation learning on explicitly\\rdisentangled metrics. We show that strongly addressing the disentanglement of\\rstyle and content leads to large gains in style-specific metrics, encoding far\\rless semantic information and achieving state-of-the-art accuracy in downstream\\rmultimodal applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05755 ,  3361kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05758\\rDate: Wed, 12 Apr 2023 10:46:23 GMT   (2327kb,D)\\r\\rTitle: Best Practices for 2-Body Pose Forecasting\\rAuthors: Muhammad Rameez Ur Rahman, Luca Scofano, Edoardo De Matteis,\\r  Alessandro Flaborea, Alessio Sampieri, Fabio Galasso\\rCategories: cs.CV\\rComments: The 5th IEEE/CVF CVPR Precognition Workshop '23\\r\\\\\\\\\\r  The task of collaborative human pose forecasting stands for predicting the\\rfuture poses of multiple interacting people, given those in previous frames.\\rPredicting two people in interaction, instead of each separately, promises\\rbetter performance, due to their body-body motion correlations. But the task\\rhas remained so far primarily unexplored.\\r  In this paper, we review the progress in human pose forecasting and provide\\ran in-depth assessment of the single-person practices that perform best for\\r2-body collaborative motion forecasting. Our study confirms the positive impact\\rof frequency input representations, space-time separable and fully-learnable\\rinteraction adjacencies for the encoding GCN and FC decoding. Other\\rsingle-person practices do not transfer to 2-body, so the proposed best ones do\\rnot include hierarchical body modeling or attention-based interaction encoding.\\r  We further contribute a novel initialization procedure for the 2-body spatial\\rinteraction parameters of the encoder, which benefits performance and\\rstability. Altogether, our proposed 2-body pose forecasting best practices\\ryield a performance improvement of 21.9% over the state-of-the-art on the most\\rrecent ExPI dataset, whereby the novel initialization accounts for 3.5%. See\\rour project page at https://www.pinlab.org/bestpractices2body\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05758 ,  2327kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05772\\rDate: Wed, 12 Apr 2023 11:30:06 GMT   (29784kb,D)\\r\\rTitle: An Image Quality Assessment Dataset for Portraits\\rAuthors: Nicolas Chahine, Ana-Stefania Calarasanu, Davide Garcia-Civiero, Theo\\r  Cayla, Sira Ferradans, Jean Ponce (NYU)\\rCategories: cs.CV\\rComments: Conference on Computer Vision and Pattern Recognition 2023, IEEE/CVF,\\r  Jun 2023, Vancouver, Canada\\r\\\\\\\\\\r  Year after year, the demand for ever-better smartphone photos continues to\\rgrow, in particular in the domain of portrait photography. Manufacturers thus\\ruse perceptual quality criteria throughout the development of smartphone\\rcameras. This costly procedure can be partially replaced by automated\\rlearning-based methods for image quality assessment (IQA). Due to its\\rsubjective nature, it is necessary to estimate and guarantee the consistency of\\rthe IQA process, a characteristic lacking in the mean opinion scores (MOS)\\rwidely used for crowdsourcing IQA. In addition, existing blind IQA (BIQA)\\rdatasets pay little attention to the difficulty of cross-content assessment,\\rwhich may degrade the quality of annotations. This paper introduces PIQ23, a\\rportrait-specific IQA dataset of 5116 images of 50 predefined scenarios\\racquired by 100 smartphones, covering a high variety of brands, models, and use\\rcases. The dataset includes individuals of various genders and ethnicities who\\rhave given explicit and informed consent for their photographs to be used in\\rpublic research. It is annotated by pairwise comparisons (PWC) collected from\\rover 30 image quality experts for three image attributes: face detail\\rpreservation, face target exposure, and overall image quality. An in-depth\\rstatistical analysis of these annotations allows us to evaluate their\\rconsistency over PIQ23. Finally, we show through an extensive comparison with\\rexisting baselines that semantic information (image context) can be used to\\rimprove IQA predictions. The dataset along with the proposed statistical\\ranalysis and BIQA algorithms are available:\\rhttps://github.com/DXOMARK-Research/PIQ2023\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05772 ,  29784kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05818\\rDate: Wed, 12 Apr 2023 12:46:27 GMT   (37137kb,D)\\r\\rTitle: Gradient-Free Textual Inversion\\rAuthors: Zhengcong Fei, Mingyuan Fan, Junshi Huang\\rCategories: cs.CV\\r\\\\\\\\\\r  Recent works on personalized text-to-image generation usually learn to bind a\\rspecial token with specific subjects or styles of a few given images by tuning\\rits embedding through gradient descent. It is natural to question whether we\\rcan optimize the textual inversions by only accessing the process of model\\rinference. As only requiring the forward computation to determine the textual\\rinversion retains the benefits of less GPU memory, simple deployment, and\\rsecure access for scalable models. In this paper, we introduce a\\r\\\\emph{gradient-free} framework to optimize the continuous textual inversion in\\ran iterative evolutionary strategy. Specifically, we first initialize an\\rappropriate token embedding for textual inversion with the consideration of\\rvisual and text vocabulary information. Then, we decompose the optimization of\\revolutionary strategy into dimension reduction of searching space and\\rnon-convex gradient-free optimization in subspace, which significantly\\raccelerates the optimization process with negligible performance loss.\\rExperiments in several applications demonstrate that the performance of\\rtext-to-image model equipped with our proposed gradient-free method is\\rcomparable to that of gradient-based counterparts with variant GPU/CPU\\rplatforms, flexible employment, as well as computational efficiency.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05818 ,  37137kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05821\\rDate: Wed, 12 Apr 2023 12:59:02 GMT   (5830kb,D)\\r\\rTitle: DUFormer: A Novel Architecture for Power Line Segmentation of Aerial\\r  Images\\rAuthors: Deyu An, Qiang Zhang, Jianshu Chao, Ting Li, Feng Qiao, Yong Deng,\\r  Zhenpeng Bian, Jia Xu\\rCategories: cs.CV\\r\\\\\\\\\\r  Power lines pose a significant safety threat to unmanned aerial vehicles\\r(UAVs) operating at low altitudes. However, detecting power lines in aerial\\rimages is challenging due to the small size of the foreground data (i.e., power\\rlines) and the abundance of background information. To address this challenge,\\rwe propose DUFormer, a semantic segmentation algorithm designed specifically\\rfor power line detection in aerial images. We assume that performing sufficient\\rfeature extraction with a convolutional neural network (CNN) that has a strong\\rinductive bias is beneficial for training an efficient Transformer model. To\\rthis end, we propose a heavy token encoder responsible for overlapping feature\\rre-mining and tokenization. The encoder comprises a pyramid CNN feature\\rextraction module and a power line feature enhancement module. Following\\rsufficient feature extraction for power lines, the feature fusion is carried\\rout, and then the Transformer block is used for global modeling. The final\\rsegmentation result is obtained by fusing local and global features in the\\rdecode head. Additionally, we demonstrate the significance of the joint\\rmulti-weight loss function in power line segmentation. The experimental results\\rdemonstrate that our proposed method achieves the state-of-the-art performance\\rin power line segmentation on the publicly available TTPLA dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05821 ,  5830kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05832\\rDate: Wed, 12 Apr 2023 13:07:37 GMT   (6609kb,D)\\r\\rTitle: Few Shot Semantic Segmentation: a review of methodologies and open\\r  challenges\\rAuthors: Nico Catalano, Matteo Matteucci\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Semantic segmentation assigns category labels to each pixel in an image,\\renabling breakthroughs in fields such as autonomous driving and robotics. Deep\\rNeural Networks have achieved high accuracies in semantic segmentation but\\rrequire large training datasets. Some domains have difficulties building such\\rdatasets due to rarity, privacy concerns, and the need for skilled annotators.\\rFew-Shot Learning (FSL) has emerged as a new research stream that allows models\\rto learn new tasks from a few samples. This contribution provides an overview\\rof FSL in semantic segmentation (FSS), proposes a new taxonomy, and describes\\rcurrent limitations and outlooks.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05832 ,  6609kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05838\\rDate: Tue, 11 Apr 2023 09:42:10 GMT   (4161kb,D)\\r\\rTitle: DartsReNet: Exploring new RNN cells in ReNet architectures\\rAuthors: Brian Moser, Federico Raue, J\\\\orn Hees, Andreas Dengel\\rCategories: cs.CV cs.AI cs.LG\\rDOI: 10.1007/978-3-030-61609-0_67\\r\\\\\\\\\\r  We present new Recurrent Neural Network (RNN) cells for image classification\\rusing a Neural Architecture Search (NAS) approach called DARTS. We are\\rinterested in the ReNet architecture, which is a RNN based approach presented\\ras an alternative for convolutional and pooling steps. ReNet can be defined\\rusing any standard RNN cells, such as LSTM and GRU. One limitation is that\\rstandard RNN cells were designed for one dimensional sequential data and not\\rfor two dimensions like it is the case for image classification. We overcome\\rthis limitation by using DARTS to find new cell designs. We compare our results\\rwith ReNet that uses GRU and LSTM cells. Our found cells outperform the\\rstandard RNN cells on CIFAR-10 and SVHN. The improvements on SVHN indicate\\rgeneralizability, as we derived the RNN cell designs from CIFAR-10 without\\rperforming a new cell search for SVHN.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05838 ,  4161kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05841\\rDate: Wed, 12 Apr 2023 13:16:07 GMT   (3128kb,D)\\r\\rTitle: Exploring Diffusion Models for Unsupervised Video Anomaly Detection\\rAuthors: Anil Osman Tur and Nicola Dall'Asen and Cigdem Beyan and Elisa Ricci\\rCategories: cs.CV\\rComments: Submitted to IEEE ICIP 2023\\r\\\\\\\\\\r  This paper investigates the performance of diffusion models for video anomaly\\rdetection (VAD) within the most challenging but also the most operational\\rscenario in which the data annotations are not used. As being sparse, diverse,\\rcontextual, and often ambiguous, detecting abnormal events precisely is a very\\rambitious task. To this end, we rely only on the information-rich\\rspatio-temporal data, and the reconstruction power of the diffusion models such\\rthat a high reconstruction error is utilized to decide the abnormality.\\rExperiments performed on two large-scale video anomaly detection datasets\\rdemonstrate the consistent improvement of the proposed method over the\\rstate-of-the-art generative models while in some cases our method achieves\\rbetter scores than the more complex models. This is the first study using a\\rdiffusion model and examining its parameters' influence to present guidance for\\rVAD in surveillance scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05841 ,  3128kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05856\\rDate: Wed, 12 Apr 2023 13:39:09 GMT   (9167kb,D)\\r\\rTitle: RESET: Revisiting Trajectory Sets for Conditional Behavior Prediction\\rAuthors: Julian Schmidt, Pascal Huissel, Julian Wiederer, Julian Jordan,\\r  Vasileios Belagiannis, Klaus Dietmayer\\rCategories: cs.CV cs.AI cs.LG cs.RO\\rComments: Accepted to the 2023 Intelligent Vehicles Symposium (IV 2023)\\r\\\\\\\\\\r  It is desirable to predict the behavior of traffic participants conditioned\\ron different planned trajectories of the autonomous vehicle. This allows the\\rdownstream planner to estimate the impact of its decisions. Recent approaches\\rfor conditional behavior prediction rely on a regression decoder, meaning that\\rcoordinates or polynomial coefficients are regressed. In this work we revisit\\rset-based trajectory prediction, where the probability of each trajectory in a\\rpredefined trajectory set is determined by a classification model, and\\rfirst-time employ it to the task of conditional behavior prediction. We propose\\rRESET, which combines a new metric-driven algorithm for trajectory set\\rgeneration with a graph-based encoder. For unconditional prediction, RESET\\rachieves comparable performance to a regression-based approach. Due to the\\rnature of set-based approaches, it has the advantageous property of being able\\rto predict a flexible number of trajectories without influencing runtime or\\rcomplexity. For conditional prediction, RESET achieves reasonable results with\\rlate fusion of the planned trajectory, which was not observed for\\rregression-based approaches before. This means that RESET is computationally\\rlightweight to combine with a planner that proposes multiple future plans of\\rthe autonomous vehicle, as large parts of the forward pass can be reused.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05856 ,  9167kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05864\\rDate: Wed, 12 Apr 2023 13:56:12 GMT   (1836kb,D)\\r\\rTitle: Scale-Equivariant Deep Learning for 3D Data\\rAuthors: Thomas Wimmer, Vladimir Golkov, Hoai Nam Dang, Moritz Zaiss, Andreas\\r  Maier, Daniel Cremers\\rCategories: cs.CV cs.LG\\rComments: 12 pages, 4 figures\\r\\\\\\\\\\r  The ability of convolutional neural networks (CNNs) to recognize objects\\rregardless of their position in the image is due to the\\rtranslation-equivariance of the convolutional operation. Group-equivariant CNNs\\rtransfer this equivariance to other transformations of the input. Dealing\\rappropriately with objects and object parts of different scale is challenging,\\rand scale can vary for multiple reasons such as the underlying object size or\\rthe resolution of the imaging modality. In this paper, we propose a\\rscale-equivariant convolutional network layer for three-dimensional data that\\rguarantees scale-equivariance in 3D CNNs. Scale-equivariance lifts the burden\\rof having to learn each possible scale separately, allowing the neural network\\rto focus on higher-level learning goals, which leads to better results and\\rbetter data-efficiency. We provide an overview of the theoretical foundations\\rand scientific work on scale-equivariant neural networks in the two-dimensional\\rdomain. We then transfer the concepts from 2D to the three-dimensional space\\rand create a scale-equivariant convolutional layer for 3D data. Using the\\rproposed scale-equivariant layer, we create a scale-equivariant U-Net for\\rmedical image segmentation and compare it with a non-scale-equivariant baseline\\rmethod. Our experiments demonstrate the effectiveness of the proposed method in\\rachieving scale-equivariance for 3D medical image analysis. We publish our code\\rat https://github.com/wimmerth/scale-equivariant-3d-convnet for further\\rresearch and application.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05864 ,  1836kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05866\\rDate: Wed, 12 Apr 2023 13:56:45 GMT   (28411kb,D)\\r\\rTitle: NoisyTwins: Class-Consistent and Diverse Image Generation through\\r  StyleGANs\\rAuthors: Harsh Rangwani, Lavish Bansal, Kartik Sharma, Tejan Karmali, Varun\\r  Jampani, R. Venkatesh Babu\\rCategories: cs.CV cs.LG\\rComments: CVPR 2023. Project Page: https://rangwani-harsh.github.io/NoisyTwins/\\r\\\\\\\\\\r  StyleGANs are at the forefront of controllable image generation as they\\rproduce a latent space that is semantically disentangled, making it suitable\\rfor image editing and manipulation. However, the performance of StyleGANs\\rseverely degrades when trained via class-conditioning on large-scale\\rlong-tailed datasets. We find that one reason for degradation is the collapse\\rof latents for each class in the $\\\\mathcal{W}$ latent space. With NoisyTwins,\\rwe first introduce an effective and inexpensive augmentation strategy for class\\rembeddings, which then decorrelates the latents based on self-supervision in\\rthe $\\\\mathcal{W}$ space. This decorrelation mitigates collapse, ensuring that\\rour method preserves intra-class diversity with class-consistency in image\\rgeneration. We show the effectiveness of our approach on large-scale real-world\\rlong-tailed datasets of ImageNet-LT and iNaturalist 2019, where our method\\routperforms other methods by $\\\\sim 19\\\\%$ on FID, establishing a new\\rstate-of-the-art.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05866 ,  28411kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05868\\rDate: Wed, 12 Apr 2023 13:58:25 GMT   (11518kb,D)\\r\\rTitle: Mesh2Tex: Generating Mesh Textures from Image Queries\\rAuthors: Alexey Bokhovkin, Shubham Tulsiani, Angela Dai\\rCategories: cs.CV\\rComments: https://alexeybokhovkin.github.io/mesh2tex/\\r\\\\\\\\\\r  Remarkable advances have been achieved recently in learning neural\\rrepresentations that characterize object geometry, while generating textured\\robjects suitable for downstream applications and 3D rendering remains at an\\rearly stage. In particular, reconstructing textured geometry from images of\\rreal objects is a significant challenge -- reconstructed geometry is often\\rinexact, making realistic texturing a significant challenge. We present\\rMesh2Tex, which learns a realistic object texture manifold from uncorrelated\\rcollections of 3D object geometry and photorealistic RGB images, by leveraging\\ra hybrid mesh-neural-field texture representation. Our texture representation\\renables compact encoding of high-resolution textures as a neural field in the\\rbarycentric coordinate system of the mesh faces. The learned texture manifold\\renables effective navigation to generate an object texture for a given 3D\\robject geometry that matches to an input RGB image, which maintains robustness\\reven under challenging real-world scenarios where the mesh geometry\\rapproximates an inexact match to the underlying geometry in the RGB image.\\rMesh2Tex can effectively generate realistic object textures for an object mesh\\rto match real images observations towards digitization of real environments,\\rsignificantly improving over previous state of the art.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05868 ,  11518kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05869\\rDate: Wed, 12 Apr 2023 13:59:04 GMT   (526kb,D)\\r\\rTitle: LMR: Lane Distance-Based Metric for Trajectory Prediction\\rAuthors: Julian Schmidt, Thomas Monninger, Julian Jordan, Klaus Dietmayer\\rCategories: cs.CV cs.AI cs.LG cs.RO\\rComments: Accepted to the 2023 IEEE Intelligent Vehicles Symposium (IV 2023)\\r\\\\\\\\\\r  The development of approaches for trajectory prediction requires metrics to\\rvalidate and compare their performance. Currently established metrics are based\\ron Euclidean distance, which means that errors are weighted equally in all\\rdirections. Euclidean metrics are insufficient for structured environments like\\rroads, since they do not properly capture the agent's intent relative to the\\runderlying lane. In order to provide a reasonable assessment of trajectory\\rprediction approaches with regard to the downstream planning task, we propose a\\rnew metric that is lane distance-based: Lane Miss Rate (LMR). For the\\rcalculation of LMR, the ground-truth and predicted endpoints are assigned to\\rlane segments, more precisely their centerlines. Measured by the distance along\\rthe lane segments, predictions that are within a certain threshold distance to\\rthe ground-truth count as hits, otherwise they count as misses. LMR is then\\rdefined as the ratio of sequences that yield a miss. Our results on three\\rstate-of-the-art trajectory prediction models show that LMR preserves the order\\rof Euclidean distance-based metrics. In contrast to the Euclidean Miss Rate,\\rqualitative results show that LMR yields misses for sequences where predictions\\rare located on wrong lanes. Hits on the other hand result for sequences where\\rpredictions are located on the correct lane. This means that LMR implicitly\\rweights Euclidean error relative to the lane and goes into the direction of\\rcapturing intents of traffic agents. The source code of LMR for Argoverse 1 is\\rpublicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05869 ,  526kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05884\\rDate: Wed, 12 Apr 2023 14:25:52 GMT   (3176kb,D)\\r\\rTitle: Unicom: Universal and Compact Representation Learning for Image\\r  Retrieval\\rAuthors: Xiang An, Jiankang Deng, Kaicheng Yang, Jaiwei Li, Ziyong Feng, Jia\\r  Guo, Jing Yang, Tongliang Liu\\rCategories: cs.CV\\rComments: Accepted at ICLR2023\\r\\\\\\\\\\r  Modern image retrieval methods typically rely on fine-tuning pre-trained\\rencoders to extract image-level descriptors. However, the most widely used\\rmodels are pre-trained on ImageNet-1K with limited classes. The pre-trained\\rfeature representation is therefore not universal enough to generalize well to\\rthe diverse open-world classes. In this paper, we first cluster the large-scale\\rLAION400M into one million pseudo classes based on the joint textual and visual\\rfeatures extracted by the CLIP model. Due to the confusion of label\\rgranularity, the automatically clustered dataset inevitably contains heavy\\rinter-class conflict. To alleviate such conflict, we randomly select partial\\rinter-class prototypes to construct the margin-based softmax loss. To further\\renhance the low-dimensional feature representation, we randomly select partial\\rfeature dimensions when calculating the similarities between embeddings and\\rclass-wise prototypes. The dual random partial selections are with respect to\\rthe class dimension and the feature dimension of the prototype matrix, making\\rthe classification conflict-robust and the feature embedding compact. Our\\rmethod significantly outperforms state-of-the-art unsupervised and supervised\\rimage retrieval approaches on multiple benchmarks. The code and pre-trained\\rmodels are released to facilitate future research\\rhttps://github.com/deepglint/unicom.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05884 ,  3176kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05887\\rDate: Wed, 12 Apr 2023 14:46:57 GMT   (5467kb,D)\\r\\rTitle: Are Local Features All You Need for Cross-Domain Visual Place\\r  Recognition?\\rAuthors: Giovanni Barbarani, Mohamad Mostafa, Hajali Bayramov, Gabriele\\r  Trivigno, Gabriele Berton, Carlo Masone, Barbara Caputo\\rCategories: cs.CV\\rComments: CVPRW 2023\\r\\\\\\\\\\r  Visual Place Recognition is a task that aims to predict the coordinates of an\\rimage (called query) based solely on visual clues. Most commonly, a retrieval\\rapproach is adopted, where the query is matched to the most similar images from\\ra large database of geotagged photos, using learned global descriptors. Despite\\rrecent advances, recognizing the same place when the query comes from a\\rsignificantly different distribution is still a major hurdle for state of the\\rart retrieval methods. Examples are heavy illumination changes (e.g. night-time\\rimages) or substantial occlusions (e.g. transient objects). In this work we\\rexplore whether re-ranking methods based on spatial verification can tackle\\rthese challenges, following the intuition that local descriptors are inherently\\rmore robust than global features to domain shifts. To this end, we provide a\\rnew, comprehensive benchmark on current state of the art models. We also\\rintroduce two new demanding datasets with night and occluded queries, to be\\rmatched against a city-wide database. Code and datasets are available at\\rhttps://github.com/gbarbarani/re-ranking-for-VPR.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05887 ,  5467kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05899\\rDate: Wed, 12 Apr 2023 15:08:34 GMT   (819kb,D)\\r\\rTitle: Cancer-Net BCa-S: Breast Cancer Grade Prediction using Volumetric Deep\\r  Radiomic Features from Synthetic Correlated Diffusion Imaging\\rAuthors: Chi-en Amy Tai, Hayden Gunraj, Alexander Wong\\rCategories: cs.CV\\r\\\\\\\\\\r  The prevalence of breast cancer continues to grow, affecting about 300,000\\rfemales in the United States in 2023. However, there are different levels of\\rseverity of breast cancer requiring different treatment strategies, and hence,\\rgrading breast cancer has become a vital component of breast cancer diagnosis\\rand treatment planning. Specifically, the gold-standard Scarff-Bloom-Richardson\\r(SBR) grade has been shown to consistently indicate a patient's response to\\rchemotherapy. Unfortunately, the current method to determine the SBR grade\\rrequires removal of some cancer cells from the patient which can lead to stress\\rand discomfort along with costly expenses. In this paper, we study the efficacy\\rof deep learning for breast cancer grading based on synthetic correlated\\rdiffusion (CDI$^s$) imaging, a new magnetic resonance imaging (MRI) modality\\rand found that it achieves better performance on SBR grade prediction compared\\rto those learnt using gold-standard imaging modalities. Hence, we introduce\\rCancer-Net BCa-S, a volumetric deep radiomics approach for predicting SBR grade\\rbased on volumetric CDI$^s$ data. Given the promising results, this proposed\\rmethod to identify the severity of the cancer would allow for better treatment\\rdecisions without the need for a biopsy. Cancer-Net BCa-S has been made\\rpublicly available as part of a global open-source initiative for advancing\\rmachine learning for cancer care.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05899 ,  819kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05919\\rDate: Wed, 12 Apr 2023 15:38:23 GMT   (8690kb,D)\\r\\rTitle: Hard Patches Mining for Masked Image Modeling\\rAuthors: Haochen Wang, Kaiyou Song, Junsong Fan, Yuxi Wang, Jin Xie, Zhaoxiang\\r  Zhang\\rCategories: cs.CV\\rComments: Accepted to CVPR 2023\\r\\\\\\\\\\r  Masked image modeling (MIM) has attracted much research attention due to its\\rpromising potential for learning scalable visual representations. In typical\\rapproaches, models usually focus on predicting specific contents of masked\\rpatches, and their performances are highly related to pre-defined mask\\rstrategies. Intuitively, this procedure can be considered as training a student\\r(the model) on solving given problems (predict masked patches). However, we\\rargue that the model should not only focus on solving given problems, but also\\rstand in the shoes of a teacher to produce a more challenging problem by\\ritself. To this end, we propose Hard Patches Mining (HPM), a brand-new\\rframework for MIM pre-training. We observe that the reconstruction loss can\\rnaturally be the metric of the difficulty of the pre-training task. Therefore,\\rwe introduce an auxiliary loss predictor, predicting patch-wise losses first\\rand deciding where to mask next. It adopts a relative relationship learning\\rstrategy to prevent overfitting to exact reconstruction loss values.\\rExperiments under various settings demonstrate the effectiveness of HPM in\\rconstructing masked images. Furthermore, we empirically find that solely\\rintroducing the loss prediction objective leads to powerful representations,\\rverifying the efficacy of the ability to be aware of where is hard to\\rreconstruct.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05919 ,  8690kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05930\\rDate: Wed, 12 Apr 2023 15:50:19 GMT   (33401kb,D)\\r\\rTitle: MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to\\r  Object Segmentation\\rAuthors: Rezaul Karim, He Zhao, Richard P. Wildes, Mennatullah Siam\\rCategories: cs.CV\\rComments: Accepted in CVPR 2023\\r\\\\\\\\\\r  Multiscale video transformers have been explored in a wide variety of vision\\rtasks. To date, however, the multiscale processing has been confined to the\\rencoder or decoder alone. We present a unified multiscale encoder-decoder\\rtransformer that is focused on dense prediction tasks in videos. Multiscale\\rrepresentation at both encoder and decoder yields key benefits of implicit\\rextraction of spatiotemporal features (i.e. without reliance on input optical\\rflow) as well as temporal consistency at encoding and coarseto-fine detection\\rfor high-level (e.g. object) semantics to guide precise localization at\\rdecoding. Moreover, we propose a transductive learning scheme through\\rmany-to-many label propagation to provide temporally consistent predictions. We\\rshowcase our Multiscale Encoder-Decoder Video Transformer (MED-VT) on Automatic\\rVideo Object Segmentation (AVOS) and actor/action segmentation, where we\\routperform state-of-the-art approaches on multiple benchmarks using only raw\\rimages, without using optical flow.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05930 ,  33401kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05934\\rDate: Wed, 12 Apr 2023 15:52:53 GMT   (1273kb,D)\\r\\rTitle: ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign\\r  Language Recognition\\rAuthors: Aashaka Desai, Lauren Berger, Fyodor O. Minakov, Vanessa Milan,\\r  Chinmay Singh, Kriston Pumphrey, Richard E. Ladner, Hal Daum\\\\'e III, Alex X.\\r  Lu, Naomi Caselli, Danielle Bragg\\rCategories: cs.CV cs.CL\\r\\\\\\\\\\r  Sign languages are used as a primary language by approximately 70 million\\rD/deaf people world-wide. However, most communication technologies operate in\\rspoken and written languages, creating inequities in access. To help tackle\\rthis problem, we release ASL Citizen, the largest Isolated Sign Language\\rRecognition (ISLR) dataset to date, collected with consent and containing\\r83,912 videos for 2,731 distinct signs filmed by 52 signers in a variety of\\renvironments. We propose that this dataset be used for sign language dictionary\\rretrieval for American Sign Language (ASL), where a user demonstrates a sign to\\rtheir own webcam with the aim of retrieving matching signs from a dictionary.\\rWe show that training supervised machine learning classifiers with our dataset\\rgreatly advances the state-of-the-art on metrics relevant for dictionary\\rretrieval, achieving, for instance, 62% accuracy and a recall-at-10 of 90%,\\revaluated entirely on videos of users who are not present in the training or\\rvalidation sets. An accessible PDF of this article is available at\\rhttps://aashakadesai.github.io/research/ASL_Dataset__arxiv_.pdf\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05934 ,  1273kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05939\\rDate: Wed, 12 Apr 2023 16:03:36 GMT   (22398kb,D)\\r\\rTitle: Explicitly Minimizing the Blur Error of Variational Autoencoders\\rAuthors: Gustav Bredell, Kyriakos Flouris, Krishna Chaitanya, Ertunc Erdil,\\r  Ender Konukoglu\\rCategories: cs.CV cs.LG eess.IV\\rComments: Accepted to ICLR 2023\\r\\\\\\\\\\r  Variational autoencoders (VAEs) are powerful generative modelling methods,\\rhowever they suffer from blurry generated samples and reconstructions compared\\rto the images they have been trained on. Significant research effort has been\\rspent to increase the generative capabilities by creating more flexible models\\rbut often flexibility comes at the cost of higher complexity and computational\\rcost. Several works have focused on altering the reconstruction term of the\\revidence lower bound (ELBO), however, often at the expense of losing the\\rmathematical link to maximizing the likelihood of the samples under the modeled\\rdistribution. Here we propose a new formulation of the reconstruction term for\\rthe VAE that specifically penalizes the generation of blurry images while at\\rthe same time still maximizing the ELBO under the modeled distribution. We show\\rthe potential of the proposed loss on three different data sets, where it\\routperforms several recently proposed reconstruction losses for VAEs.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05939 ,  22398kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05947\\rDate: Wed, 12 Apr 2023 16:15:05 GMT   (13969kb,D)\\r\\rTitle: Visual Localization using Imperfect 3D Models from the Internet\\rAuthors: Vojtech Panek, Zuzana Kukelova, Torsten Sattler\\rCategories: cs.CV\\rComments: to be presented at CVPR 2023\\rACM-class: I.2.10; I.4.8; I.4.9\\r\\\\\\\\\\r  Visual localization is a core component in many applications, including\\raugmented reality (AR). Localization algorithms compute the camera pose of a\\rquery image w.r.t. a scene representation, which is typically built from\\rimages. This often requires capturing and storing large amounts of data,\\rfollowed by running Structure-from-Motion (SfM) algorithms. An interesting, and\\runderexplored, source of data for building scene representations are 3D models\\rthat are readily available on the Internet, e.g., hand-drawn CAD models, 3D\\rmodels generated from building footprints, or from aerial images. These models\\rallow to perform visual localization right away without the time-consuming\\rscene capturing and model building steps. Yet, it also comes with challenges as\\rthe available 3D models are often imperfect reflections of reality. E.g., the\\rmodels might only have generic or no textures at all, might only provide a\\rsimple approximation of the scene geometry, or might be stretched. This paper\\rstudies how the imperfections of these models affect localization accuracy. We\\rcreate a new benchmark for this task and provide a detailed experimental\\revaluation based on multiple 3D models per scene. We show that 3D models from\\rthe Internet show promise as an easy-to-obtain scene representation. At the\\rsame time, there is significant room for improvement for visual localization\\rpipelines. To foster research on this interesting and challenging task, we\\rrelease our benchmark at v-pnk.github.io/cadloc.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05947 ,  13969kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05956\\rDate: Wed, 12 Apr 2023 16:28:29 GMT   (6534kb,D)\\r\\rTitle: OO-dMVMT: A Deep Multi-view Multi-task Classification Framework for\\r  Real-time 3D Hand Gesture Classification and Segmentation\\rAuthors: Federico Cunico, Federico Girella, Andrea Avogaro, Marco Emporio,\\r  Andrea Giachetti and Marco Cristani\\rCategories: cs.CV\\rComments: Accepted to the Computer Vision for Mixed Reality workshop at CVPR\\r  2023\\r\\\\\\\\\\r  Continuous mid-air hand gesture recognition based on captured hand pose\\rstreams is fundamental for human-computer interaction, particularly in AR / VR.\\rHowever, many of the methods proposed to recognize heterogeneous hand gestures\\rare tested only on the classification task, and the real-time low-latency\\rgesture segmentation in a continuous stream is not well addressed in the\\rliterature. For this task, we propose the On-Off deep Multi-View Multi-Task\\rparadigm (OO-dMVMT). The idea is to exploit multiple time-local views related\\rto hand pose and movement to generate rich gesture descriptions, along with\\rusing heterogeneous tasks to achieve high accuracy. OO-dMVMT extends the\\rclassical MVMT paradigm, where all of the multiple tasks have to be active at\\reach time, by allowing specific tasks to switch on/off depending on whether\\rthey can apply to the input. We show that OO-dMVMT defines the new SotA on\\rcontinuous/online 3D skeleton-based gesture recognition in terms of gesture\\rclassification accuracy, segmentation accuracy, false positives, and decision\\rlatency while maintaining real-time operation.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05956 ,  6534kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05961\\rDate: Wed, 12 Apr 2023 16:32:34 GMT   (9118kb,D)\\r\\rTitle: SpectralDiff: Hyperspectral Image Classification with Spectral-Spatial\\r  Diffusion Models\\rAuthors: Ning Chen, Jun Yue, Leyuan Fang, Shaobo Xia\\rCategories: cs.CV\\rComments: This work has been submitted to the IEEE for possible publication.\\r  Copyright may be transferred without notice, after which this version may no\\r  longer be accessible\\r\\\\\\\\\\r  Hyperspectral image (HSI) classification is an important topic in the field\\rof remote sensing, and has a wide range of applications in Earth science. HSIs\\rcontain hundreds of continuous bands, which are characterized by high dimension\\rand high correlation between adjacent bands. The high dimension and redundancy\\rof HSI data bring great difficulties to HSI classification. In recent years, a\\rlarge number of HSI feature extraction and classification methods based on deep\\rlearning have been proposed. However, their ability to model the global\\rrelationships among samples in both spatial and spectral domains is still\\rlimited. In order to solve this problem, an HSI classification method with\\rspectral-spatial diffusion models is proposed. The proposed method realizes the\\rreconstruction of spectral-spatial distribution of the training samples with\\rthe forward and reverse spectral-spatial diffusion process, thus modeling the\\rglobal spatial-spectral relationship between samples. Then, we use the\\rspectral-spatial denoising network of the reverse process to extract the\\runsupervised diffusion features. Features extracted by the spectral-spatial\\rdiffusion models can achieve cross-sample perception from the reconstructed\\rdistribution of the training samples, thus obtaining better classification\\rperformance. Experiments on three public HSI datasets show that the proposed\\rmethod can achieve better performance than the state-of-the-art methods. The\\rsource code and the pre-trained spectral-spatial diffusion model will be\\rpublicly available at https://github.com/chenning0115/SpectralDiff.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05961 ,  9118kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05977\\rDate: Wed, 12 Apr 2023 16:58:13 GMT   (17159kb,D)\\r\\rTitle: ImageReward: Learning and Evaluating Human Preferences for Text-to-Image\\r  Generation\\rAuthors: Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding,\\r  Jie Tang, Yuxiao Dong\\rCategories: cs.CV cs.LG\\rComments: 24 pages\\r\\\\\\\\\\r  We present ImageReward -- the first general-purpose text-to-image human\\rpreference reward model -- to address various prevalent issues in generative\\rmodels and align them with human values and preferences. Its training is based\\ron our systematic annotation pipeline that covers both the rating and ranking\\rcomponents, collecting a dataset of 137k expert comparisons to date. In human\\revaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by\\r38.6\\\\%), making it a promising automatic metric for evaluating and improving\\rtext-to-image synthesis. The reward model is publicly available via the\\r\\\\texttt{image-reward} package at \\\\url{https://github.com/THUDM/ImageReward}.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05977 ,  17159kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05995\\rDate: Wed, 12 Apr 2023 17:20:37 GMT   (7258kb,D)\\r\\rTitle: APPLeNet: Visual Attention Parameterized Prompt Learning for Few-Shot\\r  Remote Sensing Image Generalization using CLIP\\rAuthors: Mainak Singha, Ankit Jha, Bhupendra Solanki, Shirsha Bose and Biplab\\r  Banerjee\\rCategories: cs.CV\\rComments: 11 Pages, 6 figures, 8 tables, Accepted in Earth Vision (CVPR 2023)\\r\\\\\\\\\\r  In recent years, the success of large-scale vision-language models (VLMs)\\rsuch as CLIP has led to their increased usage in various computer vision tasks.\\rThese models enable zero-shot inference through carefully crafted instructional\\rtext prompts without task-specific supervision. However, the potential of VLMs\\rfor generalization tasks in remote sensing (RS) has not been fully realized. To\\raddress this research gap, we propose a novel image-conditioned prompt learning\\rstrategy called the Visual Attention Parameterized Prompts Learning Network\\r(APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learning\\rin RS scene classification and disentangles visual style and content primitives\\rfor domain generalization tasks. To achieve this, APPLeNet combines visual\\rcontent features obtained from different layers of the vision encoder and style\\rproperties obtained from feature statistics of domain-specific batches. An\\rattention-driven injection module is further introduced to generate visual\\rtokens from this information. We also introduce an anti-correlation regularizer\\rto ensure discrimination among the token embeddings, as this visual information\\ris combined with the textual tokens. To validate APPLeNet, we curated four\\ravailable RS benchmarks and introduced experimental protocols and datasets for\\rthree domain generalization tasks. Our results consistently outperform the\\rrelevant literature and code is available at\\rhttps://github.com/mainaksingha01/APPLeNet\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05995 ,  7258kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06002\\rDate: Wed, 12 Apr 2023 17:28:30 GMT   (713kb)\\r\\rTitle: Fast vehicle detection algorithm based on lightweight YOLO7-tiny\\rAuthors: Bo Li, YiHua Chen and Hao Xu\\rCategories: cs.CV\\r\\\\\\\\\\r  The swift and precise detection of vehicles holds significant research\\rsignificance in intelligent transportation systems (ITS). However, current\\rvehicle detection algorithms encounter challenges such as high computational\\rcomplexity, low detection rate, and limited feasibility on mobile devices. To\\raddress these issues, this paper proposes a lightweight vehicle detection\\ralgorithm for YOLOv7-tiny called Ghost-YOLOv7. The model first scales the width\\rmultiple to 0.5 and replaces the standard convolution of the backbone network\\rwith Ghost convolution to achieve a lighter network and improve the detection\\rspeed; secondly, a Ghost bi-directional feature pyramid network (Ghost-BiFPN)\\rneck network is designed to enhance feature extraction capability of the\\ralgorithm and enrich semantic information; thirdly, a Ghost Decouoled Head\\r(GDH) is employed for accurate prediction of vehicle location and class,\\renhancing model accuracy; finally, a coordinate attention mechanism is\\rintroduced in the output layer to suppress environmental interference, and the\\rWIoU loss function is employed to enhance the detection accuracy further.\\rExperimental results on the PASCAL VOC dataset demonstrate that Ghost-YOLOv7\\routperforms the original YOLOv7-tiny model, achieving a 29.8% reduction in\\rcomputation, 37.3% reduction in the number of parameters, 35.1% reduction in\\rmodel weights, and 1.1% higher mean average precision (mAP), while achieving a\\rdetection speed of 428 FPS. These results validate the effectiveness of the\\rproposed method.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06002 ,  713kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06007\\rDate: Wed, 12 Apr 2023 17:32:18 GMT   (8049kb,D)\\r\\rTitle: GPr-Net: Geometric Prototypical Network for Point Cloud Few-Shot\\r  Learning\\rAuthors: Tejas Anvekar, Dena Bazazian\\rCategories: cs.CV\\r\\\\\\\\\\r  In the realm of 3D-computer vision applications, point cloud few-shot\\rlearning plays a critical role. However, it poses an arduous challenge due to\\rthe sparsity, irregularity, and unordered nature of the data. Current methods\\rrely on complex local geometric extraction techniques such as convolution,\\rgraph, and attention mechanisms, along with extensive data-driven pre-training\\rtasks. These approaches contradict the fundamental goal of few-shot learning,\\rwhich is to facilitate efficient learning. To address this issue, we propose\\rGPr-Net (Geometric Prototypical Network), a lightweight and computationally\\refficient geometric prototypical network that captures the intrinsic topology\\rof point clouds and achieves superior performance. Our proposed method, IGI++\\r(Intrinsic Geometry Interpreter++) employs vector-based hand-crafted intrinsic\\rgeometry interpreters and Laplace vectors to extract and evaluate point cloud\\rmorphology, resulting in improved representations for FSL (Few-Shot Learning).\\rAdditionally, Laplace vectors enable the extraction of valuable features from\\rpoint clouds with fewer points. To tackle the distribution drift challenge in\\rfew-shot metric learning, we leverage hyperbolic space and demonstrate that our\\rapproach handles intra and inter-class variance better than existing point\\rcloud few-shot learning methods. Experimental results on the ModelNet40 dataset\\rshow that GPr-Net outperforms state-of-the-art methods in few-shot learning on\\rpoint clouds, achieving utmost computational efficiency that is $170\\\\times$\\rbetter than all existing works. The code is publicly available at\\rhttps://github.com/TejasAnvekar/GPr-Net.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06007 ,  8049kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06009\\rDate: Wed, 12 Apr 2023 17:33:41 GMT   (287kb,D)\\r\\rTitle: Literature Review: Computer Vision Applications in Transportation\\r  Logistics and Warehousing\\rAuthors: Alexander Naumann, Felix Hertlein, Laura Doerr, Steffen Thoma, Kai\\r  Furmans\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Computer vision applications in transportation logistics and warehousing have\\ra huge potential for process automation. We present a structured literature\\rreview on research in the field to help leverage this potential. All literature\\ris categorized w.r.t. the application, i.e. the task it tackles and w.r.t. the\\rcomputer vision techniques that are used. Regarding applications, we subdivide\\rthe literature in two areas: Monitoring, i.e. observing and retrieving relevant\\rinformation from the environment, and manipulation, where approaches are used\\rto analyze and interact with the environment. In addition to that, we point out\\rdirections for future research and link to recent developments in computer\\rvision that are suitable for application in logistics. Finally, we present an\\roverview of existing datasets and industrial solutions. We conclude that while\\ralready many research areas have been investigated, there is still huge\\rpotential for future research. The results of our analysis are also available\\ronline at https://a-nau.github.io/cv-in-logistics.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06009 ,  287kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06018\\rDate: Wed, 12 Apr 2023 17:55:59 GMT   (12476kb,D)\\r\\rTitle: Adaptive Human Matting for Dynamic Videos\\rAuthors: Chung-Ching Lin, Jiang Wang, Kun Luo, Kevin Lin, Linjie Li, Lijuan\\r  Wang, Zicheng Liu\\rCategories: cs.CV\\rComments: CVPR 2023\\r\\\\\\\\\\r  The most recent efforts in video matting have focused on eliminating trimap\\rdependency since trimap annotations are expensive and trimap-based methods are\\rless adaptable for real-time applications. Despite the latest tripmap-free\\rmethods showing promising results, their performance often degrades when\\rdealing with highly diverse and unstructured videos. We address this limitation\\rby introducing Adaptive Matting for Dynamic Videos, termed AdaM, which is a\\rframework designed for simultaneously differentiating foregrounds from\\rbackgrounds and capturing alpha matte details of human subjects in the\\rforeground. Two interconnected network designs are employed to achieve this\\rgoal: (1) an encoder-decoder network that produces alpha mattes and\\rintermediate masks which are used to guide the transformer in adaptively\\rdecoding foregrounds and backgrounds, and (2) a transformer network in which\\rlong- and short-term attention combine to retain spatial and temporal contexts,\\rfacilitating the decoding of foreground details. We benchmark and study our\\rmethods on recently introduced datasets, showing that our model notably\\rimproves matting realism and temporal coherence in complex real-world videos\\rand achieves new best-in-class generalizability. Further details and examples\\rare available at https://github.com/microsoft/AdaM.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06018 ,  12476kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06019\\rDate: Wed, 12 Apr 2023 17:56:42 GMT   (32433kb,D)\\r\\rTitle: Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image\\r  Restoration in Under-Display Camera\\rAuthors: Ruicheng Feng, Chongyi Li, Huaijin Chen, Shuai Li, Jinwei Gu, Chen\\r  Change Loy\\rCategories: cs.CV eess.IV\\rComments: Accepted by CVPR 2023\\r\\\\\\\\\\r  Due to the difficulty in collecting large-scale and perfectly aligned paired\\rtraining data for Under-Display Camera (UDC) image restoration, previous\\rmethods resort to monitor-based image systems or simulation-based methods,\\rsacrificing the realness of the data and introducing domain gaps. In this work,\\rwe revisit the classic stereo setup for training data collection -- capturing\\rtwo images of the same scene with one UDC and one standard camera. The key idea\\ris to copy details from a high-quality reference image and paste them on\\rthe UDC image. While being able to generate real training pairs, this setting\\ris susceptible to spatial misalignment due to perspective and depth of field\\rchanges. The problem is further compounded by the large domain discrepancy\\rbetween the UDC and normal images, which is unique to UDC restoration. In this\\rpaper, we mitigate the non-trivial domain discrepancy and spatial misalignment\\rthrough a novel Transformer-based framework that generates well-aligned yet\\rhigh-quality target data for the corresponding UDC input. This is made possible\\rthrough two carefully designed components, namely, the Domain Alignment Module\\r(DAM) and Geometric Alignment Module (GAM), which encourage robust and accurate\\rdiscovery of correspondence between the UDC and normal views. Extensive\\rexperiments show that high-quality and well-aligned pseudo UDC training pairs\\rare beneficial for training a robust restoration network. Code and the dataset\\rare available at https://github.com/jnjaby/AlignFormer.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06019 ,  32433kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06020\\rDate: Wed, 12 Apr 2023 17:57:15 GMT   (12896kb,D)\\r\\rTitle: VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs\\rAuthors: Moayed Haji Ali, Andrew Bond, Tolga Birdal, Duygu Ceylan, Levent\\r  Karacan, Erkut Erdem, and Aykut Erdem\\rCategories: cs.CV\\r\\\\\\\\\\r  We propose $\\\\textbf{VidStyleODE}$, a spatiotemporally continuous disentangled\\r$\\\\textbf{Vid}$eo representation based upon $\\\\textbf{Style}$GAN and\\rNeural-$\\\\textbf{ODE}$s. Effective traversal of the latent space learned by\\rGenerative Adversarial Networks (GANs) has been the basis for recent\\rbreakthroughs in image editing. However, the applicability of such advancements\\rto the video domain has been hindered by the difficulty of representing and\\rcontrolling videos in the latent space of GANs. In particular, videos are\\rcomposed of content (i.e., appearance) and complex motion components that\\rrequire a special mechanism to disentangle and control. To achieve this,\\rVidStyleODE encodes the video content in a pre-trained StyleGAN $\\\\mathcal{W}_+$\\rspace and benefits from a latent ODE component to summarize the spatiotemporal\\rdynamics of the input video. Our novel continuous video generation process then\\rcombines the two to generate high-quality and temporally consistent videos with\\rvarying frame rates. We show that our proposed method enables a variety of\\rapplications on real videos: text-guided appearance manipulation, motion\\rmanipulation, image animation, and video interpolation and extrapolation.\\rProject website: https://cyberiada.github.io/VidStyleODE\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06020 ,  12896kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06021\\rDate: Wed, 12 Apr 2023 17:57:48 GMT   (21334kb,D)\\r\\rTitle: Crowd Counting with Sparse Annotation\\rAuthors: Shiwei Zhang, Zhengzheng Wang, Qing Liu, Fei Wang, Wei Ke, Tong Zhang\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  This paper presents a new annotation method called Sparse Annotation (SA) for\\rcrowd counting, which reduces human labeling efforts by sparsely labeling\\rindividuals in an image. We argue that sparse labeling can reduce the\\rredundancy of full annotation and capture more diverse information from distant\\rindividuals that is not fully captured by Partial Annotation methods. Besides,\\rwe propose a point-based Progressive Point Matching network (PPM) to better\\rexplore the crowd from the whole image with sparse annotation, which includes a\\rProposal Matching Network (PMN) and a Performance Restoration Network (PRN).\\rThe PMN generates pseudo-point samples using a basic point classifier, while\\rthe PRN refines the point classifier with the pseudo points to maximize\\rperformance. Our experimental results show that PPM outperforms previous\\rsemi-supervised crowd counting methods with the same amount of annotation by a\\rlarge margin and achieves competitive performance with state-of-the-art\\rfully-supervised methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06021 ,  21334kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06022\\rDate: Wed, 12 Apr 2023 17:58:03 GMT   (3054kb,D)\\r\\rTitle: SAM Struggles in Concealed Scenes -- Empirical Study on Segment\\r  Anything\\rAuthors: Ge-Peng Ji, Deng-Ping Fan, Peng Xu, Ming-Ming Cheng, Bowen Zhou, Luc\\r  Van Gool\\rCategories: cs.CV\\rComments: Report\\r\\\\\\\\\\r  Segmenting anything is a ground-breaking step toward artificial general\\rintelligence, and the Segment Anything Model (SAM) greatly fosters the\\rfoundation models for computer vision. We could not be more excited to probe\\rthe performance traits of SAM. In particular, exploring situations in which SAM\\rdoes not perform well is interesting. In this report, we choose three concealed\\rscenes, i.e., camouflaged animals, industrial defects, and medical lesions, to\\revaluate SAM under unprompted settings. Our main observation is that SAM looks\\runskilled in concealed scenes.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06022 ,  3054kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06024\\rDate: Wed, 12 Apr 2023 17:58:57 GMT   (20996kb,D)\\r\\rTitle: Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views\\rAuthors: Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker,\\r  Siyu Tang\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Automatic perception of human behaviors during social interactions is crucial\\rfor AR/VR applications, and an essential component is estimation of plausible\\r3D human pose and shape of our social partners from the egocentric view. One of\\rthe biggest challenges of this task is severe body truncation due to close\\rsocial distances in egocentric scenarios, which brings large pose ambiguities\\rfor unseen body parts. To tackle this challenge, we propose a novel\\rscene-conditioned diffusion method to model the body pose distribution.\\rConditioned on the 3D scene geometry, the diffusion model generates bodies in\\rplausible human-scene interactions, with the sampling guided by a physics-based\\rcollision score to further resolve human-scene inter-penetrations. The\\rclassifier-free training enables flexible sampling with different conditions\\rand enhanced diversity. A visibility-aware graph convolution model guided by\\rper-joint visibility serves as the diffusion denoiser to incorporate\\rinter-joint dependencies and per-body-part control. Extensive evaluations show\\rthat our method generates bodies in plausible interactions with 3D scenes,\\rachieving both superior accuracy for visible joints and diversity for invisible\\rbody parts. The code will be available at\\rhttps://sanweiliti.github.io/egohmr/egohmr.html.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06024 ,  20996kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06025\\rDate: Wed, 12 Apr 2023 17:59:17 GMT   (13856kb,D)\\r\\rTitle: DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion\\rAuthors: Johanna Karras, Aleksander Holynski, Ting-Chun Wang, Ira\\r  Kemelmacher-Shlizerman\\rCategories: cs.CV\\rComments: Project page: https://grail.cs.washington.edu/projects/dreampose/\\r\\\\\\\\\\r  We present DreamPose, a diffusion-based method for generating animated\\rfashion videos from still images. Given an image and a sequence of human body\\rposes, our method synthesizes a video containing both human and fabric motion.\\rTo achieve this, we transform a pretrained text-to-image model (Stable\\rDiffusion) into a pose-and-image guided video synthesis model, using a novel\\rfinetuning strategy, a set of architectural changes to support the added\\rconditioning signals, and techniques to encourage temporal consistency. We\\rfine-tune on a collection of fashion videos from the UBC Fashion dataset. We\\revaluate our method on a variety of clothing styles and poses, and demonstrate\\rthat our method produces state-of-the-art results on fashion video animation.\\rVideo results are available on our project page.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06025 ,  13856kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06027\\rDate: Wed, 12 Apr 2023 17:59:41 GMT   (18540kb,D)\\r\\rTitle: Continual Diffusion: Continual Customization of Text-to-Image Diffusion\\r  with C-LoRA\\rAuthors: James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira,\\r  Yilin Shen, Hongxia Jin\\rCategories: cs.CV cs.AI cs.LG\\rComments: Project page: https://jamessealesmith.github.io/continual-diffusion/\\r\\\\\\\\\\r  Recent works demonstrate a remarkable ability to customize text-to-image\\rdiffusion models while only providing a few example images. What happens if you\\rtry to customize such models using multiple, fine-grained concepts in a\\rsequential (i.e., continual) manner? In our work, we show that recent\\rstate-of-the-art customization of text-to-image models suffer from catastrophic\\rforgetting when new concepts arrive sequentially. Specifically, when adding a\\rnew concept, the ability to generate high quality images of past, similar\\rconcepts degrade. To circumvent this forgetting, we propose a new method,\\rC-LoRA, composed of a continually self-regularized low-rank adaptation in cross\\rattention layers of the popular Stable Diffusion model. Furthermore, we use\\rcustomization prompts which do not include the word of the customized object\\r(i.e., person for a human face dataset) and are initialized as completely\\rrandom embeddings. Importantly, our method induces only marginal additional\\rparameter costs and requires no storage of user data for replay. We show that\\rC-LoRA not only outperforms several baselines for our proposed setting of\\rtext-to-image continual customization, which we refer to as Continual\\rDiffusion, but that we achieve a new state-of-the-art in the well-established\\rrehearsal-free continual learning setting for image classification. The high\\rachieving performance of C-LoRA in two separate domains positions it as a\\rcompelling solution for a wide range of applications, and we believe it has\\rsignificant potential for practical impact.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06027 ,  18540kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.06028\\rDate: Wed, 12 Apr 2023 17:59:58 GMT   (1673kb,D)\\r\\rTitle: RECLIP: Resource-efficient CLIP by Training with Small Images\\rAuthors: Runze Li, Dahun Kim, Bir Bhanu, Weicheng Kuo\\rCategories: cs.CV\\r\\\\\\\\\\r  We present RECLIP (Resource-efficient CLIP), a simple method that minimizes\\rcomputational resource footprint for CLIP (Contrastive Language Image\\rPretraining). Inspired by the notion of coarse-to-fine in computer vision, we\\rleverage small images to learn from large-scale language supervision\\refficiently, and finetune the model with high-resolution data in the end. Since\\rthe complexity of the vision transformer heavily depends on input image size,\\rour approach significantly reduces the training resource requirements both in\\rtheory and in practice. Using the same batch size and training epoch, RECLIP\\rachieves highly competitive zero-shot classification and image text retrieval\\raccuracy with 6 to 8$\\\\times$ less computational resources and 7 to 9$\\\\times$\\rfewer FLOPs than the baseline. Compared to the state-of-the-art contrastive\\rlearning methods, RECLIP demonstrates 5 to 59$\\\\times$ training resource savings\\rwhile maintaining highly competitive zero-shot classification and retrieval\\rperformance. We hope this work will pave the path for the broader research\\rcommunity to explore language supervised pretraining in more resource-friendly\\rsettings.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.06028 ,  1673kb)\\r%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.04011 (*cross-listing*)\\rDate: Tue, 7 Mar 2023 16:25:41 GMT   (12795kb,D)\\rDate (revised v2): Tue, 11 Apr 2023 17:48:25 GMT   (12795kb,D)\\r\\rTitle: One-4-All: Neural Potential Fields for Embodied Navigation\\rAuthors: Sacha Morin, Miguel Saavedra-Ruiz, Liam Paull\\rCategories: cs.RO cs.CV cs.LG\\rComments: Sacha Morin and Miguel Saavedra-Ruiz contributed equally. Submitted\\r  to the IEEE/RSJ International Conference on Intelligent Robots (IROS 2023)\\r\\\\\\\\\\r  A fundamental task in robotics is to navigate between two locations. In\\rparticular, real-world navigation can require long-horizon planning using\\rhigh-dimensional RGB images, which poses a substantial challenge for end-to-end\\rlearning-based approaches. Current semi-parametric methods instead achieve\\rlong-horizon navigation by combining learned modules with a topological memory\\rof the environment, often represented as a graph over previously collected\\rimages. However, using these graphs in practice typically involves tuning a\\rnumber of pruning heuristics to avoid spurious edges, limit runtime memory\\rusage and allow reasonably fast graph queries. In this work, we present\\rOne-4-All (O4A), a method leveraging self-supervised and manifold learning to\\robtain a graph-free, end-to-end navigation pipeline in which the goal is\\rspecified as an image. Navigation is achieved by greedily minimizing a\\rpotential function defined continuously over the O4A latent space. Our system\\ris trained offline on non-expert exploration sequences of RGB data and\\rcontrols, and does not require any depth or pose measurements. We show that O4A\\rcan reach long-range goals in 8 simulated Gibson indoor environments, and\\rfurther demonstrate successful real-world navigation using a Jackal UGV\\rplatform.\\r\\\\\\\\ ( https://arxiv.org/abs/2303.04011 ,  12795kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05396 (*cross-listing*)\\rDate: Mon, 10 Apr 2023 18:20:29 GMT   (2075kb,D)\\r\\rTitle: SAM.MD: Zero-shot medical image segmentation capabilities of the Segment\\r  Anything Model\\rAuthors: Saikat Roy, Tassilo Wald, Gregor Koehler, Maximilian R. Rokuss, Nico\\r  Disch, Julius Holzschuh, David Zimmerer, Klaus H. Maier-Hein\\rCategories: eess.IV cs.CV cs.LG\\rComments: 3 Pages, 1 Figure, Short paper under review for MIDL 2023\\r\\\\\\\\\\r  Foundation models have taken over natural language processing and image\\rgeneration domains due to the flexibility of prompting. With the recent\\rintroduction of the Segment Anything Model (SAM), this prompt-driven paradigm\\rhas entered image segmentation with a hitherto unexplored abundance of\\rcapabilities. The purpose of this paper is to conduct an initial evaluation of\\rthe out-of-the-box zero-shot capabilities of SAM for medical image\\rsegmentation, by evaluating its performance on an abdominal CT organ\\rsegmentation task, via point or bounding box based prompting. We show that SAM\\rgeneralizes well to CT data, making it a potential catalyst for the advancement\\rof semi-automatic segmentation tools for clinicians. We believe that this\\rfoundation model, while not reaching state-of-the-art segmentation performance\\rin our investigations, can serve as a highly potent starting point for further\\radaptations of such models to the intricacies of the medical domain. Keywords:\\rmedical image segmentation, SAM, foundation models, zero-shot learning\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05396 ,  2075kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05405 (*cross-listing*)\\rDate: Tue, 11 Apr 2023 13:15:29 GMT   (5107kb,D)\\r\\rTitle: Efficient Automation of Neural Network Design: A Survey on\\r  Differentiable Neural Architecture Search\\rAuthors: Alexandre Heuillet, Ahmad Nasser, Hichem Arioui, Hedi Tabia\\rCategories: cs.LG cs.AI cs.CV\\rComments: Under review at ACM Computing Surveys. 35 pages, 16 figures\\r\\\\\\\\\\r  In the past few years, Differentiable Neural Architecture Search (DNAS)\\rrapidly imposed itself as the trending approach to automate the discovery of\\rdeep neural network architectures. This rise is mainly due to the popularity of\\rDARTS, one of the first major DNAS methods. In contrast with previous works\\rbased on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster by\\rseveral orders of magnitude and uses fewer computational resources. In this\\rcomprehensive survey, we focus specifically on DNAS and review recent\\rapproaches in this field. Furthermore, we propose a novel challenge-based\\rtaxonomy to classify DNAS methods. We also discuss the contributions brought to\\rDNAS in the past few years and its impact on the global NAS field. Finally, we\\rconclude by giving some insights into future research directions for the DNAS\\rfield.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05405 ,  5107kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05482 (*cross-listing*)\\rDate: Tue, 11 Apr 2023 20:28:33 GMT   (5355kb,D)\\r\\rTitle: Computational Pathology: A Survey Review and The Way Forward\\rAuthors: Mahdi S. Hosseini, Babak Ehteshami Bejnordi, Vincent Quoc-Huy Trinh,\\r  Danial Hasan, Xingwen Li, Taehyo Kim, Haochen Zhang, Theodore Wu, Kajanan\\r  Chinniah, Sina Maghsoudlou, Ryan Zhang, Stephen Yang, Jiadai Zhu, Lyndon\\r  Chan, Samir Khaki, Andrei Buin, Fatemeh Chaji, Ala Salehi, Alejandra Zambrano\\r  Luna, Bich Ngoc Nguyen, Dimitris Samaras and Konstantinos N. Plataniotis\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Computational Pathology (CoPath) is an interdisciplinary science that\\raugments developments of computational approaches to analyze and model medical\\rhistopathology images. The main objective for CoPath is to develop\\rinfrastructure and workflows of digital diagnostics as an assistive CAD system\\rfor clinical pathology facilitating transformational changes in the diagnosis\\rand treatment of cancer diseases. With evergrowing developments in deep\\rlearning and computer vision algorithms, and the ease of the data flow from\\rdigital pathology, currently CoPath is witnessing a paradigm shift. Despite the\\rsheer volume of engineering and scientific works being introduced for cancer\\rimage analysis, there is still a considerable gap of adopting and integrating\\rthese algorithms in clinical practice. This raises a significant question\\rregarding the direction and trends that are undertaken in CoPath. In this\\rarticle we provide a comprehensive review of more than 700 papers to address\\rthe challenges faced in problem design all-the-way to the application and\\rimplementation viewpoints. We have catalogued each paper into a model-card by\\rexamining the key works and challenges faced to layout the current landscape in\\rCoPath. We hope this helps the community to locate relevant works and\\rfacilitate understanding of the field's future directions. In a nutshell, we\\roversee the CoPath developments in cycle of stages which are required to be\\rcohesively linked together to address the challenges associated with such\\rmultidisciplinary science. We overview this cycle from different perspectives\\rof data-centric, model-centric, and application-centric problems. We finally\\rsketch remaining challenges and provide directions for future technical\\rdevelopments and clinical integration of CoPath.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05482 ,  5355kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05547 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 00:43:30 GMT   (550kb,D)\\r\\rTitle: Taxonomic Class Incremental Learning\\rAuthors: Yuzhao Chen, Zonghuan Li, Zhiyuan Hu, Nuno Vasconcelos\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  The problem of continual learning has attracted rising attention in recent\\ryears. However, few works have questioned the commonly used learning setup,\\rbased on a task curriculum of random class. This differs significantly from\\rhuman continual learning, which is guided by taxonomic curricula. In this work,\\rwe propose the Taxonomic Class Incremental Learning (TCIL) problem. In TCIL,\\rthe task sequence is organized based on a taxonomic class tree. We unify\\rexisting approaches to CIL and taxonomic learning as parameter inheritance\\rschemes and introduce a new such scheme for the TCIL learning. This enables the\\rincremental transfer of knowledge from ancestor to descendant class of a class\\rtaxonomy through parameter inheritance. Experiments on CIFAR-100 and\\rImageNet-100 show the effectiveness of the proposed TCIL method, which\\routperforms existing SOTA methods by 2% in terms of final accuracy on CIFAR-100\\rand 3% on ImageNet-100.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05547 ,  550kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05565 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 01:57:08 GMT   (719kb)\\r\\rTitle: A Predictive Model using Machine Learning Algorithm in Identifying\\r  Students Probability on Passing Semestral Course\\rAuthors: Anabella C. Doctor\\rCategories: cs.LG cs.CV cs.CY\\rJournal-ref: International Journal of Computing Sciences Research (ISSN print:\\r  2546-0552; ISSN online: 2546-115X) Vol. 7, pp. 1830-1856\\rDOI: 10.25147/ijcsr.2017.001.1.135\\r\\\\\\\\\\r  This study aims to determine a predictive model to learn students probability\\rto pass their courses taken at the earliest stage of the semester. To\\rsuccessfully discover a good predictive model with high acceptability,\\raccurate, and precision rate which delivers a useful outcome for decision\\rmaking in education systems, in improving the processes of conveying knowledge\\rand uplifting students academic performance, the proponent applies and strictly\\rfollowed the CRISP-DM (Cross-Industry Standard Process for Data Mining)\\rmethodology. This study employs classification for data mining techniques, and\\rdecision tree for algorithm. With the utilization of the newly discovered\\rpredictive model, the prediction of students probabilities to pass the current\\rcourses they take gives 0.7619 accuracy, 0.8333 precision, 0.8823 recall, and\\r0.8571 f1 score, which shows that the model used in the prediction is reliable,\\raccurate, and recommendable. Considering the indicators and the results, it can\\rbe noted that the prediction model used in this study is highly acceptable. The\\rdata mining techniques provides effective and efficient innovative tools in\\ranalyzing and predicting student performances. The model used in this study\\rwill greatly affect the way educators understand and identify the weakness of\\rtheir students in the class, the way they improved the effectiveness of their\\rlearning processes gearing to their students, bring down academic failure\\rrates, and help institution administrators modify their learning system\\routcomes. Further study for the inclusion of some students demographic\\rinformation, vast amount of data within the dataset, automated and manual\\rprocess of predictive criteria indicators where the students can regulate to\\rwhich criteria, they must improve more for them to pass their courses taken at\\rthe end of the semester as early as midterm period are highly needed.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05565 ,  719kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05589 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 03:35:23 GMT   (10832kb,D)\\r\\rTitle: Ill-Posed Image Reconstruction Without an Image Prior\\rAuthors: Oscar Leong and Angela F. Gao and He Sun and Katherine L. Bouman\\rCategories: eess.IV cs.CV\\rComments: Extended version of arXiv:2303.12217\\r\\\\\\\\\\r  We consider solving ill-posed imaging inverse problems without access to an\\rimage prior or ground-truth examples. An overarching challenge in these inverse\\rproblems is that an infinite number of images, including many that are\\rimplausible, are consistent with the observed measurements. Thus, image priors\\rare required to reduce the space of possible solutions to more desireable\\rreconstructions. However, in many applications it is difficult or potentially\\rimpossible to obtain example images to construct an image prior. Hence\\rinaccurate priors are often used, which inevitably result in biased solutions.\\rRather than solving an inverse problem using priors that encode the spatial\\rstructure of any one image, we propose to solve a set of inverse problems\\rjointly by incorporating prior constraints on the collective structure of the\\runderlying images. The key assumption of our work is that the underlying images\\rwe aim to reconstruct share common, low-dimensional structure. We show that\\rsuch a set of inverse problems can be solved simultaneously without the use of\\ra spatial image prior by instead inferring a shared image generator with a\\rlow-dimensional latent space. The parameters of the generator and latent\\rembeddings are found by maximizing a proxy for the Evidence Lower Bound (ELBO).\\rOnce identified, the generator and latent embeddings can be combined to provide\\rreconstructed images for each inverse problem. The framework we propose can\\rhandle general forward model corruptions, and we show that measurements derived\\rfrom only a small number of ground-truth images ($\\\\leqslant 150$) are\\rsufficient for prior-free image reconstruction. We demonstrate our approach\\ron a variety of convex and non-convex inverse problems, ranging from denoising,\\rphase retrieval, and black hole video reconstruction.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05589 ,  10832kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05600 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 04:17:45 GMT   (11646kb,D)\\r\\rTitle: Looking Similar, Sounding Different: Leveraging Counterfactual\\r  Cross-Modal Pairs for Audiovisual Representation Learning\\rAuthors: Nikhil Singh, Chih-Wei Wu, Iroro Orife, Mahdi Kalayeh\\rCategories: cs.SD cs.CV cs.LG cs.MM eess.AS\\rComments: 17 pages, 5 figures\\r\\\\\\\\\\r  Audiovisual representation learning typically relies on the correspondence\\rbetween sight and sound. However, there are often multiple audio tracks that\\rcan correspond with a visual scene. Consider, for example, different\\rconversations on the same crowded street. The effect of such counterfactual\\rpairs on audiovisual representation learning has not been previously explored.\\rTo investigate this, we use dubbed versions of movies to augment cross-modal\\rcontrastive learning. Our approach learns to represent alternate audio tracks,\\rdiffering only in speech content, similarly to the same video. Our results show\\rthat dub-augmented training improves performance on a range of auditory and\\raudiovisual tasks, without significantly affecting linguistic task performance\\roverall. We additionally compare this approach to a strong baseline where we\\rremove speech before pretraining, and find that dub-augmented training is more\\reffective, including for paralinguistic and audiovisual tasks where speech\\rremoval leads to worse performance. These findings highlight the importance of\\rconsidering speech variation when learning scene-level audiovisual\\rcorrespondences and suggest that dubbed audio can be a useful augmentation\\rtechnique for training audiovisual models toward more robust performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05600 ,  11646kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05622 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 05:39:38 GMT   (4244kb,D)\\r\\rTitle: SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM\\rAuthors: Yihao Liu, Jiaming Zhang, Zhangcong She, Amir Kheradmand and Mehran\\r  Armand\\rCategories: eess.IV cs.CV cs.LG\\rComments: 3 pages, 2 figures\\r\\\\\\\\\\r  The Segment Anything Model (SAM) is a new image segmentation tool trained\\rwith the largest segmentation dataset at this time. The model has demonstrated\\rthat it can create high-quality masks for image segmentation with good\\rpromptability and generalizability. However, the performance of the model on\\rmedical images requires further validation. To assist with the development,\\rassessment, and utilization of SAM on medical images, we introduce Segment Any\\rMedical Model (SAMM), an extension of SAM on 3D Slicer, a widely-used\\ropen-source image processing and visualization software that has been\\rextensively used in the medical imaging community. This open-source extension\\rto 3D Slicer and its demonstrations are posted on GitHub\\r(https://github.com/bingogome/samm). SAMM achieves 0.6-second latency of a\\rcomplete cycle and can infer image masks in nearly real-time.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05622 ,  4244kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05623 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 05:41:44 GMT   (1077kb,D)\\r\\rTitle: A Multi-Institutional Open-Source Benchmark Dataset for Breast Cancer\\r  Clinical Decision Support using Synthetic Correlated Diffusion Imaging Data\\rAuthors: Chi-en Amy Tai, Hayden Gunraj, Alexander Wong\\rCategories: eess.IV cs.CV q-bio.QM\\r\\\\\\\\\\r  Recently, a new form of magnetic resonance imaging (MRI) called synthetic\\rcorrelated diffusion (CDI$^s$) imaging was introduced and showed considerable\\rpromise for clinical decision support for cancers such as prostate cancer when\\rcompared to current gold-standard MRI techniques. However, the efficacy for\\rCDI$^s$ for other forms of cancers such as breast cancer has not been as\\rwell-explored nor have CDI$^s$ data been previously made publicly available.\\rMotivated to advance efforts in the development of computer-aided clinical\\rdecision support for breast cancer using CDI$^s$, we introduce Cancer-Net BCa,\\ra multi-institutional open-source benchmark dataset of volumetric CDI$^s$\\rimaging data of breast cancer patients. Cancer-Net BCa contains CDI$^s$\\rvolumetric images from a pre-treatment cohort of 253 patients across ten\\rinstitutions, along with detailed annotation metadata (the lesion type, genetic\\rsubtype, longest diameter on the MRI (MRLD), the Scarff-Bloom-Richardson (SBR)\\rgrade, and the post-treatment breast cancer pathologic complete response (pCR)\\rto neoadjuvant chemotherapy). We further examine the demographic and tumour\\rdiversity of the Cancer-Net BCa dataset to gain deeper insights into potential\\rbiases. Cancer-Net BCa is publicly available as a part of a global open-source\\rinitiative dedicated to accelerating advancement in machine learning to aid\\rclinicians in the fight against cancer.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05623 ,  1077kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05635 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 06:32:08 GMT   (30843kb,D)\\r\\rTitle: Unifying and Personalizing Weakly-supervised Federated Medical Image\\r  Segmentation via Adaptive Representation and Aggregation\\rAuthors: Li Lin, Jiewei Wu, Yixiang Liu, Kenneth K. Y. Wong, Xiaoying Tang\\rCategories: eess.IV cs.CV\\rComments: 13 pages, 7 figures\\r\\\\\\\\\\r  Federated learning (FL) enables multiple sites to collaboratively train\\rpowerful deep models without compromising data privacy and security. The\\rstatistical heterogeneity (e.g., non-IID data and domain shifts) is a primary\\robstacle in FL, impairing the generalization performance of the global model.\\rWeakly supervised segmentation, which uses sparsely-grained (i.e., point-,\\rbounding box-, scribble-, block-wise) supervision, is increasingly being paid\\rattention to due to its great potential of reducing annotation costs. However,\\rthere may exist label heterogeneity, i.e., different annotation forms across\\rsites. In this paper, we propose a novel personalized FL framework for medical\\rimage segmentation, named FedICRA, which uniformly leverages heterogeneous weak\\rsupervision via adaptIve Contrastive Representation and Aggregation.\\rConcretely, to facilitate personalized modeling and to avoid confusion, a\\rchannel selection based site contrastive representation module is employed to\\radaptively cluster intra-site embeddings and separate inter-site ones. To\\reffectively integrate the common knowledge from the global model with the\\runique knowledge from each local model, an adaptive aggregation module is\\rapplied for updating and initializing local models at the element level.\\rAdditionally, a weakly supervised objective function that leverages a\\rmultiscale tree energy loss and a gated CRF loss is employed to generate more\\rprecise pseudo-labels and further boost the segmentation performance. Through\\rextensive experiments on two distinct medical image segmentation tasks of\\rdifferent modalities, the proposed FedICRA demonstrates overwhelming\\rperformance over other state-of-the-art personalized FL methods. Its\\rperformance even approaches that of fully supervised training on centralized\\rdata. Our code and data are available at https://github.com/llmir/FedICRA.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05635 ,  30843kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05724 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 09:28:53 GMT   (6552kb)\\r\\rTitle: Universal Polarization Transformations: Spatial programming of\\r  polarization scattering matrices using a deep learning-designed diffractive\\r  polarization transformer\\rAuthors: Yuhang Li, Jingxi Li, Yifan Zhao, Tianyi Gan, Jingtian Hu, Mona\\r  Jarrahi, Aydogan Ozcan\\rCategories: physics.optics cs.CV\\rComments: 33 Pages, 7 Figures\\r\\\\\\\\\\r  We demonstrate universal polarization transformers based on an engineered\\rdiffractive volume, which can synthesize a large set of arbitrarily-selected,\\rcomplex-valued polarization scattering matrices between the polarization states\\rat different positions within its input and output field-of-views (FOVs). This\\rframework comprises 2D arrays of linear polarizers with diverse angles, which\\rare positioned between isotropic diffractive layers, each containing tens of\\rthousands of diffractive features with optimizable transmission coefficients.\\rWe demonstrate that, after its deep learning-based training, this diffractive\\rpolarization transformer could successfully implement N_i x N_o = 10,000\\rdifferent spatially-encoded polarization scattering matrices with negligible\\rerror within a single diffractive volume, where N_i and N_o represent the\\rnumber of pixels in the input and output FOVs, respectively. We experimentally\\rvalidated this universal polarization transformation framework in the terahertz\\rpart of the spectrum by fabricating wire-grid polarizers and integrating them\\rwith 3D-printed diffractive layers to form a physical polarization transformer\\roperating at 0.75 mm wavelength. Through this set-up, we demonstrated an\\rall-optical polarization permutation operation of spatially-varying\\rpolarization fields, and simultaneously implemented distinct spatially-encoded\\rpolarization scattering matrices between the input and output FOVs of a compact\\rdiffractive processor that axially spans 200 wavelengths. This framework opens\\rup new avenues for developing novel optical devices for universal polarization\\rcontrol, and may find various applications in, e.g., remote sensing, medical\\rimaging, security, material inspection and machine vision.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05724 ,  6552kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05727 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 09:34:13 GMT   (5383kb,D)\\r\\rTitle: Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks\\rAuthors: Lorenz Linhardt, Klaus-Robert M\\\\uller, Gr\\\\'egoire Montavon\\rCategories: cs.LG cs.AI cs.CV\\rComments: 14 pages + supplement\\r\\\\\\\\\\r  Explainable AI has become a popular tool for validating machine learning\\rmodels. Mismatches between the explained model's decision strategy and the\\ruser's domain knowledge (e.g. Clever Hans effects) have also been recognized as\\ra starting point for improving faulty models. However, it is less clear what to\\rdo when the user and the explanation agree. In this paper, we demonstrate that\\racceptance of explanations by the user is not a guarantee for a ML model to\\rfunction well, in particular, some Clever Hans effects may remain undetected.\\rSuch hidden flaws of the model can nevertheless be mitigated, and we\\rdemonstrate this by contributing a new method, Explanation-Guided Exposure\\rMinimization (EGEM), that premptively prunes variations in the ML model that\\rhave not been the subject of positive explanation feedback. Experiments on\\rnatural image data demonstrate that our approach leads to models that strongly\\rreduce their reliance on hidden Clever Hans strategies, and consequently\\rachieve higher accuracy on new data.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05727 ,  5383kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05826 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 13:02:08 GMT   (18037kb,D)\\r\\rTitle: HaDR: Applying Domain Randomization for Generating Synthetic Multimodal\\r  Dataset for Hand Instance Segmentation in Cluttered Industrial Environments\\rAuthors: Stefan Grushko, Ale\\\\v{s} Vysock\\\\'y, Jakub Chlebek, Petr Prokop\\rCategories: cs.HC cs.CV\\r\\\\\\\\\\r  This study uses domain randomization to generate a synthetic RGB-D dataset\\rfor training multimodal instance segmentation models, aiming to achieve\\rcolour-agnostic hand localization in cluttered industrial environments. Domain\\rrandomization is a simple technique for addressing the reality gap by\\rrandomly rendering unrealistic features in a simulation scene to force the\\rneural network to learn essential domain features. We provide a new synthetic\\rdataset for various hand detection applications in industrial environments, as\\rwell as ready-to-use pretrained instance segmentation models. To achieve robust\\rresults in a complex unstructured environment, we use multimodal input that\\rincludes both colour and depth information, which we hypothesize helps to\\rimprove the accuracy of the model prediction. In order to test this assumption,\\rwe analyze the influence of each modality and their synergy. The evaluated\\rmodels were trained solely on our synthetic dataset; yet we show that our\\rapproach enables the models to outperform corresponding models trained on\\rexisting state-of-the-art datasets in terms of Average Precision and\\rProbability-based Detection Quality.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05826 ,  18037kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05901 (*cross-listing*)\\rDate: Wed, 12 Apr 2023 15:14:41 GMT   (6090kb,D)\\r\\rTitle: Automated computed tomography and magnetic resonance imaging\\r  segmentation using deep learning: a beginner's guide\\rAuthors: Diedre Carmo, Gustavo Pinheiro, L\\\\'ivia Rodrigues, Thays Abreu,\\r  Roberto Lotufo, Let\\\\'icia Rittner\\rCategories: eess.IV cs.CV\\rComments: Equal contribution from Diedre Carmo, Gustavo Pinheiro, and L\\\\'ivia\\r  Rodrigues\\r\\\\\\\\\\r  Medical image segmentation is an increasingly popular area of research in\\rmedical imaging processing and analysis. However, many researchers who are new\\rto the field struggle with basic concepts. This tutorial paper aims to provide\\ran overview of the fundamental concepts of medical imaging, with a focus on\\rMagnetic Resonance and Computerized Tomography. We will also discuss deep\\rlearning algorithms, tools, and frameworks used for segmentation tasks, and\\rsuggest best practices for method development and image analysis. Our tutorial\\rincludes sample tasks using public data, and accompanying code is available on\\rGitHub (https://github.com/MICLab-Unicamp/Medical-ImagingTutorial). By sharing\\rour insights gained from years of experience in the field and learning from\\rrelevant literature, we hope to assist researchers in overcoming the initial\\rchallenges they may encounter in this exciting and important area of research.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05901 ,  6090kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05989 (*cross-listing*)\\rDate: Thu, 30 Mar 2023 15:04:04 GMT   (10945kb,D)\\r\\rTitle: Object-agnostic Affordance Categorization via Unsupervised Learning of\\r  Graph Embeddings\\rAuthors: Alexia Toumpa and Anthony G. Cohn\\rCategories: cs.AI cs.CV cs.LG\\rComments: Accepted at Journal of Artificial Intelligence Research (JAIR)\\r\\\\\\\\\\r  Acquiring knowledge about object interactions and affordances can facilitate\\rscene understanding and human-robot collaboration tasks. As humans tend to use\\robjects in many different ways depending on the scene and the objects'\\ravailability, learning object affordances in everyday-life scenarios is a\\rchallenging task, particularly in the presence of an open set of interactions\\rand objects. We address the problem of affordance categorization for\\rclass-agnostic objects with an open set of interactions; we achieve this by\\rlearning similarities between object interactions in an unsupervised way and\\rthus inducing clusters of object affordances. A novel depth-informed\\rqualitative spatial representation is proposed for the construction of Activity\\rGraphs (AGs), which abstract from the continuous representation of\\rspatio-temporal interactions in RGB-D videos. These AGs are clustered to obtain\\rgroups of objects with similar affordances. Our experiments in a real-world\\rscenario demonstrate that our method learns to create object affordance\\rclusters with a high V-measure even in cluttered scenes. The proposed approach\\rhandles object occlusions by capturing effectively possible interactions and\\rwithout imposing any object or scene constraints.\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05989 ,  10945kb)\\r%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2109.08103\\rreplaced with revised version Wed, 12 Apr 2023 17:27:02 GMT   (1653kb,D)\\r\\rTitle: Aesthetics and neural network image representations\\rAuthors: Romuald A. Janik\\rCategories: cs.CV eess.IV q-bio.NC\\rComments: 11 pages, 6 figures; v2: expanded discussion, appendix with 2 figures\\r  added\\r\\\\\\\\ ( https://arxiv.org/abs/2109.08103 ,  1653kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2111.02018\\rreplaced with revised version Wed, 12 Apr 2023 14:36:38 GMT   (2512kb,D)\\r\\rTitle: Multi-Glimpse Network: A Robust and Efficient Classification\\r  Architecture based on Recurrent Downsampled Attention\\rAuthors: Sia Huat Tan, Runpei Dong, Kaisheng Ma\\rCategories: cs.CV\\rComments: Accepted at BMVC 2021\\rJournal-ref: The British Machine Vision Conference (BMVC) 2021\\r\\\\\\\\ ( https://arxiv.org/abs/2111.02018 ,  2512kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2203.14205\\rreplaced with revised version Wed, 12 Apr 2023 05:26:58 GMT   (5452kb,D)\\r\\rTitle: Recent Few-Shot Object Detection Algorithms: A Survey with Performance\\r  Comparison\\rAuthors: Tianying Liu, Lu Zhang, Yang Wang, Jihong Guan, Yanwei Fu, Jiajia\\r  Zhao, Shuigeng Zhou\\rCategories: cs.CV\\rComments: 35 pages, 14 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2203.14205 ,  5452kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2205.10760\\rreplaced with revised version Wed, 12 Apr 2023 17:33:41 GMT   (20692kb,D)\\r\\rTitle: CNNs Avoid Curse of Dimensionality by Learning on Patches\\rAuthors: Vamshi C. Madala and Shivkumar Chandrasekaran and Jason Bunk\\rCategories: cs.CV cs.AI cs.LG\\rComments: Reorganization\\r\\\\\\\\ ( https://arxiv.org/abs/2205.10760 ,  20692kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2206.12126\\rreplaced with revised version Wed, 12 Apr 2023 08:08:50 GMT   (1300kb,D)\\r\\rTitle: Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive\\r  Learning\\rAuthors: Cheng Tan, Zhangyang Gao, Lirong Wu, Yongjie Xu, Jun Xia, Siyuan Li,\\r  Stan Z. Li\\rCategories: cs.CV cs.AI\\rComments: Accepted by CVPR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2206.12126 ,  1300kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2209.14072\\rreplaced with revised version Wed, 12 Apr 2023 12:06:09 GMT   (7767kb,D)\\r\\rTitle: City-scale Incremental Neural Mapping with Three-layer Sampling and\\r  Panoptic Representation\\rAuthors: Yongliang Shi, Runyi Yang, Pengfei Li, Zirui Wu, Hao Zhao, Guyue Zhou\\rCategories: cs.CV cs.RO\\rComments: 8 pages, 9 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2209.14072 ,  7767kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.03105\\rreplaced with revised version Wed, 12 Apr 2023 09:22:53 GMT   (3006kb,D)\\r\\rTitle: Mask3D: Mask Transformer for 3D Semantic Instance Segmentation\\rAuthors: Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu\\r  Tang, Bastian Leibe\\rCategories: cs.CV\\rComments: ICRA 2023 camera-ready version\\r\\\\\\\\ ( https://arxiv.org/abs/2210.03105 ,  3006kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.02048\\rreplaced with revised version Tue, 11 Apr 2023 19:08:09 GMT   (37577kb,D)\\r\\rTitle: Efficient Spatially Sparse Inference for Conditional GANs and Diffusion\\r  Models\\rAuthors: Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, and Jun-Yan\\r  Zhu\\rCategories: cs.CV cs.GR cs.LG\\rComments: NeurIPS 2022 Website: https://www.cs.cmu.edu/~sige/ Code:\\r  https://github.com/lmxyy/sige\\r\\\\\\\\ ( https://arxiv.org/abs/2211.02048 ,  37577kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.05207\\rreplaced with revised version Tue, 11 Apr 2023 18:50:03 GMT   (5222kb,D)\\r\\rTitle: Interpretable Machine Learning System to EEG Patterns on the\\r  Ictal-Interictal-Injury Continuum\\rAuthors: Alina Jade Barnett, Zhicheng Guo, Jin Jing, Wendong Ge, Cynthia Rudin,\\r  M. Brandon Westover\\rCategories: cs.CV cs.AI cs.LG\\rComments: 20 pages including appendices, 7 figures, submitted for peer review\\rACM-class: I.2.6; I.4.9; I.5.4\\r\\\\\\\\ ( https://arxiv.org/abs/2211.05207 ,  5222kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.12483\\rreplaced with revised version Wed, 12 Apr 2023 14:38:50 GMT   (7688kb,D)\\r\\rTitle: PIC-Score: Probabilistic Interpretable Comparison Score for Optimal\\r  Matching Confidence in Single- and Multi-Biometric (Face) Recognition\\rAuthors: Pedro C. Neto, Ana F. Sequeira, Jaime S. Cardoso, Philipp Terh\\\\orst\\rCategories: cs.CV\\rComments: Accepted at IEEE/CVF Conference on Computer Vision and Pattern\\r  Recognition (CVPR) 2023 Workshop on Biometrics\\r\\\\\\\\ ( https://arxiv.org/abs/2211.12483 ,  7688kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.15088\\rreplaced with revised version Wed, 12 Apr 2023 06:51:01 GMT   (778kb,D)\\r\\rTitle: Class Adaptive Network Calibration\\rAuthors: Bingyuan Liu, J\\\\'er\\\\^ome Rony, Adrian Galdran, Jose Dolz, Ismail Ben\\r  Ayed\\rCategories: cs.CV\\rComments: CVPR 2023. Code: https://github.com/by-liu/CALS\\r\\\\\\\\ ( https://arxiv.org/abs/2211.15088 ,  778kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.00256\\rreplaced with revised version Tue, 11 Apr 2023 22:31:23 GMT   (0kb,I)\\r\\rTitle: Face Animation with Multiple Source Images\\rAuthors: Zhaoying Pan, Jinge Ma\\rCategories: cs.CV\\rComments: Withdrawal to supplement more experiments\\r\\\\\\\\ ( https://arxiv.org/abs/2212.00256 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.11613\\rreplaced with revised version Wed, 12 Apr 2023 06:32:37 GMT   (14299kb,D)\\r\\rTitle: DDColor: Towards Photo-Realistic and Semantic-Aware Image Colorization\\r  via Dual Decoders\\rAuthors: Xiaoyang Kang, Tao Yang, Wenqi Ouyang, Peiran Ren, Lingzhi Li,\\r  Xuansong Xie\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2212.11613 ,  14299kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.13967\\rreplaced with revised version Tue, 11 Apr 2023 18:58:50 GMT   (6883kb,D)\\r\\rTitle: Extreme Image Transformations Affect Humans and Machines Differently\\rAuthors: Girik Malik and Dakarai Crowder and Ennio Mingolla\\rCategories: cs.CV cs.AI cs.LG\\rComments: Under review. 35 pages, 11 figures, 12 tables. arXiv admin note: text\\r  overlap with arXiv:2205.05167\\r\\\\\\\\ ( https://arxiv.org/abs/2212.13967 ,  6883kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.03410\\rreplaced with revised version Wed, 12 Apr 2023 15:19:16 GMT   (4476kb,D)\\r\\rTitle: In Defense of Structural Symbolic Representation for Video\\r  Event-Relation Prediction\\rAuthors: Andrew Lu, Xudong Lin, Yulei Niu, Shih-Fu Chang\\rCategories: cs.CV\\rComments: CVPRW 23, Learning with Limited Labelled Data\\r\\\\\\\\ ( https://arxiv.org/abs/2301.03410 ,  4476kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.07805\\rreplaced with revised version Tue, 11 Apr 2023 22:18:33 GMT   (2515kb,D)\\r\\rTitle: Multi-target multi-camera vehicle tracking using transformer-based\\r  camera link model and spatial-temporal information\\rAuthors: Hsiang-Wei Huang, Cheng-Yen Yang, Jenq-Neng Hwang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2301.07805 ,  2515kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.05757\\rreplaced with revised version Wed, 12 Apr 2023 00:26:19 GMT   (18551kb,D)\\r\\rTitle: Multispectral Contrastive Learning with Viewmaker Networks\\rAuthors: Jasmine Bayrooti, Noah Goodman, Alex Tamkin\\rCategories: cs.CV cs.AI\\rComments: Appearing in CVPR-PBVS 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2302.05757 ,  18551kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.05991\\rreplaced with revised version Tue, 11 Apr 2023 20:31:38 GMT   (8935kb,D)\\r\\rTitle: Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for\\r  Longer-Range Object Tracking Applications\\rAuthors: Weiyu Feng, Seth Z. Zhao, Chuanyu Pan, Adam Chang, Yichen Chen, Zekun\\r  Wang, Allen Y. Yang\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2302.05991 ,  8935kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.08788\\rreplaced with revised version Wed, 12 Apr 2023 05:15:11 GMT   (9308kb,D)\\r\\rTitle: MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis\\r  from Sparse Inputs\\rAuthors: Seunghyeon Seo, Donghoon Han, Yeonjin Chang, Nojun Kwak\\rCategories: cs.CV\\rComments: CVPR 2023. Project Page: https://shawn615.github.io/mixnerf/\\r\\\\\\\\ ( https://arxiv.org/abs/2302.08788 ,  9308kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.05072\\rreplaced with revised version Wed, 12 Apr 2023 10:05:41 GMT   (10524kb,D)\\r\\rTitle: Identification of Systematic Errors of Image Classifiers on Rare\\r  Subgroups\\rAuthors: Jan Hendrik Metzen, Robin Hutmacher, N. Grace Hua, Valentyn Boreiko,\\r  Dan Zhang\\rCategories: cs.CV cs.AI cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2303.05072 ,  10524kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.05725\\rreplaced with revised version Wed, 12 Apr 2023 10:07:11 GMT   (495kb,D)\\r\\rTitle: CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language\\r  Recognition with Variational Alignment\\rAuthors: Jiangbin Zheng, Yile Wang, Cheng Tan, Siyuan Li, Ge Wang, Jun Xia,\\r  Yidong Chen, Stan Z. Li\\rCategories: cs.CV cs.AI\\rComments: Accepted to CVPR 2023 (Highlight paper, 2.5% acceptance rate); Open\\r  source\\r\\\\\\\\ ( https://arxiv.org/abs/2303.05725 ,  495kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.07771\\rreplaced with revised version Wed, 12 Apr 2023 05:49:57 GMT   (4600kb,D)\\r\\rTitle: Imbalanced Domain Generalization for Robust Single Cell Classification\\r  in Hematological Cytomorphology\\rAuthors: Rao Muhammad Umer, Armin Gruber, Sayedali Shetab Boushehri, Christian\\r  Metak, Carsten Marr\\rCategories: cs.CV\\rComments: Published as a ICLR 2023 workshop paper: What do we need for\\r  successful domain generalization?\\r\\\\\\\\ ( https://arxiv.org/abs/2303.07771 ,  4600kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.08622\\rreplaced with revised version Wed, 12 Apr 2023 14:17:00 GMT   (9635kb,D)\\r\\rTitle: Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style\\r  Transfer\\rAuthors: Serin Yang, Hyunmin Hwang, Jong Chul Ye\\rCategories: cs.CV cs.AI cs.LG stat.ML\\r\\\\\\\\ ( https://arxiv.org/abs/2303.08622 ,  9635kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.14897\\rreplaced with revised version Wed, 12 Apr 2023 03:10:37 GMT   (13716kb,D)\\r\\rTitle: Seer: Language Instructed Video Prediction with Latent Diffusion Models\\rAuthors: Xianfan Gu, Chuan Wen, Jiaming Song, Yang Gao\\rCategories: cs.CV\\rComments: 17 pages, 15 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2303.14897 ,  13716kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.16897\\rreplaced with revised version Tue, 11 Apr 2023 19:33:09 GMT   (12905kb,D)\\r\\rTitle: Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos\\rAuthors: Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Torralba, Chuang Gan\\rCategories: cs.CV cs.LG cs.SD eess.AS\\rComments: CVPR 2023. Project page:\\r  https://sukun1045.github.io/video-physics-sound-diffusion/\\r\\\\\\\\ ( https://arxiv.org/abs/2303.16897 ,  12905kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.17249\\rreplaced with revised version Wed, 12 Apr 2023 13:06:55 GMT   (2624kb)\\r\\rTitle: Model-agnostic explainable artificial intelligence for object detection\\r  in image data\\rAuthors: Milad Moradi, Ke Yan, David Colwell, Matthias Samwald, Rhona Asgari\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2303.17249 ,  2624kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.18181\\rreplaced with revised version Wed, 12 Apr 2023 14:41:12 GMT   (16885kb,D)\\r\\rTitle: A Closer Look at Parameter-Efficient Tuning in Diffusion Models\\rAuthors: Chendong Xiang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu\\rCategories: cs.CV cs.LG\\rComments: 8pages, now our code is available at:\\r  https://github.com/Xiang-cd/unet-finetune\\r\\\\\\\\ ( https://arxiv.org/abs/2303.18181 ,  16885kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.00668\\rreplaced with revised version Wed, 12 Apr 2023 04:42:21 GMT   (17542kb,D)\\r\\rTitle: Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR\\rAuthors: Weijie Li, Wei Yang, Li Liu, Wenpeng Zhang, Yongxiang Liu\\rCategories: cs.CV cs.LG\\rDOI: 10.1109/LGRS.2023.3266493\\r\\\\\\\\ ( https://arxiv.org/abs/2304.00668 ,  17542kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.03198\\rreplaced with revised version Wed, 12 Apr 2023 15:14:14 GMT   (2163kb)\\r\\rTitle: RFAConv: Innovating Spatital Attention and Standard Convolutional\\r  Operation\\rAuthors: Xin Zhang, Chen Liu, Degang Yang, Tingting Song, Yichen Ye, Ke Li, and\\r  Yingze Song\\rCategories: cs.CV\\rComments: 14 pages, 5 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2304.03198 ,  2163kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.04133\\rreplaced with revised version Wed, 12 Apr 2023 01:11:10 GMT   (5516kb,D)\\r\\rTitle: NeRF applied to satellite imagery for surface reconstruction\\rAuthors: Federico Semeraro, Yi Zhang, Wenying Wu, Patrick Carroll\\rCategories: cs.CV cs.AI cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2304.04133 ,  5516kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.04175\\rreplaced with revised version Wed, 12 Apr 2023 04:35:47 GMT   (880kb,D)\\r\\rTitle: Token Boosting for Robust Self-Supervised Visual Transformer\\r  Pre-training\\rAuthors: Tianjiao Li, Lin Geng Foo, Ping Hu, Xindi Shang, Hossein Rahmani,\\r  Zehuan Yuan, Jun Liu\\rCategories: cs.CV\\rComments: Accepted to CVPR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2304.04175 ,  880kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.04415\\rreplaced with revised version Wed, 12 Apr 2023 07:19:48 GMT   (24910kb,D)\\r\\rTitle: Meta Compositional Referring Expression Segmentation\\rAuthors: Li Xu, Mark He Huang, Xindi Shang, Zehuan Yuan, Ying Sun, Jun Liu\\rCategories: cs.CV\\rComments: Accepted by CVPR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2304.04415 ,  24910kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.04546\\rreplaced with revised version Wed, 12 Apr 2023 12:57:40 GMT   (637kb,D)\\r\\rTitle: Kinship Representation Learning with Face Componential Relation\\rAuthors: Weng-Tai Su, Min-Hung Chen, Chien-Yi Wang, Shang-Hong Lai, Trista\\r  Pei-Chun Chen\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2304.04546 ,  637kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05265\\rreplaced with revised version Wed, 12 Apr 2023 07:59:16 GMT   (22001kb,D)\\r\\rTitle: Controllable Textual Inversion for Personalized Text-to-Image Generation\\rAuthors: Jianan Yang, Haobo Wang, Ruixuan Xiao, Sai Wu, Gang Chen, Junbo Zhao\\rCategories: cs.CV cs.AI cs.CL cs.LG\\rComments: 10 pages, 6 figures, 2 tables. Project Page:\\r  https://github.com/jnzju/COTI\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05265 ,  22001kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.05370\\rreplaced with revised version Wed, 12 Apr 2023 05:17:53 GMT   (4696kb,D)\\r\\rTitle: Overload: Latency Attacks on Object Detection for Edge Devices\\rAuthors: Erh-Chung Chen, Pin-Yu Chen, I-Hsin Chung, Che-rung Lee\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2304.05370 ,  4696kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2111.10050\\rreplaced with revised version Wed, 12 Apr 2023 08:26:28 GMT   (974kb,D)\\r\\rTitle: Combined Scaling for Zero-shot Transfer Learning\\rAuthors: Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu,\\r  Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing\\r  Tan, Quoc V. Le\\rCategories: cs.LG cs.CL cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2111.10050 ,  974kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2112.05074 (*cross-listing*)\\rreplaced with revised version Wed, 12 Apr 2023 12:44:23 GMT   (1885kb,D)\\r\\rTitle: Critical configurations for two projective views, a new approach\\rAuthors: Martin Br{\\\\aa}telund\\rCategories: math.AG cs.CV\\rComments: 28 pages, 4 figures, to appear in Journal of Symbolic Computations\\r\\\\\\\\ ( https://arxiv.org/abs/2112.05074 ,  1885kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.05140 (*cross-listing*)\\rreplaced with revised version Wed, 12 Apr 2023 10:58:04 GMT   (19718kb,D)\\r\\rTitle: Self-supervised Multi-modal Training from Uncurated Image and Reports\\r  Enables Zero-shot Oversight Artificial Intelligence in Radiology\\rAuthors: Sangjoon Park, Eun Sun Lee, Kyung Sook Shin, Jeong Eun Lee, and Jong\\r  Chul Ye\\rCategories: eess.IV cs.CL cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2208.05140 ,  19718kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.07839\\rreplaced with revised version Tue, 11 Apr 2023 22:47:19 GMT   (20663kb,D)\\r\\rTitle: Contrastive Audio-Visual Masked Autoencoder\\rAuthors: Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath,\\r  Leonid Karlinsky, Hilde Kuehne, James Glass\\rCategories: cs.MM cs.CV cs.SD eess.AS\\rComments: Accepted at ICLR 2023 as a notable top 25% paper. Code and pretrained\\r  models are at https://github.com/yuangongnd/cav-mae\\r\\\\\\\\ ( https://arxiv.org/abs/2210.07839 ,  20663kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.09224\\rreplaced with revised version Wed, 12 Apr 2023 07:20:21 GMT   (1390kb,D)\\r\\rTitle: Are we certain it's anomalous?\\rAuthors: Alessandro Flaborea, Bardh Prenkaj, Bharti Munjal, Marco Aurelio\\r  Sterpa, Dario Aragona, Luca Podo, Fabio Galasso\\rCategories: cs.LG cs.CV\\rComments: Accepted at CVPR '23 Visual Anomaly and Novelty Detection (VAND)\\r  Workshop\\r\\\\\\\\ ( https://arxiv.org/abs/2211.09224 ,  1390kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2301.09799 (*cross-listing*)\\rreplaced with revised version Wed, 12 Apr 2023 14:57:13 GMT   (1728kb,D)\\r\\rTitle: LDMIC: Learning-based Distributed Multi-view Image Coding\\rAuthors: Xinjie Zhang, Jiawei Shao, Jun Zhang\\rCategories: eess.IV cs.AI cs.CV cs.LG cs.MM\\rComments: Accepted by ICLR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2301.09799 ,  1728kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.08416 (*cross-listing*)\\rreplaced with revised version Wed, 12 Apr 2023 00:53:00 GMT   (2167kb,D)\\r\\rTitle: Lung Nodule Segmentation and Low-Confidence Region Prediction with\\r  Uncertainty-Aware Attention Mechanism\\rAuthors: Han Yang, Qiuli Wang, Yue Zhang, Zhulin An, Chen Liu, Xiaohong Zhang,\\r  S. Kevin Zhou\\rCategories: eess.IV cs.CV\\rComments: 10 pages, 10 figures. We have reported a preliminary version of this\\r  work in MICCAI 2022\\r\\\\\\\\ ( https://arxiv.org/abs/2303.08416 ,  2167kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.02798\\rreplaced with revised version Wed, 12 Apr 2023 15:50:35 GMT   (864kb,D)\\r\\rTitle: Source-free Domain Adaptation Requires Penalized Diversity\\rAuthors: Laya Rafiee Sevyeri, Ivaxi Sheth, Farhood Farahnak, Alexandre See,\\r  Samira Ebrahimi Kahou, Thomas Fevens, Mohammad Havaei\\rCategories: cs.LG cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2304.02798 ,  864kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.02834\\rreplaced with revised version Wed, 12 Apr 2023 06:17:23 GMT   (3074kb,D)\\r\\rTitle: Probing the Purview of Neural Networks via Gradient Analysis\\rAuthors: Jinsol Lee, Charlie Lehman, Mohit Prabhushankar, Ghassan AlRegib\\rCategories: cs.LG cs.AI cs.CV\\rComments: Published in IEEE Access. 17 pages, 6 figures\\rJournal-ref: in IEEE Access, vol. 11, pp. 32716-32732, 2023\\rDOI: 10.1109/ACCESS.2023.3263210\\r\\\\\\\\ ( https://arxiv.org/abs/2304.02834 ,  3074kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.03895 (*cross-listing*)\\rreplaced with revised version Wed, 12 Apr 2023 03:55:25 GMT   (13233kb,D)\\r\\rTitle: Multi-code deep image prior based plug-and-play ADMM for image denoising\\r  and CT reconstruction\\rAuthors: Chen Cheng, Qingping Zhou\\rCategories: eess.IV cs.CV\\rComments: 32 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2304.03895 ,  13233kb)\\r%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%--- ------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\rSend any comments regarding submissions directly to submitter.\\r------------------------------------------------------------------------------\\rArchives at http://arxiv.org/\\rTo unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel\\r------------------------------------------------------------------------------\\r Submissions to:\\rComputer Vision and Pattern Recognition\\r received from  Mon  3 Jul 23 18:00:00 GMT  to  Wed  5 Jul 23 18:00:00 GMT\\r------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01346\\rDate: Mon, 3 Jul 2023 20:39:48 GMT   (15242kb)\\r\\rTitle: Patch-CNN: Training data-efficient deep learning for high-fidelity\\r  diffusion tensor estimation from minimal diffusion protocols\\rAuthors: Tobias Goodwin-Allcock, Ting Gong, Robert Gray, Parashkev Nachev and\\r  Hui Zhang\\rCategories: cs.CV cs.LG eess.IV\\rComments: 12 pages, 6 figures\\r\\\\\\\\\\r  We propose a new method, Patch-CNN, for diffusion tensor (DT) estimation from\\ronly six-direction diffusion weighted images (DWI). Deep learning-based methods\\rhave been recently proposed for dMRI parameter estimation, using either\\rvoxel-wise fully-connected neural networks (FCN) or image-wise convolutional\\rneural networks (CNN). In the acute clinical context -- where pressure of time\\rlimits the number of imaged directions to a minimum -- existing approaches\\reither require an infeasible number of training images volumes (image-wise\\rCNNs), or do not estimate the fibre orientations (voxel-wise FCNs) required for\\rtractogram estimation. To overcome these limitations, we propose Patch-CNN, a\\rneural network with a minimal (non-voxel-wise) convolutional kernel\\r(3$\\\\times$3$\\\\times$3). Compared with voxel-wise FCNs, this has the advantage of\\rallowing the network to leverage local anatomical information. Compared with\\rimage-wise CNNs, the minimal kernel vastly reduces training data demand.\\rEvaluated against both conventional model fitting and a voxel-wise FCN,\\rPatch-CNN, trained with a single subject is shown to improve the estimation of\\rboth scalar dMRI parameters and fibre orientation from six-direction DWIs. The\\rimproved fibre orientation estimation is shown to produce improved tractogram.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01346 ,  15242kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01362\\rDate: Mon, 3 Jul 2023 21:33:40 GMT   (4510kb,D)\\r\\rTitle: Direct Superpoints Matching for Fast and Robust Point Cloud Registration\\rAuthors: Aniket Gupta, Yiming Xie, Hanumant Singh, Huaizu Jiang\\rCategories: cs.CV\\r\\\\\\\\\\r  Although deep neural networks endow the downsampled superpoints with\\rdiscriminative feature representations, directly matching them is usually not\\rused alone in state-of-the-art methods, mainly for two reasons. First, the\\rcorrespondences are inevitably noisy, so RANSAC-like refinement is usually\\radopted. Such ad hoc postprocessing, however, is slow and not differentiable,\\rwhich can not be jointly optimized with feature learning. Second, superpoints\\rare sparse and thus more RANSAC iterations are needed. Existing approaches use\\rthe coarse-to-fine strategy to propagate the superpoints correspondences to the\\rpoint level, which are not discriminative enough and further necessitates the\\rpostprocessing refinement. In this paper, we present a simple yet effective\\rapproach to extract correspondences by directly matching superpoints using a\\rglobal softmax layer in an end-to-end manner, which are used to determine the\\rrigid transformation between the source and target point cloud. Compared with\\rmethods that directly predict corresponding points, by leveraging the rich\\rinformation from the superpoints matchings, we can obtain more accurate\\restimation of the transformation and effectively filter out outliers without\\rany postprocessing refinement. As a result, our approach is not only fast, but\\ralso achieves state-of-the-art results on the challenging ModelNet and 3DMatch\\rbenchmarks. Our code and model weights will be publicly released.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01362 ,  4510kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01378\\rDate: Mon, 3 Jul 2023 22:16:17 GMT   (2707kb,D)\\r\\rTitle: A CNN regression model to estimate buildings height maps using\\r  Sentinel-1 SAR and Sentinel-2 MSI time series\\rAuthors: Ritu Yadav, Andrea Nascetti, Yifang Ban\\rCategories: cs.CV cs.AI eess.IV\\r\\\\\\\\\\r  Accurate estimation of building heights is essential for urban planning,\\rinfrastructure management, and environmental analysis. In this study, we\\rpropose a supervised Multimodal Building Height Regression Network (MBHR-Net)\\rfor estimating building heights at 10m spatial resolution using Sentinel-1 (S1)\\rand Sentinel-2 (S2) satellite time series. S1 provides Synthetic Aperture Radar\\r(SAR) data that offers valuable information on building structures, while S2\\rprovides multispectral data that is sensitive to different land cover types,\\rvegetation phenology, and building shadows. Our MBHR-Net aims to extract\\rmeaningful features from the S1 and S2 images to learn complex spatio-temporal\\rrelationships between image patterns and building heights. The model is trained\\rand tested in 10 cities in the Netherlands. Root Mean Squared Error (RMSE),\\rIntersection over Union (IOU), and R-squared (R2) score metrics are used to\\revaluate the performance of the model. The preliminary results (3.73m RMSE,\\r0.95 IoU, 0.61 R2) demonstrate the effectiveness of our deep learning model in\\raccurately estimating building heights, showcasing its potential for urban\\rplanning, environmental impact analysis, and other related applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01378 ,  2707kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01383\\rDate: Mon, 3 Jul 2023 22:27:37 GMT   (2032kb,D)\\r\\rTitle: Depth video data-enabled predictions of longitudinal dairy cow body\\r  weight using thresholding and Mask R-CNN algorithms\\rAuthors: Ye Bi, Leticia M.Campos, Jin Wang, Haipeng Yu, Mark D.Hanigan, Gota\\r  Morota\\rCategories: cs.CV cs.AI q-bio.QM\\r\\\\\\\\\\r  Monitoring cow body weight is crucial to support farm management decisions\\rdue to its direct relationship with the growth, nutritional status, and health\\rof dairy cows. Cow body weight is a repeated trait, however, the majority of\\rprevious body weight prediction research only used data collected at a single\\rpoint in time. Furthermore, the utility of deep learning-based segmentation for\\rbody weight prediction using videos remains unanswered. Therefore, the\\robjectives of this study were to predict cow body weight from repeatedly\\rmeasured video data, to compare the performance of the thresholding and Mask\\rR-CNN deep learning approaches, to evaluate the predictive ability of body\\rweight regression models, and to promote open science in the animal science\\rcommunity by releasing the source code for video-based body weight prediction.\\rA total of 40,405 depth images and depth map files were obtained from 10\\rlactating Holstein cows and 2 non-lactating Jersey cows. Three approaches were\\rinvestigated to segment the cow's body from the background, including single\\rthresholding, adaptive thresholding, and Mask R-CNN. Four image-derived\\rbiometric features, such as dorsal length, abdominal width, height, and volume,\\rwere estimated from the segmented images. On average, the Mask-RCNN approach\\rcombined with a linear mixed model resulted in the best prediction coefficient\\rof determination and mean absolute percentage error of 0.98 and 2.03%,\\rrespectively, in the forecasting cross-validation. The Mask-RCNN approach was\\ralso the best in the leave-three-cows-out cross-validation. The prediction\\rcoefficients of determination and mean absolute percentage error of the\\rMask-RCNN coupled with the linear mixed model were 0.90 and 4.70%,\\rrespectively. Our results suggest that deep learning-based segmentation\\rimproves the prediction performance of cow body weight from longitudinal depth\\rvideo data.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01383 ,  2032kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01421\\rDate: Tue, 4 Jul 2023 01:26:26 GMT   (45067kb,D)\\r\\rTitle: Unsupervised Feature Learning with Emergent Data-Driven Prototypicality\\rAuthors: Yunhui Guo, Youren Zhang, Yubei Chen, Stella X. Yu\\rCategories: cs.CV cs.AI\\rComments: 17 pages\\r\\\\\\\\\\r  Given an image set without any labels, our goal is to train a model that maps\\reach image to a point in a feature space such that, not only proximity\\rindicates visual similarity, but where it is located directly encodes how\\rprototypical the image is according to the dataset.\\r  Our key insight is to perform unsupervised feature learning in hyperbolic\\rinstead of Euclidean space, where the distance between points still reflect\\rimage similarity, and yet we gain additional capacity for representing\\rprototypicality with the location of the point: The closer it is to the origin,\\rthe more prototypical it is. The latter property is simply emergent from\\roptimizing the usual metric learning objective: The image similar to many\\rtraining instances is best placed at the center of corresponding points in\\rEuclidean space, but closer to the origin in hyperbolic space.\\r  We propose an unsupervised feature learning algorithm in Hyperbolic space\\rwith sphere pACKing. HACK first generates uniformly packed particles in the\\rPoincar\\\\'e ball of hyperbolic space and then assigns each image uniquely to\\reach particle. Images after congealing are regarded more typical of the dataset\\rit belongs to. With our feature mapper simply trained to spread out training\\rinstances in hyperbolic space, we observe that images move closer to the origin\\rwith congealing, validating our idea of unsupervised prototypicality discovery.\\rWe demonstrate that our data-driven prototypicality provides an easy and\\rsuperior unsupervised instance selection to reduce sample complexity, increase\\rmodel generalization with atypical instances and robustness with typical ones.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01421 ,  45067kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01425\\rDate: Tue, 4 Jul 2023 01:33:20 GMT   (5777kb,D)\\r\\rTitle: Consistent Multimodal Generation via A Unified GAN Framework\\rAuthors: Zhen Zhu, Yijun Li, Weijie Lyu, Krishna Kumar Singh, Zhixin Shu,\\r  Soeren Pirk, Derek Hoiem\\rCategories: cs.CV\\rComments: In review\\r\\\\\\\\\\r  We investigate how to generate multimodal image outputs, such as RGB, depth,\\rand surface normals, with a single generative model. The challenge is to\\rproduce outputs that are realistic, and also consistent with each other. Our\\rsolution builds on the StyleGAN3 architecture, with a shared backbone and\\rmodality-specific branches in the last layers of the synthesis network, and we\\rpropose per-modality fidelity discriminators and a cross-modality consistency\\rdiscriminator. In experiments on the Stanford2D3D dataset, we demonstrate\\rrealistic and consistent generation of RGB, depth, and normal images. We also\\rshow a training recipe to easily extend our pretrained model on a new domain,\\reven with a few pairwise data. We further evaluate the use of synthetically\\rgenerated RGB and depth pairs for training or fine-tuning depth estimators.\\rCode will be available at https://github.com/jessemelpolio/MultimodalGAN.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01425 ,  5777kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01426\\rDate: Tue, 4 Jul 2023 01:34:41 GMT   (7694kb,D)\\r\\rTitle: DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection\\rAuthors: Zhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, Baoyuan Wu\\rCategories: cs.CV\\r\\\\\\\\\\r  A critical yet frequently overlooked challenge in the field of deepfake\\rdetection is the lack of a standardized, unified, comprehensive benchmark. This\\rissue leads to unfair performance comparisons and potentially misleading\\rresults. Specifically, there is a lack of uniformity in data processing\\rpipelines, resulting in inconsistent data inputs for detection models.\\rAdditionally, there are noticeable differences in experimental settings, and\\revaluation strategies and metrics lack standardization. To fill this gap, we\\rpresent the first comprehensive benchmark for deepfake detection, called\\rDeepfakeBench, which offers three key contributions: 1) a unified data\\rmanagement system to ensure consistent input across all detectors, 2) an\\rintegrated framework for state-of-the-art methods implementation, and 3)\\rstandardized evaluation metrics and protocols to promote transparency and\\rreproducibility. Featuring an extensible, modular-based codebase, DeepfakeBench\\rcontains 15 state-of-the-art detection methods, 9 deepfake datasets, a series\\rof deepfake detection evaluation protocols and analysis tools, as well as\\rcomprehensive evaluations. Moreover, we provide new insights based on extensive\\ranalysis of these evaluations from various perspectives (e.g., data\\raugmentations, backbones). We hope that our efforts could facilitate future\\rresearch and foster innovation in this increasingly critical domain. All codes,\\revaluations, and analyses of our benchmark are publicly available at\\rhttps://github.com/SCLBD/DeepfakeBench.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01426 ,  7694kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01430\\rDate: Tue, 4 Jul 2023 01:47:34 GMT   (3150kb,D)\\r\\rTitle: Continual Learning in Open-vocabulary Classification with Complementary\\r  Memory Systems\\rAuthors: Zhen Zhu, Weijie Lyu, Yao Xiao, Derek Hoiem\\rCategories: cs.CV\\rComments: In review\\r\\\\\\\\\\r  We introduce a method for flexible continual learning in open-vocabulary\\rimage classification, drawing inspiration from the complementary learning\\rsystems observed in human cognition. We propose a tree probe method, an\\radaption of lazy learning principles, which enables fast learning from new\\rexamples with competitive accuracy to batch-trained linear models. Further, we\\rpropose a method to combine predictions from a CLIP zero-shot model and the\\rexemplar-based model, using the zero-shot estimated probability that a sample's\\rclass is within any of the exemplar classes. We test in data incremental, class\\rincremental, and task incremental settings, as well as ability to perform\\rflexible inference on varying subsets of zero-shot and learned categories. Our\\rproposed method achieves a good balance of learning speed, target task\\reffectiveness, and zero-shot effectiveness.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01430 ,  3150kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01447\\rDate: Tue, 4 Jul 2023 02:50:44 GMT   (8617kb,D)\\r\\rTitle: Learning Feature Matching via Matchable Keypoint-Assisted Graph Neural\\r  Network\\rAuthors: Zizhuo Li and Jiayi Ma\\rCategories: cs.CV\\r\\\\\\\\\\r  Accurately matching local features between a pair of images is a challenging\\rcomputer vision task. Previous studies typically use attention based graph\\rneural networks (GNNs) with fully-connected graphs over keypoints within/across\\rimages for visual and geometric information reasoning. However, in the context\\rof feature matching, considerable keypoints are non-repeatable due to occlusion\\rand failure of the detector, and thus irrelevant for message passing. The\\rconnectivity with non-repeatable keypoints not only introduces redundancy,\\rresulting in limited efficiency, but also interferes with the representation\\raggregation process, leading to limited accuracy. Targeting towards high\\raccuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN\\rarchitecture which bypasses non-repeatable keypoints and leverages matchable\\rones to guide compact and meaningful message passing. More specifically, our\\rBilateral Context-Aware Sampling Module first dynamically samples two small\\rsets of well-distributed keypoints with high matchability scores from the image\\rpair. Then, our Matchable Keypoint-Assisted Context Aggregation Module regards\\rsampled informative keypoints as message bottlenecks and thus constrains each\\rkeypoint only to retrieve favorable contextual information from intra- and\\rinter- matchable keypoints, evading the interference of irrelevant and\\rredundant connectivity with non-repeatable ones. Furthermore, considering the\\rpotential noise in initial keypoints and sampled matchable ones, the MKACA\\rmodule adopts a matchability-guided attentional aggregation operation for purer\\rdata-dependent context propagation. By these means, we achieve the\\rstate-of-the-art performance on relative camera estimation, fundamental matrix\\restimation, and visual localization, while significantly reducing computational\\rand memory complexity compared to typical attentional GNNs.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01447 ,  8617kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01464\\rDate: Tue, 4 Jul 2023 03:53:05 GMT   (3194kb,D)\\r\\rTitle: Unsupervised Quality Prediction for Improved Single-Frame and Weighted\\r  Sequential Visual Place Recognition\\rAuthors: Helen Carson, Jason J. Ford, Michael Milford\\rCategories: cs.CV\\r\\\\\\\\\\r  While substantial progress has been made in the absolute performance of\\rlocalization and Visual Place Recognition (VPR) techniques, it is becoming\\rincreasingly clear from translating these systems into applications that other\\rcapabilities like integrity and predictability are just as important,\\respecially for safety- or operationally-critical autonomous systems. In this\\rresearch we present a new, training-free approach to predicting the likely\\rquality of localization estimates, and a novel method for using these\\rpredictions to bias a sequence-matching process to produce additional\\rperformance gains beyond that of a naive sequence matching approach. Our\\rcombined system is lightweight, runs in real-time and is agnostic to the\\runderlying VPR technique. On extensive experiments across four datasets and\\rthree VPR techniques, we demonstrate our system improves precision performance,\\respecially at the high-precision/low-recall operating point. We also present\\rablation and analysis identifying the performance contributions of the\\rprediction and weighted sequence matching components in isolation, and the\\rrelationship between the quality of the prediction system and the benefits of\\rthe weighted sequential matcher.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01464 ,  3194kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01465\\rDate: Tue, 4 Jul 2023 03:56:43 GMT   (34944kb,D)\\r\\rTitle: AdAM: Few-Shot Image Generation via Adaptation-Aware Kernel Modulation\\rAuthors: Yunqing Zhao, Keshigeyan Chandrasegaran, Abdollahzadeh Milad, Chao Du,\\r  Tianyu Pang, Ruoteng Li, Henghui Ding, Ngai-Man Cheung\\rCategories: cs.CV\\rComments: 33 pages, 35 figures, 13 tables. Extension of NeurIPS-2022 paper\\r  arXiv:2210.16559\\r\\\\\\\\\\r  Few-shot image generation (FSIG) aims to learn to generate new and diverse\\rimages given few (e.g., 10) training samples. Recent work has addressed FSIG by\\rleveraging a GAN pre-trained on a large-scale source domain and adapting it to\\rthe target domain with few target samples. Central to recent FSIG methods are\\rknowledge preservation criteria, which select and preserve a subset of source\\rknowledge to the adapted model. However, a major limitation of existing methods\\ris that their knowledge preserving criteria consider only source domain/task\\rand fail to consider target domain/adaptation in selecting source knowledge,\\rcasting doubt on their suitability for setups of different proximity between\\rsource and target domain. Our work makes two contributions. Firstly, we revisit\\rrecent FSIG works and their experiments. We reveal that under setups which\\rassumption of close proximity between source and target domains is relaxed,\\rmany existing state-of-the-art (SOTA) methods which consider only source domain\\rin knowledge preserving perform no better than a baseline method. As our second\\rcontribution, we propose Adaptation-Aware kernel Modulation (AdAM) for general\\rFSIG of different source-target domain proximity. Extensive experiments show\\rthat AdAM consistently achieves SOTA performance in FSIG, including challenging\\rsetups where source and target domains are more apart.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01465 ,  34944kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01467\\rDate: Tue, 4 Jul 2023 04:12:49 GMT   (416kb)\\r\\rTitle: Technical Report for Ego4D Long Term Action Anticipation Challenge 2023\\rAuthors: Tatsuya Ishibashi, Kosuke Ono, Noriyuki Kugo, Yuji Sato\\rCategories: cs.CV\\r\\\\\\\\\\r  In this report, we describe the technical details of our approach for the\\rEgo4D Long-Term Action Anticipation Challenge 2023. The aim of this task is to\\rpredict a sequence of future actions that will take place at an arbitrary time\\ror later, given an input video. To accomplish this task, we introduce three\\rimprovements to the baseline model, which consists of an encoder that generates\\rclip-level features from the video, an aggregator that integrates multiple\\rclip-level features, and a decoder that outputs Z future actions. 1) Model\\rensemble of SlowFast and SlowFast-CLIP; 2) Label smoothing to relax order\\rconstraints for future actions; 3) Constraining the prediction of the action\\rclass (verb, noun) based on word co-occurrence. Our method outperformed the\\rbaseline performance and recorded as second place solution on the public\\rleaderboard.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01467 ,  416kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01468\\rDate: Tue, 4 Jul 2023 04:12:50 GMT   (22924kb,D)\\r\\rTitle: Generating Animatable 3D Cartoon Faces from Single Portraits\\rAuthors: Chuanyu Pan, Guowei Yang, Taijiang Mu, and Yu-Kun Lai\\rCategories: cs.CV\\rComments: 12 pages, accepted by CGI2023 and journal Virtual Reality Intelligent\\r  Hardware (VRIH)\\r\\\\\\\\\\r  With the booming of virtual reality (VR) technology, there is a growing need\\rfor customized 3D avatars. However, traditional methods for 3D avatar modeling\\rare either time-consuming or fail to retain similarity to the person being\\rmodeled. We present a novel framework to generate animatable 3D cartoon faces\\rfrom a single portrait image. We first transfer an input real-world portrait to\\ra stylized cartoon image with a StyleGAN. Then we propose a two-stage\\rreconstruction method to recover the 3D cartoon face with detailed texture,\\rwhich first makes a coarse estimation based on template models, and then\\rrefines the model by non-rigid deformation under landmark supervision. Finally,\\rwe propose a semantic preserving face rigging method based on manually created\\rtemplates and deformation transfer. Compared with prior arts, qualitative and\\rquantitative results show that our method achieves better accuracy, aesthetics,\\rand similarity criteria. Furthermore, we demonstrate the capability of\\rreal-time facial animation of our 3D model.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01468 ,  22924kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01470\\rDate: Tue, 4 Jul 2023 04:29:03 GMT   (9163kb,D)\\r\\rTitle: A Review of Driver Gaze Estimation and Application in Gaze Behavior\\r  Understanding\\rAuthors: Pavan Kumar Sharma and Pranamesh Chakraborty\\rCategories: cs.CV cs.HC cs.LG\\r\\\\\\\\\\r  Driver gaze plays an important role in different gaze-based applications such\\ras driver attentiveness detection, visual distraction detection, gaze behavior\\runderstanding, and building driver assistance system. The main objective of\\rthis study is to perform a comprehensive summary of driver gaze fundamentals,\\rmethods to estimate driver gaze, and it's applications in real world driving\\rscenarios. We first discuss the fundamentals related to driver gaze, involving\\rhead-mounted and remote setup based gaze estimation and the terminologies used\\rfor each of these data collection methods. Next, we list out the existing\\rbenchmark driver gaze datasets, highlighting the collection methodology and the\\requipment used for such data collection. This is followed by a discussion of\\rthe algorithms used for driver gaze estimation, which primarily involves\\rtraditional machine learning and deep learning based techniques. The estimated\\rdriver gaze is then used for understanding gaze behavior while maneuvering\\rthrough intersections, on-ramps, off-ramps, lane changing, and determining the\\reffect of roadside advertising structures. Finally, we have discussed the\\rlimitations in the existing literature, challenges, and the future scope in\\rdriver gaze estimation and gaze-based applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01470 ,  9163kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01473\\rDate: Tue, 4 Jul 2023 04:46:44 GMT   (29307kb,D)\\r\\rTitle: Mitigating Bias: Enhancing Image Classification by Improving Model\\r  Explanations\\rAuthors: Raha Ahmadi, Mohammad Javad Rajabi, Mohamamd Khalooiem Mohammad\\r  Sabokrou\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Deep learning models have demonstrated remarkable capabilities in learning\\rcomplex patterns and concepts from training data. However, recent findings\\rindicate that these models tend to rely heavily on simple and easily\\rdiscernible features present in the background of images rather than the main\\rconcepts or objects they are intended to classify. This phenomenon poses a\\rchallenge to image classifiers as the crucial elements of interest in images\\rmay be overshadowed. In this paper, we propose a novel approach to address this\\rissue and improve the learning of main concepts by image classifiers. Our\\rcentral idea revolves around concurrently guiding the model's attention toward\\rthe foreground during the classification task. By emphasizing the foreground,\\rwhich encapsulates the primary objects of interest, we aim to shift the focus\\rof the model away from the dominant influence of the background. To accomplish\\rthis, we introduce a mechanism that encourages the model to allocate sufficient\\rattention to the foreground. We investigate various strategies, including\\rmodifying the loss function or incorporating additional architectural\\rcomponents, to enable the classifier to effectively capture the primary concept\\rwithin an image. Additionally, we explore the impact of different foreground\\rattention mechanisms on model performance and provide insights into their\\reffectiveness. Through extensive experimentation on benchmark datasets, we\\rdemonstrate the efficacy of our proposed approach in improving the\\rclassification accuracy of image classifiers. Our findings highlight the\\rimportance of foreground attention in enhancing model understanding and\\rrepresentation of the main concepts within images. The results of this study\\rcontribute to advancing the field of image classification and provide valuable\\rinsights for developing more robust and accurate deep-learning models.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01473 ,  29307kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01489\\rDate: Tue, 4 Jul 2023 05:44:13 GMT   (37640kb,D)\\r\\rTitle: Semantic Segmentation on 3D Point Clouds with High Density Variations\\rAuthors: Ryan Faulkner, Luke Haub, Simon Ratcliffe, Ian Reid, Tat-Jun Chin\\rCategories: cs.CV\\rACM-class: I.4.6\\r\\\\\\\\\\r  LiDAR scanning for surveying applications acquire measurements over wide\\rareas and long distances, which produces large-scale 3D point clouds with\\rsignificant local density variations. While existing 3D semantic segmentation\\rmodels conduct downsampling and upsampling to build robustness against varying\\rpoint densities, they are less effective under the large local density\\rvariations characteristic of point clouds from surveying applications. To\\ralleviate this weakness, we propose a novel architecture called HDVNet that\\rcontains a nested set of encoder-decoder pathways, each handling a specific\\rpoint density range. Limiting the interconnections between the feature maps\\renables HDVNet to gauge the reliability of each feature based on the density of\\ra point, e.g., downweighting high density features not existing in low density\\robjects. By effectively handling input density variations, HDVNet outperforms\\rstate-of-the-art models in segmentation accuracy on real point clouds with\\rinconsistent density, using just over half the weights.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01489 ,  37640kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01492\\rDate: Tue, 4 Jul 2023 05:55:54 GMT   (1322kb,D)\\r\\rTitle: FB-OCC: 3D Occupancy Prediction based on Forward-Backward View\\r  Transformation\\rAuthors: Zhiqi Li, Zhiding Yu, David Austin, Mingsheng Fang, Shiyi Lan, Jan\\r  Kautz, Jose M. Alvarez\\rCategories: cs.CV cs.RO\\rComments: Outstanding Champion and Innovation Award in the 3D Occupancy\\r  Prediction Challenge (CVPR23)\\r\\\\\\\\\\r  This technical report summarizes the winning solution for the 3D Occupancy\\rPrediction Challenge, which is held in conjunction with the CVPR 2023 Workshop\\ron End-to-End Autonomous Driving and CVPR 23 Workshop on Vision-Centric\\rAutonomous Driving Workshop. Our proposed solution FB-OCC builds upon FB-BEV, a\\rcutting-edge camera-based bird's-eye view perception design using\\rforward-backward projection. On top of FB-BEV, we further study novel designs\\rand optimization tailored to the 3D occupancy prediction task, including joint\\rdepth-semantic pre-training, joint voxel-BEV representation, model scaling up,\\rand effective post-processing strategies. These designs and optimization result\\rin a state-of-the-art mIoU score of 54.19% on the nuScenes dataset, ranking the\\r1st place in the challenge track. Code and models will be released at:\\rhttps://github.com/NVlabs/FB-BEV.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01492 ,  1322kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01502\\rDate: Tue, 4 Jul 2023 06:15:06 GMT   (1634kb)\\r\\rTitle: HEDI: First-Time Clinical Application and Results of a Biomechanical\\r  Evaluation and Visualisation Tool for Incisional Hernia Repair\\rAuthors: Jacob J. Relle, Samuel Vo{\\\\ss}, Ramesch Raschidi, Regine Nessel,\\r  Johannes G\\\\orich, Mark O. Wielp\\\\utz, Thorsten L\\\\offler, Vincent Heuveline,\\r  Friedrich Kallinowski, Philipp D. L\\\\osel\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Abdominal wall defects often lead to pain, discomfort, and recurrence of\\rincisional hernias, resulting in significant morbidity and repeated surgical\\rrepairs worldwide. Mesh repair for large hernias is usually based on the defect\\rarea with a fixed overlap, without considering biomechanical aspects such as\\rmuscle activation, intra-abdominal pressure, tissue elasticity, and abdominal\\rwall distention. To address this issue, we present a biomechanical approach to\\rincisional hernia repair that takes into account the unstable abdominal wall.\\rAdditionally, we introduce HEDI, a tool that uses dynamic computed tomography\\rwith Valsalva maneuver to automatically detect and assess hernia size, volume,\\rand abdominal wall instability. Our first clinical application of HEDI in the\\rpreoperative evaluation of 31 patients shows significantly improved success\\rrates compared to reported rates, with all patients remaining pain-free and\\rshowing no hernia recurrence after three years of follow-up.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01502 ,  1634kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01515\\rDate: Tue, 4 Jul 2023 06:54:01 GMT   (27531kb,D)\\r\\rTitle: LPN: Language-guided Prototypical Network for few-shot classification\\rAuthors: Kaihui Cheng, Chule Yang\\rCategories: cs.CV\\r\\\\\\\\\\r  Few-shot classification aims to adapt to new tasks with limited labeled\\rexamples. To fully use the accessible data, recent methods explore suitable\\rmeasures for the similarity between the query and support images and better\\rhigh-dimensional features with meta-training and pre-training strategies.\\rHowever, the potential of multi-modality information has barely been explored,\\rwhich may bring promising improvement for few-shot classification. In this\\rpaper, we propose a Language-guided Prototypical Network (LPN) for few-shot\\rclassification, which leverages the complementarity of vision and language\\rmodalities via two parallel branches. Concretely, to introduce language\\rmodality with limited samples in the visual task, we leverage a pre-trained\\rtext encoder to extract class-level text features directly from class names\\rwhile processing images with a conventional image encoder. Then, a\\rlanguage-guided decoder is introduced to obtain text features corresponding to\\reach image by aligning class-level features with visual features. In addition,\\rto take advantage of class-level features and prototypes, we build a refined\\rprototypical head that generates robust prototypes in the text branch for\\rfollow-up measurement. Finally, we aggregate the visual and text logits to\\rcalibrate the deviation of a single modality. Extensive experiments demonstrate\\rthe competitiveness of LPN against state-of-the-art methods on benchmark\\rdatasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01515 ,  27531kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01520\\rDate: Tue, 4 Jul 2023 07:00:37 GMT   (4102kb,D)\\r\\rTitle: LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via\\r  Latent Ensemble Attack\\rAuthors: Joonkyo Shim, Hyunsoo Yoon\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Deepfakes, malicious visual contents created by generative models, pose an\\rincreasingly harmful threat to society. To proactively mitigate deepfake\\rdamages, recent studies have employed adversarial perturbation to disrupt\\rdeepfake model outputs. However, previous approaches primarily focus on\\rgenerating distorted outputs based on only predetermined target attributes,\\rleading to a lack of robustness in real-world scenarios where target attributes\\rare unknown. Additionally, the transferability of perturbations between two\\rprominent generative models, Generative Adversarial Networks (GANs) and\\rDiffusion Models, remains unexplored. In this paper, we emphasize the\\rimportance of target attribute-transferability and model-transferability for\\rachieving robust deepfake disruption. To address this challenge, we propose a\\rsimple yet effective disruption method called Latent Ensemble ATtack (LEAT),\\rwhich attacks the independent latent encoding process. By disrupting the latent\\rencoding process, it generates distorted output images in subsequent generation\\rprocesses, regardless of the given target attributes. This target\\rattribute-agnostic attack ensures robust disruption even when the target\\rattributes are unknown. Additionally, we introduce a Normalized Gradient\\rEnsemble strategy that effectively aggregates gradients for iterative gradient\\rattacks, enabling simultaneous attacks on various types of deepfake models,\\rinvolving both GAN-based and Diffusion-based models. Moreover, we demonstrate\\rthe insufficiency of evaluating disruption quality solely based on pixel-level\\rdifferences. As a result, we propose an alternative protocol for\\rcomprehensively evaluating the success of defense. Extensive experiments\\rconfirm the efficacy of our method in disrupting deepfakes in real-world\\rscenarios, reporting a higher defense success rate compared to previous\\rmethods.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01520 ,  4102kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01524\\rDate: Tue, 4 Jul 2023 07:10:39 GMT   (15697kb,D)\\r\\rTitle: Exploiting Richness of Learned Compressed Representation of Images for\\r  Semantic Segmentation\\rAuthors: Ravi Kakaiya, Rakshith Sathish, Ramanathan Sethuraman\\rCategories: cs.CV cs.LG eess.IV\\rComments: Accepted at ICME 2023 (Industry Track)\\r\\\\\\\\\\r  Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the\\rpotential to radically change the way we travel. Many such vehicles currently\\rrely on segmentation and object detection algorithms to detect and track\\robjects around its surrounding. The data collected from the vehicles are often\\rsent to cloud servers to facilitate continual/life-long learning of these\\ralgorithms. Considering the bandwidth constraints, the data is compressed\\rbefore sending it to servers, where it is typically decompressed for training\\rand analysis. In this work, we propose the use of a learning-based compression\\rCodec to reduce the overhead in latency incurred for the decompression\\roperation in the standard pipeline. We demonstrate that the learned compressed\\rrepresentation can also be used to perform tasks like semantic segmentation in\\raddition to decompression to obtain the images. We experimentally validate the\\rproposed pipeline on the Cityscapes dataset, where we achieve a compression\\rfactor up to $66 \\\\times$ while preserving the information required to perform\\rsegmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved\\rusing decompressed images while reducing the overall compute by $11\\\\%$.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01524 ,  15697kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01530\\rDate: Tue, 4 Jul 2023 07:33:53 GMT   (6595kb,D)\\r\\rTitle: Convolutional Transformer for Autonomous Recognition and Grading of\\r  Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions\\rAuthors: Asim Khan, Taimur Hassan, Muhammad Shafay, Israa Fahmy, Naoufel\\r  Werghi, Lakmal Seneviratne and Irfan Hussain\\rCategories: cs.CV cs.AI eess.IV\\rComments: 22 pages and 6 figures\\r\\\\\\\\\\r  Harvesting fully ripe tomatoes with mobile robots presents significant\\rchallenges in real-world scenarios. These challenges arise from factors such as\\rocclusion caused by leaves and branches, as well as the color similarity\\rbetween tomatoes and the surrounding foliage during the fruit development\\rstage. The natural environment further compounds these issues with varying\\rlight conditions, viewing angles, occlusion factors, and different maturity\\rlevels. To overcome these obstacles, this research introduces a novel framework\\rthat leverages a convolutional transformer architecture to autonomously\\rrecognize and grade tomatoes, irrespective of their occlusion level, lighting\\rconditions, and ripeness. The proposed model is trained and tested using\\rcarefully annotated images curated specifically for this purpose. The dataset\\ris prepared under various lighting conditions, viewing perspectives, and\\remploys different mobile camera sensors, distinguishing it from existing\\rdatasets such as Laboro Tomato and Rob2Pheno Annotated Tomato. The\\reffectiveness of the proposed framework in handling cluttered and occluded\\rtomato instances was evaluated using two additional public datasets, Laboro\\rTomato and Rob2Pheno Annotated Tomato, as benchmarks. The evaluation results\\racross these three datasets demonstrate the exceptional performance of our\\rproposed framework, surpassing the state-of-the-art by 58.14%, 65.42%, and\\r66.39% in terms of mean average precision scores for KUTomaData, Laboro Tomato,\\rand Rob2Pheno Annotated Tomato, respectively. The results underscore the\\rsuperiority of the proposed model in accurately detecting and delineating\\rtomatoes compared to baseline methods and previous approaches. Specifically,\\rthe model achieves an F1-score of 80.14%, a Dice coefficient of 73.26%, and a\\rmean IoU of 66.41% on the KUTomaData image dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01530 ,  6595kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01533\\rDate: Tue, 4 Jul 2023 07:36:48 GMT   (6811kb,D)\\r\\rTitle: Unsupervised Video Anomaly Detection with Diffusion Models Conditioned\\r  on Compact Motion Representations\\rAuthors: Anil Osman Tur and Nicola Dall'Asen and Cigdem Beyan and Elisa Ricci\\rCategories: cs.CV\\rComments: Accepted to ICIAP 2023\\r\\\\\\\\\\r  This paper aims to address the unsupervised video anomaly detection (VAD)\\rproblem, which involves classifying each frame in a video as normal or\\rabnormal, without any access to labels. To accomplish this, the proposed method\\remploys conditional diffusion models, where the input data is the\\rspatiotemporal features extracted from a pre-trained network, and the condition\\ris the features extracted from compact motion representations that summarize a\\rgiven video segment in terms of its motion and appearance. Our method utilizes\\ra data-driven threshold and considers a high reconstruction error as an\\rindicator of anomalous events. This study is the first to utilize compact\\rmotion representations for VAD and the experiments conducted on two large-scale\\rVAD benchmarks demonstrate that they supply relevant information to the\\rdiffusion model, and consequently improve VAD performances w.r.t the prior art.\\rImportantly, our method exhibits better generalization performance across\\rdifferent datasets, notably outperforming both the state-of-the-art and\\rbaseline methods. The code of our method is available at\\rhttps://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01533 ,  6811kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01545\\rDate: Tue, 4 Jul 2023 07:58:23 GMT   (435kb,D)\\r\\rTitle: EffSeg: Efficient Fine-Grained Instance Segmentation using\\r  Structure-Preserving Sparsity\\rAuthors: C\\\\'edric Picron, Tinne Tuytelaars\\rCategories: cs.CV\\r\\\\\\\\\\r  Many two-stage instance segmentation heads predict a coarse 28x28 mask per\\rinstance, which is insufficient to capture the fine-grained details of many\\robjects. To address this issue, PointRend and RefineMask predict a 112x112\\rsegmentation mask resulting in higher quality segmentations. Both methods\\rhowever have limitations by either not having access to neighboring features\\r(PointRend) or by performing computation at all spatial locations instead of\\rsparsely (RefineMask). In this work, we propose EffSeg performing fine-grained\\rinstance segmentation in an efficient way by using our Structure-Preserving\\rSparsity (SPS) method based on separately storing the active features, the\\rpassive features and a dense 2D index map containing the feature indices. The\\rgoal of the index map is to preserve the 2D spatial configuration or structure\\rbetween the features such that any 2D operation can still be performed. EffSeg\\rachieves similar performance on COCO compared to RefineMask, while reducing the\\rnumber of FLOPs by 71% and increasing the FPS by 29%. Code will be released.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01545 ,  435kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01557\\rDate: Tue, 4 Jul 2023 08:21:39 GMT   (331kb,D)\\r\\rTitle: Separated RoadTopoFormer\\rAuthors: Mingjie Lu, Yuanxian Huang, Ji Liu, Jinzhang Peng, Lu Tian, Ashish\\r  Sirasao\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Understanding driving scenarios is crucial to realizing autonomous driving.\\rPrevious works such as map learning and BEV lane detection neglect the\\rconnection relationship between lane instances, and traffic elements detection\\rtasks usually neglect the relationship with lane lines. To address these\\rissues, the task is presented which includes 4 sub-tasks, the detection of\\rtraffic elements, the detection of lane centerlines, reasoning connection\\rrelationships among lanes, and reasoning assignment relationships between lanes\\rand traffic elements. We present Separated RoadTopoFormer to tackle the issues,\\rwhich is an end-to-end framework that detects lane centerline and traffic\\relements with reasoning relationships among them. We optimize each module\\rseparately to prevent interaction with each other and aggregate them together\\rwith few finetunes. For two detection heads, we adopted a DETR-like\\rarchitecture to detect objects, and for the relationship head, we concat two\\rinstance features from front detectors and feed them to the classifier to\\robtain relationship probability. Our final submission achieves 0.445 OLS, which\\ris competitive in both sub-task and combined scores.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01557 ,  331kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01582\\rDate: Tue, 4 Jul 2023 09:22:50 GMT   (873kb,D)\\r\\rTitle: IAdet: Simplest human-in-the-loop object detection\\rAuthors: Franco Marchesoni-Acland, Gabriele Facciolo\\rCategories: cs.CV cs.AI cs.HC cs.LG\\r\\\\\\\\\\r  This work proposes a strategy for training models while annotating data named\\rIntelligent Annotation (IA). IA involves three modules: (1) assisted data\\rannotation, (2) background model training, and (3) active selection of the next\\rdatapoints. Under this framework, we open-source the IAdet tool, which is\\rspecific for single-class object detection. Additionally, we devise a method\\rfor automatically evaluating such a human-in-the-loop system. For the PASCAL\\rVOC dataset, the IAdet tool reduces the database annotation time by $25\\\\%$\\rwhile providing a trained model for free. These results are obtained for a\\rdeliberately very simple IAdet design. As a consequence, IAdet is susceptible\\rto multiple easy improvements, paving the way for powerful human-in-the-loop\\robject detection systems.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01582 ,  873kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01630\\rDate: Tue, 4 Jul 2023 10:26:53 GMT   (21933kb,D)\\r\\rTitle: ChildPlay: A New Benchmark for Understanding Children's Gaze Behaviour\\rAuthors: Samy Tafasca, Anshul Gupta, Jean-Marc Odobez\\rCategories: cs.CV\\rComments: First submitted for CVPR 2022. Current draft is in review\\r\\\\\\\\\\r  Gaze behaviors such as eye-contact or shared attention are important markers\\rfor diagnosing developmental disorders in children. While previous studies have\\rlooked at some of these elements, the analysis is usually performed on private\\rdatasets and is restricted to lab settings. Furthermore, all publicly available\\rgaze target prediction benchmarks mostly contain instances of adults, which\\rmakes models trained on them less applicable to scenarios with young children.\\rIn this paper, we propose the first study for predicting the gaze target of\\rchildren and interacting adults. To this end, we introduce the ChildPlay\\rdataset: a curated collection of short video clips featuring children playing\\rand interacting with adults in uncontrolled environments (e.g. kindergarten,\\rtherapy centers, preschools etc.), which we annotate with rich gaze\\rinformation. We further propose a new model for gaze target prediction that is\\rgeometrically grounded by explicitly identifying the scene parts in the 3D\\rfield of view (3DFoV) of the person, leveraging recent geometry preserving\\rdepth inference methods. Our model achieves state of the art results on\\rbenchmark datasets and ChildPlay. Furthermore, results show that looking at\\rfaces prediction performance on children is much worse than on adults, and can\\rbe significantly improved by fine-tuning models using child gaze annotations.\\rOur dataset and models will be made publicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01630 ,  21933kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01645\\rDate: Tue, 4 Jul 2023 10:57:52 GMT   (521kb,D)\\r\\rTitle: In-Domain Self-Supervised Learning Can Lead to Improvements in Remote\\r  Sensing Image Classification\\rAuthors: Ivica Dimitrovski, Ivan Kitanovski, Nikola Simidjievski, Dragi Kocev\\rCategories: cs.CV\\r\\\\\\\\\\r  Self-supervised learning (SSL) has emerged as a promising approach for remote\\rsensing image classification due to its ability to leverage large amounts of\\runlabeled data. In contrast to traditional supervised learning, SSL aims to\\rlearn representations of data without the need for explicit labels. This is\\rachieved by formulating auxiliary tasks that can be used to create\\rpseudo-labels for the unlabeled data and learn pre-trained models. The\\rpre-trained models can then be fine-tuned on downstream tasks such as remote\\rsensing image scene classification. The paper analyzes the effectiveness of SSL\\rpre-training using Million AID - a large unlabeled remote sensing dataset on\\rvarious remote sensing image scene classification datasets as downstream tasks.\\rMore specifically, we evaluate the effectiveness of SSL pre-training using the\\riBOT framework coupled with Vision transformers (ViT) in contrast to supervised\\rpre-training of ViT using the ImageNet dataset. The comprehensive experimental\\rwork across 14 datasets with diverse properties reveals that in-domain SSL\\rleads to improved predictive performance of models compared to the supervised\\rcounterparts.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01645 ,  521kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01651\\rDate: Tue, 4 Jul 2023 11:15:27 GMT   (17448kb,D)\\r\\rTitle: Task Planning Support for Arborists and Foresters: Comparing Deep\\r  Learning Approaches for Tree Inventory and Tree Vitality Assessment Based on\\r  UAV-Data\\rAuthors: Jonas-Dario Troles and Richard Nieding and Sonia Simons and Ute Schmid\\rCategories: cs.CV cs.CY\\rComments: I4CS 2023: 23rd International Conference on Innovations for Community\\r  Services\\r\\\\\\\\\\r  Climate crisis and correlating prolonged, more intense periods of drought\\rthreaten tree health in cities and forests. In consequence, arborists and\\rforesters suffer from increasing workloads and, in the best case, a consistent\\rbut often declining workforce. To optimise workflows and increase productivity,\\rwe propose a novel open-source end-to-end approach that generates helpful\\rinformation and improves task planning of those who care for trees in and\\raround cities. Our approach is based on RGB and multispectral UAV data, which\\ris used to create tree inventories of city parks and forests and to deduce tree\\rvitality assessments through statistical indices and Deep Learning. Due to EU\\rrestrictions regarding flying drones in urban areas, we will also use\\rmultispectral satellite data and fifteen soil moisture sensors to extend our\\rtree vitality-related basis of data. Furthermore, Bamberg already has a\\rgeoreferenced tree cadastre of around 15,000 solitary trees in the city area,\\rwhich is also used to generate helpful information. All mentioned data is then\\rjoined and visualised in an interactive web application allowing arborists and\\rforesters to generate individual and flexible evaluations, thereby improving\\rdaily task planning.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01651 ,  17448kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01663\\rDate: Tue, 4 Jul 2023 11:51:56 GMT   (443kb,D)\\r\\rTitle: Exploring Transformers for On-Line Handwritten Signature Verification\\rAuthors: Pietro Melzi, Ruben Tolosana, Ruben Vera-Rodriguez, Paula\\r  Delgado-Santos, Giuseppe Stragapede, Julian Fierrez, Javier Ortega-Garcia\\rCategories: cs.CV\\rComments: 2 pages, 2 figures\\r\\\\\\\\\\r  The application of mobile biometrics as a user-friendly authentication method\\rhas increased in the last years. Recent studies have proposed novel behavioral\\rbiometric recognition systems based on Transformers, which currently outperform\\rthe state of the art in several application scenarios. On-line handwritten\\rsignature verification aims to verify the identity of subjects, based on their\\rbiometric signatures acquired using electronic devices such as tablets or\\rsmartphones. This paper investigates the suitability of architectures based on\\rrecent Transformers for on-line signature verification. In particular, four\\rdifferent configurations are studied, two of them rely on the Vanilla\\rTransformer encoder, and the two others have been successfully applied to the\\rtasks of gait and activity recognition. We evaluate the four proposed\\rconfigurations according to the experimental protocol proposed in the\\rSVC-onGoing competition. The results obtained in our experiments are promising,\\rand promote the use of Transformers for on-line signature verification.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01663 ,  443kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01703\\rDate: Tue, 4 Jul 2023 13:18:39 GMT   (26893kb,D)\\r\\rTitle: Augment Features Beyond Color for Domain Generalized Segmentation\\rAuthors: Qiyu Sun, Pavlo Melnyk, Michael Felsberg, Yang Tang\\rCategories: cs.CV\\r\\\\\\\\\\r  Domain generalized semantic segmentation (DGSS) is an essential but highly\\rchallenging task, in which the model is trained only on source data and any\\rtarget data is not available. Previous DGSS methods can be partitioned into\\raugmentation-based and normalization-based ones. The former either introduces\\rextra biased data or only conducts channel-wise adjustments for data\\raugmentation, and the latter may discard beneficial visual information, both of\\rwhich lead to limited performance in DGSS. Contrarily, our method performs\\rinter-channel transformation and meanwhile evades domain-specific biases, thus\\rdiversifying data and enhancing model generalization performance. Specifically,\\rour method consists of two modules: random image color augmentation (RICA) and\\rrandom feature distribution augmentation (RFDA). RICA converts images from RGB\\rto the CIELAB color model and randomizes color maps in a perception-based way\\rfor image enhancement purposes. We further this augmentation by extending it\\rbeyond color to feature space using a CycleGAN-based generative network, which\\rcomplements RICA and further boosts generalization capability. We conduct\\rextensive experiments, and the generalization results from the synthetic GTAV\\rand SYNTHIA to the real Cityscapes, BDDS, and Mapillary datasets show that our\\rmethod achieves state-of-the-art performance in DGSS.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01703 ,  26893kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01704\\rDate: Tue, 4 Jul 2023 13:19:57 GMT   (1546kb,D)\\r\\rTitle: Graph-Ensemble Learning Model for Multi-label Skin Lesion Classification\\r  using Dermoscopy and Clinical Images\\rAuthors: Peng Tang, Yang Nan, Tobias Lasser\\rCategories: cs.CV\\rComments: Submitted to TNNLS in 1st July 2023\\r\\\\\\\\\\r  Many skin lesion analysis (SLA) methods recently focused on developing a\\rmulti-modal-based multi-label classification method due to two factors. The\\rfirst is multi-modal data, i.e., clinical and dermoscopy images, which can\\rprovide complementary information to obtain more accurate results than\\rsingle-modal data. The second one is that multi-label classification, i.e.,\\rseven-point checklist (SPC) criteria as an auxiliary classification task can\\rnot only boost the diagnostic accuracy of melanoma in the deep learning (DL)\\rpipeline but also provide more useful functions to the clinical doctor as it is\\rcommonly used in clinical dermatologist's diagnosis. However, most methods only\\rfocus on designing a better module for multi-modal data fusion; few methods\\rexplore utilizing the label correlation between SPC and skin disease for\\rperformance improvement. This study fills the gap that introduces a Graph\\rConvolution Network (GCN) to exploit prior co-occurrence between each category\\ras a correlation matrix into the DL model for the multi-label classification.\\rHowever, directly applying GCN degraded the performances in our experiments; we\\rattribute this to the weak generalization ability of GCN in the scenario of\\rinsufficient statistical samples of medical data. We tackle this issue by\\rproposing a Graph-Ensemble Learning Model (GELN) that views the prediction from\\rGCN as complementary information of the predictions from the fusion model and\\radaptively fuses them by a weighted averaging scheme, which can utilize the\\rvaluable information from GCN while avoiding its negative influences as much as\\rpossible. To evaluate our method, we conduct experiments on public datasets.\\rThe results illustrate that our GELN can consistently improve the\\rclassification performance on different datasets and that the proposed method\\rcan achieve state-of-the-art performance in SPC and diagnosis classification.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01704 ,  1546kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01740\\rDate: Tue, 4 Jul 2023 14:16:49 GMT   (1050kb)\\r\\rTitle: Synchronous Image-Label Diffusion Probability Model with Application to\\r  Stroke Lesion Segmentation on Non-contrast CT\\rAuthors: Jianhai Zhang and Tonghua Wan and Ethan MacDonald and Aravind Ganesh\\r  and Qiu Wu\\rCategories: cs.CV\\r\\\\\\\\\\r  Stroke lesion volume is a key radiologic measurement for assessing the\\rprognosis of Acute Ischemic Stroke (AIS) patients, which is challenging to be\\rautomatically measured on Non-Contrast CT (NCCT) scans. Recent diffusion\\rprobabilistic models have shown potentials of being used for image\\rsegmentation. In this paper, a novel Synchronous image-label Diffusion\\rProbability Model (SDPM) is proposed for stroke lesion segmentation on NCCT\\rusing Markov diffusion process. The proposed SDPM is fully based on a Latent\\rVariable Model (LVM), offering a complete probabilistic elaboration. An\\radditional net-stream, parallel with a noise prediction stream, is introduced\\rto obtain initial noisy label estimates for efficiently inferring the final\\rlabels. By optimizing the specified variational boundaries, the trained model\\rcan infer multiple label estimates for reference given the input images with\\rnoises. The proposed model was assessed on three stroke lesion datasets\\rincluding one public and two private datasets. Compared to several U-net and\\rtransformer-based segmentation methods, our proposed SDPM model is able to\\rachieve state-of-the-art performance. The code is publicly available.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01740 ,  1050kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01741\\rDate: Tue, 4 Jul 2023 14:17:54 GMT   (872kb,D)\\r\\rTitle: Ben-ge: Extending BigEarthNet with Geographical and Environmental Data\\rAuthors: Michael Mommert, Nicolas Kesseli, Jo\\\\elle Hanna, Linus Scheibenreif,\\r  Damian Borth, Beg\\\\um Demir\\rCategories: cs.CV\\rComments: Accepted for presentation at the IEEE International Geoscience and\\r  Remote Sensing Symposium 2023\\r\\\\\\\\\\r  Deep learning methods have proven to be a powerful tool in the analysis of\\rlarge amounts of complex Earth observation data. However, while Earth\\robservation data are multi-modal in most cases, only single or few modalities\\rare typically considered. In this work, we present the ben-ge dataset, which\\rsupplements the BigEarthNet-MM dataset by compiling freely and globally\\ravailable geographical and environmental data. Based on this dataset, we\\rshowcase the value of combining different data modalities for the downstream\\rtasks of patch-based land-use/land-cover classification and land-use/land-cover\\rsegmentation. ben-ge is freely available and expected to serve as a test bed\\rfor fully supervised and self-supervised Earth observation applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01741 ,  872kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01750\\rDate: Tue, 4 Jul 2023 14:39:59 GMT   (19744kb,D)\\r\\rTitle: SRCD: Semantic Reasoning with Compound Domains for Single-Domain\\r  Generalized Object Detection\\rAuthors: Zhijie Rao, Jingcai Guo, Luyao Tang, Yue Huang, Xinghao Ding, Song Guo\\rCategories: cs.CV cs.LG\\rComments: 10 pages, 5 figures\\r\\\\\\\\\\r  This paper provides a novel framework for single-domain generalized object\\rdetection (i.e., Single-DGOD), where we are interested in learning and\\rmaintaining the semantic structures of self-augmented compound cross-domain\\rsamples to enhance the model's generalization ability. Different from DGOD\\rtrained on multiple source domains, Single-DGOD is far more challenging to\\rgeneralize well to multiple target domains with only one single source domain.\\rExisting methods mostly adopt a similar treatment from DGOD to learn\\rdomain-invariant features by decoupling or compressing the semantic space.\\rHowever, there may have two potential limitations: 1) pseudo attribute-label\\rcorrelation, due to extremely scarce single-domain data; and 2) the semantic\\rstructural information is usually ignored, i.e., we found the affinities of\\rinstance-level semantic relations in samples are crucial to model\\rgeneralization. In this paper, we introduce Semantic Reasoning with Compound\\rDomains (SRCD) for Single-DGOD. Specifically, our SRCD contains two main\\rcomponents, namely, the texture-based self-augmentation (TBSA) module, and the\\rlocal-global semantic reasoning (LGSR) module. TBSA aims to eliminate the\\reffects of irrelevant attributes associated with labels, such as light, shadow,\\rcolor, etc., at the image level by a light-yet-efficient self-augmentation.\\rMoreover, LGSR is used to further model the semantic relationships on instance\\rfeatures to uncover and maintain the intrinsic semantic structures. Extensive\\rexperiments on multiple benchmarks demonstrate the effectiveness of the\\rproposed SRCD.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01750 ,  19744kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01754\\rDate: Tue, 4 Jul 2023 14:51:03 GMT   (3839kb,D)\\r\\rTitle: K-complex Detection Using Fourier Spectrum Analysis In EEG\\rAuthors: Alexey Protopopov\\rCategories: cs.CV\\rComments: 11 pages, 6 figures, 3 tables\\r\\\\\\\\\\r  K-complexes are an important marker of brain activity and are used both in\\rclinical practice to perform sleep scoring, and in research. However, due to\\rthe size of electroencephalography (EEG) records, as well as the subjective\\rnature of K-complex detection performed by somnologists, it is reasonable to\\rautomate K-complex detection. Previous works in this field of research have\\rrelied on the values of true positive rate and false positive rate to quantify\\rthe effectiveness of proposed methods, however this set of metrics may be\\rmisleading. The objective of the present research is to find a more accurate\\rset of metrics and use them to develop a new method of K-complex detection,\\rwhich would not rely on neural networks. Thus, the present article proposes two\\rnew methods for K-complex detection based on the fast Fourier transform. The\\rresults achieved demonstrated that the proposed methods offered a quality of\\rK-complex detection that is either similar or superior to the quality of the\\rmethods demonstrated in previous works, including the methods employing neural\\rnetworks, while requiring less computational power, meaning that K-complex\\rdetection does not require the use of neural networks. The proposed methods\\rwere evaluated using a new set of metrics, which is more representative of the\\rquality of K-complex detection.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01754 ,  3839kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01759\\rDate: Tue, 4 Jul 2023 15:00:06 GMT   (841kb,D)\\r\\rTitle: Pretraining is All You Need: A Multi-Atlas Enhanced Transformer\\r  Framework for Autism Spectrum Disorder Classification\\rAuthors: Lucas Mahler, Qi Wang, Julius Steiglechner, Florian Birk, Samuel\\r  Heczko, Klaus Scheffler, Gabriele Lohmann\\rCategories: cs.CV cs.LG eess.IV\\r\\\\\\\\\\r  Autism spectrum disorder (ASD) is a prevalent psychiatric condition\\rcharacterized by atypical cognitive, emotional, and social patterns. Timely and\\raccurate diagnosis is crucial for effective interventions and improved outcomes\\rin individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced\\rTransformer framework, METAFormer, ASD classification. Our framework utilizes\\rresting-state functional magnetic resonance imaging data from the ABIDE I\\rdataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer\\remploys a multi-atlas approach, where flattened connectivity matrices from the\\rAAL, CC200, and DOS160 atlases serve as input to the transformer encoder.\\rNotably, we demonstrate that self-supervised pretraining, involving the\\rreconstruction of masked values from the input, significantly enhances\\rclassification performance without the need for additional or separate training\\rdata. Through stratified cross-validation, we evaluate the proposed framework\\rand show that it surpasses state-of-the-art performance on the ABIDE I dataset,\\rwith an average accuracy of 83.7% and an AUC-score of 0.832. The code for our\\rframework is available at https://github.com/Lugges991/METAFormer\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01759 ,  841kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01778\\rDate: Tue, 4 Jul 2023 15:31:03 GMT   (44402kb,D)\\r\\rTitle: Physically Realizable Natural-Looking Clothing Textures Evade Person\\r  Detectors via 3D Modeling\\rAuthors: Zhanhao Hu, Wenda Chu, Xiaopei Zhu, Hui Zhang, Bo Zhang, Xiaolin Hu\\rCategories: cs.CV cs.AI cs.CR\\rComments: Accepted by CVPR 2023\\r\\\\\\\\\\r  Recent works have proposed to craft adversarial clothes for evading person\\rdetectors, while they are either only effective at limited viewing angles or\\rvery conspicuous to humans. We aim to craft adversarial texture for clothes\\rbased on 3D modeling, an idea that has been used to craft rigid adversarial\\robjects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes\\rare non-rigid, leading to difficulties in physical realization. In order to\\rcraft natural-looking adversarial clothes that can evade person detectors at\\rmultiple viewing angles, we propose adversarial camouflage textures (AdvCaT)\\rthat resemble one kind of the typical textures of daily clothes, camouflage\\rtextures. We leverage the Voronoi diagram and Gumbel-softmax trick to\\rparameterize the camouflage textures and optimize the parameters via 3D\\rmodeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes\\rcombining topologically plausible projection (TopoProj) and Thin Plate Spline\\r(TPS) to narrow the gap between digital and real-world objects. We printed the\\rdeveloped 3D texture pieces on fabric materials and tailored them into T-shirts\\rand trousers. Experiments show high attack success rates of these clothes\\ragainst multiple detectors.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01778 ,  44402kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01806\\rDate: Tue, 4 Jul 2023 16:21:39 GMT   (6119kb,D)\\r\\rTitle: DeepFlorist: Rethinking Deep Neural Networks and Ensemble Learning as A\\r  Meta-Classifier For Object Classification\\rAuthors: Afshin Khadangi\\rCategories: cs.CV cs.AI cs.DC\\r\\\\\\\\\\r  In this paper, we propose a novel learning paradigm called DeepFlorist for\\rflower classification using ensemble learning as a meta-classifier. DeepFlorist\\rcombines the power of deep learning with the robustness of ensemble methods to\\rachieve accurate and reliable flower classification results. The proposed\\rnetwork architecture leverages a combination of dense convolutional and\\rconvolutional neural networks (DCNNs and CNNs) to extract high-level features\\rfrom flower images, followed by a fully connected layer for classification. To\\renhance the performance and generalization of DeepFlorist, an ensemble learning\\rapproach is employed, incorporating multiple diverse models to improve the\\rclassification accuracy. Experimental results on benchmark flower datasets\\rdemonstrate the effectiveness of DeepFlorist, outperforming state-of-the-art\\rmethods in terms of accuracy and robustness. The proposed framework holds\\rsignificant potential for automated flower recognition systems in real-world\\rapplications, enabling advancements in plant taxonomy, conservation efforts,\\rand ecological studies.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01806 ,  6119kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01807\\rDate: Tue, 4 Jul 2023 16:22:10 GMT   (1446kb,D)\\r\\rTitle: SUIT: Learning Significance-guided Information for 3D Temporal Detection\\rAuthors: Zheyuan Zhou, Jiachen Lu, Yihan Zeng, Hang Xu, Li Zhang\\rCategories: cs.CV cs.RO\\rComments: Accepted to IROS 2023\\r\\\\\\\\\\r  3D object detection from LiDAR point cloud is of critical importance for\\rautonomous driving and robotics. While sequential point cloud has the potential\\rto enhance 3D perception through temporal information, utilizing these temporal\\rfeatures effectively and efficiently remains a challenging problem. Based on\\rthe observation that the foreground information is sparsely distributed in\\rLiDAR scenes, we believe sufficient knowledge can be provided by sparse format\\rrather than dense maps. To this end, we propose to learn Significance-gUided\\rInformation for 3D Temporal detection (SUIT), which simplifies temporal\\rinformation as sparse features for information fusion across frames.\\rSpecifically, we first introduce a significant sampling mechanism that extracts\\rinformation-rich yet sparse features based on predicted object centroids. On\\rtop of that, we present an explicit geometric transformation learning\\rtechnique, which learns the object-centric transformations among sparse\\rfeatures across frames. We evaluate our method on large-scale nuScenes and\\rWaymo dataset, where our SUIT not only significantly reduces the memory and\\rcomputation cost of temporal fusion, but also performs well over the\\rstate-of-the-art baselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01807 ,  1446kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01817\\rDate: Tue, 4 Jul 2023 16:45:21 GMT   (2324kb,D)\\r\\rTitle: Human Trajectory Forecasting with Explainable Behavioral Uncertainty\\rAuthors: Jiangbei Yue, Dinesh Manocha and He Wang\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Human trajectory forecasting helps to understand and predict human behaviors,\\renabling applications from social robots to self-driving cars, and therefore\\rhas been heavily investigated. Most existing methods can be divided into\\rmodel-free and model-based methods. Model-free methods offer superior\\rprediction accuracy but lack explainability, while model-based methods provide\\rexplainability but cannot predict well. Combining both methodologies, we\\rpropose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM,\\rwhere a behavior SDE model is combined with Bayesian neural networks (BNNs).\\rWhile the NNs provide superior predictive power, the SDE offers strong\\rexplainability with quantifiable uncertainty in behavior and observation. We\\rshow that BNSP-SFM achieves up to a 50% improvement in prediction accuracy,\\rcompared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to\\rdrastically different scenes with different environments and crowd densities (~\\r20 times higher than the testing data). Finally, BNSP-SFM can provide\\rpredictions with confidence to better explain potential causes of behaviors.\\rThe code will be released upon acceptance.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01817 ,  2324kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01831\\rDate: Tue, 4 Jul 2023 17:15:46 GMT   (4051kb,D)\\r\\rTitle: DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation\\rAuthors: Shentong Mo, Enze Xie, Ruihang Chu, Lewei Yao, Lanqing Hong, Matthias\\r  Nie{\\\\ss}ner, Zhenguo Li\\rCategories: cs.CV cs.AI cs.LG\\rComments: Project Page: https://dit-3d.github.io/\\r\\\\\\\\\\r  Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful\\reffectiveness in generating high-quality 2D images. However, it is still being\\rdetermined whether the Transformer architecture performs equally well in 3D\\rshape generation, as previous 3D diffusion methods mostly adopted the U-Net\\rarchitecture. To bridge this gap, we propose a novel Diffusion Transformer for\\r3D shape generation, namely DiT-3D, which can directly operate the denoising\\rprocess on voxelized point clouds using plain Transformers. Compared to\\rexisting U-Net approaches, our DiT-3D is more scalable in model size and\\rproduces much higher quality generations. Specifically, the DiT-3D adopts the\\rdesign philosophy of DiT but modifies it by incorporating 3D positional and\\rpatch embeddings to adaptively aggregate input from voxelized point clouds. To\\rreduce the computational cost of self-attention in 3D shape generation, we\\rincorporate 3D window attention into Transformer blocks, as the increased 3D\\rtoken length resulting from the additional dimension of voxels can lead to high\\rcomputation. Finally, linear and devoxelization layers are used to predict the\\rdenoised point clouds. In addition, our transformer architecture supports\\refficient fine-tuning from 2D to 3D, where the pre-trained DiT-2D checkpoint on\\rImageNet can significantly improve DiT-3D on ShapeNet. Experimental results on\\rthe ShapeNet dataset demonstrate that the proposed DiT-3D achieves\\rstate-of-the-art performance in high-fidelity and diverse 3D point cloud\\rgeneration. In particular, our DiT-3D decreases the 1-Nearest Neighbor Accuracy\\rof the state-of-the-art method by 4.59 and increases the Coverage metric by\\r3.51 when evaluated on Chamfer Distance.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01831 ,  4051kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01836\\rDate: Tue, 4 Jul 2023 17:28:58 GMT   (293kb,D)\\r\\rTitle: On the Matrix Form of the Quaternion Fourier Transform and Quaternion\\r  Convolution\\rAuthors: Giorgos Sfikas and George Retsinas\\rCategories: cs.CV math.RA\\rComments: 22 pages, 2 figures\\rMSC-class: 15B05, 15B33, 65F15, 65F99\\rACM-class: I.4.0\\r\\\\\\\\\\r  We study matrix forms of quaternionic versions of the Fourier Transform and\\rConvolution operations. Quaternions offer a powerful representation unit,\\rhowever they are related to difficulties in their use that stem foremost from\\rnon-commutativity of quaternion multiplication, and due to that $\\\\mu^2 = -1$\\rposseses infinite solutions in the quaternion domain. Handling of quaternionic\\rmatrices is consequently complicated in several aspects (definition of\\reigenstructure, determinant, etc.). Our research findings clarify the relation\\rof the Quaternion Fourier Transform matrix to the standard (complex) Discrete\\rFourier Transform matrix, and the extend on which well-known complex-domain\\rtheorems extend to quaternions. We focus especially on the relation of\\rQuaternion Fourier Transform matrices to Quaternion Circulant matrices\\r(representing quaternionic convolution), and the eigenstructure of the latter.\\rA proof-of-concept application that makes direct use of our theoretical results\\ris presented, where we produce a method to bound the spectral norm of a\\rQuaternionic Convolution.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01836 ,  293kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01838\\rDate: Tue, 4 Jul 2023 17:30:19 GMT   (924kb,D)\\r\\rTitle: EdgeFace: Efficient Face Recognition Model for Edge Devices\\rAuthors: Anjith George and Christophe Ecabert and Hatef Otroshi Shahreza and\\r  Ketan Kotwal and Sebastien Marcel\\rCategories: cs.CV cs.CR\\rComments: 8 pages\\r\\\\\\\\\\r  In this paper, we present EdgeFace, a lightweight and efficient face\\rrecognition network inspired by the hybrid architecture of EdgeNeXt. By\\reffectively combining the strengths of both CNN and Transformer models, and a\\rlow rank linear layer, EdgeFace achieves excellent face recognition performance\\roptimized for edge devices. The proposed EdgeFace network not only maintains\\rlow computational costs and compact storage, but also achieves high face\\rrecognition accuracy, making it suitable for deployment on edge devices.\\rExtensive experiments on challenging benchmark face datasets demonstrate the\\reffectiveness and efficiency of EdgeFace in comparison to state-of-the-art\\rlightweight models and deep face recognition models. Our EdgeFace model with\\r1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B\\r(92.67%), and IJB-C (94.85%), outperforming other efficient models with larger\\rcomputational complexities. The code to replicate the experiments will be made\\ravailable publicly.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01838 ,  924kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01844\\rDate: Tue, 4 Jul 2023 17:46:02 GMT   (6465kb,D)\\r\\rTitle: Advancing Wound Filling Extraction on 3D Faces: A Auto-Segmentation and\\r  Wound Face Regeneration Approach\\rAuthors: Duong Q. Nguyen and Thinh D. Le and Phuong D. Nguyen and H.\\r  Nguyen-Xuan\\rCategories: cs.CV\\r\\\\\\\\\\r  Facial wound segmentation plays a crucial role in preoperative planning and\\roptimizing patient outcomes in various medical applications. In this paper, we\\rpropose an efficient approach for automating 3D facial wound segmentation using\\ra two-stream graph convolutional network. Our method leverages the Cir3D-FaIR\\rdataset and addresses the challenge of data imbalance through extensive\\rexperimentation with different loss functions. To achieve accurate\\rsegmentation, we conducted thorough experiments and selected a high-performing\\rmodel from the trained models. The selected model demonstrates exceptional\\rsegmentation performance for complex 3D facial wounds. Furthermore, based on\\rthe segmentation model, we propose an improved approach for extracting 3D\\rfacial wound fillers and compare it to the results of the previous study. Our\\rmethod achieved a remarkable accuracy of 0.9999986\\\\% on the test suite,\\rsurpassing the performance of the previous method. From this result, we use 3D\\rprinting technology to illustrate the shape of the wound filling. The outcomes\\rof this study have significant implications for physicians involved in\\rpreoperative planning and intervention design. By automating facial wound\\rsegmentation and improving the accuracy of wound-filling extraction, our\\rapproach can assist in carefully assessing and optimizing interventions,\\rleading to enhanced patient outcomes. Additionally, it contributes to advancing\\rfacial reconstruction techniques by utilizing machine learning and 3D\\rbioprinting for printing skin tissue implants. Our source code is available at\\r\\\\url{https://github.com/SIMOGroup/WoundFilling3D}.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01844 ,  6465kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01845\\rDate: Tue, 4 Jul 2023 17:46:20 GMT   (2349kb,D)\\r\\rTitle: Deep Features for Contactless Fingerprint Presentation Attack Detection:\\r  Can They Be Generalized?\\rAuthors: Hailin Li and Raghavendra Ramachandra\\rCategories: cs.CV\\rComments: Preprint paper accepted by First Workshop on Contactless Hand\\r  Biometrics and Gesture Recognition (CHBGR-2023)\\r\\\\\\\\\\r  The rapid evolution of high-end smartphones with advanced high-resolution\\rcameras has resulted in contactless capture of fingerprint biometrics that are\\rmore reliable and suitable for verification. Similar to other biometric\\rsystems, contactless fingerprint-verification systems are vulnerable to\\rpresentation attacks. In this paper, we present a comparative study on the\\rgeneralizability of seven different pre-trained Convolutional Neural Networks\\r(CNN) and a Vision Transformer (ViT) to reliably detect presentation attacks.\\rExtensive experiments were carried out on publicly available smartphone-based\\rpresentation attack datasets using four different Presentation Attack\\rInstruments (PAI). The detection performance of the eighth deep feature\\rtechnique was evaluated using the leave-one-out protocol to benchmark the\\rgeneralization performance for unseen PAI. The obtained results indicated the\\rbest generalization performance with the ResNet50 CNN.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01845 ,  2349kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01848\\rDate: Tue, 4 Jul 2023 17:58:25 GMT   (26088kb,D)\\r\\rTitle: Embodied Task Planning with Large Language Models\\rAuthors: Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan\\rCategories: cs.CV cs.RO\\rComments: Project Page: https://gary3410.github.io/TaPA\\r\\\\\\\\\\r  Equipping embodied agents with commonsense is important for robots to\\rsuccessfully complete complex human instructions in general environments.\\rRecent large language models (LLM) can embed rich semantic knowledge for agents\\rin plan generation of complex tasks, while they lack the information about the\\rrealistic world and usually yield infeasible action sequences. In this paper,\\rwe propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning\\rwith physical scene constraint, where the agent generates executable plans\\raccording to the existed objects in the scene by aligning LLMs with the visual\\rperception models. Specifically, we first construct a multimodal dataset\\rcontaining triplets of indoor scenes, instructions and action plans, where we\\rprovide the designed prompts and the list of existing objects in the scene for\\rGPT-3.5 to generate a large number of instructions and corresponding planned\\ractions. The generated data is leveraged for grounded plan tuning of\\rpre-trained LLMs. During inference, we discover the objects in the scene by\\rextending open-vocabulary object detectors to multi-view RGB images collected\\rin different achievable locations. Experimental results show that the generated\\rplan from our TaPA framework can achieve higher success rate than LLaVA and\\rGPT-3.5 by a sizable margin, which indicates the practicality of embodied task\\rplanning in general and complex environments.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01848 ,  26088kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01864\\rDate: Tue, 4 Jul 2023 18:22:00 GMT   (2219kb,D)\\r\\rTitle: MaskBEV: Joint Object Detection and Footprint Completion for Bird's-eye\\r  View 3D Point Clouds\\rAuthors: William Guimont-Martin, Jean-Michel Fortin, Fran\\\\c{c}ois Pomerleau,\\r  Philippe Gigu\\\\`ere\\rCategories: cs.CV\\rComments: This work has been submitted to the IEEE for possible publication.\\r  Copyright may be transferred without notice, after which this version may no\\r  longer be accessible\\r\\\\\\\\\\r  Recent works in object detection in LiDAR point clouds mostly focus on\\rpredicting bounding boxes around objects. This prediction is commonly achieved\\rusing anchor-based or anchor-free detectors that predict bounding boxes,\\rrequiring significant explicit prior knowledge about the objects to work\\rproperly. To remedy these limitations, we propose MaskBEV, a bird's-eye view\\r(BEV) mask-based object detector neural architecture. MaskBEV predicts a set of\\rBEV instance masks that represent the footprints of detected objects. Moreover,\\rour approach allows object detection and footprint completion in a single pass.\\rMaskBEV also reformulates the detection problem purely in terms of\\rclassification, doing away with regression usually done to predict bounding\\rboxes. We evaluate the performance of MaskBEV on both SemanticKITTI and KITTI\\rdatasets while analyzing the architecture advantages and limitations.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01864 ,  2219kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01893\\rDate: Tue, 4 Jul 2023 19:34:53 GMT   (484kb,D)\\r\\rTitle: EANet: Enhanced Attribute-based RGBT Tracker Network\\rAuthors: Abbas T\\\\urko\\\\u{g}lu, Erdem Akag\\\\und\\\\uz\\rCategories: cs.CV\\r\\\\\\\\\\r  Tracking objects can be a difficult task in computer vision, especially when\\rfaced with challenges such as occlusion, changes in lighting, and motion blur.\\rRecent advances in deep learning have shown promise in challenging these\\rconditions. However, most deep learning-based object trackers only use visible\\rband (RGB) images. Thermal infrared electromagnetic waves (TIR) can provide\\radditional information about an object, including its temperature, when faced\\rwith challenging conditions. We propose a deep learning-based image tracking\\rapproach that fuses RGB and thermal images (RGBT). The proposed model consists\\rof two main components: a feature extractor and a tracker. The feature\\rextractor encodes deep features from both the RGB and the TIR images. The\\rtracker then uses these features to track the object using an enhanced\\rattribute-based architecture. We propose a fusion of attribute-specific feature\\rselection with an aggregation module. The proposed methods are evaluated on the\\rRGBT234 \\\\cite{LiCLiang2018} and LasHeR \\\\cite{LiLasher2021} datasets, which are\\rthe most widely used RGBT object-tracking datasets in the literature. The\\rresults show that the proposed system outperforms state-of-the-art RGBT object\\rtrackers on these datasets, with a relatively smaller number of parameters.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01893 ,  484kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01924\\rDate: Tue, 4 Jul 2023 21:18:39 GMT   (4980kb,D)\\r\\rTitle: ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype\\r  Learning\\rAuthors: Gulcin Baykal, Halil Faruk Karagoz, Taha Binhuraib, Gozde Unal\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Diffusion models are generative models that have shown significant advantages\\rcompared to other generative models in terms of higher generation quality and\\rmore stable training. However, the computational need for training diffusion\\rmodels is considerably increased. In this work, we incorporate prototype\\rlearning into diffusion models to achieve high generation quality faster than\\rthe original diffusion model. Instead of randomly initialized class embeddings,\\rwe use separately learned class prototypes as the conditioning information to\\rguide the diffusion process. We observe that our method, called ProtoDiffusion,\\rachieves better performance in the early stages of training compared to the\\rbaseline method, signifying that using the learned prototypes shortens the\\rtraining time. We demonstrate the performance of ProtoDiffusion using various\\rdatasets and experimental settings, achieving the best performance in shorter\\rtimes across all settings.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01924 ,  4980kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01938\\rDate: Tue, 4 Jul 2023 21:57:05 GMT   (24422kb,D)\\r\\rTitle: Physics-based Motion Retargeting from Sparse Inputs\\rAuthors: Daniele Reda, Jungdam Won, Yuting Ye, Michiel van de Panne, Alexander\\r  Winkler\\rCategories: cs.CV\\rComments: More info at: https://www.cs.ubc.ca/~dreda/retargeting.html\\r\\\\\\\\\\r  Avatars are important to create interactive and immersive experiences in\\rvirtual worlds. One challenge in animating these characters to mimic a user's\\rmotion is that commercial AR/VR products consist only of a headset and\\rcontrollers, providing very limited sensor data of the user's pose. Another\\rchallenge is that an avatar might have a different skeleton structure than a\\rhuman and the mapping between them is unclear. In this work we address both of\\rthese challenges. We introduce a method to retarget motions in real-time from\\rsparse human sensor data to characters of various morphologies. Our method uses\\rreinforcement learning to train a policy to control characters in a physics\\rsimulator. We only require human motion capture data for training, without\\rrelying on artist-generated animations for each avatar. This allows us to use\\rlarge motion capture datasets to train general policies that can track unseen\\rusers from real and sparse data in real-time. We demonstrate the feasibility of\\rour approach on three characters with different skeleton structure: a dinosaur,\\ra mouse-like creature and a human. We show that the avatar poses often match\\rthe user surprisingly well, despite having no sensor information of the lower\\rbody available. We discuss and ablate the important components in our\\rframework, specifically the kinematic retargeting step, the imitation, contact\\rand action reward as well as our asymmetric actor-critic observations. We\\rfurther explore the robustness of our method in a variety of settings including\\runbalancing, dancing and sports motions.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01938 ,  24422kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01945\\rDate: Tue, 4 Jul 2023 22:28:17 GMT   (13456kb,D)\\r\\rTitle: Query-based Video Summarization with Pseudo Label Supervision\\rAuthors: Jia-Hong Huang, Luka Murn, Marta Mrak, Marcel Worring\\rCategories: cs.CV cs.AI cs.IR\\rComments: This paper is accepted by IEEE International Conference on Image\\r  Processing (ICIP), 2023\\r\\\\\\\\\\r  Existing datasets for manually labelled query-based video summarization are\\rcostly and thus small, limiting the performance of supervised deep video\\rsummarization models. Self-supervision can address the data sparsity challenge\\rby using a pretext task and defining a method to acquire extra data with pseudo\\rlabels to pre-train a supervised deep model. In this work, we introduce\\rsegment-level pseudo labels from input videos to properly model both the\\rrelationship between a pretext task and a target task, and the implicit\\rrelationship between the pseudo label and the human-defined label. The pseudo\\rlabels are generated based on existing human-defined frame-level labels. To\\rcreate more accurate query-dependent video summaries, a semantics booster is\\rproposed to generate context-aware query representations. Furthermore, we\\rpropose mutual attention to help capture the interactive information between\\rvisual and textual modalities. Three commonly-used video summarization\\rbenchmarks are used to thoroughly validate the proposed approach. Experimental\\rresults show that the proposed video summarization algorithm achieves\\rstate-of-the-art performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01945 ,  13456kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01946\\rDate: Tue, 4 Jul 2023 22:42:55 GMT   (15161kb,D)\\r\\rTitle: A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to\\r  Facilitate Deep Learning-Based Scanned ECG Digitization\\rAuthors: Kshama Kodthalu Shivashankara and Reza Sameni\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Access to medical data is often limited as it contains protected health\\rinformation (PHI). There are privacy concerns regarding using records\\rcontaining personally identifiable information. Recent advancements have been\\rmade in applying deep learning-based algorithms for clinical diagnosis and\\rdecision-making. However, deep learning models are data-greedy, whereas the\\ravailability of medical datasets for training and evaluating these models is\\rrelatively limited. Data augmentation with so-called \\\\textit{digital twins} is\\ran emerging technique to address this need. This paper presents a novel\\rapproach for generating synthetic electrocardiogram (ECG) images with realistic\\rartifacts from time-series data for use in developing algorithms for\\rdigitization of ECG images. Synthetic data is generated in a privacy-preserving\\rmanner by generating distortionless ECG images on standard ECG paper\\rbackground. Next, various distortions, including handwritten text artifacts,\\rwrinkles, creases, and perspective transforms are applied to the ECG images.\\rThe artifacts are generated synthetically, without personally identifiable\\rinformation. As a use case, we generated a large ECG image dataset of 21,801\\rrecords from the PhysioNet PTB-XL dataset, with 12 lead ECG time-series data\\rfrom 18,869 patients. A deep ECG image digitization model was developed and\\rtrained on the synthetic dataset, and was employed to convert the synthetic\\rimages to time-series data for evaluation. The signal-to-noise ratio (SNR) was\\rcalculated to assess the image digitization quality vs the ground truth ECG\\rtime-series. The results show an average signal recovery SNR of 27$\\\\pm$2.8\\\\,dB,\\rdemonstrating the significance of the proposed synthetic ECG image dataset for\\rtraining deep learning models.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01946 ,  15161kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01947\\rDate: Tue, 4 Jul 2023 22:52:16 GMT   (17264kb,D)\\r\\rTitle: Causal Video Summarizer for Video Exploration\\rAuthors: Jia-Hong Huang, Chao-Han Huck Yang, Pin-Yu Chen, Andrew Brown, Marcel\\r  Worring\\rCategories: cs.CV cs.AI cs.IR\\rComments: This paper is accepted by IEEE International Conference on Multimedia\\r  and Expo (ICME), 2022\\r\\\\\\\\\\r  Recently, video summarization has been proposed as a method to help video\\rexploration. However, traditional video summarization models only generate a\\rfixed video summary which is usually independent of user-specific needs and\\rhence limits the effectiveness of video exploration. Multi-modal video\\rsummarization is one of the approaches utilized to address this issue.\\rMulti-modal video summarization has a video input and a text-based query input.\\rHence, effective modeling of the interaction between a video input and\\rtext-based query is essential to multi-modal video summarization. In this work,\\ra new causality-based method named Causal Video Summarizer (CVS) is proposed to\\reffectively capture the interactive information between the video and query to\\rtackle the task of multi-modal video summarization. The proposed method\\rconsists of a probabilistic encoder and a probabilistic decoder. Based on the\\revaluation of the existing multi-modal video summarization dataset,\\rexperimental results show that the proposed approach is effective with the\\rincrease of +5.4% in accuracy and +4.92% increase of F 1- score, compared with\\rthe state-of-the-art method.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01947 ,  17264kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01952\\rDate: Tue, 4 Jul 2023 23:04:57 GMT   (17167kb,D)\\r\\rTitle: SDXL: Improving Latent Diffusion Models for High-Resolution Image\\r  Synthesis\\rAuthors: Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim\\r  Dockhorn, Jonas M\\\\uller, Joe Penna, Robin Rombach\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  We present SDXL, a latent diffusion model for text-to-image synthesis.\\rCompared to previous versions of Stable Diffusion, SDXL leverages a three times\\rlarger UNet backbone: The increase of model parameters is mainly due to more\\rattention blocks and a larger cross-attention context as SDXL uses a second\\rtext encoder. We design multiple novel conditioning schemes and train SDXL on\\rmultiple aspect ratios. We also introduce a refinement model which is used to\\rimprove the visual fidelity of samples generated by SDXL using a post-hoc\\rimage-to-image technique. We demonstrate that SDXL shows drastically improved\\rperformance compared the previous versions of Stable Diffusion and achieves\\rresults competitive with those of black-box state-of-the-art image generators.\\rIn the spirit of promoting open research and fostering transparency in large\\rmodel training and evaluation, we provide access to code and model weights at\\rhttps://github.com/Stability-AI/generative-models\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01952 ,  17167kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01953\\rDate: Tue, 4 Jul 2023 23:06:57 GMT   (15577kb,D)\\r\\rTitle: Toward more frugal models for functional cerebral networks automatic\\r  recognition with resting-state fMRI\\rAuthors: Lukman Ismaila, Pejman Rasti, Jean-Michel Lem\\\\'ee, David Rousseau\\rCategories: cs.CV\\r\\\\\\\\\\r  We refer to a machine learning situation where models based on classical\\rconvolutional neural networks have shown good performance. We are investigating\\rdifferent encoding techniques in the form of supervoxels, then graphs to reduce\\rthe complexity of the model while tracking the loss of performance. This\\rapproach is illustrated on a recognition task of resting-state functional\\rnetworks for patients with brain tumors. Graphs encoding supervoxels preserve\\ractivation characteristics of functional brain networks from images, optimize\\rmodel parameters by 26 times while maintaining CNN model performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01953 ,  15577kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01957\\rDate: Tue, 4 Jul 2023 23:28:01 GMT   (7319kb,D)\\r\\rTitle: Hybrid Neural Diffeomorphic Flow for Shape Representation and Generation\\r  via Triplane\\rAuthors: Kun Han, Shanlin Sun, Xiaohui Xie\\rCategories: cs.CV\\r\\\\\\\\\\r  Deep Implicit Functions (DIFs) have gained popularity in 3D computer vision\\rdue to their compactness and continuous representation capabilities. However,\\raddressing dense correspondences and semantic relationships across DIF-encoded\\rshapes remains a critical challenge, limiting their applications in texture\\rtransfer and shape analysis. Moreover, recent endeavors in 3D shape generation\\rusing DIFs often neglect correspondence and topology preservation. This paper\\rpresents HNDF (Hybrid Neural Diffeomorphic Flow), a method that implicitly\\rlearns the underlying representation and decomposes intricate dense\\rcorrespondences into explicitly axis-aligned triplane features. To avoid\\rsuboptimal representations trapped in local minima, we propose hybrid\\rsupervision that captures both local and global correspondences. Unlike\\rconventional approaches that directly generate new 3D shapes, we further\\rexplore the idea of shape generation with deformed template shape via\\rdiffeomorphic flows, where the deformation is encoded by the generated triplane\\rfeatures. Leveraging a pre-existing 2D diffusion model, we produce high-quality\\rand diverse 3D diffeomorphic flows through generated triplanes features,\\rensuring topological consistency with the template shape. Extensive experiments\\ron medical image organ segmentation datasets evaluate the effectiveness of HNDF\\rin 3D shape representation and generation.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01957 ,  7319kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01968\\rDate: Wed, 5 Jul 2023 00:40:19 GMT   (3221kb,D)\\r\\rTitle: Muti-scale Graph Neural Network with Signed-attention for Social Bot\\r  Detection: A Frequency Perspective\\rAuthors: Shuhao Shi, Kai Qiao, Zhengyan Wang, Jie Yang, Baojie Song, Jian Chen,\\r  Bin Yan\\rCategories: cs.CV\\rComments: 13 pages, 10 figures\\r\\\\\\\\\\r  The presence of a large number of bots on social media has adverse effects.\\rThe graph neural network (GNN) can effectively leverage the social\\rrelationships between users and achieve excellent results in detecting bots.\\rRecently, more and more GNN-based methods have been proposed for bot detection.\\rHowever, the existing GNN-based bot detection methods only focus on\\rlow-frequency information and seldom consider high-frequency information, which\\rlimits the representation ability of the model. To address this issue, this\\rpaper proposes a Multi-scale with Signed-attention Graph Filter for social bot\\rdetection called MSGS. MSGS could effectively utilize both high and\\rlow-frequency information in the social graph. Specifically, MSGS utilizes a\\rmulti-scale structure to produce representation vectors at different scales.\\rThese representations are then combined using a signed-attention mechanism.\\rFinally, multi-scale representations via MLP after polymerization to produce\\rthe final result. We analyze the frequency response and demonstrate that MSGS\\ris a more flexible and expressive adaptive graph filter. MSGS can effectively\\rutilize high-frequency information to alleviate the over-smoothing problem of\\rdeep GNNs. Experimental results on real-world datasets demonstrate that our\\rmethod achieves better performance compared with several state-of-the-art\\rsocial bot detection methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01968 ,  3221kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01969\\rDate: Wed, 5 Jul 2023 00:40:40 GMT   (311kb,D)\\r\\rTitle: Multimodal Prompt Learning for Product Title Generation with Extremely\\r  Limited Labels\\rAuthors: Bang Yang, Fenglin Liu, Zheng Li, Qingyu Yin, Chenyu You, Bing Yin,\\r  and Yuexian Zou\\rCategories: cs.CV\\rComments: accepted by ACL Findings 2023\\r\\\\\\\\\\r  Generating an informative and attractive title for the product is a crucial\\rtask for e-commerce. Most existing works follow the standard multimodal natural\\rlanguage generation approaches, e.g., image captioning, and employ the large\\rscale of human-labelled datasets to train desirable models. However, for novel\\rproducts, especially in a different domain, there are few existing labelled\\rdata. In this paper, we propose a prompt-based approach, i.e., the Multimodal\\rPrompt Learning framework, to accurately and efficiently generate titles for\\rnovel products with limited labels. We observe that the core challenges of\\rnovel product title generation are the understanding of novel product\\rcharacteristics and the generation of titles in a novel writing style. To this\\rend, we build a set of multimodal prompts from different modalities to preserve\\rthe corresponding characteristics and writing styles of novel products. As a\\rresult, with extremely limited labels for training, the proposed method can\\rretrieve the multimodal prompts to generate desirable titles for novel\\rproducts. The experiments and analyses are conducted on five novel product\\rcategories under both the in-domain and out-of-domain experimental settings.\\rThe results show that, with only 1% of downstream labelled data for training,\\rour proposed approach achieves the best few-shot results and even achieves\\rcompetitive results with fully-supervised methods trained on 100% of training\\rdata; With the full labelled data for training, our method achieves\\rstate-of-the-art results.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01969 ,  311kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01984\\rDate: Wed, 5 Jul 2023 02:00:14 GMT   (12281kb,D)\\r\\rTitle: The KiTS21 Challenge: Automatic segmentation of kidneys, renal tumors,\\r  and renal cysts in corticomedullary-phase CT\\rAuthors: Nicholas Heller, Fabian Isensee, Dasha Trofimova, Resha Tejpaul,\\r  Zhongchen Zhao, Huai Chen, Lisheng Wang, Alex Golts, Daniel Khapun, Daniel\\r  Shats, Yoel Shoshan, Flora Gilboa-Solomon, Yasmeen George, Xi Yang, Jianpeng\\r  Zhang, Jing Zhang, Yong Xia, Mengran Wu, Zhiyang Liu, Ed Walczak, Sean\\r  McSweeney, Ranveer Vasdev, Chris Hornung, Rafat Solaiman, Jamee\\r  Schoephoerster, Bailey Abernathy, David Wu, Safa Abdulkadir, Ben Byun,\\r  Justice Spriggs, Griffin Struyk, Alexandra Austin, Ben Simpson, Michael\\r  Hagstrom, Sierra Virnig, John French, Nitin Venkatesh, Sarah Chan, Keenan\\r  Moore, Anna Jacobsen, Susan Austin, Mark Austin, Subodh Regmi, Nikolaos\\r  Papanikolopoulos, and Christopher Weight\\rCategories: cs.CV cs.AI cs.LG\\rComments: 34 pages, 12 figures\\r\\\\\\\\\\r  This paper presents the challenge report for the 2021 Kidney and Kidney Tumor\\rSegmentation Challenge (KiTS21) held in conjunction with the 2021 international\\rconference on Medical Image Computing and Computer Assisted Interventions\\r(MICCAI). KiTS21 is a sequel to its first edition in 2019, and it features a\\rvariety of innovations in how the challenge was designed, in addition to a\\rlarger dataset. A novel annotation method was used to collect three separate\\rannotations for each region of interest, and these annotations were performed\\rin a fully transparent setting using a web-based annotation tool. Further, the\\rKiTS21 test set was collected from an outside institution, challenging\\rparticipants to develop methods that generalize well to new populations.\\rNonetheless, the top-performing teams achieved a significant improvement over\\rthe state of the art set in 2019, and this performance is shown to inch ever\\rcloser to human-level performance. An in-depth meta-analysis is presented\\rdescribing which methods were used and how they faired on the leaderboard, as\\rwell as the characteristics of which cases generally saw good performance, and\\rwhich did not. Overall KiTS21 facilitated a significant advancement in the\\rstate of the art in kidney tumor segmentation, and provides useful insights\\rthat are applicable to the field of semantic segmentation as a whole.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01984 ,  12281kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01985\\rDate: Wed, 5 Jul 2023 02:13:25 GMT   (3421kb,D)\\r\\rTitle: Task-Specific Alignment and Multiple Level Transformer for Few-Shot\\r  Action Recognition\\rAuthors: Fei Guo, Li Zhu, YiWang Wang\\rCategories: cs.CV cs.DM\\r\\\\\\\\\\r  In the research field of few-shot learning, the main difference between\\rimage-based and video-based is the additional temporal dimension for videos. In\\rrecent years, many approaches for few-shot action recognition have followed the\\rmetric-based methods, especially, since some works use the Transformer to get\\rthe cross-attention feature of the videos or the enhanced prototype, and the\\rresults are competitive. However, they do not mine enough information from the\\rTransformer because they only focus on the feature of a single level. In our\\rpaper, we have addressed this problem. We propose an end-to-end method named\\rTask-Specific Alignment and Multiple Level Transformer Network (TSA-MLT). In\\rour model, the Multiple Level Transformer focuses on the multiple-level feature\\rof the support video and query video. Especially before Multiple Level\\rTransformer, we use task-specific TSA to filter unimportant or misleading\\rframes as a pre-processing. Furthermore, we adopt a fusion loss using two kinds\\rof distance, the first is L2 sequence distance, which focuses on temporal order\\ralignment. The second one is Optimal transport distance, which focuses on\\rmeasuring the gap between the appearance and semantics of the videos. Using a\\rsimple fusion network, we fuse the two distances element-wise, then use the\\rcross-entropy loss as our fusion loss. Extensive experiments show our method\\rachieves state-of-the-art results on the HMDB51 and UCF101 datasets and a\\rcompetitive result on the benchmark of Kinetics and something-2-something V2\\rdatasets. Our code will be available at the URL:\\rhttps://github.com/cofly2014/tsa-mlt.git\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01985 ,  3421kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02003\\rDate: Wed, 5 Jul 2023 03:27:31 GMT   (9627kb,D)\\r\\rTitle: Multi-Modal Prototypes for Open-Set Semantic Segmentation\\rAuthors: Yuhuan Yang, Chaofan Ma, Chen Ju, Ya Zhang, Yanfeng Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  In semantic segmentation, adapting a visual system to novel object categories\\rat inference time has always been both valuable and challenging. To enable such\\rgeneralization, existing methods rely on either providing several support\\rexamples as visual cues or class names as textual cues. Through the development\\ris relatively optimistic, these two lines have been studied in isolation,\\rneglecting the complementary intrinsic of low-level visual and high-level\\rlanguage information. In this paper, we define a unified setting termed as\\ropen-set semantic segmentation (O3S), which aims to learn seen and unseen\\rsemantics from both visual examples and textual names. Our pipeline extracts\\rmulti-modal prototypes for segmentation task, by first single modal\\rself-enhancement and aggregation, then multi-modal complementary fusion. To be\\rspecific, we aggregate visual features into several tokens as visual\\rprototypes, and enhance the class name with detailed descriptions for textual\\rprototype generation. The two modalities are then fused to generate multi-modal\\rprototypes for final segmentation. On both \\\\pascal and \\\\coco datasets, we\\rconduct extensive experiments to evaluate the framework effectiveness.\\rState-of-the-art results are achieved even on more detailed part-segmentation,\\rPascal-Animals, by only training on coarse-grained datasets. Thorough ablation\\rstudies are performed to dissect each component, both quantitatively and\\rqualitatively.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02003 ,  9627kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02007\\rDate: Wed, 5 Jul 2023 03:32:49 GMT   (16473kb)\\r\\rTitle: Remote Sensing Image Change Detection with Graph Interaction\\rAuthors: Chenglong Liu\\rCategories: cs.CV cs.IR\\r\\\\\\\\\\r  Modern remote sensing image change detection has witnessed substantial\\radvancements by harnessing the potent feature extraction capabilities of CNNs\\rand Transforms.Yet,prevailing change detection techniques consistently\\rprioritize extracting semantic features related to significant\\ralterations,overlooking the viability of directly interacting with bitemporal\\rimage features.In this letter,we propose a bitemporal image graph Interaction\\rnetwork for remote sensing change detection,namely BGINet-CD. More\\rspecifically,by leveraging the concept of non-local operations and mapping the\\rfeatures obtained from the backbone network to the graph structure space,we\\rpropose a unified self-focus mechanism for bitemporal images.This approach\\renhances the information coupling between the two temporal images while\\reffectively suppressing task-irrelevant interference,Based on a streamlined\\rbackbone architecture,namely ResNet18,our model demonstrates superior\\rperformance compared to other state-of-the-art methods (SOTA) on the GZ CD\\rdataset. Moreover,the model exhibits an enhanced trade-off between accuracy and\\rcomputational efficiency,further improving its overall effectiveness\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02007 ,  16473kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02010\\rDate: Wed, 5 Jul 2023 03:43:15 GMT   (579kb,D)\\r\\rTitle: ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised\\r  Video Object Segmentation\\rAuthors: Jiahao Li, Yuanyou Xu, Zongxin Yang, Yi Yang, Yueting Zhuang\\rCategories: cs.CV\\rComments: Top 1 solution for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video\\r  Object Segmentation\\r\\\\\\\\\\r  The Associating Objects with Transformers (AOT) framework has exhibited\\rexceptional performance in a wide range of complex scenarios for video object\\rsegmentation. In this study, we introduce MSDeAOT, a variant of the AOT series\\rthat incorporates transformers at multiple feature scales. Leveraging the\\rhierarchical Gated Propagation Module (GPM), MSDeAOT efficiently propagates\\robject masks from previous frames to the current frame using a feature scale\\rwith a stride of 16. Additionally, we employ GPM in a more refined feature\\rscale with a stride of 8, leading to improved accuracy in detecting and\\rtracking small objects. Through the implementation of test-time augmentations\\rand model ensemble techniques, we achieve the top-ranking position in the\\rEPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02010 ,  579kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02019\\rDate: Wed, 5 Jul 2023 04:14:57 GMT   (5968kb)\\r\\rTitle: Generative Adversarial Networks for Dental Patient Identity Protection\\r  in Orthodontic Educational Imaging\\rAuthors: Mingchuan Tian, Wilson Weixun Lu, Kelvin Weng Chiong Foong, Eugene Loh\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  Objectives: This research introduces a novel area-preserving Generative\\rAdversarial Networks (GAN) inversion technique for effectively de-identifying\\rdental patient images. This innovative method addresses privacy concerns while\\rpreserving key dental features, thereby generating valuable resources for\\rdental education and research.\\r  Methods: We enhanced the existing GAN Inversion methodology to maximize the\\rpreservation of dental characteristics within the synthesized images. A\\rcomprehensive technical framework incorporating several deep learning models\\rwas developed to provide end-to-end development guidance and practical\\rapplication for image de-identification.\\r  Results: Our approach was assessed with varied facial pictures, extensively\\rused for diagnosing skeletal asymmetry and facial anomalies. Results\\rdemonstrated our model's ability to adapt the context from one image to\\ranother, maintaining compatibility, while preserving dental features essential\\rfor oral diagnosis and dental education. A panel of five clinicians conducted\\ran evaluation on a set of original and GAN-processed images. The generated\\rimages achieved effective de-identification, maintaining the realism of\\rimportant dental features and were deemed useful for dental diagnostics and\\reducation.\\r  Clinical Significance: Our GAN model and the encompassing framework can\\rstreamline the de-identification process of dental patient images, enhancing\\refficiency in dental education. This method improves students' diagnostic\\rcapabilities by offering more exposure to orthodontic malocclusions.\\rFurthermore, it facilitates the creation of de-identified datasets for broader\\r2D image research at major research institutions.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02019 ,  5968kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02025\\rDate: Wed, 5 Jul 2023 05:23:49 GMT   (765kb,D)\\r\\rTitle: NMS Threshold matters for Ego4D Moment Queries -- 2nd place solution to\\r  the Ego4D Moment Queries Challenge 2023\\rAuthors: Lin Sui, Fangzhou Mu, Yin Li\\rCategories: cs.CV\\r\\\\\\\\\\r  This report describes our submission to the Ego4D Moment Queries Challenge\\r2023. Our submission extends ActionFormer, a latest method for temporal action\\rlocalization. Our extension combines an improved ground-truth assignment\\rstrategy during training and a refined version of SoftNMS at inference time.\\rOur solution is ranked 2nd on the public leaderboard with 26.62% average mAP\\rand 45.69% Recall@1x at tIoU=0.5 on the test set, significantly outperforming\\rthe strong baseline from 2023 challenge. Our code is available at\\rhttps://github.com/happyharrycn/actionformer_release.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02025 ,  765kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02041\\rDate: Wed, 5 Jul 2023 05:55:10 GMT   (1661kb,D)\\r\\rTitle: Multimodal Imbalance-Aware Gradient Modulation for Weakly-supervised\\r  Audio-Visual Video Parsing\\rAuthors: Jie Fu, Junyu Gao, Changsheng Xu\\rCategories: cs.CV\\r\\\\\\\\\\r  Weakly-supervised audio-visual video parsing (WS-AVVP) aims to localize the\\rtemporal extents of audio, visual and audio-visual event instances as well as\\ridentify the corresponding event categories with only video-level category\\rlabels for training. Most previous methods pay much attention to refining the\\rsupervision for each modality or extracting fruitful cross-modality information\\rfor more reliable feature learning. None of them have noticed the imbalanced\\rfeature learning between different modalities in the task. In this paper, to\\rbalance the feature learning processes of different modalities, a dynamic\\rgradient modulation (DGM) mechanism is explored, where a novel and effective\\rmetric function is designed to measure the imbalanced feature learning between\\raudio and visual modalities. Furthermore, principle analysis indicates that the\\rmultimodal confusing calculation will hamper the precise measurement of\\rmultimodal imbalanced feature learning, which further weakens the effectiveness\\rof our DGM mechanism. To cope with this issue, a modality-separated decision\\runit (MSDU) is designed for more precise measurement of imbalanced feature\\rlearning between audio and visual modalities. Comprehensive experiments are\\rconducted on public benchmarks and the corresponding experimental results\\rdemonstrate the effectiveness of our proposed method.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02041 ,  1661kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02055\\rDate: Wed, 5 Jul 2023 06:40:08 GMT   (1046kb)\\r\\rTitle: Adversarial Attacks on Image Classification Models: FGSM and Patch\\r  Attacks and their Impact\\rAuthors: Jaydip Sen and Subhasis Dasgupta\\rCategories: cs.CV cs.CR cs.LG\\rComments: This is the preprint of the chapter titled Adversarial Attacks on\\r  Image Classification Models: FGSM and Patch Attacks and their Impact which\\r  will be published in the volume titled Information Security and Privacy in\\r  the Digital World - Some Selected Cases, edited by Jaydip Sen. The book will\\r  be published by IntechOpen, London, UK, in 2023. This is not the final\\r  version of the chapter\\r\\\\\\\\\\r  This chapter introduces the concept of adversarial attacks on image\\rclassification models built on convolutional neural networks (CNN). CNNs are\\rvery popular deep-learning models which are used in image classification tasks.\\rHowever, very powerful and pre-trained CNN models working very accurately on\\rimage datasets for image classification tasks may perform disastrously when the\\rnetworks are under adversarial attacks. In this work, two very well-known\\radversarial attacks are discussed and their impact on the performance of image\\rclassifiers is analyzed. These two adversarial attacks are the fast gradient\\rsign method (FGSM) and adversarial patch attack. These attacks are launched on\\rthree powerful pre-trained image classifier architectures, ResNet-34,\\rGoogleNet, and DenseNet-161. The classification accuracy of the models in the\\rabsence and presence of the two attacks are computed on images from the\\rpublicly accessible ImageNet dataset. The results are analyzed to evaluate the\\rimpact of the attacks on the image classification task.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02055 ,  1046kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02065\\rDate: Wed, 5 Jul 2023 07:08:58 GMT   (3893kb,D)\\r\\rTitle: Line Graphics Digitization: A Step Towards Full Automation\\rAuthors: Omar Moured, Jiaming Zhang, Alina Roitberg, Thorsten Schwarz, Rainer\\r  Stiefelhagen\\rCategories: cs.CV cs.AI cs.LG\\rComments: Accepted at The 17th International Conference on Document Analysis\\r  and Recognition (ICDAR 2023)\\r\\\\\\\\\\r  The digitization of documents allows for wider accessibility and\\rreproducibility. While automatic digitization of document layout and text\\rcontent has been a long-standing focus of research, this problem in regard to\\rgraphical elements, such as statistical plots, has been under-explored. In this\\rpaper, we introduce the task of fine-grained visual understanding of\\rmathematical graphics and present the Line Graphics (LG) dataset, which\\rincludes pixel-wise annotations of 5 coarse and 10 fine-grained categories. Our\\rdataset covers 520 images of mathematical graphics collected from 450 documents\\rfrom different disciplines. Our proposed dataset can support two different\\rcomputer vision tasks, i.e., semantic segmentation and object detection. To\\rbenchmark our LG dataset, we explore 7 state-of-the-art models. To foster\\rfurther research on the digitization of statistical graphs, we will make the\\rdataset, code, and models publicly available to the community.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02065 ,  3893kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02090\\rDate: Wed, 5 Jul 2023 08:06:26 GMT   (3585kb,D)\\r\\rTitle: Interactive Conversational Head Generation\\rAuthors: Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao\\rCategories: cs.CV\\rComments: arXiv admin note: text overlap with arXiv:2112.13548\\r\\\\\\\\\\r  We introduce a new conversation head generation benchmark for synthesizing\\rbehaviors of a single interlocutor in a face-to-face conversation. The\\rcapability to automatically synthesize interlocutors which can participate in\\rlong and multi-turn conversations is vital and offer benefits for various\\rapplications, including digital humans, virtual agents, and social robots.\\rWhile existing research primarily focuses on talking head generation (one-way\\rinteraction), hindering the ability to create a digital human for conversation\\r(two-way) interaction due to the absence of listening and interaction parts. In\\rthis work, we construct two datasets to address this issue, ``ViCo'' for\\rindependent talking and listening head generation tasks at the sentence level,\\rand ``ViCo-X'', for synthesizing interlocutors in multi-turn conversational\\rscenarios. Based on ViCo and ViCo-X, we define three novel tasks targeting the\\rinteraction modeling during the face-to-face conversation: 1) responsive\\rlistening head generation making listeners respond actively to the speaker with\\rnon-verbal signals, 2) expressive talking head generation guiding speakers to\\rbe aware of listeners' behaviors, and 3) conversational head generation to\\rintegrate the talking/listening ability in one interlocutor. Along with the\\rdatasets, we also propose corresponding baseline solutions to the three\\raforementioned tasks. Experimental results show that our baseline method could\\rgenerate responsive and vivid agents that can collaborate with real person to\\rfulfil the whole conversation. Project page: https://vico.solutions/.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02090 ,  3585kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02100\\rDate: Wed, 5 Jul 2023 08:19:29 GMT   (2802kb,D)\\r\\rTitle: MDViT: Multi-domain Vision Transformer for Small Medical Image\\r  Segmentation Datasets\\rAuthors: Siyi Du, Nourhan Bayasi, Ghassan Harmarneh, Rafeef Garbi\\rCategories: cs.CV\\rComments: 12 pages, 2 figures, accepted by 26th International Conference on\\r  Medical Image Computing and Computer Assisted Intervention (MICCAI 2023)\\r\\\\\\\\\\r  Despite its clinical utility, medical image segmentation (MIS) remains a\\rdaunting task due to images' inherent complexity and variability. Vision\\rtransformers (ViTs) have recently emerged as a promising solution to improve\\rMIS; however, they require larger training datasets than convolutional neural\\rnetworks. To overcome this obstacle, data-efficient ViTs were proposed, but\\rthey are typically trained using a single source of data, which overlooks the\\rvaluable knowledge that could be leveraged from other available datasets.\\rNaivly combining datasets from different domains can result in negative\\rknowledge transfer (NKT), i.e., a decrease in model performance on some domains\\rwith non-negligible inter-domain heterogeneity. In this paper, we propose\\rMDViT, the first multi-domain ViT that includes domain adapters to mitigate\\rdata-hunger and combat NKT by adaptively exploiting knowledge in multiple small\\rdata resources (domains). Further, to enhance representation learning across\\rdomains, we integrate a mutual knowledge distillation paradigm that transfers\\rknowledge between a universal network (spanning all the domains) and auxiliary\\rdomain-specific branches. Experiments on 4 skin lesion segmentation datasets\\rshow that MDViT outperforms state-of-the-art algorithms, with superior\\rsegmentation performance and a fixed model size, at inference time, even as\\rmore domains are added. Our code is available at\\rhttps://github.com/siyi-wind/MDViT.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02100 ,  2802kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02138\\rDate: Wed, 5 Jul 2023 09:28:25 GMT   (11030kb,D)\\r\\rTitle: Prompting Diffusion Representations for Cross-Domain Semantic\\r  Segmentation\\rAuthors: Rui Gong, Martin Danelljan, Han Sun, Julio Delgado Mangas, Luc Van\\r  Gool\\rCategories: cs.CV\\rComments: 17 pages, 3 figures, 11 tables\\r\\\\\\\\\\r  While originally designed for image generation, diffusion models have\\rrecently shown to provide excellent pretrained feature representations for\\rsemantic segmentation. Intrigued by this result, we set out to explore how well\\rdiffusion-pretrained representations generalize to new domains, a crucial\\rability for any representation. We find that diffusion-pretraining achieves\\rextraordinary domain generalization results for semantic segmentation,\\routperforming both supervised and self-supervised backbone networks. Motivated\\rby this, we investigate how to utilize the model's unique ability of taking an\\rinput prompt, in order to further enhance its cross-domain performance. We\\rintroduce a scene prompt and a prompt randomization strategy to help further\\rdisentangle the domain-invariant information when training the segmentation\\rhead. Moreover, we propose a simple but highly effective approach for test-time\\rdomain adaptation, based on learning a scene prompt on the target domain in an\\runsupervised manner. Extensive experiments conducted on four synthetic-to-real\\rand clear-to-adverse weather benchmarks demonstrate the effectiveness of our\\rapproaches. Without resorting to any complex techniques, such as image\\rtranslation, augmentation, or rare-class sampling, we set a new\\rstate-of-the-art on all benchmarks. Our implementation will be publicly\\ravailable at \\\\url{https://github.com/ETHRuiGong/PTDiffSeg}.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02138 ,  11030kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02203\\rDate: Wed, 5 Jul 2023 10:54:50 GMT   (11692kb,D)\\r\\rTitle: Neural Fields for Interactive Visualization of Statistical Dependencies\\r  in 3D Simulation Ensembles\\rAuthors: Fatemeh Farokhmanesh, Kevin H\\\\ohlein, Christoph Neuhauser, and\\r  R\\\\udiger Westermann\\rCategories: cs.CV\\r\\\\\\\\\\r  We present the first neural network that has learned to compactly represent\\rand can efficiently reconstruct the statistical dependencies between the values\\rof physical variables at different spatial locations in large 3D simulation\\rensembles. Going beyond linear dependencies, we consider mutual information as\\ra measure of non-linear dependence. We demonstrate learning and reconstruction\\rwith a large weather forecast ensemble comprising 1000 members, each storing\\rmultiple physical variables at a 250 x 352 x 20 simulation grid. By\\rcircumventing compute-intensive statistical estimators at runtime, we\\rdemonstrate significantly reduced memory and computation requirements for\\rreconstructing the major dependence structures. This enables embedding the\\restimator into a GPU-accelerated direct volume renderer and interactively\\rvisualizing all mutual dependencies for a selected domain point.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02203 ,  11692kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02211\\rDate: Wed, 5 Jul 2023 11:37:17 GMT   (6212kb,D)\\r\\rTitle: Object Recognition System on a Tactile Device for Visually Impaired\\rAuthors: Souayah Abdelkader, Mokretar Kraroubi Abderrahmene, Slimane Larabi\\rCategories: cs.CV\\r\\\\\\\\\\r  People with visual impairments face numerous challenges when interacting with\\rtheir environment. Our objective is to develop a device that facilitates\\rcommunication between individuals with visual impairments and their\\rsurroundings. The device will convert visual information into auditory\\rfeedback, enabling users to understand their environment in a way that suits\\rtheir sensory needs. Initially, an object detection model is selected from\\rexisting machine learning models based on its accuracy and cost considerations,\\rincluding time and power consumption. The chosen model is then implemented on a\\rRaspberry Pi, which is connected to a specifically designed tactile device.\\rWhen the device is touched at a specific position, it provides an audio signal\\rthat communicates the identification of the object present in the scene at that\\rcorresponding position to the visually impaired individual. Conducted tests\\rhave demonstrated the effectiveness of this device in scene understanding,\\rencompassing static or dynamic objects, as well as screen contents such as TVs,\\rcomputers, and mobile phones.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02211 ,  6212kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02227\\rDate: Wed, 5 Jul 2023 12:08:56 GMT   (3892kb,D)\\r\\rTitle: MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic\\r  Facial Expression Recognition\\rAuthors: Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao\\rCategories: cs.CV cs.AI cs.HC cs.MM\\rComments: 17 pages, 11 figures, 14 tables\\r\\\\\\\\\\r  Dynamic facial expression recognition (DFER) is essential to the development\\rof intelligent and empathetic machines. Prior efforts in this field mainly fall\\rinto supervised learning paradigm, which is restricted by the limited labeled\\rdata in existing datasets. Inspired by recent unprecedented success of masked\\rautoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel\\rself-supervised method which leverages large-scale self-supervised pre-training\\ron abundant unlabeled data to advance the development of DFER. Since the\\rvanilla Vision Transformer (ViT) employed in VideoMAE requires substantial\\rcomputation during fine-tuning, MAE-DFER develops an efficient local-global\\rinteraction Transformer (LGI-Former) as the encoder. LGI-Former first\\rconstrains self-attention in local spatiotemporal regions and then utilizes a\\rsmall set of learnable representative tokens to achieve efficient local-global\\rinformation exchange, thus avoiding the expensive computation of global\\rspace-time self-attention in ViT. Moreover, in addition to the standalone\\rappearance content reconstruction in VideoMAE, MAE-DFER also introduces\\rexplicit facial motion modeling to encourage LGI-Former to excavate both static\\rappearance and dynamic motion information. Extensive experiments on six\\rdatasets show that MAE-DFER consistently outperforms state-of-the-art\\rsupervised methods by significant margins, verifying that it can learn powerful\\rdynamic facial representations via large-scale self-supervised pre-training.\\rBesides, it has comparable or even better performance than VideoMAE, while\\rlargely reducing the computational cost (about 38\\\\% FLOPs). We believe MAE-DFER\\rhas paved a new way for the advancement of DFER and can inspire more relavant\\rresearch in this field and even other related tasks. Codes and models are\\rpublicly available at https://github.com/sunlicai/MAE-DFER.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02227 ,  3892kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02238\\rDate: Wed, 5 Jul 2023 12:27:58 GMT   (4047kb,D)\\r\\rTitle: Source Identification: A Self-Supervision Task for Dense Prediction\\rAuthors: Shuai Chen and Subhradeep Kayal and Marleen de Bruijne\\rCategories: cs.CV\\rComments: Under review\\r\\\\\\\\\\r  The paradigm of self-supervision focuses on representation learning from raw\\rdata without the need of labor-consuming annotations, which is the main\\rbottleneck of current data-driven methods. Self-supervision tasks are often\\rused to pre-train a neural network with a large amount of unlabeled data and\\rextract generic features of the dataset. The learned model is likely to contain\\ruseful information which can be transferred to the downstream main task and\\rimprove performance compared to random parameter initialization. In this paper,\\rwe propose a new self-supervision task called source identification (SI), which\\ris inspired by the classic blind source separation problem. Synthetic images\\rare generated by fusing multiple source images and the network's task is to\\rreconstruct the original images, given the fused images. A proper understanding\\rof the image content is required to successfully solve the task. We validate\\rour method on two medical image segmentation tasks: brain tumor segmentation\\rand white matter hyperintensities segmentation. The results show that the\\rproposed SI task outperforms traditional self-supervision tasks for dense\\rpredictions including inpainting, pixel shuffling, intensity shift, and\\rsuper-resolution. Among variations of the SI task fusing images of different\\rtypes, fusing images from different patients performs best.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02238 ,  4047kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02246\\rDate: Wed, 5 Jul 2023 12:41:46 GMT   (1011kb,D)\\r\\rTitle: S3C: Self-Supervised Stochastic Classifiers for Few-Shot\\r  Class-Incremental Learning\\rAuthors: Jayateja Kalla and Soma Biswas\\rCategories: cs.CV\\rComments: Accepted in ECCV 2022\\r\\\\\\\\\\r  Few-shot class-incremental learning (FSCIL) aims to learn progressively about\\rnew classes with very few labeled samples, without forgetting the knowledge of\\ralready learnt classes. FSCIL suffers from two major challenges: (i)\\rover-fitting on the new classes due to limited amount of data, (ii)\\rcatastrophically forgetting about the old classes due to unavailability of data\\rfrom these classes in the incremental stages. In this work, we propose a\\rself-supervised stochastic classifier (S3C) to counter both these challenges in\\rFSCIL. The stochasticity of the classifier weights (or class prototypes) not\\ronly mitigates the adverse effect of absence of large number of samples of the\\rnew classes, but also the absence of samples from previously learnt classes\\rduring the incremental steps. This is complemented by the self-supervision\\rcomponent, which helps to learn features from the base classes which generalize\\rwell to unseen classes that are encountered in future, thus reducing\\rcatastrophic forgetting. Extensive evaluation on three benchmark datasets using\\rmultiple evaluation metrics show the effectiveness of the proposed framework.\\rWe also experiment on two additional realistic scenarios of FSCIL, namely where\\rthe number of annotated data available for each of the new classes can be\\rdifferent, and also where the number of base classes is much lesser, and show\\rthat the proposed S3C performs significantly better than the state-of-the-art\\rfor all these challenging scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02246 ,  1011kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02249\\rDate: Wed, 5 Jul 2023 12:44:52 GMT   (7236kb,D)\\r\\rTitle: Rethinking Multiple Instance Learning for Whole Slide Image\\r  Classification: A Good Instance Classifier is All You Need\\rAuthors: Linhao Qu, Yingfan Ma, Xiaoyuan Luo, Manning Wang, and Zhijian Song\\rCategories: cs.CV\\r\\\\\\\\\\r  Weakly supervised whole slide image classification is usually formulated as a\\rmultiple instance learning (MIL) problem, where each slide is treated as a bag,\\rand the patches cut out of it are treated as instances. Existing methods either\\rtrain an instance classifier through pseudo-labeling or aggregate instance\\rfeatures into a bag feature through attention mechanisms and then train a bag\\rclassifier, where the attention scores can be used for instance-level\\rclassification. However, the pseudo instance labels constructed by the former\\rusually contain a lot of noise, and the attention scores constructed by the\\rlatter are not accurate enough, both of which affect their performance. In this\\rpaper, we propose an instance-level MIL framework based on contrastive learning\\rand prototype learning to effectively accomplish both instance classification\\rand bag classification tasks. To this end, we propose an instance-level weakly\\rsupervised contrastive learning algorithm for the first time under the MIL\\rsetting to effectively learn instance feature representation. We also propose\\ran accurate pseudo label generation method through prototype learning. We then\\rdevelop a joint training strategy for weakly supervised contrastive learning,\\rprototype learning, and instance classifier training. Extensive experiments and\\rvisualizations on four datasets demonstrate the powerful performance of our\\rmethod. Codes will be available.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02249 ,  7236kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02270\\rDate: Wed, 5 Jul 2023 13:10:37 GMT   (24076kb,D)\\r\\rTitle: SVDM: Single-View Diffusion Model for Pseudo-Stereo 3D Object Detection\\rAuthors: Yuguang Shi\\rCategories: cs.CV cs.AI\\rComments: arXiv admin note: text overlap with arXiv:2203.02112,\\r  arXiv:2303.01469 by other authors\\r\\\\\\\\\\r  One of the key problems in 3D object detection is to reduce the accuracy gap\\rbetween methods based on LiDAR sensors and those based on monocular cameras. A\\rrecently proposed framework for monocular 3D detection based on Pseudo-Stereo\\rhas received considerable attention in the community. However, so far these two\\rproblems are discovered in existing practices, including (1) monocular depth\\restimation and Pseudo-Stereo detector must be trained separately, (2) Difficult\\rto be compatible with different stereo detectors and (3) the overall\\rcalculation is large, which affects the reasoning speed. In this work, we\\rpropose an end-to-end, efficient pseudo-stereo 3D detection framework by\\rintroducing a Single-View Diffusion Model (SVDM) that uses a few iterations to\\rgradually deliver right informative pixels to the left image. SVDM allows the\\rentire pseudo-stereo 3D detection pipeline to be trained end-to-end and can\\rbenefit from the training of stereo detectors. Afterwards, we further explore\\rthe application of SVDM in depth-free stereo 3D detection, and the final\\rframework is compatible with most stereo detectors. Among multiple benchmarks\\ron the KITTI dataset, we achieve new state-of-the-art performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02270 ,  24076kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02273\\rDate: Wed, 5 Jul 2023 13:17:14 GMT   (7457kb,D)\\r\\rTitle: Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient\\r  Neural Image Compression\\rAuthors: Ahmed Ghorbel, Wassim Hamidouche and Luce Morin\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  Recently, the performance of neural image compression (NIC) has steadily\\rimproved thanks to the last line of study, reaching or outperforming\\rstate-of-the-art conventional codecs. Despite significant progress, current NIC\\rmethods still rely on ConvNet-based entropy coding, limited in modeling\\rlong-range dependencies due to their local connectivity and the increasing\\rnumber of architectural biases and priors, resulting in complex underperforming\\rmodels with high decoding latency. Motivated by the efficiency investigation of\\rthe Tranformer-based transform coding framework, namely SwinT-ChARM, we propose\\rto enhance the latter, as first, with a more straightforward yet effective\\rTranformer-based channel-wise auto-regressive prior model, resulting in an\\rabsolute image compression transformer (ICT). Through the proposed ICT, we can\\rcapture both global and local contexts from the latent representations and\\rbetter parameterize the distribution of the quantized latents. Further, we\\rleverage a learnable scaling module with a sandwich ConvNeXt-based\\rpre-/post-processor to accurately extract more compact latent codes while\\rreconstructing higher-quality images. Extensive experimental results on\\rbenchmark datasets showed that the proposed framework significantly improves\\rthe trade-off between coding efficiency and decoder complexity over the\\rversatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec\\rSwinT-ChARM. Moreover, we provide model scaling studies to verify the\\rcomputational efficiency of our approach and conduct several objective and\\rsubjective analyses to bring to the fore the performance gap between the\\radaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02273 ,  7457kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02280\\rDate: Wed, 5 Jul 2023 13:29:05 GMT   (21425kb,D)\\r\\rTitle: Interactive Image Segmentation with Cross-Modality Vision Transformers\\rAuthors: Kun Li, George Vosselman, Michael Ying Yang\\rCategories: cs.CV\\rComments: 16 pages\\r\\\\\\\\\\r  Interactive image segmentation aims to segment the target from the background\\rwith the manual guidance, which takes as input multimodal data such as images,\\rclicks, scribbles, and bounding boxes. Recently, vision transformers have\\rachieved a great success in several downstream visual tasks, and a few efforts\\rhave been made to bring this powerful architecture to interactive segmentation\\rtask. However, the previous works neglect the relations between two modalities\\rand directly mock the way of processing purely visual information with\\rself-attentions. In this paper, we propose a simple yet effective network for\\rclick-based interactive segmentation with cross-modality vision transformers.\\rCross-modality transformers exploits mutual information to better guide the\\rlearning process. The experiments on several benchmarks show that the proposed\\rmethod achieves superior performance in comparison to the previous\\rstate-of-the-art models. The stability of our method in term of avoiding\\rfailure cases shows its potential to be a practical annotation tool. The code\\rand pretrained models will be released under\\rhttps://github.com/lik1996/iCMFormer.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02280 ,  21425kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02291\\rDate: Wed, 5 Jul 2023 13:42:31 GMT   (5658kb,D)\\r\\rTitle: Focusing on what to decode and what to train: Efficient Training with\\r  HOI Split Decoders and Specific Target Guided DeNoising\\rAuthors: Junwen Chen, Yingcheng Wang, Keiji Yanai\\rCategories: cs.CV\\r\\\\\\\\\\r  Recent one-stage transformer-based methods achieve notable gains in the\\rHuman-object Interaction Detection (HOI) task by leveraging the detection of\\rDETR. However, the current methods redirect the detection target of the object\\rdecoder, and the box target is not explicitly separated from the query\\rembeddings, which leads to long and hard training. Furthermore, matching the\\rpredicted HOI instances with the ground-truth is more challenging than object\\rdetection, simply adapting training strategies from the object detection makes\\rthe training more difficult. To clear the ambiguity between human and object\\rdetection and share the prediction burden, we propose a novel one-stage\\rframework (SOV), which consists of a subject decoder, an object decoder, and a\\rverb decoder. Moreover, we propose a novel Specific Target Guided (STG)\\rDeNoising strategy, which leverages learnable object and verb label embeddings\\rto guide the training and accelerates the training convergence. In addition,\\rfor the inference part, the label-specific information is directly fed into the\\rdecoders by initializing the query embeddings from the learnable label\\rembeddings. Without additional features or prior language knowledge, our method\\r(SOV-STG) achieves higher accuracy than the state-of-the-art method in\\rone-third of training epochs. The code is available at\\r\\\\url{https://github.com/cjw2021/SOV-STG}.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02291 ,  5658kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02308\\rDate: Wed, 5 Jul 2023 14:10:29 GMT   (759kb)\\r\\rTitle: Multi-Scale Prototypical Transformer for Whole Slide Image\\r  Classification\\rAuthors: Saisai Ding, Jun Wang, Juncheng Li, and Jun Shi\\rCategories: cs.CV\\r\\\\\\\\\\r  Whole slide image (WSI) classification is an essential task in computational\\rpathology. Despite the recent advances in multiple instance learning (MIL) for\\rWSI classification, accurate classification of WSIs remains challenging due to\\rthe extreme imbalance between the positive and negative instances in bags, and\\rthe complicated pre-processing to fuse multi-scale information of WSI. To this\\rend, we propose a novel multi-scale prototypical Transformer (MSPT) for WSI\\rclassification, which includes a prototypical Transformer (PT) module and a\\rmulti-scale feature fusion module (MFFM). The PT is developed to reduce\\rredundant instances in bags by integrating prototypical learning into the\\rTransformer architecture. It substitutes all instances with cluster prototypes,\\rwhich are then re-calibrated through the self-attention mechanism of the\\rTrans-former. Thereafter, an MFFM is proposed to fuse the clustered prototypes\\rof different scales, which employs MLP-Mixer to enhance the information\\rcommunication between prototypes. The experimental results on two public WSI\\rdatasets demonstrate that the proposed MSPT outperforms all the compared\\ralgorithms, suggesting its potential applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02308 ,  759kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02321\\rDate: Wed, 5 Jul 2023 14:22:31 GMT   (27870kb,D)\\r\\rTitle: MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers\\rAuthors: Jakob Drachmann Havtorn and Amelie Royer and Tijmen Blankevoort and\\r  Babak Ehteshami Bejnordi\\rCategories: cs.CV\\r\\\\\\\\\\r  The input tokens to Vision Transformers carry little semantic meaning as they\\rare defined as regular equal-sized patches of the input image, regardless of\\rits content. However, processing uniform background areas of an image should\\rnot necessitate as much compute as dense, cluttered areas. To address this\\rissue, we propose a dynamic mixed-scale tokenization scheme for ViT, MSViT. Our\\rmethod introduces a conditional gating mechanism that selects the optimal token\\rscale for every image region, such that the number of tokens is dynamically\\rdetermined per input. The proposed gating module is lightweight, agnostic to\\rthe choice of transformer backbone, and trained within a few epochs (e.g., 20\\repochs on ImageNet) with little training overhead. In addition, to enhance the\\rconditional behavior of the gate during training, we introduce a novel\\rgeneralization of the batch-shaping loss. We show that our gating module is\\rable to learn meaningful semantics despite operating locally at the coarse\\rpatch-level. We validate MSViT on the tasks of classification and segmentation\\rwhere it leads to improved accuracy-complexity trade-off.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02321 ,  27870kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02339\\rDate: Wed, 5 Jul 2023 14:50:36 GMT   (1040kb,D)\\r\\rTitle: GAFAR: Graph-Attention Feature-Augmentation for Registration A Fast and\\r  Light-weight Point Set Registration Algorithm\\rAuthors: Ludwig Mohr, Ismail Geles and Friedrich Fraundorfer\\rCategories: cs.CV\\rComments: Accepted to the 11th European Conference on Mobile Robots (ECMR2023)\\r\\\\\\\\\\r  Rigid registration of point clouds is a fundamental problem in computer\\rvision with many applications from 3D scene reconstruction to geometry capture\\rand robotics. If a suitable initial registration is available, conventional\\rmethods like ICP and its many variants can provide adequate solutions. In\\rabsence of a suitable initialization and in the presence of a high outlier rate\\ror in the case of small overlap though the task of rigid registration still\\rpresents great challenges. The advent of deep learning in computer vision has\\rbrought new drive to research on this topic, since it provides the possibility\\rto learn expressive feature-representations and provide one-shot estimates\\rinstead of depending on time-consuming iterations of conventional robust\\rmethods. Yet, the rotation and permutation invariant nature of point clouds\\rposes its own challenges to deep learning, resulting in loss of performance and\\rlow generalization capability due to sensitivity to outliers and\\rcharacteristics of 3D scans not present during network training. In this work,\\rwe present a novel fast and light-weight network architecture using the\\rattention mechanism to augment point descriptors at inference time to optimally\\rsuit the registration task of the specific point clouds it is presented with.\\rEmploying a fully-connected graph both within and between point clouds lets the\\rnetwork reason about the importance and reliability of points for registration,\\rmaking our approach robust to outliers, low overlap and unseen data. We test\\rthe performance of our registration algorithm on different registration and\\rgeneralization tasks and provide information on runtime and resource\\rconsumption. The code and trained weights are available at\\rhttps://github.com/mordecaimalignatius/GAFAR/.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02339 ,  1040kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02347\\rDate: Wed, 5 Jul 2023 15:03:10 GMT   (2353kb,D)\\r\\rTitle: Detecting Images Generated by Deep Diffusion Models using their Local\\r  Intrinsic Dimensionality\\rAuthors: Peter Lorenz, Ricard Durall and Janis Keuper\\rCategories: cs.CV cs.CR\\r\\\\\\\\\\r  Diffusion models recently have been successfully applied for the visual\\rsynthesis of strikingly realistic appearing images. This raises strong concerns\\rabout their potential for malicious purposes. In this paper, we propose using\\rthe lightweight multi Local Intrinsic Dimensionality (multiLID), which has been\\roriginally developed in context of the detection of adversarial examples, for\\rthe automatic detection of synthetic images and the identification of the\\raccording generator networks. In contrast to many existing detection\\rapproaches, which often only work for GAN-generated images, the proposed method\\rprovides close to perfect detection results in many realistic use cases.\\rExtensive experiments on known and newly created datasets demonstrate that\\rmultiLID exhibits superiority in diffusion detection and model identification.\\rSince the empirical evaluations of recent publications on the detection of\\rgenerated images is often too focused on the LSUN-Bedroom dataset, we further\\restablish a comprehensive benchmark for the detection of diffusion-generated\\rimages, including samples from several diffusion models with different image\\rsizes to evaluate the performance of their multiLID.\\r  Code for our experiments is provided at\\rhttps://github.com/deepfake-study/deepfake_multiLID.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02347 ,  2353kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02392\\rDate: Wed, 5 Jul 2023 16:04:44 GMT   (13100kb,D)\\r\\rTitle: RADiff: Controllable Diffusion Models for Radio Astronomical Maps\\r  Generation\\rAuthors: Renato Sortino, Thomas Cecconello, Andrea DeMarco, Giuseppe Fiameni,\\r  Andrea Pilzer, Andrew M. Hopkins, Daniel Magro, Simone Riggi, Eva Sciacca,\\r  Adriano Ingallinera, Cristobal Bordiu, Filomena Bufano, Concetto Spampinato\\rCategories: cs.CV\\r\\\\\\\\\\r  Along with the nearing completion of the Square Kilometre Array (SKA), comes\\ran increasing demand for accurate and reliable automated solutions to extract\\rvaluable information from the vast amount of data it will allow acquiring.\\rAutomated source finding is a particularly important task in this context, as\\rit enables the detection and classification of astronomical objects.\\rDeep-learning-based object detection and semantic segmentation models have\\rproven to be suitable for this purpose. However, training such deep networks\\rrequires a high volume of labeled data, which is not trivial to obtain in the\\rcontext of radio astronomy. Since data needs to be manually labeled by experts,\\rthis process is not scalable to large dataset sizes, limiting the possibilities\\rof leveraging deep networks to address several tasks. In this work, we propose\\rRADiff, a generative approach based on conditional diffusion models trained\\rover an annotated radio dataset to generate synthetic images, containing radio\\rsources of different morphologies, to augment existing datasets and reduce the\\rproblems caused by class imbalances. We also show that it is possible to\\rgenerate fully-synthetic image-annotation pairs to automatically augment any\\rannotated dataset. We evaluate the effectiveness of this approach by training a\\rsemantic segmentation model on a real dataset augmented in two ways: 1) using\\rsynthetic images obtained from real masks, and 2) generating images from\\rsynthetic semantic masks. We show an improvement in performance when applying\\raugmentation, gaining up to 18% in performance when using real masks and 4%\\rwhen augmenting with synthetic masks. Finally, we employ this model to generate\\rlarge-scale radio maps with the objective of simulating Data Challenges.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02392 ,  13100kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02402\\rDate: Wed, 5 Jul 2023 16:21:52 GMT   (3466kb,D)\\r\\rTitle: Unbalanced Optimal Transport: A Unified Framework for Object Detection\\rAuthors: Henri De Plaen, Pierre-Fran\\\\c{c}ois De Plaen, Johan A. K. Suykens,\\r  Marc Proesmans, Tinne Tuytelaars and Luc Van Gool\\rCategories: cs.CV cs.LG\\rComments: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\r  Recognition (CVPR 2023)\\r\\\\\\\\\\r  During training, supervised object detection tries to correctly match the\\rpredicted bounding boxes and associated classification scores to the ground\\rtruth. This is essential to determine which predictions are to be pushed\\rtowards which solutions, or to be discarded. Popular matching strategies\\rinclude matching to the closest ground truth box (mostly used in combination\\rwith anchors), or matching via the Hungarian algorithm (mostly used in\\ranchor-free methods). Each of these strategies comes with its own properties,\\runderlying losses, and heuristics. We show how Unbalanced Optimal Transport\\runifies these different approaches and opens a whole continuum of methods in\\rbetween. This allows for a finer selection of the desired properties.\\rExperimentally, we show that training an object detection model with Unbalanced\\rOptimal Transport is able to reach the state-of-the-art both in terms of\\rAverage Precision and Average Recall as well as to provide a faster initial\\rconvergence. The approach is well suited for GPU implementation, which proves\\rto be an advantage for large-scale models.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02402 ,  3466kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02421\\rDate: Wed, 5 Jul 2023 16:43:56 GMT   (6669kb,D)\\r\\rTitle: DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models\\rAuthors: Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  Despite the ability of existing large-scale text-to-image (T2I) models to\\rgenerate high-quality images from detailed textual descriptions, they often\\rlack the ability to precisely edit the generated or real images. In this paper,\\rwe propose a novel image editing method, DragonDiffusion, enabling Drag-style\\rmanipulation on Diffusion models. Specifically, we construct classifier\\rguidance based on the strong correspondence of intermediate features in the\\rdiffusion model. It can transform the editing signals into gradients via\\rfeature correspondence loss to modify the intermediate representation of the\\rdiffusion model. Based on this guidance strategy, we also build a multi-scale\\rguidance to consider both semantic and geometric alignment. Moreover, a\\rcross-branch self-attention is added to maintain the consistency between the\\roriginal image and the editing result. Our method, through an efficient design,\\rachieves various editing modes for the generated or real images, such as object\\rmoving, object resizing, object appearance replacement, and content dragging.\\rIt is worth noting that all editing and content preservation signals come from\\rthe image itself, and the model does not require fine-tuning or additional\\rmodules. Our source code will be available at\\rhttps://github.com/MC-E/DragonDiffusion.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02421 ,  6669kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02457\\rDate: Wed, 5 Jul 2023 17:31:44 GMT   (17763kb,D)\\r\\rTitle: DeSRA: Detect and Delete the Artifacts of GAN-based Real-World\\r  Super-Resolution Models\\rAuthors: Liangbin Xie, Xintao Wang, Xiangyu Chen, Gen Li, Ying Shan, Jiantao\\r  Zhou, Chao Dong\\rCategories: cs.CV cs.AI cs.MM\\rComments: The code and models will be made publicly at\\r  https://github.com/TencentARC/DeSRA\\r\\\\\\\\\\r  Image super-resolution (SR) with generative adversarial networks (GAN) has\\rachieved great success in restoring realistic details. However, it is notorious\\rthat GAN-based SR models will inevitably produce unpleasant and undesirable\\rartifacts, especially in practical scenarios. Previous works typically suppress\\rartifacts with an extra loss penalty in the training phase. They only work for\\rin-distribution artifact types generated during training. When applied in\\rreal-world scenarios, we observe that those improved methods still generate\\robviously annoying artifacts during inference. In this paper, we analyze the\\rcause and characteristics of the GAN artifacts produced in unseen test data\\rwithout ground-truths. We then develop a novel method, namely, DeSRA, to Detect\\rand then Delete those SR Artifacts in practice. Specifically, we propose to\\rmeasure a relative local variance distance from MSE-SR results and GAN-SR\\rresults, and locate the problematic areas based on the above distance and\\rsemantic-aware thresholds. After detecting the artifact regions, we develop a\\rfinetune procedure to improve GAN-based SR models with a few samples, so that\\rthey can deal with similar types of artifacts in more unseen real data.\\rEquipped with our DeSRA, we can successfully eliminate artifacts from inference\\rand improve the ability of SR models to be applied in real-world scenarios. The\\rcode will be available at https://github.com/TencentARC/DeSRA.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02457 ,  17763kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02465\\rDate: Wed, 5 Jul 2023 17:38:48 GMT   (28251kb,D)\\r\\rTitle: Large-scale Detection of Marine Debris in Coastal Areas with Sentinel-2\\rAuthors: Marc Ru{\\\\ss}wurm, Sushen Jilla Venkatesa, Devis Tuia\\rCategories: cs.CV\\rComments: in review\\r\\\\\\\\\\r  Detecting and quantifying marine pollution and macro-plastics is an\\rincreasingly pressing ecological issue that directly impacts ecology and human\\rhealth. Efforts to quantify marine pollution are often conducted with sparse\\rand expensive beach surveys, which are difficult to conduct on a large scale.\\rHere, remote sensing can provide reliable estimates of plastic pollution by\\rregularly monitoring and detecting marine debris in coastal areas.\\rMedium-resolution satellite data of coastal areas is readily available and can\\rbe leveraged to detect aggregations of marine debris containing plastic litter.\\rIn this work, we present a detector for marine debris built on a deep\\rsegmentation model that outputs a probability for marine debris at the pixel\\rlevel. We train this detector with a combination of annotated datasets of\\rmarine debris and evaluate it on specifically selected test sites where it is\\rhighly probable that plastic pollution is present in the detected marine\\rdebris. We demonstrate quantitatively and qualitatively that a deep learning\\rmodel trained on this dataset issued from multiple sources outperforms existing\\rdetection models trained on previous datasets by a large margin. Our\\rexperiments show, consistent with the principles of data-centric AI, that this\\rperformance is due to our particular dataset design with extensive sampling of\\rnegative examples and label refinements rather than depending on the particular\\rdeep learning model. We hope to accelerate advances in the large-scale\\rautomated detection of marine debris, which is a step towards quantifying and\\rmonitoring marine litter with remote sensing at global scales, and release the\\rmodel weights and training source code under\\rhttps://github.com/marccoru/marinedebrisdetector\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02465 ,  28251kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02469\\rDate: Wed, 5 Jul 2023 17:44:28 GMT   (38351kb,D)\\r\\rTitle: What Matters in Training a GPT4-Style Language Model with Multimodal\\r  Inputs?\\rAuthors: Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang\\r  Wei, Yuchen Zhang, Tao Kong\\rCategories: cs.CV cs.CL\\rComments: 32 pages\\r\\\\\\\\\\r  Recent advancements in Large Language Models (LLMs) such as GPT4 have\\rdisplayed exceptional multi-modal capabilities in following open-ended\\rinstructions given images. However, the performance of these models heavily\\rrelies on design choices such as network structures, training data, and\\rtraining strategies, and these choices have not been extensively discussed in\\rthe literature, making it difficult to quantify progress in this field. To\\raddress this issue, this paper presents a systematic and comprehensive study,\\rquantitatively and qualitatively, on training such models. We implement over 20\\rvariants with controlled settings. Concretely, for network structures, we\\rcompare different LLM backbones and model designs. For training data, we\\rinvestigate the impact of data and sampling strategies. For instructions, we\\rexplore the influence of diversified prompts on the instruction-following\\rability of the trained models. For benchmarks, we contribute the first, to our\\rbest knowledge, comprehensive evaluation set including both image and video\\rtasks through crowd-sourcing. Based on our findings, we present Lynx, which\\rperforms the most accurate multi-modal understanding while keeping the best\\rmulti-modal generation ability compared to existing open-sourced GPT4-style\\rmodels.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02469 ,  38351kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02480\\rDate: Wed, 5 Jul 2023 17:54:36 GMT   (2525kb,D)\\r\\rTitle: A Dataset of Inertial Measurement Units for Handwritten English\\r  Alphabets\\rAuthors: Hari Prabhat Gupta and Rahul Mishra\\rCategories: cs.CV\\rComments: 10 pages, 12 figures\\rDOI: 10.21227/av6q-jj17\\r\\\\\\\\\\r  This paper presents an end-to-end methodology for collecting datasets to\\rrecognize handwritten English alphabets by utilizing Inertial Measurement Units\\r(IMUs) and leveraging the diversity present in the Indian writing style. The\\rIMUs are utilized to capture the dynamic movement patterns associated with\\rhandwriting, enabling more accurate recognition of alphabets. The Indian\\rcontext introduces various challenges due to the heterogeneity in writing\\rstyles across different regions and languages. By leveraging this diversity,\\rthe collected dataset and the collection system aim to achieve higher\\rrecognition accuracy. Some preliminary experimental results demonstrate the\\reffectiveness of the dataset in accurately recognizing handwritten English\\ralphabet in the Indian context. This research can be extended and contributes\\rto the field of pattern recognition and offers valuable insights for developing\\rimproved systems for handwriting recognition, particularly in diverse\\rlinguistic and cultural contexts.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02480 ,  2525kb)\\r%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01088 (*cross-listing*)\\rDate: Mon, 3 Jul 2023 15:08:28 GMT   (1596kb,D)\\r\\rTitle: Empirically Validating Conformal Prediction on Modern Vision\\r  Architectures Under Distribution Shift and Long-tailed Data\\rAuthors: Kevin Kasa and Graham W. Taylor\\rCategories: cs.LG cs.CV stat.ML\\r\\\\\\\\\\r  Conformal prediction has emerged as a rigorous means of providing deep\\rlearning models with reliable uncertainty estimates and safety guarantees. Yet,\\rits performance is known to degrade under distribution shift and long-tailed\\rclass distributions, which are often present in real world applications. Here,\\rwe characterize the performance of several post-hoc and training-based\\rconformal prediction methods under these settings, providing the first\\rempirical evaluation on large-scale datasets and models. We show that across\\rnumerous conformal methods and neural network families, performance greatly\\rdegrades under distribution shifts violating safety guarantees. Similarly, we\\rshow that in long-tailed settings the guarantees are frequently violated on\\rmany classes. Understanding the limitations of these methods is necessary for\\rdeployment in real world and safety-critical applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01088 ,  1596kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01220 (*cross-listing*)\\rDate: Sun, 2 Jul 2023 10:39:29 GMT   (5033kb,D)\\r\\rTitle: ARHNet: Adaptive Region Harmonization for Lesion-aware Augmentation to\\r  Improve Segmentation Performance\\rAuthors: Jiayu Huo, Yang Liu, Xi Ouyang, Alejandro Granados, Sebastien\\r  Ourselin, Rachel Sparks\\rCategories: eess.IV cs.CV\\rComments: 9 pages, 4 figures, 3 tables\\r\\\\\\\\\\r  Accurately segmenting brain lesions in MRI scans is critical for providing\\rpatients with prognoses and neurological monitoring. However, the performance\\rof CNN-based segmentation methods is constrained by the limited training set\\rsize. Advanced data augmentation is an effective strategy to improve the\\rmodel's robustness. However, they often introduce intensity disparities between\\rforeground and background areas and boundary artifacts, which weakens the\\reffectiveness of such strategies. In this paper, we propose a foreground\\rharmonization framework (ARHNet) to tackle intensity disparities and make\\rsynthetic images look more realistic. In particular, we propose an Adaptive\\rRegion Harmonization (ARH) module to dynamically align foreground feature maps\\rto the background with an attention mechanism. We demonstrate the efficacy of\\rour method in improving the segmentation performance using real and synthetic\\rimages. Experimental results on the ATLAS 2.0 dataset show that ARHNet\\routperforms other methods for image harmonization tasks, and boosts the\\rdown-stream segmentation performance. Our code is publicly available at\\rhttps://github.com/King-HAW/ARHNet.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01220 ,  5033kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01232 (*cross-listing*)\\rDate: Mon, 3 Jul 2023 08:12:56 GMT   (5186kb,D)\\r\\rTitle: Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data\\rAuthors: Adnan Qayyum, Hassan Ali, Massimo Caputo, Hunaid Vohra, Taofeek\\r  Akinosho, Sofiat Abioye, Ilhem Berrou, Pawe{\\\\l} Capik, Junaid Qadir, and\\r  Muhammad Bilal\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  Over the past few years, surgical data science has attracted substantial\\rinterest from the machine learning (ML) community. Various studies have\\rdemonstrated the efficacy of emerging ML techniques in analysing surgical data,\\rparticularly recordings of procedures, for digitizing clinical and non-clinical\\rfunctions like preoperative planning, context-aware decision-making, and\\roperating skill assessment. However, this field is still in its infancy and\\rlacks representative, well-annotated datasets for training robust models in\\rintermediate ML tasks. Also, existing datasets suffer from inaccurate labels,\\rhindering the development of reliable models. In this paper, we propose a\\rsystematic methodology for developing robust models for surgical tool detection\\rusing noisy data. Our methodology introduces two key innovations: (1) an\\rintelligent active learning strategy for minimal dataset identification and\\rlabel correction by human experts; and (2) an assembling strategy for a\\rstudent-teacher model-based self-training framework to achieve the robust\\rclassification of 14 surgical tools in a semi-supervised fashion. Furthermore,\\rwe employ weighted data loaders to handle difficult class labels and address\\rclass imbalance issues. The proposed methodology achieves an average F1-score\\rof 85.88\\\\% for the ensemble model-based self-training with class weights, and\\r80.88\\\\% without class weights for noisy labels. Also, our proposed method\\rsignificantly outperforms existing approaches, which effectively demonstrates\\rits effectiveness.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01232 ,  5186kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01462 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 03:49:42 GMT   (3141kb,D)\\r\\rTitle: Practical Collaborative Perception: A Framework for Asynchronous and\\r  Multi-Agent 3D Object Detection\\rAuthors: Minh-Quan Dao, Julie Stephany Berrio, Vincent Fr\\\\'emont, Mao Shan,\\r  Elwan H\\\\'ery, and Stewart Worrall\\rCategories: cs.RO cs.CV\\rComments: Work in progress\\r\\\\\\\\\\r  In this paper, we improve the single-vehicle 3D object detection models using\\rLiDAR by extending their capacity to process point cloud sequences instead of\\rindividual point clouds. In this step, we extend our previous work on\\rrectification of the shadow effect in the concatenation of point clouds to\\rboost the detection accuracy of multi-frame detection models. Our extension\\rincludes incorporating HD Map and distilling an Oracle model. Next, we further\\rincrease the performance of single-vehicle perception using multi-agent\\rcollaboration via Vehicle-to-everything (V2X) communication. We devise a simple\\ryet effective collaboration method that achieves better bandwidth-performance\\rtradeoffs than prior arts while minimizing changes made to single-vehicle\\rdetection models and assumptions on inter-agent synchronization. Experiments on\\rthe V2X-Sim dataset show that our collaboration method achieves 98% performance\\rof the early collaboration while consuming the equivalent amount of bandwidth\\rusage of late collaboration which is 0.03% of early collaboration. The code\\rwill be released at https://github.com/quan-dao/practical-collab-perception.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01462 ,  3141kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01486 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 05:31:09 GMT   (7456kb,D)\\r\\rTitle: H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for\\r  Multimodal Tumor Segmentation\\rAuthors: Jun Shi, Hongyu Kan, Shulan Ruan, Ziqi Zhu, Minfan Zhao, Liang Qiao,\\r  Zhaohui Wang, Hong An, Xudong Xue\\rCategories: eess.IV cs.CV\\rComments: 11 pages, 2 figures. This paper has been accepted by Medical Image\\r  Computing and Computer-Assisted Intervention(MICCAI) 2023\\r\\\\\\\\\\r  Recently, deep learning methods have been widely used for tumor segmentation\\rof multimodal medical images with promising results. However, most existing\\rmethods are limited by insufficient representational ability, specific modality\\rnumber and high computational complexity. In this paper, we propose a hybrid\\rdensely connected network for tumor segmentation, named H-DenseFormer, which\\rcombines the representational power of the Convolutional Neural Network (CNN)\\rand the Transformer structures. Specifically, H-DenseFormer integrates a\\rTransformer-based Multi-path Parallel Embedding (MPE) module that can take an\\rarbitrary number of modalities as input to extract the fusion features from\\rdifferent modalities. Then, the multimodal fusion features are delivered to\\rdifferent levels of the encoder to enhance multimodal learning representation.\\rBesides, we design a lightweight Densely Connected Transformer (DCT) block to\\rreplace the standard Transformer block, thus significantly reducing\\rcomputational complexity. We conduct extensive experiments on two public\\rmultimodal datasets, HECKTOR21 and PI-CAI22. The experimental results show that\\rour proposed method outperforms the existing state-of-the-art methods while\\rhaving lower computational complexity. The source code is available at\\rhttps://github.com/shijun18/H-DenseFormer.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01486 ,  7456kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01514 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 06:50:16 GMT   (3840kb,D)\\r\\rTitle: SelfFed: Self-supervised Federated Learning for Data Heterogeneity and\\r  Label Scarcity in IoMT\\rAuthors: Sunder Ali Khowaja, Kapal Dev, Syed Muhammad Anwar, Marius George\\r  Linguraru\\rCategories: cs.LG cs.CV\\rComments: 8 pages, 6 figures\\r\\\\\\\\\\r  Self-supervised learning in federated learning paradigm has been gaining a\\rlot of interest both in industry and research due to the collaborative learning\\rcapability on unlabeled yet isolated data. However, self-supervised based\\rfederated learning strategies suffer from performance degradation due to label\\rscarcity and diverse data distributions, i.e., data heterogeneity. In this\\rpaper, we propose the SelfFed framework for Internet of Medical Things (IoMT).\\rOur proposed SelfFed framework works in two phases. The first phase is the\\rpre-training paradigm that performs augmentive modeling using Swin Transformer\\rbased encoder in a decentralized manner. The first phase of SelfFed framework\\rhelps to overcome the data heterogeneity issue. The second phase is the\\rfine-tuning paradigm that introduces contrastive network and a novel\\raggregation strategy that is trained on limited labeled data for a target task\\rin a decentralized manner. This fine-tuning stage overcomes the label scarcity\\rproblem. We perform our experimental analysis on publicly available medical\\rimaging datasets and show that our proposed SelfFed framework performs better\\rwhen compared to existing baselines concerning non-independent and identically\\rdistributed (IID) data and label scarcity. Our method achieves a maximum\\rimprovement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID\\rdataset. Further, our proposed method outperforms existing baselines even when\\rtrained on a few (10%) labeled instances.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01514 ,  3840kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01578 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 09:11:33 GMT   (694kb,D)\\r\\rTitle: Optimal and Efficient Binary Questioning for Human-in-the-Loop\\r  Annotation\\rAuthors: Franco Marchesoni-Acland, Jean-Michel Morel, Josselin Kherroubi,\\r  Gabriele Facciolo\\rCategories: cs.LG cs.AI cs.CV cs.HC cs.IT math.IT\\rComments: 8 pages + references + appendix\\r\\\\\\\\\\r  Even though data annotation is extremely important for interpretability,\\rresearch and development of artificial intelligence solutions, most research\\refforts such as active learning or few-shot learning focus on the sample\\refficiency problem. This paper studies the neglected complementary problem of\\rgetting annotated data given a predictor. For the simple binary classification\\rsetting, we present the spectrum ranging from optimal general solutions to\\rpractical efficient methods. The problem is framed as the full annotation of a\\rbinary classification dataset with the minimal number of yes/no questions when\\ra predictor is available. For the case of general binary questions the solution\\ris found in coding theory, where the optimal questioning strategy is given by\\rthe Huffman encoding of the possible labelings. However, this approach is\\rcomputationally intractable even for small dataset sizes. We propose an\\ralternative practical solution based on several heuristics and lookahead\\rminimization of proxy cost functions. The proposed solution is analysed,\\rcompared with optimal solutions and evaluated on several synthetic and\\rreal-world datasets. On these datasets, the method allows a significant\\rimprovement ($23-86\\\\%$) in annotation efficiency.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01578 ,  694kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01583 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 09:23:24 GMT   (1141kb,D)\\r\\rTitle: Learning Lie Group Symmetry Transformations with Neural Networks\\rAuthors: Alex Gabel, Victoria Klein, Riccardo Valperga, Jeroen S. W. Lamb,\\r  Kevin Webster, Rick Quax, Efstratios Gavves\\rCategories: cs.LG cs.CV\\rComments: 9 pages, 5 figures, Proceedings of the 2nd Annual Workshop on\\r  Topology, Algebra, and Geometry in Machine Learning (TAG-ML) at the 40th\\r  International Conference on Machine Learning, Honolulu, Hawaii, USA. 2023\\r\\\\\\\\\\r  The problem of detecting and quantifying the presence of symmetries in\\rdatasets is useful for model selection, generative modeling, and data analysis,\\ramongst others. While existing methods for hard-coding transformations in\\rneural networks require prior knowledge of the symmetries of the task at hand,\\rthis work focuses on discovering and characterizing unknown symmetries present\\rin the dataset, namely, Lie group symmetry transformations beyond the\\rtraditional ones usually considered in the field (rotation, scaling, and\\rtranslation). Specifically, we consider a scenario in which a dataset has been\\rtransformed by a one-parameter subgroup of transformations with different\\rparameter values for each data point. Our goal is to characterize the\\rtransformation group and the distribution of the parameter values. The results\\rshowcase the effectiveness of the approach in both these settings.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01583 ,  1141kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01666 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 11:57:23 GMT   (1335kb)\\r\\rTitle: Sensors and Systems for Monitoring Mental Fatigue: A systematic review\\rAuthors: Prabin Sharma, Joanna C. Justus, Govinda R. Poudel\\rCategories: cs.HC cs.CV\\rComments: 19 Pages, 3 Figures\\r\\\\\\\\\\r  Mental fatigue is a leading cause of motor vehicle accidents, medical errors,\\rloss of workplace productivity, and student disengagements in e-learning\\renvironment. Development of sensors and systems that can reliably track mental\\rfatigue can prevent accidents, reduce errors, and help increase workplace\\rproductivity. This review provides a critical summary of theoretical models of\\rmental fatigue, a description of key enabling sensor technologies, and a\\rsystematic review of recent studies using biosensor-based systems for tracking\\rmental fatigue in humans. We conducted a systematic search and review of recent\\rliterature which focused on detection and tracking of mental fatigue in humans.\\rThe search yielded 57 studies (N=1082), majority of which used\\relectroencephalography (EEG) based sensors for tracking mental fatigue. We\\rfound that EEG-based sensors can provide a moderate to good sensitivity for\\rfatigue detection. Notably, we found no incremental benefit of using\\rhigh-density EEG sensors for application in mental fatigue detection. Given the\\rfindings, we provide a critical discussion on the integration of wearable EEG\\rand ambient sensors in the context of achieving real-world monitoring. Future\\rwork required to advance and adapt the technologies toward widespread\\rdeployment of wearable sensors and systems for fatigue monitoring in\\rsemi-autonomous and autonomous industries is examined.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01666 ,  1335kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01668 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 12:00:06 GMT   (8927kb,D)\\r\\rTitle: Training Energy-Based Models with Diffusion Contrastive Divergences\\rAuthors: Weijian Luo and Hao Jiang and Tianyang Hu and Jiacheng Sun and Zhenguo\\r  Li and Zhihua Zhang\\rCategories: cs.LG cs.CV stat.ML\\r\\\\\\\\\\r  Energy-Based Models (EBMs) have been widely used for generative modeling.\\rContrastive Divergence (CD), a prevailing training objective for EBMs, requires\\rsampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which\\rleads to an irreconcilable trade-off between the computational burden and the\\rvalidity of the CD. Running MCMCs till convergence is computationally\\rintensive. On the other hand, short-run MCMC brings in an extra non-negligible\\rparameter gradient term that is difficult to handle. In this paper, we provide\\ra general interpretation of CD, viewing it as a special instance of our\\rproposed Diffusion Contrastive Divergence (DCD) family. By replacing the\\rLangevin dynamic used in CD with other EBM-parameter-free diffusion processes,\\rwe propose a more efficient divergence. We show that the proposed DCDs are both\\rmore computationally efficient than the CD and are not limited to a\\rnon-negligible gradient term. We conduct intensive experiments, including both\\rsynthesis data modeling and high-dimensional image denoising and generation, to\\rshow the advantages of the proposed DCDs. On the synthetic data learning and\\rimage denoising experiments, our proposed DCD outperforms CD by a large margin.\\rIn image generation experiments, the proposed DCD is capable of training an\\renergy-based model for generating the Celab-A $32\\\\times 32$ dataset, which is\\rcomparable to existing EBMs.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01668 ,  8927kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01683 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 12:27:10 GMT   (136kb,D)\\r\\rTitle: Learning Discrete Weights and Activations Using the Local\\r  Reparameterization Trick\\rAuthors: Guy Berger, Aviv Navon, Ethan Fetaya\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\\\r  In computer vision and machine learning, a crucial challenge is to lower the\\rcomputation and memory demands for neural network inference. A commonplace\\rsolution to address this challenge is through the use of binarization. By\\rbinarizing the network weights and activations, one can significantly reduce\\rcomputational complexity by substituting the computationally expensive floating\\roperations with faster bitwise operations. This leads to a more efficient\\rneural network inference that can be deployed on low-resource devices. In this\\rwork, we extend previous approaches that trained networks with discrete weights\\rusing the local reparameterization trick to also allow for discrete\\ractivations. The original approach optimized a distribution over the discrete\\rweights and uses the central limit theorem to approximate the pre-activation\\rwith a continuous Gaussian distribution. Here we show that the probabilistic\\rmodeling can also allow effective training of networks with discrete activation\\ras well. This further reduces runtime and memory footprint at inference time\\rwith state-of-the-art results for networks with binary activations.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01683 ,  136kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01694 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 13:00:18 GMT   (4979kb,D)\\r\\rTitle: Spike-driven Transformer\\rAuthors: Man Yao, Jiakui Hu, Zhaokun Zhou, Li Yuan, Yonghong Tian, Bo Xu, Guoqi\\r  Li\\rCategories: cs.NE cs.CV\\r\\\\\\\\\\r  Spiking Neural Networks (SNNs) provide an energy-efficient deep learning\\roption due to their unique spike-based event-driven (i.e., spike-driven)\\rparadigm. In this paper, we incorporate the spike-driven paradigm into\\rTransformer by the proposed Spike-driven Transformer with four unique\\rproperties: 1) Event-driven, no calculation is triggered when the input of\\rTransformer is zero; 2) Binary spike communication, all matrix multiplications\\rassociated with the spike matrix can be transformed into sparse additions; 3)\\rSelf-attention with linear complexity at both token and channel dimensions; 4)\\rThe operations between spike-form Query, Key, and Value are mask and addition.\\rTogether, there are only sparse addition operations in the Spike-driven\\rTransformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA),\\rwhich exploits only mask and addition operations without any multiplication,\\rand thus having up to $87.2\\\\times$ lower computation energy than vanilla\\rself-attention. Especially in SDSA, the matrix multiplication between Query,\\rKey, and Value is designed as the mask operation. In addition, we rearrange all\\rresidual connections in the vanilla Transformer before the activation functions\\rto ensure that all neurons transmit binary spike signals. It is shown that the\\rSpike-driven Transformer can achieve 77.1\\\\% top-1 accuracy on ImageNet-1K,\\rwhich is the state-of-the-art result in the SNN field. The source code is\\ravailable at https://github.com/BICLab/Spike-Driven-Transformer.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01694 ,  4979kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01738 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 14:14:12 GMT   (5383kb,D)\\r\\rTitle: Mitigating Calibration Bias Without Fixed Attribute Grouping for\\r  Improved Fairness in Medical Imaging Analysis\\rAuthors: Changjian Shui, Justin Szeto, Raghav Mehta, Douglas Arnold, Tal Arbel\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Trustworthy deployment of deep learning medical imaging models into\\rreal-world clinical practice requires that they be calibrated. However, models\\rthat are well calibrated overall can still be poorly calibrated for a\\rsub-population, potentially resulting in a clinician unwittingly making poor\\rdecisions for this group based on the recommendations of the model. Although\\rmethods have been shown to successfully mitigate biases across subgroups in\\rterms of model accuracy, this work focuses on the open problem of mitigating\\rcalibration biases in the context of medical image analysis. Our method does\\rnot require subgroup attributes during training, permitting the flexibility to\\rmitigate biases for different choices of sensitive attributes without\\rre-training. To this end, we propose a novel two-stage method: Cluster-Focal to\\rfirst identify poorly calibrated samples, cluster them into groups, and then\\rintroduce group-wise focal loss to improve calibration bias. We evaluate our\\rmethod on skin lesion classification with the public HAM10000 dataset, and on\\rpredicting future lesional activity for multiple sclerosis (MS) patients. In\\raddition to considering traditional sensitive attributes (e.g. age, sex) with\\rdemographic subgroups, we also consider biases among groups with different\\rimage-derived attributes, such as lesion load, which are required in medical\\rimage analysis. Our results demonstrate that our method effectively controls\\rcalibration error in the worst-performing subgroups while preserving prediction\\rperformance, and outperforming recent baselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01738 ,  5383kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01767 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 15:14:59 GMT   (20025kb,D)\\r\\rTitle: Localized Data Work as a Precondition for Data-Centric ML: A Case Study\\r  of Full Lifecycle Crop Disease Identification in Ghana\\rAuthors: Darlington Akogo, Issah Samori, Cyril Akafia, Harriet Fiagbor, Andrews\\r  Kangah, Donald Kwame Asiedu, Kwabena Fuachie, Luis Oala\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  The Ghana Cashew Disease Identification with Artificial Intelligence (CADI\\rAI) project demonstrates the importance of sound data work as a precondition\\rfor the delivery of useful, localized datacentric solutions for public good\\rtasks such as agricultural productivity and food security. Drone collected data\\rand machine learning are utilized to determine crop stressors. Data, model and\\rthe final app are developed jointly and made available to local farmers via a\\rdesktop application.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01767 ,  20025kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01798 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 16:08:18 GMT   (5921kb)\\r\\rTitle: Edge-aware Multi-task Network for Integrating Quantification\\r  Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality\\r  Non-contrast MRI\\rAuthors: Xiaojiao Xiao, Qinmin Hu, Guanghui Wang\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  Simultaneous multi-index quantification, segmentation, and uncertainty\\restimation of liver tumors on multi-modality non-contrast magnetic resonance\\rimaging (NCMRI) are crucial for accurate diagnosis. However, existing methods\\rlack an effective mechanism for multi-modality NCMRI fusion and accurate\\rboundary information capture, making these tasks challenging. To address these\\rissues, this paper proposes a unified framework, namely edge-aware multi-task\\rnetwork (EaMtNet), to associate multi-index quantification, segmentation, and\\runcertainty of liver tumors on the multi-modality NCMRI. The EaMtNet employs\\rtwo parallel CNN encoders and the Sobel filters to extract local features and\\redge maps, respectively. The newly designed edge-aware feature aggregation\\rmodule (EaFA) is used for feature fusion and selection, making the network\\redge-aware by capturing long-range dependency between feature and edge maps.\\rMulti-tasking leverages prediction discrepancy to estimate uncertainty and\\rimprove segmentation and quantification performance. Extensive experiments are\\rperformed on multi-modality NCMRI with 250 clinical subjects. The proposed\\rmodel outperforms the state-of-the-art by a large margin, achieving a dice\\rsimilarity coefficient of 90.01$\\\\pm$1.23 and a mean absolute error of\\r2.72$\\\\pm$0.58 mm for MD. The results demonstrate the potential of EaMtNet as a\\rreliable clinical-aided tool for medical image analysis.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01798 ,  5921kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01846 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 17:49:46 GMT   (1463kb,D)\\r\\rTitle: Grad-FEC: Unequal Loss Protection of Deep Features in Collaborative\\r  Intelligence\\rAuthors: Korcan Uyanik, S. Faegheh Yeganli, Ivan V. Baji\\\\'c\\rCategories: eess.IV cs.CV\\rComments: 5 pages, 6 figures, IEEE ICIP 2023\\r\\\\\\\\\\r  Collaborative intelligence (CI) involves dividing an artificial intelligence\\r(AI) model into two parts: front-end, to be deployed on an edge device, and\\rback-end, to be deployed in the cloud. The deep feature tensors produced by the\\rfront-end are transmitted to the cloud through a communication channel, which\\rmay be subject to packet loss. To address this issue, in this paper, we propose\\ra novel approach to enhance the resilience of the CI system in the presence of\\rpacket loss through Unequal Loss Protection (ULP). The proposed ULP approach\\rinvolves a feature importance estimator, which estimates the importance of\\rfeature packets produced by the front-end, and then selectively applies Forward\\rError Correction (FEC) codes to protect important packets. Experimental results\\rdemonstrate that the proposed approach can significantly improve the\\rreliability and robustness of the CI system in the presence of packet loss.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01846 ,  1463kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01849 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 17:59:29 GMT   (15752kb,D)\\r\\rTitle: Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via\\r  Self-supervised Learning\\rAuthors: Xiang Li, Varun Belagali, Jinghuan Shang, Michael S. Ryoo\\rCategories: cs.RO cs.CV cs.LG\\rComments: 18 pages, 10 figures\\r\\\\\\\\\\r  Sequence modeling approaches have shown promising results in robot imitation\\rlearning. Recently, diffusion models have been adopted for behavioral cloning,\\rbenefiting from their exceptional capabilities in modeling complex data\\rdistribution. In this work, we propose Crossway Diffusion, a method to enhance\\rdiffusion-based visuomotor policy learning by using an extra self-supervised\\rlearning (SSL) objective. The standard diffusion-based policy generates action\\rsequences from random noise conditioned on visual observations and other\\rlow-dimensional states. We further extend this by introducing a new decoder\\rthat reconstructs raw image pixels (and other state information) from the\\rintermediate representations of the reverse diffusion process, and train the\\rmodel jointly using the SSL loss. Our experiments demonstrate the effectiveness\\rof Crossway Diffusion in various simulated and real-world robot tasks,\\rconfirming its advantages over the standard diffusion-based policy. We\\rdemonstrate that such self-supervised reconstruction enables better\\rrepresentation for policy learning, especially when the demonstrations have\\rdifferent proficiencies.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01849 ,  15752kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01850 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 17:59:31 GMT   (46840kb,D)\\r\\rTitle: Self-Consuming Generative Models Go MAD\\rAuthors: Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz\\r  Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, Richard G. Baraniuk\\rCategories: cs.LG cs.AI cs.CV\\rComments: 31 pages, 31 figures, pre-print\\r\\\\\\\\\\r  Seismic advances in generative AI algorithms for imagery, text, and other\\rdata types has led to the temptation to use synthetic data to train\\rnext-generation models. Repeating this process creates an autophagous\\r(self-consuming) loop whose properties are poorly understood. We conduct a\\rthorough analytical and empirical analysis using state-of-the-art generative\\rimage models of three families of autophagous loops that differ in how fixed or\\rfresh real training data is available through the generations of training and\\rin whether the samples from previous generation models have been biased to\\rtrade off data quality versus diversity. Our primary conclusion across all\\rscenarios is that without enough fresh real data in each generation of an\\rautophagous loop, future generative models are doomed to have their quality\\r(precision) or diversity (recall) progressively decrease. We term this\\rcondition Model Autophagy Disorder (MAD), making analogy to mad cow disease.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01850 ,  46840kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01930 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 21:35:49 GMT   (200kb,D)\\r\\rTitle: Learning ECG signal features without backpropagation\\rAuthors: P\\\\'eter P\\\\'osfay, Marcell T. Kurbucz, P\\\\'eter Kov\\\\'acs, Antal\\r  Jakov\\\\'ac\\rCategories: cs.LG cs.AI cs.CV stat.AP stat.ML\\rComments: 28 pages, 1 figure, 1 table\\rMSC-class: 62H30, 68T10, 62M10, 92C50\\rACM-class: J.3; I.5; I.2.0; G.3\\r\\\\\\\\\\r  Representation learning has become a crucial area of research in machine\\rlearning, as it aims to discover efficient ways of representing raw data with\\ruseful features to increase the effectiveness, scope and applicability of\\rdownstream tasks such as classification and prediction. In this paper, we\\rpropose a novel method to generate representations for time series-type data.\\rThis method relies on ideas from theoretical physics to construct a compact\\rrepresentation in a data-driven way, and it can capture both the underlying\\rstructure of the data and task-specific information while still remaining\\rintuitive, interpretable and verifiable. This novel methodology aims to\\ridentify linear laws that can effectively capture a shared characteristic among\\rsamples belonging to a specific class. By subsequently utilizing these laws to\\rgenerate a classifier-agnostic representation in a forward manner, they become\\rapplicable in a generalized setting. We demonstrate the effectiveness of our\\rapproach on the task of ECG signal classification, achieving state-of-the-art\\rperformance.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01930 ,  200kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01944 (*cross-listing*)\\rDate: Tue, 4 Jul 2023 22:26:20 GMT   (33611kb,D)\\r\\rTitle: Text + Sketch: Image Compression at Ultra Low Rates\\rAuthors: Eric Lei, Yi\\\\u{g}it Berkay Uslu, Hamed Hassani, Shirin Saeedi Bidokhti\\rCategories: cs.LG cs.CV cs.IT math.IT\\rComments: ICML 2023 Neural Compression Workshop\\r\\\\\\\\\\r  Recent advances in text-to-image generative models provide the ability to\\rgenerate high-quality images from short text descriptions. These foundation\\rmodels, when pre-trained on billion-scale datasets, are effective for various\\rdownstream tasks with little or no further training. A natural question to ask\\ris how such models may be adapted for image compression. We investigate several\\rtechniques in which the pre-trained models can be directly used to implement\\rcompression schemes targeting novel low rate regimes. We show how text\\rdescriptions can be used in conjunction with side information to generate\\rhigh-fidelity reconstructions that preserve both semantics and spatial\\rstructure of the original. We demonstrate that at very low bit-rates, our\\rmethod can significantly improve upon learned compressors in terms of\\rperceptual and semantic fidelity, despite no end-to-end training.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01944 ,  33611kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01979 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 01:41:24 GMT   (46549kb,D)\\r\\rTitle: ToothSegNet: Image Degradation meets Tooth Segmentation in CBCT Images\\rAuthors: Jiaxiang Liu, Tianxiang Hu, Yang Feng, Wanghui Ding, Zuozhu Liu\\rCategories: eess.IV cs.CV\\rComments: IEEE ISBI 2023\\r\\\\\\\\\\r  In computer-assisted orthodontics, three-dimensional tooth models are\\rrequired for many medical treatments. Tooth segmentation from cone-beam\\rcomputed tomography (CBCT) images is a crucial step in constructing the models.\\rHowever, CBCT image quality problems such as metal artifacts and blurring\\rcaused by shooting equipment and patients' dental conditions make the\\rsegmentation difficult. In this paper, we propose ToothSegNet, a new framework\\rwhich acquaints the segmentation model with generated degraded images during\\rtraining. ToothSegNet merges the information of high and low quality images\\rfrom the designed degradation simulation module using channel-wise cross fusion\\rto reduce the semantic gap between encoder and decoder, and also refines the\\rshape of tooth prediction through a structural constraint loss. Experimental\\rresults suggest that ToothSegNet produces more precise segmentation and\\routperforms the state-of-the-art medical image segmentation methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01979 ,  46549kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01981 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 01:45:19 GMT   (41525kb,D)\\r\\rTitle: A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image\\r  Diagnosis\\rAuthors: Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, Zuozhu\\r  Liu\\rCategories: eess.IV cs.CV cs.LG\\rComments: Workshop on Interpretable ML in Healthcare at International\\r  Conference on Machine Learning (ICML) 2023\\r\\\\\\\\\\r  Zero-shot medical image classification is a critical process in real-world\\rscenarios where we have limited access to all possible diseases or large-scale\\rannotated data. It involves computing similarity scores between a query medical\\rimage and possible disease categories to determine the diagnostic result.\\rRecent advances in pretrained vision-language models (VLMs) such as CLIP have\\rshown great performance for zero-shot natural image recognition and exhibit\\rbenefits in medical applications. However, an explainable zero-shot medical\\rimage recognition framework with promising performance is yet under\\rdevelopment. In this paper, we propose a novel CLIP-based zero-shot medical\\rimage classification framework supplemented with ChatGPT for explainable\\rdiagnosis, mimicking the diagnostic process performed by human experts. The key\\ridea is to query large language models (LLMs) with category names to\\rautomatically generate additional cues and knowledge, such as disease symptoms\\ror descriptions other than a single category name, to help provide more\\raccurate and explainable diagnosis in CLIP. We further design specific prompts\\rto enhance the quality of generated texts by ChatGPT that describe visual\\rmedical features. Extensive results on one private dataset and four public\\rdatasets along with detailed analysis demonstrate the effectiveness and\\rexplainability of our training-free zero-shot diagnosis pipeline, corroborating\\rthe great potential of VLMs and LLMs for medical applications.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01981 ,  41525kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01990 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 02:45:44 GMT   (24691kb)\\r\\rTitle: Unsupervised Spectral Demosaicing with Lightweight Spectral Attention\\r  Networks\\rAuthors: Kai Feng, Yongqiang Zhao, Seong G. Kong, and Haijin Zeng\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  This paper presents a deep learning-based spectral demosaicing technique\\rtrained in an unsupervised manner. Many existing deep learning-based techniques\\rrelying on supervised learning with synthetic images, often underperform on\\rreal-world images especially when the number of spectral bands increases.\\rAccording to the characteristics of the spectral mosaic image, this paper\\rproposes a mosaic loss function, the corresponding model structure, a\\rtransformation strategy, and an early stopping strategy, which form a complete\\runsupervised spectral demosaicing framework. A challenge in real-world spectral\\rdemosaicing is inconsistency between the model parameters and the computational\\rresources of the imager. We reduce the complexity and parameters of the\\rspectral attention module by dividing the spectral attention tensor into\\rspectral attention matrices in the spatial dimension and spectral attention\\rvector in the channel dimension, which is more suitable for unsupervised\\rframework. This paper also presents Mosaic25, a real 25-band hyperspectral\\rmosaic image dataset of various objects, illuminations, and materials for\\rbenchmarking. Extensive experiments on synthetic and real-world datasets\\rdemonstrate that the proposed method outperforms conventional unsupervised\\rmethods in terms of spatial distortion suppression, spectral fidelity,\\rrobustness, and computational cost.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01990 ,  24691kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01998 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 03:07:00 GMT   (10726kb,D)\\r\\rTitle: Zero-Shot Neural Architecture Search: Challenges, Solutions, and\\r  Opportunities\\rAuthors: Guihong Li, Duc Hoang, Kartikeya Bhardwaj, Ming Lin, Zhangyang Wang,\\r  Radu Marculescu\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Recently, zero-shot (or training-free) Neural Architecture Search (NAS)\\rapproaches have been proposed to liberate the NAS from training requirements.\\rThe key idea behind zero-shot NAS approaches is to design proxies that predict\\rthe accuracies of the given networks without training network parameters. The\\rproxies proposed so far are usually inspired by recent progress in theoretical\\rdeep learning and have shown great potential on several NAS benchmark datasets.\\rThis paper aims to comprehensively review and compare the state-of-the-art\\r(SOTA) zero-shot NAS approaches, with an emphasis on their hardware awareness.\\rTo this end, we first review the mainstream zero-shot proxies and discuss their\\rtheoretical underpinnings. We then compare these zero-shot proxies through\\rlarge-scale experiments and demonstrate their effectiveness in both\\rhardware-aware and hardware-oblivious NAS scenarios. Finally, we point out\\rseveral promising ideas to design better proxies. Our source code and the\\rrelated paper list are available on\\rhttps://github.com/SLDGroup/survey-zero-shot-nas.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01998 ,  10726kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02000 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 03:08:38 GMT   (1751kb,D)\\r\\rTitle: Distilling Missing Modality Knowledge from Ultrasound for Endometriosis\\r  Diagnosis with Magnetic Resonance Images\\rAuthors: Yuan Zhang, Hu Wang, David Butler, Minh-Son To, Jodie Avery, M Louise\\r  Hull and Gustavo Carneiro\\rCategories: eess.IV cs.CV cs.LG\\rComments: This paper is accepted by 2023 IEEE 20th International Symposium on\\r  Biomedical Imaging(ISBI 2023)\\r\\\\\\\\\\r  Endometriosis is a common chronic gynecological disorder that has many\\rcharacteristics, including the pouch of Douglas (POD) obliteration, which can\\rbe diagnosed using Transvaginal gynecological ultrasound (TVUS) scans and\\rmagnetic resonance imaging (MRI). TVUS and MRI are complementary non-invasive\\rendometriosis diagnosis imaging techniques, but patients are usually not\\rscanned using both modalities and, it is generally more challenging to detect\\rPOD obliteration from MRI than TVUS. To mitigate this classification imbalance,\\rwe propose in this paper a knowledge distillation training algorithm to improve\\rthe POD obliteration detection from MRI by leveraging the detection results\\rfrom unpaired TVUS data. More specifically, our algorithm pre-trains a teacher\\rmodel to detect POD obliteration from TVUS data, and it also pre-trains a\\rstudent model with 3D masked auto-encoder using a large amount of unlabelled\\rpelvic 3D MRI volumes. Next, we distill the knowledge from the teacher TVUS POD\\robliteration detector to train the student MRI model by minimizing a regression\\rloss that approximates the output of the student to the teacher using unpaired\\rTVUS and MRI data. Experimental results on our endometriosis dataset containing\\rTVUS and MRI data demonstrate the effectiveness of our method to improve the\\rPOD detection accuracy from MRI.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02000 ,  1751kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02092 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 08:10:17 GMT   (8599kb,D)\\r\\rTitle: Make A Long Image Short: Adaptive Token Length for Vision Transformers\\rAuthors: Qiqi Zhou and Yichen Zhu\\rCategories: cs.LG cs.CV\\rComments: accepted to ECML PKDD. arXiv admin note: substantial text overlap\\r  with arXiv:2112.01686\\r\\\\\\\\\\r  The vision transformer is a model that breaks down each image into a sequence\\rof tokens with a fixed length and processes them similarly to words in natural\\rlanguage processing. Although increasing the number of tokens typically results\\rin better performance, it also leads to a considerable increase in\\rcomputational cost. Motivated by the saying A picture is worth a thousand\\rwords, we propose an innovative approach to accelerate the ViT model by\\rshortening long images. Specifically, we introduce a method for adaptively\\rassigning token length for each image at test time to accelerate inference\\rspeed. First, we train a Resizable-ViT (ReViT) model capable of processing\\rinput with diverse token lengths. Next, we extract token-length labels from\\rReViT that indicate the minimum number of tokens required to achieve accurate\\rpredictions. We then use these labels to train a lightweight Token-Length\\rAssigner (TLA) that allocates the optimal token length for each image during\\rinference. The TLA enables ReViT to process images with the minimum sufficient\\rnumber of tokens, reducing token numbers in the ViT model and improving\\rinference speed. Our approach is general and compatible with modern vision\\rtransformer architectures, significantly reducing computational costs. We\\rverified the effectiveness of our methods on multiple representative ViT models\\ron image classification and action recognition.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02092 ,  8599kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02129 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 09:11:09 GMT   (6325kb,D)\\r\\rTitle: How Deep Neural Networks Learn Compositional Data: The Random Hierarchy\\r  Model\\rAuthors: Leonardo Petrini, Francesco Cagnetta, Umberto M. Tomasini, Alessandro\\r  Favero, Matthieu Wyart\\rCategories: cs.LG cs.CV stat.ML\\r\\\\\\\\\\r  Learning generic high-dimensional tasks is notably hard, as it requires a\\rnumber of training data exponential in the dimension. Yet, deep convolutional\\rneural networks (CNNs) have shown remarkable success in overcoming this\\rchallenge. A popular hypothesis is that learnable tasks are highly structured\\rand that CNNs leverage this structure to build a low-dimensional representation\\rof the data. However, little is known about how much training data they\\rrequire, and how this number depends on the data structure. This paper answers\\rthis question for a simple classification task that seeks to capture relevant\\raspects of real data: the Random Hierarchy Model. In this model, each of the\\r$n_c$ classes corresponds to $m$ synonymic compositions of high-level features,\\rwhich are in turn composed of sub-features through an iterative process\\rrepeated $L$ times. We find that the number of training data $P^*$ required by\\rdeep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is\\ronly polynomial in the input dimensionality; (ii) coincides with the training\\rset size such that the representation of a trained network becomes invariant to\\rexchanges of synonyms; (iii) corresponds to the number of data at which the\\rcorrelations between low-level features and classes become detectable. Overall,\\rour results indicate how deep CNNs can overcome the curse of dimensionality by\\rbuilding invariant representations, and provide an estimate of the number of\\rdata required to learn a task based on its hierarchically compositional\\rstructure.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02129 ,  6325kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02148 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 09:44:02 GMT   (1173kb)\\r\\rTitle: Compound Attention and Neighbor Matching Network for Multi-contrast MRI\\r  Super-resolution\\rAuthors: Wenxuan Chen, Sirui Wu, Shuai Wang, Zhongsen Li, Jia Yang, Xiaolei\\r  Song\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Multi-contrast magnetic resonance imaging (MRI) reflects information about\\rhuman tissue from different perspectives and has many clinical applications. By\\rutilizing the complementary information among different modalities,\\rmulti-contrast super-resolution (SR) of MRI can achieve better results than\\rsingle-image super-resolution. However, existing methods of multi-contrast MRI\\rSR have the following shortcomings that may limit their performance: First,\\rexisting methods either simply concatenate the reference and degraded features\\ror exploit global feature-matching between them, which are unsuitable for\\rmulti-contrast MRI SR. Second, although many recent methods employ transformers\\rto capture long-range dependencies in the spatial dimension, they neglect that\\rself-attention in the channel dimension is also important for low-level vision\\rtasks. To address these shortcomings, we proposed a novel network architecture\\rwith compound-attention and neighbor matching (CANM-Net) for multi-contrast MRI\\rSR: The compound self-attention mechanism effectively captures the dependencies\\rin both spatial and channel dimension; the neighborhood-based feature-matching\\rmodules are exploited to match degraded features and adjacent reference\\rfeatures and then fuse them to obtain the high-quality images. We conduct\\rexperiments of SR tasks on the IXI, fastMRI, and real-world scanning datasets.\\rThe CANM-Net outperforms state-of-the-art approaches in both retrospective and\\rprospective experiments. Moreover, the robustness study in our work shows that\\rthe CANM-Net still achieves good performance when the reference and degraded\\rimages are imperfectly registered, proving good potential in clinical\\rapplications.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02148 ,  1173kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02150 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 09:46:41 GMT   (286kb,D)\\r\\rTitle: Harmonizing Feature Attributions Across Deep Learning Architectures:\\r  Enhancing Interpretability and Consistency\\rAuthors: Md Abdul Kadir, Gowtham Krishna Addluri, Daniel Sonntag\\rCategories: cs.LG cs.CV\\rComments: This version of the contribution has been accepted for publication,\\r  after peer review (when applicable) but is not the Version of Record and does\\r  not reflect post-acceptance improvements, or any corrections\\r\\\\\\\\\\r  Ensuring the trustworthiness and interpretability of machine learning models\\ris critical to their deployment in real-world applications. Feature attribution\\rmethods have gained significant attention, which provide local explanations of\\rmodel predictions by attributing importance to individual input features. This\\rstudy examines the generalization of feature attributions across various deep\\rlearning architectures, such as convolutional neural networks (CNNs) and vision\\rtransformers. We aim to assess the feasibility of utilizing a feature\\rattribution method as a future detector and examine how these features can be\\rharmonized across multiple models employing distinct architectures but trained\\ron the same data distribution. By exploring this harmonization, we aim to\\rdevelop a more coherent and optimistic understanding of feature attributions,\\renhancing the consistency of local explanations across diverse deep-learning\\rmodels. Our findings highlight the potential for harmonized feature attribution\\rmethods to improve interpretability and foster trust in machine learning\\rapplications, regardless of the underlying architecture.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02150 ,  286kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02159 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 10:00:53 GMT   (92kb,D)\\r\\rTitle: DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and\\r  Generative Adversarial Networks\\rAuthors: Jingwei Zhang, Han Shi, Jincheng Yu, Enze Xie, and Zhenguo Li\\rCategories: stat.ML cs.CV cs.LG math.AP\\rComments: Tech Report\\r\\\\\\\\\\r  Generative models can be categorized into two types: explicit generative\\rmodels that define explicit density forms and allow exact likelihood inference,\\rsuch as score-based diffusion models (SDMs) and normalizing flows; implicit\\rgenerative models that directly learn a transformation from the prior to the\\rdata distribution, such as generative adversarial nets (GANs). While these two\\rtypes of models have shown great success, they suffer from respective\\rlimitations that hinder them from achieving fast sampling and high sample\\rquality simultaneously. In this paper, we propose a unified theoretic framework\\rfor SDMs and GANs. We shown that: i) the learning dynamics of both SDMs and\\rGANs can be described as a novel SDE named Discriminator Denoising Diffusion\\rFlow (DiffFlow) where the drift can be determined by some weighted combinations\\rof scores of the real data and the generated data; ii) By adjusting the\\rrelative weights between different score terms, we can obtain a smooth\\rtransition between SDMs and GANs while the marginal distribution of the SDE\\rremains invariant to the change of the weights; iii) we prove the asymptotic\\roptimality and maximal likelihood training scheme of the DiffFlow dynamics; iv)\\runder our unified theoretic framework, we introduce several instantiations of\\rthe DiffFLow that provide new algorithms beyond GANs and SDMs with exact\\rlikelihood inference and have potential to achieve flexible trade-off between\\rhigh sample quality and fast sampling speed.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02159 ,  92kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02191 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 10:33:45 GMT   (1481kb,D)\\r\\rTitle: Evaluating AI systems under uncertain ground truth: a case study in\\r  dermatology\\rAuthors: David Stutz, Ali Taylan Cemgil, Abhijit Guha Roy, Tatiana\\r  Matejovicova, Melih Barsbey, Patricia Strachan, Mike Schaekermann, Jan\\r  Freyberg, Rajeev Rikhye, Beverly Freeman, Javier Perez Matos, Umesh Telang,\\r  Dale R. Webster, Yuan Liu, Greg S. Corrado, Yossi Matias, Pushmeet Kohli, Yun\\r  Liu, Arnaud Doucet, Alan Karthikesalingam\\rCategories: cs.LG cs.CV stat.ME stat.ML\\r\\\\\\\\\\r  For safety, AI systems in health undergo thorough evaluations before\\rdeployment, validating their predictions against a ground truth that is assumed\\rcertain. However, this is actually not the case and the ground truth may be\\runcertain. Unfortunately, this is largely ignored in standard evaluation of AI\\rmodels but can have severe consequences such as overestimating the future\\rperformance. To avoid this, we measure the effects of ground truth uncertainty,\\rwhich we assume decomposes into two main components: annotation uncertainty\\rwhich stems from the lack of reliable annotations, and inherent uncertainty due\\rto limited observational information. This ground truth uncertainty is ignored\\rwhen estimating the ground truth by deterministically aggregating annotations,\\re.g., by majority voting or averaging. In contrast, we propose a framework\\rwhere aggregation is done using a statistical model. Specifically, we frame\\raggregation of annotations as posterior inference of so-called plausibilities,\\rrepresenting distributions over classes in a classification setting, subject to\\ra hyper-parameter encoding annotator reliability. Based on this model, we\\rpropose a metric for measuring annotation uncertainty and provide\\runcertainty-adjusted metrics for performance evaluation. We present a case\\rstudy applying our framework to skin condition classification from images where\\rannotations are provided in the form of differential diagnoses. The\\rdeterministic adjudication process called inverse rank normalization (IRN) from\\rprevious work ignores ground truth uncertainty in evaluation. Instead, we\\rpresent two alternative statistical models: a probabilistic version of IRN and\\ra Plackett-Luce-based model. We find that a large portion of the dataset\\rexhibits significant ground truth uncertainty and standard IRN-based evaluation\\rseverely over-estimates performance without providing uncertainty estimates.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02191 ,  1481kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02223 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 11:59:46 GMT   (1233kb,D)\\r\\rTitle: Direct segmentation of brain white matter tracts in diffusion MRI\\rAuthors: Hamza Kebiri, and Ali Gholipour, Meritxell Bach Cuadra, Davood Karimi\\rCategories: eess.IV cs.CV q-bio.NC\\r\\\\\\\\\\r  The brain white matter consists of a set of tracts that connect distinct\\rregions of the brain. Segmentation of these tracts is often needed for clinical\\rand research studies. Diffusion-weighted MRI offers unique contrast to\\rdelineate these tracts. However, existing segmentation methods rely on\\rintermediate computations such as tractography or estimation of fiber\\rorientation density. These intermediate computations, in turn, entail complex\\rcomputations that can result in unnecessary errors. Moreover, these\\rintermediate computations often require dense multi-shell measurements that are\\runavailable in many clinical and research applications. As a result, current\\rmethods suffer from low accuracy and poor generalizability. Here, we propose a\\rnew deep learning method that segments these tracts directly from the diffusion\\rMRI data, thereby sidestepping the intermediate computation errors. Our\\rexperiments show that this method can achieve segmentation accuracy that is on\\rpar with the state of the art methods (mean Dice Similarity Coefficient of\\r0.826). Compared with the state of the art, our method offers far superior\\rgeneralizability to undersampled data that are typical of clinical studies and\\rto data obtained with different acquisition protocols. Moreover, we propose a\\rnew method for detecting inaccurate segmentations and show that it is more\\raccurate than standard methods that are based on estimation uncertainty\\rquantification. The new methods can serve many critically important clinical\\rand scientific applications that require accurate and reliable non-invasive\\rsegmentation of white matter tracts.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02223 ,  1233kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02245 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 12:39:58 GMT   (6866kb,D)\\r\\rTitle: Set Learning for Accurate and Calibrated Models\\rAuthors: Lukas Muttenthaler and Robert A. Vandermeulen and Qiuyi (Richard)\\r  Zhang and Thomas Unterthiner and Klaus-Robert M\\\\uller\\rCategories: cs.LG cs.CV cs.IT math.IT\\r\\\\\\\\\\r  Model overconfidence and poor calibration are common in machine learning and\\rdifficult to account for when applying standard empirical risk minimization. In\\rthis work, we propose a novel method to alleviate these problems that we call\\rodd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets\\rrather than for single examples. This naturally allows the model to capture\\rcorrelations across data examples and achieves both better accuracy and\\rcalibration, especially in limited training data and class-imbalanced regimes.\\rPerhaps surprisingly, OKO often yields better calibration even when training\\rwith hard labels and dropping any additional calibration parameter tuning, such\\ras temperature scaling. We provide theoretical justification, establishing that\\rOKO naturally yields better calibration, and provide extensive experimental\\ranalyses that corroborate our theoretical findings. We emphasize that OKO is a\\rgeneral framework that can be easily adapted to many settings and the trained\\rmodel can be applied to single examples at inference time, without introducing\\rsignificant run-time overhead or architecture changes.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02245 ,  6866kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02251 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 12:49:02 GMT   (2191kb,D)\\r\\rTitle: RanPAC: Random Projections and Pre-trained Models for Continual Learning\\rAuthors: Mark D. McDonnell, Dong Gong, Amin Parveneh, Ehsan Abbasnejad, Anton\\r  van den Hengel\\rCategories: cs.LG cs.CV\\rComments: 30 pages, 11 figures\\r\\\\\\\\\\r  Continual learning (CL) aims to incrementally learn different tasks (such as\\rclassification) in a non-stationary data stream without forgetting old ones.\\rMost CL works focus on tackling catastrophic forgetting under a\\rlearning-from-scratch paradigm. However, with the increasing prominence of\\rfoundation models, pre-trained models equipped with informative representations\\rhave become available for various downstream requirements. Several CL methods\\rbased on pre-trained models have been explored, either utilizing pre-extracted\\rfeatures directly (which makes bridging distribution gaps challenging) or\\rincorporating adaptors (which may be subject to forgetting). In this paper, we\\rpropose a concise and effective approach for CL with pre-trained models. Given\\rthat forgetting occurs during parameter updating, we contemplate an alternative\\rapproach that exploits training-free random projectors and class-prototype\\raccumulation, which thus bypasses the issue. Specifically, we inject a frozen\\rRandom Projection layer with nonlinear activation between the pre-trained\\rmodel's feature representations and output head, which captures interactions\\rbetween features with expanded dimensionality, providing enhanced linear\\rseparability for class-prototype-based CL. We also demonstrate the importance\\rof decorrelating the class-prototypes to reduce the distribution disparity when\\rusing pre-trained representations. These techniques prove to be effective and\\rcircumvent the problem of forgetting for both class- and domain-incremental\\rcontinual learning. Compared to previous methods applied to pre-trained\\rViT-B/16 models, we reduce final error rates by between 10\\\\% and 62\\\\% on seven\\rclass-incremental benchmark datasets, despite not using any rehearsal memory.\\rWe conclude that the full potential of pre-trained models for simple,\\reffective, and fast continual learning has not hitherto been fully tapped.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02251 ,  2191kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02275 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 13:19:41 GMT   (1973kb,D)\\r\\rTitle: Convolutions Through the Lens of Tensor Networks\\rAuthors: Felix Dangel\\rCategories: cs.LG cs.CV stat.ML\\rComments: 10 pages main text + appendix, pre-print version\\r\\\\\\\\\\r  Despite their simple intuition, convolutions are more tedious to analyze than\\rdense layers, which complicates the generalization of theoretical and\\ralgorithmic ideas. We provide a new perspective onto convolutions through\\rtensor networks (TNs) which allow reasoning about the underlying tensor\\rmultiplications by drawing diagrams, and manipulating them to perform function\\rtransformations, sub-tensor access, and fusion. We demonstrate this expressive\\rpower by deriving the diagrams of various autodiff operations and popular\\rapproximations of second-order information with full hyper-parameter support,\\rbatching, channel groups, and generalization to arbitrary convolution\\rdimensions. Further, we provide convolution-specific transformations based on\\rthe connectivity pattern which allow to re-wire and simplify diagrams before\\revaluation. Finally, we probe computational performance, relying on established\\rmachinery for efficient TN contraction. Our TN implementation speeds up a\\rrecently-proposed KFAC variant up to 4.5x and enables new hardware-efficient\\rtensor dropout for approximate backpropagation.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02275 ,  1973kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02334 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 14:43:26 GMT   (5426kb)\\r\\rTitle: Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI\\rAuthors: Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, Yapeng Tian\\rCategories: eess.IV cs.CV\\rComments: Accepted by MICCAI2023\\r\\\\\\\\\\r  Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging\\r(MRI) images from partial measurement is essential to medical imaging research.\\rBenefiting from the diverse and complementary information of multi-contrast MR\\rimages in different imaging modalities, multi-contrast Super-Resolution (SR)\\rreconstruction is promising to yield SR images with higher quality. In the\\rmedical scenario, to fully visualize the lesion, radiologists are accustomed to\\rzooming the MR images at arbitrary scales rather than using a fixed scale, as\\rused by most MRI SR methods. In addition, existing multi-contrast MRI SR\\rmethods often require a fixed resolution for the reference image, which makes\\racquiring reference images difficult and imposes limitations on arbitrary scale\\rSR tasks. To address these issues, we proposed an implicit neural\\rrepresentations based dual-arbitrary multi-contrast MRI super-resolution\\rmethod, called Dual-ArbNet. First, we decouple the resolution of the target and\\rreference images by a feature encoder, enabling the network to input target and\\rreference images at arbitrary scales. Then, an implicit fusion decoder fuses\\rthe multi-contrast features and uses an Implicit Decoding Function~(IDF) to\\robtain the final MRI SR results. Furthermore, we introduce a curriculum\\rlearning strategy to train our network, which improves the generalization and\\rperformance of our Dual-ArbNet. Extensive experiments in two public MRI\\rdatasets demonstrate that our method outperforms state-of-the-art approaches\\runder different scale factors and has great potential in clinical practice.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02334 ,  5426kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02430 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 16:52:06 GMT   (917kb,D)\\r\\rTitle: Base Layer Efficiency in Scalable Human-Machine Coding\\rAuthors: Yalda Foroutan, Alon Harell, Anderson de Andrade, Ivan V. Baji\\\\'c\\rCategories: eess.IV cs.CV\\rComments: 5 pages, 6 figures, IEEE ICIP 2023\\r\\\\\\\\\\r  A basic premise in scalable human-machine coding is that the base layer is\\rintended for automated machine analysis and is therefore more compressible than\\rthe same content would be for human viewing. Use cases for such coding include\\rvideo surveillance and traffic monitoring, where the majority of the content\\rwill never be seen by humans. Therefore, base layer efficiency is of paramount\\rimportance because the system would most frequently operate at the base-layer\\rrate. In this paper, we analyze the coding efficiency of the base layer in a\\rstate-of-the-art scalable human-machine image codec, and show that it can be\\rimproved. In particular, we demonstrate that gains of 20-40% in BD-Rate\\rcompared to the currently best results on object detection and instance\\rsegmentation are possible.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02430 ,  917kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02452 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 17:23:42 GMT   (1163kb,D)\\r\\rTitle: LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved\\r  Wavelet Attention and Reverse Diffusion\\rAuthors: Long Bai, Tong Chen, Yanan Wu, An Wang, Mobarakol Islam, Hongliang Ren\\rCategories: eess.IV cs.CV cs.RO\\rComments: To appear in MICCAI 2023. Code availability:\\r  https://github.com/longbai1006/LLCaps\\r\\\\\\\\\\r  Wireless capsule endoscopy (WCE) is a painless and non-invasive diagnostic\\rtool for gastrointestinal (GI) diseases. However, due to GI anatomical\\rconstraints and hardware manufacturing limitations, WCE vision signals may\\rsuffer from insufficient illumination, leading to a complicated screening and\\rexamination procedure. Deep learning-based low-light image enhancement (LLIE)\\rin the medical field gradually attracts researchers. Given the exuberant\\rdevelopment of the denoising diffusion probabilistic model (DDPM) in computer\\rvision, we introduce a WCE LLIE framework based on the multi-scale\\rconvolutional neural network (CNN) and reverse diffusion process. The\\rmulti-scale design allows models to preserve high-resolution representation and\\rcontext information from low-resolution, while the curved wavelet attention\\r(CWA) block is proposed for high-frequency and local feature learning.\\rFurthermore, we combine the reverse diffusion procedure to further optimize the\\rshallow output and generate the most realistic image. The proposed method is\\rcompared with ten state-of-the-art (SOTA) LLIE methods and significantly\\routperforms quantitatively and qualitatively. The superior performance on GI\\rdisease segmentation further demonstrates the clinical potential of our\\rproposed model. Our code is publicly accessible.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02452 ,  1163kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02460 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 17:33:41 GMT   (4158kb,D)\\r\\rTitle: Performance Scaling via Optimal Transport: Enabling Data Selection from\\r  Partially Revealed Sources\\rAuthors: Feiyang Kang, Hoang Anh Just, Anit Kumar Sahu, Ruoxi Jia\\rCategories: cs.LG cs.AI cs.CE cs.CV\\rComments: An extended abstract of this work appears in Data-centric Machine\\r  Learning Research (DMLR) Workshop at 40th International Conference on Machine\\r  Learning, Honolulu HI, USA. July 29, 2023\\r\\\\\\\\\\r  Traditionally, data selection has been studied in settings where all samples\\rfrom prospective sources are fully revealed to a machine learning developer.\\rHowever, in practical data exchange scenarios, data providers often reveal only\\ra limited subset of samples before an acquisition decision is made. Recently,\\rthere have been efforts to fit scaling laws that predict model performance at\\rany size and data source composition using the limited available samples.\\rHowever, these scaling functions are black-box, computationally expensive to\\rfit, highly susceptible to overfitting, or/and difficult to optimize for data\\rselection. This paper proposes a framework called , which predicts\\rmodel performance and supports data selection decisions based on partial\\rsamples of prospective data sources. Our approach distinguishes itself from\\rexisting work by introducing a novel *two-stage* performance inference process.\\rIn the first stage, we leverage the Optimal Transport distance to predict the\\rmodel's performance for any data mixture ratio within the range of disclosed\\rdata sizes. In the second stage, we extrapolate the performance to larger\\rundisclosed data sizes based on a novel parameter-free mapping technique\\rinspired by neural scaling laws. We further derive an efficient gradient-based\\rmethod to select data sources based on the projected model performance.\\rEvaluation over a diverse range of applications demonstrates that \\rsignificantly improves existing performance scaling approaches in terms of both\\rthe accuracy of performance inference and the computation costs associated with\\rconstructing the performance predictor. Also,  outperforms by a wide\\rmargin in data selection effectiveness compared to a range of other\\roff-the-shelf solutions.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02460 ,  4158kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02462 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 17:34:58 GMT   (2968kb,D)\\r\\rTitle: Expert-Agnostic Ultrasound Image Quality Assessment using Deep\\r  Variational Clustering\\rAuthors: Deepak Raina, Dimitrios Ntentia, SH Chandrashekhara, Richard Voyles,\\r  Subir Kumar Saha\\rCategories: eess.IV cs.CV\\rComments: Accepted in IEEE International Conference on Robotics and Automation\\r  (ICRA) 2023\\rJournal-ref: 10.1109/ICRA48891.2023.10160435\\r\\\\\\\\\\r  Ultrasound imaging is a commonly used modality for several diagnostic and\\rtherapeutic procedures. However, the diagnosis by ultrasound relies heavily on\\rthe quality of images assessed manually by sonographers, which diminishes the\\robjectivity of the diagnosis and makes it operator-dependent. The supervised\\rlearning-based methods for automated quality assessment require manually\\rannotated datasets, which are highly labour-intensive to acquire. These\\rultrasound images are low in quality and suffer from noisy annotations caused\\rby inter-observer perceptual variations, which hampers learning efficiency. We\\rpropose an UnSupervised UltraSound image Quality assessment Network, US2QNet,\\rthat eliminates the burden and uncertainty of manual annotations. US2QNet uses\\rthe variational autoencoder embedded with the three modules, pre-processing,\\rclustering and post-processing, to jointly enhance, extract, cluster and\\rvisualize the quality feature representation of ultrasound images. The\\rpre-processing module uses filtering of images to point the network's attention\\rtowards salient quality features, rather than getting distracted by noise.\\rPost-processing is proposed for visualizing the clusters of feature\\rrepresentations in 2D space. We validated the proposed framework for quality\\rassessment of the urinary bladder ultrasound images. The proposed framework\\rachieved 78% accuracy and superior performance to state-of-the-art clustering\\rmethods.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02462 ,  2968kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02464 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 17:38:01 GMT   (4242kb,D)\\r\\rTitle: AxonCallosumEM Dataset: Axon Semantic Segmentation of Whole Corpus\\r  Callosum cross section from EM Images\\rAuthors: Ao Cheng and Guoqiang Zhao and Lirong Wang and Ruobing Zhang\\rCategories: eess.IV cs.CV\\rComments: 9 pages, 6 figures\\r\\\\\\\\\\r  The electron microscope (EM) remains the predominant technique for\\relucidating intricate details of the animal nervous system at the nanometer\\rscale. However, accurately reconstructing the complex morphology of axons and\\rmyelin sheaths poses a significant challenge. Furthermore, the absence of\\rpublicly available, large-scale EM datasets encompassing complete cross\\rsections of the corpus callosum, with dense ground truth segmentation for axons\\rand myelin sheaths, hinders the advancement and evaluation of holistic corpus\\rcallosum reconstructions. To surmount these obstacles, we introduce the\\rAxonCallosumEM dataset, comprising a 1.83 times 5.76mm EM image captured from\\rthe corpus callosum of the Rett Syndrome (RTT) mouse model, which entail\\rextensive axon bundles. We meticulously proofread over 600,000 patches at a\\rresolution of 1024 times 1024, thus providing a comprehensive ground truth for\\rmyelinated axons and myelin sheaths. Additionally, we extensively annotated\\rthree distinct regions within the dataset for the purposes of training,\\rtesting, and validation. Utilizing this dataset, we develop a fine-tuning\\rmethodology that adapts Segment Anything Model (SAM) to EM images segmentation\\rtasks, called EM-SAM, enabling outperforms other state-of-the-art methods.\\rFurthermore, we present the evaluation results of EM-SAM as a baseline.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02464 ,  4242kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.02485 (*cross-listing*)\\rDate: Wed, 5 Jul 2023 17:59:27 GMT   (11764kb,D)\\r\\rTitle: Building Cooperative Embodied Agents Modularly with Large Language\\r  Models\\rAuthors: Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua\\r  B. Tenenbaum, Tianmin Shu, Chuang Gan\\rCategories: cs.AI cs.CL cs.CV\\rComments: Project page: https://vis-www.cs.umass.edu/Co-LLM-Agents/\\r\\\\\\\\\\r  Large Language Models (LLMs) have demonstrated impressive planning abilities\\rin single-agent embodied tasks across various domains. However, their capacity\\rfor planning and communication in multi-agent cooperation remains unclear, even\\rthough these are crucial skills for intelligent embodied agents. In this paper,\\rwe present a novel framework that utilizes LLMs for multi-agent cooperation and\\rtests it in various embodied environments. Our framework enables embodied\\ragents to plan, communicate, and cooperate with other embodied agents or humans\\rto accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs,\\rsuch as GPT-4, can surpass strong planning-based methods and exhibit emergent\\reffective communication using our framework without requiring fine-tuning or\\rfew-shot prompting. We also discover that LLM-based agents that communicate in\\rnatural language can earn more trust and cooperate more effectively with\\rhumans. Our research underscores the potential of LLMs for embodied AI and lays\\rthe foundation for future research in multi-agent cooperation. Videos can be\\rfound on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.\\r\\\\\\\\ ( https://arxiv.org/abs/2307.02485 ,  11764kb)\\r%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2012.13392\\rreplaced with revised version Mon, 3 Jul 2023 21:21:19 GMT   (19529kb,D)\\r\\rTitle: Deep Learning-Based Human Pose Estimation: A Survey\\rAuthors: Ce Zheng and Wenhan Wu and Chen Chen and Taojiannan Yang and Sijie Zhu\\r  and Ju Shen and Nasser Kehtarnavaz and Mubarak Shah\\rCategories: cs.CV cs.GR cs.MM\\r\\\\\\\\ ( https://arxiv.org/abs/2012.13392 ,  19529kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2101.07458\\rreplaced with revised version Wed, 5 Jul 2023 06:46:37 GMT   (23614kb,D)\\r\\rTitle: Hybrid Trilinear and Bilinear Programming for Aligning Partially\\r  Overlapping Point Sets\\rAuthors: Wei Lian and Wangmeng Zuo\\rCategories: cs.CV\\rJournal-ref: Neurocomputing, July, 2023\\rDOI: 10.1016/j.neucom.2023.126482\\r\\\\\\\\ ( https://arxiv.org/abs/2101.07458 ,  23614kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2101.11183\\rreplaced with revised version Tue, 4 Jul 2023 07:30:21 GMT   (5109kb,D)\\r\\rTitle: DeepOIS: Gyroscope-Guided Deep Optical Image Stabilizer Compensation\\rAuthors: Haipeng Li, Shuaicheng Liu, Jue Wang\\rCategories: cs.CV\\rReport-no: 21690602\\rJournal-ref: IEEE Transactions on Circuits and Systems for Video Technology (\\r  Volume: 32, Issue: 5, May 2022)\\rDOI: 10.1109/TCSVT.2021.3103281\\r\\\\\\\\ ( https://arxiv.org/abs/2101.11183 ,  5109kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2105.00717\\rreplaced with revised version Wed, 5 Jul 2023 15:59:52 GMT   (8730kb,D)\\r\\rTitle: Synthetic Data for Model Selection\\rAuthors: Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Matan Fintz, Gerard\\r  Medioni\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2105.00717 ,  8730kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2106.06847\\rreplaced with revised version Tue, 4 Jul 2023 15:30:58 GMT   (10354kb,D)\\r\\rTitle: Video Super-Resolution Transformer\\rAuthors: Jiezhang Cao, Yawei Li, Kai Zhang, Luc Van Gool\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2106.06847 ,  10354kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2111.15430\\rreplaced with revised version Wed, 5 Jul 2023 07:10:35 GMT   (1951kb,D)\\r\\rTitle: The Devil is in the Margin: Margin-based Label Smoothing for Network\\r  Calibration\\rAuthors: Bingyuan Liu, Ismail Ben Ayed, Adrian Galdran, Jose Dolz\\rCategories: cs.CV cs.LG\\rComments: CVPR 2022. Code: https://github.com/by-liu/MbLS\\r\\\\\\\\ ( https://arxiv.org/abs/2111.15430 ,  1951kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.05121\\rreplaced with revised version Wed, 5 Jul 2023 17:52:07 GMT   (12887kb,D)\\r\\rTitle: STEdge: Self-training Edge Detection with Multi-layer Teaching and\\r  Regularization\\rAuthors: Yunfan Ye, Renjiao Yi, Zhiping Cai, Kai Xu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2201.05121 ,  12887kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2203.14092\\rreplaced with revised version Wed, 5 Jul 2023 13:48:43 GMT   (1006kb,D)\\r\\rTitle: Towards Visual Affordance Learning: A Benchmark for Affordance\\r  Segmentation and Recognition\\rAuthors: Zeyad Khalifa, Syed Afaq Ali Shah\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2203.14092 ,  1006kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2204.12900\\rreplaced with revised version Tue, 4 Jul 2023 02:20:38 GMT   (6278kb,D)\\r\\rTitle: Cross-Camera Trajectories Help Person Retrieval in a Camera Network\\rAuthors: Xin Zhang and Xiaohua Xie and Jianhuang Lai and Wei-Shi Zheng\\rCategories: cs.CV cs.AI\\rComments: IEEE Transactions on Image Processing (TIP), 2023\\rDOI: 10.1109/TIP.2023.3290515\\r\\\\\\\\ ( https://arxiv.org/abs/2204.12900 ,  6278kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2205.11018\\rreplaced with revised version Wed, 5 Jul 2023 06:08:29 GMT   (10190kb,D)\\r\\rTitle: A Comprehensive Handwritten Paragraph Text Recognition System:\\r  LexiconNet\\rAuthors: Lalita Kumari, Sukhdeep Singh, Vaibhav Varish Singh Rathore and Anuj\\r  Sharma\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2205.11018 ,  10190kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2205.14330\\rreplaced with revised version Wed, 5 Jul 2023 15:17:19 GMT   (14296kb,D)\\r\\rTitle: Differentiable Point-Based Radiance Fields for Efficient View Synthesis\\rAuthors: Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, Felix Heide\\rCategories: cs.CV cs.GR\\r\\\\\\\\ ( https://arxiv.org/abs/2205.14330 ,  14296kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2207.03341\\rreplaced with revised version Wed, 5 Jul 2023 00:28:06 GMT   (10692kb,D)\\r\\rTitle: Softmax-free Linear Transformers\\rAuthors: Li Zhang, Jiachen Lu, Junge Zhang, Xiatian Zhu, Jianfeng Feng, Tao\\r  Xiang\\rCategories: cs.CV cs.AI cs.LG\\rComments: Extended journal version of NeurIPS conference submission\\r  arXiv:2110.11945\\r\\\\\\\\ ( https://arxiv.org/abs/2207.03341 ,  10692kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2207.06079\\rreplaced with revised version Wed, 5 Jul 2023 04:31:58 GMT   (11250kb,D)\\r\\rTitle: Teachers in concordance for pseudo-labeling of 3D sequential data\\rAuthors: Awet Haileslassie Gebrehiwot, Patrik Vacek, David Hurych, Karel\\r  Zimmermann, Patrick Perez, Tom\\\\'a\\\\v{s} Svoboda\\rCategories: cs.CV cs.RO\\rComments: This work has been submitted to the IEEE for publication\\rMSC-class: 68T07\\rACM-class: I.4.6; I.4.8\\rJournal-ref: in IEEE Robotics and Automation Letters, vol. 8, no. 2, pp.\\r  536-543, Feb. 2023\\rDOI: 10.1109/LRA.2022.3226029\\r\\\\\\\\ ( https://arxiv.org/abs/2207.06079 ,  11250kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2207.06825\\rreplaced with revised version Mon, 3 Jul 2023 19:10:55 GMT   (4774kb,D)\\r\\rTitle: Refign: Align and Refine for Adaptation of Semantic Segmentation to\\r  Adverse Conditions\\rAuthors: David Bruggemann, Christos Sakaridis, Prune Truong, Luc Van Gool\\rCategories: cs.CV\\rComments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\\r  2023\\r\\\\\\\\ ( https://arxiv.org/abs/2207.06825 ,  4774kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2207.07268\\rreplaced with revised version Wed, 5 Jul 2023 16:11:41 GMT   (4236kb,D)\\r\\rTitle: Lightweight Vision Transformer with Cross Feature Attention\\rAuthors: Youpeng Zhao, Huadong Tang, Yingying Jiang, Yong A and Qiang Wu\\rCategories: cs.CV\\rComments: Technical Report. A shorter version has been accepted to ICIP 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2207.07268 ,  4236kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.02894\\rreplaced with revised version Tue, 4 Jul 2023 01:55:13 GMT   (2293kb,D)\\r\\rTitle: Redesigning Multi-Scale Neural Network for Crowd Counting\\rAuthors: Zhipeng Du, Miaojing Shi, Jiankang Deng, Stefanos Zafeiriou\\rCategories: cs.CV\\rComments: IEEE Transactions on Image Processing\\r\\\\\\\\ ( https://arxiv.org/abs/2208.02894 ,  2293kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.12776\\rreplaced with revised version Tue, 4 Jul 2023 14:50:31 GMT   (3299kb,D)\\r\\rTitle: SFusion: Self-attention based N-to-One Multimodal Fusion Block\\rAuthors: Zecheng Liu and Jia Wei and Rui Li and Jianlong Zhou\\rCategories: cs.CV cs.AI\\rComments: This paper has been accepted by MICCAI 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2208.12776 ,  3299kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2208.13648\\rreplaced with revised version Tue, 4 Jul 2023 01:57:51 GMT   (1116kb,D)\\r\\rTitle: Dynamic Data-Free Knowledge Distillation by Easy-to-Hard Learning\\r  Strategy\\rAuthors: Jingru Li, Sheng Zhou, Liangcheng Li, Haishuai Wang, Zhi Yu, Jiajun Bu\\rCategories: cs.CV\\rComments: Accepted by Information Sciences, Proof version provided\\rDOI: 10.1016/j.ins.2023.119202\\r\\\\\\\\ ( https://arxiv.org/abs/2208.13648 ,  1116kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2209.06606\\rreplaced with revised version Tue, 4 Jul 2023 09:48:35 GMT   (572kb,D)\\r\\rTitle: PlaStIL: Plastic and Stable Memory-Free Class-Incremental Learning\\rAuthors: Gr\\\\'egoire Petit, Adrian Popescu, Eden Belouadah, David Picard,\\r  Bertrand Delezoide\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2209.06606 ,  572kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.00708\\rreplaced with revised version Tue, 4 Jul 2023 13:28:48 GMT   (721kb)\\r\\rTitle: EraseNet: A Recurrent Residual Network for Supervised Document Cleaning\\rAuthors: Yashowardhan Shinde, Kishore Kulkarni, Sachin Kuberkar\\rCategories: cs.CV cs.LG eess.IV\\rComments: 10 pages, 5 figures, attempting for publication in International\\r  Journal on Document Analysis and Recognition (IJDAR)\\r\\\\\\\\ ( https://arxiv.org/abs/2210.00708 ,  721kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.01361\\rreplaced with revised version Wed, 5 Jul 2023 05:30:46 GMT   (1882kb,D)\\r\\rTitle: Uncertainty-Aware Lidar Place Recognition in Novel Environments\\rAuthors: Keita Mason, Joshua Knights, Milad Ramezani, Peyman Moghadam and\\r  Dimity Miller\\rCategories: cs.CV\\rComments: 8 pages, 4 figures. Accepted for publication at IEEE IROS 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2210.01361 ,  1882kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.09477\\rreplaced with revised version Wed, 5 Jul 2023 12:35:29 GMT   (46109kb,D)\\r\\rTitle: UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a\\r  Single Image\\rAuthors: Dani Valevski, Matan Kalman, Eyal Molad, Eyal Segalis, Yossi Matias,\\r  Yaniv Leviathan\\rCategories: cs.CV cs.GR cs.LG\\rComments: SIGGRAPH 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2210.09477 ,  46109kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.10209\\rreplaced with revised version Wed, 5 Jul 2023 16:57:43 GMT   (336kb,D)\\r\\rTitle: Exclusive Supermask Subnetwork Training for Continual Learning\\rAuthors: Prateek Yadav, Mohit Bansal\\rCategories: cs.CV cs.AI cs.CL cs.LG\\rComments: ACL Findings 2023 (17 pages, 7 figures)\\r\\\\\\\\ ( https://arxiv.org/abs/2210.10209 ,  336kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.17106\\rreplaced with revised version Tue, 4 Jul 2023 15:26:36 GMT   (594kb)\\r\\rTitle: Intelligent Painter: Picture Composition With Resampling Diffusion Model\\rAuthors: Wing-Fung Ku, Wan-Chi Siu, Xi Cheng, H. Anthony Chan\\rCategories: cs.CV\\rComments: ICIP 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2210.17106 ,  594kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.12193\\rreplaced with revised version Tue, 4 Jul 2023 14:26:19 GMT   (1912kb,D)\\r\\rTitle: Anatomy-guided domain adaptation for 3D in-bed human pose estimation\\rAuthors: Alexander Bigalke, Lasse Hansen, Jasper Diesel, Carlotta Hennigs,\\r  Philipp Rostalski, Mattias P. Heinrich\\rCategories: cs.CV\\rComments: accepted at Medical Image Analysis\\r\\\\\\\\ ( https://arxiv.org/abs/2211.12193 ,  1912kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.02090\\rreplaced with revised version Tue, 4 Jul 2023 09:17:49 GMT   (2487kb,D)\\r\\rTitle: Breaking the Spurious Causality of Conditional Generation via Fairness\\r  Intervention with Corrective Sampling\\rAuthors: Junhyun Nam, Sangwoo Mo, Jaeho Lee, Jinwoo Shin\\rCategories: cs.CV cs.AI cs.LG\\rComments: TMLR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2212.02090 ,  2487kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.05136\\rreplaced with revised version Mon, 3 Jul 2023 23:03:22 GMT   (2538kb,D)\\r\\rTitle: CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised\\r  Video Anomaly Detection\\rAuthors: Hyekang Kevin Joo, Khoa Vo, Kashu Yamazaki, Ngan Le\\rCategories: cs.CV\\rComments: Published at the 30th IEEE International Conference on Image\\r  Processing (IEEE ICIP 2023)\\r\\\\\\\\ ( https://arxiv.org/abs/2212.05136 ,  2538kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.08846\\rreplaced with revised version Tue, 4 Jul 2023 07:31:18 GMT   (16324kb,D)\\r\\rTitle: Painterly Image Harmonization in Dual Domains\\rAuthors: Junyan Cao, Yan Hong, Li Niu\\rCategories: cs.CV\\rComments: Accepted by AAAI2023\\r\\\\\\\\ ( https://arxiv.org/abs/2212.08846 ,  16324kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.09530\\rreplaced with revised version Mon, 3 Jul 2023 21:16:17 GMT   (2874kb,D)\\r\\rTitle: HARP: Personalized Hand Reconstruction from a Monocular RGB Video\\rAuthors: Korrawe Karunratanakul, Sergey Prokudin, Otmar Hilliges, Siyu Tang\\rCategories: cs.CV\\rComments: CVPR 2023. Project page: https://korrawe.github.io/harp-project/\\r\\\\\\\\ ( https://arxiv.org/abs/2212.09530 ,  2874kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2302.11325\\rreplaced with revised version Tue, 4 Jul 2023 15:51:23 GMT   (2884kb,D)\\r\\rTitle: Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS\\r  Instance Segmentation\\rAuthors: Chengxi Zeng, Xinyu Yang, David Smithard, Majid Mirmehdi, Alberto M\\r  Gambaruto, Tilo Burghardt\\rCategories: cs.CV cs.AI\\r\\\\\\\\ ( https://arxiv.org/abs/2302.11325 ,  2884kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.00865\\rreplaced with revised version Wed, 5 Jul 2023 13:25:47 GMT   (39657kb,D)\\r\\rTitle: AMIGO: Sparse Multi-Modal Graph Transformer with Shared-Context\\r  Processing for Representation Learning of Giga-pixel Images\\rAuthors: Ramin Nakhli, Puria Azadi Moghadam, Haoyang Mi, Hossein Farahani,\\r  Alexander Baras, Blake Gilks, Ali Bashashati\\rCategories: cs.CV\\rComments: Accepted at CVPR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2303.00865 ,  39657kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.12776\\rreplaced with revised version Wed, 5 Jul 2023 13:36:43 GMT   (3598kb,D)\\r\\rTitle: Dense Distinct Query for End-to-End Object Detection\\rAuthors: Shilong Zhang, Xinjiang Wang, Jiaqi Wang, Jiangmiao Pang, Chengqi Lyu,\\r  Wenwei Zhang, Ping Luo, Kai Chen\\rCategories: cs.CV\\rComments: Accepted to CVPR2023. Code has been released at\\r  https://github.com/jshilong/DDQ\\r\\\\\\\\ ( https://arxiv.org/abs/2303.12776 ,  3598kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.04963\\rreplaced with revised version Wed, 5 Jul 2023 14:49:09 GMT   (940kb,D)\\r\\rTitle: PlantDet: A benchmark for Plant Detection in the Three-Rivers-Source\\r  Region\\rAuthors: Huanhuan Li, Xuechao Zou, Yu-an Zhang, Jiangcai Zhaba, Guomei Li,\\r  Lamao Yongga\\rCategories: cs.CV\\rComments: 10 pages, 5 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2304.04963 ,  940kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.12235\\rreplaced with revised version Wed, 5 Jul 2023 07:30:58 GMT   (2719kb)\\r\\rTitle: Multi-cropping Contrastive Learning and Domain Consistency for\\r  Unsupervised Image-to-Image Translation\\rAuthors: Chen Zhao, Wei-Ling Cai, Zheng Yuan, Cheng-Wei Hu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2304.12235 ,  2719kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.13009\\rreplaced with revised version Wed, 5 Jul 2023 14:09:09 GMT   (15927kb,D)\\r\\rTitle: The Potential of Visual ChatGPT For Remote Sensing\\rAuthors: Lucas Prado Osco, Eduardo Lopes de Lemos, Wesley Nunes Gon\\\\c{c}alves,\\r  Ana Paula Marques Ramos and Jos\\\\'e Marcato Junior\\rCategories: cs.CV eess.IV\\r\\\\\\\\ ( https://arxiv.org/abs/2304.13009 ,  15927kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.00432\\rreplaced with revised version Tue, 4 Jul 2023 10:43:22 GMT   (31158kb,D)\\r\\rTitle: Synthetic Data-based Detection of Zebras in Drone Imagery\\rAuthors: Elia Bonetto and Aamir Ahmad\\rCategories: cs.CV cs.RO\\rComments: 8 pages, 7 figures, 3 tables. Published in IEEE ECMR 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2305.00432 ,  31158kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.01361\\rreplaced with revised version Wed, 5 Jul 2023 08:59:44 GMT   (739kb,D)\\r\\rTitle: Boosting Adversarial Transferability via Fusing Logits of Top-1\\r  Decomposed Feature\\rAuthors: Juanjuan Weng and Zhiming Luo and Dazhen Lin and Shaozi Li and Zhun\\r  Zhong\\rCategories: cs.CV cs.CR\\r\\\\\\\\ ( https://arxiv.org/abs/2305.01361 ,  739kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.11019\\rreplaced with revised version Tue, 4 Jul 2023 03:16:40 GMT   (6382kb,D)\\r\\rTitle: Annotation-free Audio-Visual Segmentation\\rAuthors: Jinxiang Liu, Yu Wang, Chen Ju, Chaofan Ma, Ya Zhang, Weidi Xie\\rCategories: cs.CV cs.AI cs.MM\\rComments: Update the dataset and the models which are available soon; project\\r  page is https://jinxiang-liu.github.io/anno-free-AVS/\\r\\\\\\\\ ( https://arxiv.org/abs/2305.11019 ,  6382kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.17376\\rreplaced with revised version Tue, 4 Jul 2023 15:23:24 GMT   (37634kb,D)\\r\\rTitle: DePF: A Novel Fusion Approach based on Decomposition Pooling for\\r  Infrared and Visible Images\\rAuthors: Hui Li, Yongbiao Xiao, Chunyang Cheng, Zhongwei Shen, Xiaoning Song\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2305.17376 ,  37634kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.04719\\rreplaced with revised version Tue, 4 Jul 2023 19:39:25 GMT   (8250kb,D)\\r\\rTitle: Don't trust your eyes: on the (un)reliability of feature visualizations\\rAuthors: Robert Geirhos, Roland S. Zimmermann, Blair Bilodeau, Wieland Brendel,\\r  Been Kim\\rCategories: cs.CV cs.AI cs.HC cs.LG q-bio.NC\\rComments: Added github link to\\r  https://github.com/google-research/fooling-feature-visualizations/\\r\\\\\\\\ ( https://arxiv.org/abs/2306.04719 ,  8250kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.06722\\rreplaced with revised version Tue, 4 Jul 2023 18:22:09 GMT   (1671kb,D)\\r\\rTitle: $E(2)$-Equivariant Vision Transformer\\rAuthors: Renjun Xu and Kaifan Yang and Ke Liu and Fengxiang He\\rCategories: cs.CV cs.AI\\rComments: Accept to UAI2023\\r\\\\\\\\ ( https://arxiv.org/abs/2306.06722 ,  1671kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.07520\\rreplaced with revised version Tue, 4 Jul 2023 13:59:04 GMT   (3500kb,D)\\r\\rTitle: Retrieve Anyone: A General-purpose Person Re-identification Task with\\r  Instructions\\rAuthors: Weizhen He and Shixiang Tang and Yiheng Deng and Qihao Chen and\\r  Qingsong Xie and Yizhou Wang and Lei Bai and Feng Zhu and Rui Zhao and Wanli\\r  Ouyang and Donglian Qi and Yunfeng Yan\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.07520 ,  3500kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.12070\\rreplaced with revised version Wed, 5 Jul 2023 06:09:50 GMT   (2723kb,D)\\r\\rTitle: Task-Robust Pre-Training for Worst-Case Downstream Adaptation\\rAuthors: Jianghui Wang, Yang Chen, Xingyu Xie, Cong Fang, Zhouchen Lin\\rCategories: cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2306.12070 ,  2723kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.14749\\rreplaced with revised version Tue, 4 Jul 2023 10:18:23 GMT   (5795kb,D)\\r\\rTitle: A denoised Mean Teacher for domain adaptive point cloud registration\\rAuthors: Alexander Bigalke, Mattias P. Heinrich\\rCategories: cs.CV\\rComments: early accepted at MICCAI 2023; corrected confused reference\\r\\\\\\\\ ( https://arxiv.org/abs/2306.14749 ,  5795kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.14937\\rreplaced with revised version Sun, 2 Jul 2023 08:50:00 GMT   (9062kb,D)\\r\\rTitle: Minimum Description Length Clustering to Measure Meaningful Image\\r  Complexity\\rAuthors: Louis Mahon, Thomas Lukasiewicz\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.14937 ,  9062kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.15416\\rreplaced with revised version Tue, 4 Jul 2023 07:00:25 GMT   (4563kb,D)\\r\\rTitle: Irregular Change Detection in Sparse Bi-Temporal Point Clouds using\\r  Learned Place Recognition Descriptors and Point-to-Voxel Comparison\\rAuthors: Nikolaos Stathoulopoulos, Anton Koval and George Nikolakopoulos\\rCategories: cs.CV cs.RO\\rComments: Accepted for publication in the 2023 IEEE/RSJ International\\r  Conference on Intelligent Robots and Systems (IROS 2023)\\r\\\\\\\\ ( https://arxiv.org/abs/2306.15416 ,  4563kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.15767\\rreplaced with revised version Tue, 4 Jul 2023 18:59:31 GMT   (2784kb,D)\\r\\rTitle: Evidential Detection and Tracking Collaboration: New Problem, Benchmark\\r  and Algorithm for Robust Anti-UAV System\\rAuthors: Xue-Feng Zhu, Tianyang Xu, Jian Zhao, Jia-Wei Liu, Kai Wang, Gang\\r  Wang, Jianan Li, Qiang Wang, Lei Jin, Zheng Zhu, Junliang Xing, Xiao-Jun Wu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.15767 ,  2784kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.16759\\rreplaced with revised version Tue, 4 Jul 2023 05:51:58 GMT   (48146kb,D)\\r\\rTitle: SaaFormer: Spectral-spatial Axial Aggregation Transformer for\\r  Hyperspectral Image Classification\\rAuthors: Enzhe Zhao, Zhichang Guo, Yao Li, Dazhi Zhang\\rCategories: cs.CV\\rComments: arXiv admin note: text overlap with arXiv:2107.02988 by other authors\\r\\\\\\\\ ( https://arxiv.org/abs/2306.16759 ,  48146kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.17010\\rreplaced with revised version Mon, 3 Jul 2023 21:23:18 GMT   (26207kb,D)\\r\\rTitle: milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human\\r  Motion Sensing\\rAuthors: Fangqiang Ding, Zhen Luo, Peijun Zhao, Chris Xiaoxuan Lu\\rCategories: cs.CV cs.AI cs.LG\\rComments: 15 pages, 8 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2306.17010 ,  26207kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.17431\\rreplaced with revised version Wed, 5 Jul 2023 16:15:10 GMT   (32782kb,D)\\r\\rTitle: Defense against Adversarial Cloud Attack on Remote Sensing Salient\\r  Object Detection\\rAuthors: Huiming Sun, Lan Fu, Jinlong Li, Qing Guo, Zibo Meng, Tianyun Zhang,\\r  Yuewei Lin, Hongkai Yu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.17431 ,  32782kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.17643\\rreplaced with revised version Tue, 4 Jul 2023 03:54:44 GMT   (25837kb,D)\\r\\rTitle: Neural 3D Scene Reconstruction from Multiple 2D Images without 3D\\r  Supervision\\rAuthors: Yi Guo, Che Sun, Yunde Jia, and Yuwei Wu\\rCategories: cs.CV\\rComments: 10 pages, 6 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2306.17643 ,  25837kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.00397\\rreplaced with revised version Tue, 4 Jul 2023 06:23:36 GMT   (3797kb,D)\\r\\rTitle: Improving CNN-based Person Re-identification using score Normalization\\rAuthors: Ammar Chouchane, Abdelmalik Ouamane, Yassine Himeur, Wathiq Mansoor,\\r  Shadi Atalla, Afaf Benzaibak and Chahrazed Boudellal\\rCategories: cs.CV\\rComments: 5 pages, 6 figures and 2 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2307.00397 ,  3797kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.00804\\rreplaced with revised version Tue, 4 Jul 2023 12:21:18 GMT   (42731kb,D)\\r\\rTitle: SketchMetaFace: A Learning-based Sketching Interface for High-fidelity\\r  3D Character Face Modeling\\rAuthors: Zhongjin Luo, Dong Du, Heming Zhu, Yizhou Yu, Hongbo Fu, Xiaoguang Han\\rCategories: cs.CV cs.GR cs.HC\\r\\\\\\\\ ( https://arxiv.org/abs/2307.00804 ,  42731kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2307.01146\\rreplaced with revised version Wed, 5 Jul 2023 04:19:18 GMT   (19160kb,D)\\r\\rTitle: AVSegFormer: Audio-Visual Segmentation with Transformer\\rAuthors: Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, Tong Lu\\rCategories: cs.CV cs.LG cs.SD eess.AS\\rComments: 9 pages, 7 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2307.01146 ,  19160kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2011.14956\\rreplaced with revised version Wed, 5 Jul 2023 00:35:33 GMT   (6526kb)\\r\\rTitle: Handling Noisy Labels via One-Step Abductive Multi-Target Learning and\\r  Its Application to Helicobacter Pylori Segmentation\\rAuthors: Yongquan Yang, Yiming Yang, Jie Chen, Jiayi Zheng, Zhongxi Zheng\\rCategories: cs.LG cs.AI cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2011.14956 ,  6526kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.12926\\rreplaced with revised version Wed, 5 Jul 2023 17:59:33 GMT   (2886kb,D)\\r\\rTitle: Compositionality as Lexical Symmetry\\rAuthors: Ekin Aky\\\\urek and Jacob Andreas\\rCategories: cs.CL cs.CV cs.LG\\rComments: ACL2023 Final Version\\r\\\\\\\\ ( https://arxiv.org/abs/2201.12926 ,  2886kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2210.09643\\rreplaced with revised version Tue, 4 Jul 2023 12:08:26 GMT   (14153kb,D)\\r\\rTitle: Improving Adversarial Robustness by Contrastive Guided Diffusion Process\\rAuthors: Yidong Ouyang, Liyan Xie, Guang Cheng\\rCategories: cs.LG cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2210.09643 ,  14153kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2211.06757 (*cross-listing*)\\rreplaced with revised version Mon, 3 Jul 2023 18:24:44 GMT   (7952kb,D)\\r\\rTitle: DriftRec: Adapting diffusion models to blind JPEG restoration\\rAuthors: Simon Welker, Henry N. Chapman, Timo Gerkmann\\rCategories: eess.IV cs.CV cs.LG\\rComments: This work has been submitted to the IEEE for possible publication.\\r  Copyright may be transferred without notice, after which this version may no\\r  longer be accessible\\r\\\\\\\\ ( https://arxiv.org/abs/2211.06757 ,  7952kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2212.10549\\rreplaced with revised version Tue, 4 Jul 2023 13:18:29 GMT   (10182kb,D)\\r\\rTitle: Cross-modal Attention Congruence Regularization for Vision-Language\\r  Relation Alignment\\rAuthors: Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov,\\r  Louis-Philippe Morency\\rCategories: cs.CL cs.CV cs.LG\\rComments: ACL 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2212.10549 ,  10182kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2303.08325\\rreplaced with revised version Tue, 4 Jul 2023 05:17:09 GMT   (263kb,D)\\r\\rTitle: FairAdaBN: Mitigating unfairness with adaptive batch normalization and\\r  its application to dermatological disease classification\\rAuthors: Zikang Xu, Shang Zhao, Quan Quan, Qingsong Yao, and S. Kevin Zhou\\rCategories: cs.LG cs.CV\\rComments: Accepted by MICCAI 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2303.08325 ,  263kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.01992 (*cross-listing*)\\rreplaced with revised version Tue, 4 Jul 2023 06:51:49 GMT   (1397kb,D)\\r\\rTitle: Cross-modulated Few-shot Image Generation for Colorectal Tissue\\r  Classification\\rAuthors: Amandeep Kumar, Ankan kumar Bhunia, Sanath Narayan, Hisham Cholakkal,\\r  Rao Muhammad Anwer, Jorma Laaksonen and Fahad Shahbaz Khan\\rCategories: eess.IV cs.CV\\rComments: Early Accept in MICCAI 2023\\r\\\\\\\\ ( https://arxiv.org/abs/2304.01992 ,  1397kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2304.04106 (*cross-listing*)\\rreplaced with revised version Tue, 4 Jul 2023 23:06:50 GMT   (12685kb,D)\\r\\rTitle: MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask\\r  Generation\\rAuthors: Kun Han, Yifeng Xiong, Chenyu You, Pooya Khosravi, Shanlin Sun,\\r  Xiangyi Yan, James Duncan, Xiaohui Xie\\rCategories: eess.IV cs.CV\\rComments: Accepted by MICCAI 2023. Project Page:\\r  https://krishan999.github.io/MedGen3D/\\r\\\\\\\\ ( https://arxiv.org/abs/2304.04106 ,  12685kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.00417\\rreplaced with revised version Wed, 5 Jul 2023 05:50:38 GMT   (0kb,I)\\r\\rTitle: Transformer-based Sequence Labeling for Audio Classification based on\\r  MFCCs\\rAuthors: C. S. Sonali, Chinmayi B S, Ahana Balasubramanian\\rCategories: cs.SD cs.CV eess.AS\\rComments: Error in the explanation as well inadequate results and conclusion\\r\\\\\\\\ ( https://arxiv.org/abs/2305.00417 ,  0kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.07611\\rreplaced with revised version Mon, 3 Jul 2023 19:05:30 GMT   (1046kb,D)\\r\\rTitle: Multimodal Sentiment Analysis: A Survey\\rAuthors: Songning Lai, Xifeng Hu, Haoxuan Xu, Zhaoxia Ren and Zhi Liu\\rCategories: cs.CL cs.CV\\rComments: It needs to be returned for major modifications\\r\\\\\\\\ ( https://arxiv.org/abs/2305.07611 ,  1046kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.16656 (*cross-listing*)\\rreplaced with revised version Wed, 5 Jul 2023 06:06:14 GMT   (1215kb)\\r\\rTitle: Clustering Method for Time-Series Images Using Quantum-Inspired\\r  Computing Technology\\rAuthors: Tomoki Inoue, Koyo Kubota, Tsubasa Ikami, Yasuhiro Egami, Hiroki\\r  Nagai, Takahiro Kashikawa, Koichi Kimura, Yu Matsuda\\rCategories: eess.SP cs.CV physics.flu-dyn\\rComments: 13 pages, 4 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2305.16656 ,  1215kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2305.20030\\rreplaced with revised version Tue, 4 Jul 2023 03:52:06 GMT   (17996kb,D)\\r\\rTitle: Tree-Ring Watermarks: Fingerprints for Diffusion Images that are\\r  Invisible and Robust\\rAuthors: Yuxin Wen, John Kirchenbauer, Jonas Geiping, Tom Goldstein\\rCategories: cs.LG cs.CR cs.CV\\rComments: 16 pages, 8 figures, code is available at\\r  https://github.com/YuxinWenRick/tree-ring-watermark, fixed the repo link\\r\\\\\\\\ ( https://arxiv.org/abs/2305.20030 ,  17996kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.02176 (*cross-listing*)\\rreplaced with revised version Wed, 5 Jul 2023 15:29:55 GMT   (510kb,D)\\r\\rTitle: TransRUPNet for Improved Out-of-Distribution Generalization in Polyp\\r  Segmentation\\rAuthors: Debesh Jha, Nikhil Kumar Tomar, Debayan Bhattacharya, Ulas Bagci\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2306.02176 ,  510kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.11730 (*cross-listing*)\\rreplaced with revised version Tue, 4 Jul 2023 18:56:15 GMT   (25608kb,D)\\r\\rTitle: Segment Anything Model (SAM) for Radiation Oncology\\rAuthors: Lian Zhang, Zhengliang Liu, Lu Zhang, Zihao Wu, Xiaowei Yu, Jason\\r  Holmes, Hongying Feng, Haixing Dai, Xiang Li, Quanzheng Li, Dajiang Zhu,\\r  Tianming Liu, Wei Liu\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2306.11730 ,  25608kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.13695 (*cross-listing*)\\rreplaced with revised version Wed, 5 Jul 2023 17:03:43 GMT   (21051kb,D)\\r\\rTitle: Phase Unwrapping of Color Doppler Echocardiography using Deep Learning\\rAuthors: Hang Jung Ling, Olivier Bernard, Nicolas Ducros, Damien Garcia\\rCategories: eess.IV cs.AI cs.CV cs.LG\\rComments: 11 pages, accepted for publication in IEEE TUFFC, modified graphical\\r  abstract\\rDOI: 10.1109/TUFFC.2023.3289621\\r\\\\\\\\ ( https://arxiv.org/abs/2306.13695 ,  21051kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2306.17059\\rreplaced with revised version Mon, 3 Jul 2023 19:38:37 GMT   (6315kb,D)\\r\\rTitle: The mapKurator System: A Complete Pipeline for Extracting and Linking\\r  Text from Historical Maps\\rAuthors: Jina Kim, Zekun Li, Yijun Lin, Min Namgung, Leeje Jang, Yao-Yi Chiang\\rCategories: cs.AI cs.CL cs.CV\\rComments: 4 pages, 4 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2306.17059 ,  6315kb)\\r%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%--- ------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\rSend any comments regarding submissions directly to submitter.\\r------------------------------------------------------------------------------\\rArchives at http://arxiv.org/\\rTo unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel\\r------------------------------------------------------------------------------\\r Submissions to:\\rComputer Vision and Pattern Recognition\\r received from  Fri 11 Feb 22 19:00:00 GMT  to  Mon 14 Feb 22 19:00:00 GMT\\r------------------------------------------------------------------------------\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05863\\rDate: Fri, 11 Feb 2022 19:11:16 GMT   (8951kb,D)\\r\\rTitle: Motion Correction and Volumetric Reconstruction for Fetal Functional\\r  Magnetic Resonance Imaging Data\\rAuthors: Daniel Sobotka, Michael Ebner, Ernst Schwartz, Karl-Heinz Nenning,\\r  Athena Taymourtash, Tom Vercauteren, Sebastien Ourselin, Gregor Kasprian,\\r  Daniela Prayer, Georg Langs, Roxane Licandro\\rCategories: cs.CV\\rComments: Preprint submitted to NeuroImage\\r\\\\\\\\\\r  Motion correction is an essential preprocessing step in functional Magnetic\\rResonance Imaging (fMRI) of the fetal brain with the aim to remove artifacts\\rcaused by fetal movement and maternal breathing and consequently to suppress\\rerroneous signal correlations. Current motion correction approaches for fetal\\rfMRI choose a single 3D volume from a specific acquisition timepoint with least\\rmotion artefacts as reference volume, and perform interpolation for the\\rreconstruction of the motion corrected time series. The results can suffer, if\\rno low-motion frame is available, and if reconstruction does not exploit any\\rassumptions about the continuity of the fMRI signal. Here, we propose a novel\\rframework, which estimates a high-resolution reference volume by using\\routlier-robust motion correction, and by utilizing Huber L2 regularization for\\rintra-stack volumetric reconstruction of the motion-corrected fetal brain fMRI.\\rWe performed an extensive parameter study to investigate the effectiveness of\\rmotion estimation and present in this work benchmark metrics to quantify the\\reffect of motion correction and regularised volumetric reconstruction\\rapproaches on functional connectivity computations. We demonstrate the proposed\\rframework's ability to improve functional connectivity estimates,\\rreproducibility and signal interpretability, which is clinically highly\\rdesirable for the establishment of prognostic noninvasive imaging biomarkers.\\rThe motion correction and volumetric reconstruction framework is made available\\ras an open-source package of NiftyMIC.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05863 ,  8951kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05910\\rDate: Fri, 11 Feb 2022 21:26:17 GMT   (16688kb,D)\\r\\rTitle: Multi-level Latent Space Structuring for Generative Control\\rAuthors: Oren Katzir, Vicky Perepelook, Dani Lischinski and Daniel Cohen-Or\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Truncation is widely used in generative models for improving the quality of\\rthe generated samples, at the expense of reducing their diversity. We propose\\rto leverage the StyleGAN generative architecture to devise a new truncation\\rtechnique, based on a decomposition of the latent space into clusters, enabling\\rcustomized truncation to be performed at multiple semantic levels. We do so by\\rlearning to re-generate W-space, the extended intermediate latent space of\\rStyleGAN, using a learnable mixture of Gaussians, while simultaneously training\\ra classifier to identify, for each latent vector, the cluster that it belongs\\rto. The resulting truncation scheme is more faithful to the original\\runtruncated samples and allows a better trade-off between quality and\\rdiversity. We compare our method to other truncation approaches for StyleGAN,\\rboth qualitatively and quantitatively.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05910 ,  16688kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05922\\rDate: Fri, 11 Feb 2022 22:34:15 GMT   (17459kb,D)\\r\\rTitle: Deep Signatures -- Learning Invariants of Planar Curves\\rAuthors: Roy Velich, Ron Kimmel\\rCategories: cs.CV\\r\\\\\\\\\\r  We propose a learning paradigm for numerical approximation of differential\\rinvariants of planar curves. Deep neural-networks' (DNNs) universal\\rapproximation properties are utilized to estimate geometric measures. The\\rproposed framework is shown to be a preferable alternative to axiomatic\\rconstructions. Specifically, we show that DNNs can learn to overcome\\rinstabilities and sampling artifacts and produce numerically-stable signatures\\rfor curves subject to a given group of transformations in the plane. We compare\\rthe proposed schemes to alternative state-of-the-art axiomatic constructions of\\rgroup invariant arc-lengths and curvatures.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05922 ,  17459kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05930\\rDate: Fri, 11 Feb 2022 23:15:01 GMT   (12711kb,D)\\r\\rTitle: Detecting out-of-context objects using contextual cues\\rAuthors: Manoj Acharya, Anirban Roy, Kaushik Koneripalli, Susmit Jha,\\r  Christopher Kanan, Ajay Divakaran\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  This paper presents an approach to detect out-of-context (OOC) objects in an\\rimage. Given an image with a set of objects, our goal is to determine if an\\robject is inconsistent with the scene context and detect the OOC object with a\\rbounding box. In this work, we consider commonly explored contextual relations\\rsuch as co-occurrence relations, the relative size of an object with respect to\\rother objects, and the position of the object in the scene. We posit that\\rcontextual cues are useful to determine object labels for in-context objects\\rand inconsistent context cues are detrimental to determining object labels for\\rout-of-context objects. To realize this hypothesis, we propose a graph\\rcontextual reasoning network (GCRN) to detect OOC objects. GCRN consists of two\\rseparate graphs to predict object labels based on the contextual cues in the\\rimage: 1) a representation graph to learn object features based on the\\rneighboring objects and 2) a context graph to explicitly capture contextual\\rcues from the neighboring objects. GCRN explicitly captures the contextual cues\\rto improve the detection of in-context objects and identify objects that\\rviolate contextual relations. In order to evaluate our approach, we create a\\rlarge-scale dataset by adding OOC object instances to the COCO images. We also\\revaluate on recent OCD benchmark. Our results show that GCRN outperforms\\rcompetitive baselines in detecting OOC objects and correctly detecting\\rin-context objects.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05930 ,  12711kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05941\\rDate: Sat, 12 Feb 2022 00:21:27 GMT   (1112kb,D)\\r\\rTitle: Domain-Invariant Proposals based on a Balanced Domain Classifier for\\r  Object Detection\\rAuthors: Zhize Wu, Xiaofeng Wang, Tong Xu, Xuebin Yang, Le Zou, Lixiang Xu and\\r  Thomas Weise\\rCategories: cs.CV\\r\\\\\\\\\\r  Object recognition from images means to automatically find object(s) of\\rinterest and to return their category and location information. Benefiting from\\rresearch on deep learning, like convolutional neural networks~(CNNs) and\\rgenerative adversarial networks, the performance in this field has been\\rimproved significantly, especially when training and test data are drawn from\\rsimilar distributions. However, mismatching distributions, i.e., domain shifts,\\rlead to a significant performance drop. In this paper, we build\\rdomain-invariant detectors by learning domain classifiers via adversarial\\rtraining. Based on the previous works that align image and instance level\\rfeatures, we mitigate the domain shift further by introducing a domain\\radaptation component at the region level within Faster \\\\mbox{R-CNN}. We embed a\\rdomain classification network in the region proposal network~(RPN) using\\radversarial learning. The RPN can now generate accurate region proposals in\\rdifferent domains by effectively aligning the features between them. To\\rmitigate the unstable convergence during the adversarial learning, we introduce\\ra balanced domain classifier as well as a network learning rate adjustment\\rstrategy. We conduct comprehensive experiments using four standard datasets.\\rThe results demonstrate the effectiveness and robustness of our object\\rdetection approach in domain shift scenarios.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05941 ,  1112kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05953\\rDate: Sat, 12 Feb 2022 02:13:55 GMT   (3074kb,D)\\r\\rTitle: Open-set Adversarial Defense with Clean-Adversarial Mutual Learning\\rAuthors: Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel\\rCategories: cs.CV\\rComments: Accepted by International Journal of Computer Vision (IJCV) 2022.\\r  Code will be available at https://github.com/rshaojimmy/ECCV2020-OSAD. arXiv\\r  admin note: text overlap with arXiv:2009.00814\\r\\\\\\\\\\r  Open-set recognition and adversarial defense study two key aspects of deep\\rlearning that are vital for real-world deployment. The objective of open-set\\rrecognition is to identify samples from open-set classes during testing, while\\radversarial defense aims to robustify the network against images perturbed by\\rimperceptible adversarial noise. This paper demonstrates that open-set\\rrecognition systems are vulnerable to adversarial samples. Furthermore, this\\rpaper shows that adversarial defense mechanisms trained on known classes are\\runable to generalize well to open-set samples. Motivated by these observations,\\rwe emphasize the necessity of an Open-Set Adversarial Defense (OSAD) mechanism.\\rThis paper proposes an Open-Set Defense Network with Clean-Adversarial Mutual\\rLearning (OSDN-CAML) as a solution to the OSAD problem. The proposed network\\rdesigns an encoder with dual-attentive feature-denoising layers coupled with a\\rclassifier to learn a noise-free latent feature representation, which\\radaptively removes adversarial noise guided by channel and spatial-wise\\rattentive filters. Several techniques are exploited to learn a noise-free and\\rinformative latent feature space with the aim of improving the performance of\\radversarial defense and open-set recognition. First, we incorporate a decoder\\rto ensure that clean images can be well reconstructed from the obtained latent\\rfeatures. Then, self-supervision is used to ensure that the latent features are\\rinformative enough to carry out an auxiliary task. Finally, to exploit more\\rcomplementary knowledge from clean image classification to facilitate feature\\rdenoising and search for a more generalized local minimum for open-set\\rrecognition, we further propose clean-adversarial mutual learning, where a peer\\rnetwork (classifying clean images) is further introduced to mutually learn with\\rthe classifier (classifying adversarial images).\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05953 ,  3074kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05961\\rDate: Sat, 12 Feb 2022 02:56:22 GMT   (18600kb,D)\\r\\rTitle: Audio-Visual Fusion Layers for Event Type Aware Video Recognition\\rAuthors: Arda Senocak, Junsik Kim, Tae-Hyun Oh, Hyeonggon Ryu, Dingzeyu Li, In\\r  So Kweon\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  Human brain is continuously inundated with the multisensory information and\\rtheir complex interactions coming from the outside world at any given moment.\\rSuch information is automatically analyzed by binding or segregating in our\\rbrain. While this task might seem effortless for human brains, it is extremely\\rchallenging to build a machine that can perform similar tasks since complex\\rinteractions cannot be dealt with single type of integration but requires more\\rsophisticated approaches. In this paper, we propose a new model to address the\\rmultisensory integration problem with individual event-specific layers in a\\rmulti-task learning scheme. Unlike previous works where single type of fusion\\ris used, we design event-specific layers to deal with different audio-visual\\rrelationship tasks, enabling different ways of audio-visual formation.\\rExperimental results show that our event-specific layers can discover unique\\rproperties of the audio-visual relationships in the videos. Moreover, although\\rour network is formulated with single labels, it can output additional true\\rmulti-labels to represent the given videos. We demonstrate that our proposed\\rframework also exposes the modality bias of the video data category-wise and\\rdataset-wise manner in popular benchmark datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05961 ,  18600kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05972\\rDate: Sat, 12 Feb 2022 03:59:38 GMT   (16422kb,D)\\r\\rTitle: Low-light Image Enhancement by Retinex Based Algorithm Unrolling and\\r  Adjustment\\rAuthors: Xinyi Liu and Qi Xie and Qian Zhao and Hong Qang and Deyu Meng\\rCategories: cs.CV eess.IV\\r\\\\\\\\\\r  Motivated by their recent advances, deep learning techniques have been widely\\rapplied to low-light image enhancement (LIE) problem. Among which, Retinex\\rtheory based ones, mostly following a decomposition-adjustment pipeline, have\\rtaken an important place due to its physical interpretation and promising\\rperformance. However, current investigations on Retinex based deep learning are\\rstill not sufficient, ignoring many useful experiences from traditional\\rmethods. Besides, the adjustment step is either performed with simple image\\rprocessing techniques, or by complicated networks, both of which are\\runsatisfactory in practice. To address these issues, we propose a new deep\\rlearning framework for the LIE problem. The proposed framework contains a\\rdecomposition network inspired by algorithm unrolling, and adjustment networks\\rconsidering both global brightness and local brightness sensitivity. By virtue\\rof algorithm unrolling, both implicit priors learned from data and explicit\\rpriors borrowed from traditional methods can be embedded in the network,\\rfacilitate to better decomposition. Meanwhile, the consideration of global and\\rlocal brightness can guide designing simple yet effective network modules for\\radjustment. Besides, to avoid manually parameter tuning, we also propose a\\rself-supervised fine-tuning strategy, which can always guarantee a promising\\rperformance. Experiments on a series of typical LIE datasets demonstrated the\\reffectiveness of the proposed method, both quantitatively and visually, as\\rcompared with existing methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05972 ,  16422kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05988\\rDate: Sat, 12 Feb 2022 05:19:37 GMT   (2346kb,D)\\r\\rTitle: RSINet: Inpainting Remotely Sensed Images Using Triple GAN Framework\\rAuthors: Advait Kumar, Dipesh Tamboli, Shivam Pande, Biplab Banerjee\\rCategories: cs.CV cs.LG eess.IV\\r\\\\\\\\\\r  We tackle the problem of image inpainting in the remote sensing domain.\\rRemote sensing images possess high resolution and geographical variations, that\\rrender the conventional inpainting methods less effective. This further entails\\rthe requirement of models with high complexity to sufficiently capture the\\rspectral, spatial and textural nuances within an image, emerging from its high\\rspatial variability. To this end, we propose a novel inpainting method that\\rindividually focuses on each aspect of an image such as edges, colour and\\rtexture using a task specific GAN. Moreover, each individual GAN also\\rincorporates the attention mechanism that explicitly extracts the spectral and\\rspatial features. To ensure consistent gradient flow, the model uses residual\\rlearning paradigm, thus simultaneously working with high and low level\\rfeatures. We evaluate our model, alongwith previous state of the art models, on\\rthe two well known remote sensing datasets, Open Cities AI and Earth on Canvas,\\rand achieve competitive performance.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05988 ,  2346kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06014\\rDate: Sat, 12 Feb 2022 08:22:47 GMT   (2202kb,D)\\r\\rTitle: Multi-direction and Multi-scale Pyramid in Transformer for Video-based\\r  Pedestrian Retrieval\\rAuthors: Xianghao Zang, Ge Li, Wei Gao\\rCategories: cs.CV\\rComments: 10 pages, 6 figures, Accepted for publication in IEEE Transactions on\\r  Industrial Informatics\\r\\\\\\\\\\r  In video surveillance, pedestrian retrieval (also called person\\rre-identification) is a critical task. This task aims to retrieve the\\rpedestrian of interest from non-overlapping cameras. Recently,\\rtransformer-based models have achieved significant progress for this task.\\rHowever, these models still suffer from ignoring fine-grained, part-informed\\rinformation. This paper proposes a multi-direction and multi-scale Pyramid in\\rTransformer (PiT) to solve this problem. In transformer-based architecture,\\reach pedestrian image is split into many patches. Then, these patches are fed\\rto transformer layers to obtain the feature representation of this image. To\\rexplore the fine-grained information, this paper proposes to apply vertical\\rdivision and horizontal division on these patches to generate\\rdifferent-direction human parts. These parts provide more fine-grained\\rinformation. To fuse multi-scale feature representation, this paper presents a\\rpyramid structure containing global-level information and many pieces of\\rlocal-level information from different scales. The feature pyramids of all the\\rpedestrian images from the same video are fused to form the final\\rmulti-direction and multi-scale feature representation. Experimental results on\\rtwo challenging video-based benchmarks, MARS and iLIDS-VID, show the proposed\\rPiT achieves state-of-the-art performance. Extensive ablation studies\\rdemonstrate the superiority of the proposed pyramid structure. The code is\\ravailable at https://git.openi.org.cn/zangxh/PiT.git.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06014 ,  2202kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06022\\rDate: Sat, 12 Feb 2022 09:12:31 GMT   (28091kb,D)\\r\\rTitle: Fun Selfie Filters in Face Recognition: Impact Assessment and Removal\\rAuthors: Cristian Botezatu, Mathias Ibsen, Christian Rathgeb, Christoph Busch\\rCategories: cs.CV\\r\\\\\\\\\\r  This work investigates the impact of fun selfie filters, which are frequently\\rused to modify selfies, on face recognition systems. Based on a qualitative\\rassessment and classification of freely available mobile applications, ten\\rrelevant fun selfie filters are selected to create a database. To this end, the\\rselected filters are automatically applied to face images of public face image\\rdatabases. Different state-of-the-art methods are used to evaluate the\\rinfluence of fun selfie filters on the performance of face detection using\\rdlib, RetinaFace, and a COTS method, sample quality estimated by FaceQNet and\\rMagFace, and recognition accuracy employing ArcFace and a COTS algorithm. The\\robtained results indicate that selfie filters negatively affect face\\rrecognition modules, especially if fun selfie filters cover a large region of\\rthe face, where the mouth, nose, and eyes are covered. To mitigate such\\runwanted effects, a GAN-based selfie filter removal algorithm is proposed which\\rconsists of a segmentation module, a perceptual network, and a generation\\rmodule. In a cross-database experiment the application of the presented selfie\\rfilter removal technique has shown to significantly improve the biometric\\rperformance of the underlying face recognition systems.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06022 ,  28091kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06028\\rDate: Sat, 12 Feb 2022 10:06:12 GMT   (6361kb,D)\\r\\rTitle: OctAttention: Octree-based Large-scale Contexts Model for Point Cloud\\r  Compression\\rAuthors: Chunyang Fu, Ge Li, Rui Song, Wei Gao, Shan Liu\\rCategories: cs.CV\\rComments: Accepted by AAAI 2022\\r\\\\\\\\\\r  In point cloud compression, sufficient contexts are significant for modeling\\rthe point cloud distribution. However, the contexts gathered by the previous\\rvoxel-based methods decrease when handling sparse point clouds. To address this\\rproblem, we propose a multiple-contexts deep learning framework called\\rOctAttention employing the octree structure, a memory-efficient representation\\rfor point clouds. Our approach encodes octree symbol sequences in a lossless\\rway by gathering the information of sibling and ancestor nodes. Expressly, we\\rfirst represent point clouds with octree to reduce spatial redundancy, which is\\rrobust for point clouds with different resolutions. We then design a\\rconditional entropy model with a large receptive field that models the sibling\\rand ancestor contexts to exploit the strong dependency among the neighboring\\rnodes and employ an attention mechanism to emphasize the correlated nodes in\\rthe context. Furthermore, we introduce a mask operation during training and\\rtesting to make a trade-off between encoding time and performance. Compared to\\rthe previous state-of-the-art works, our approach obtains a 10%-35% BD-Rate\\rgain on the LiDAR benchmark (e.g. SemanticKITTI) and object point cloud dataset\\r(e.g. MPEG 8i, MVUB), and saves 95% coding time compared to the voxel-based\\rbaseline. The code is available at https://github.com/zb12138/OctAttention.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06028 ,  6361kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06060\\rDate: Sat, 12 Feb 2022 13:04:16 GMT   (644kb,D)\\r\\rTitle: Depth-Cooperated Trimodal Network for Video Salient Object Detection\\rAuthors: Yukang Lu, Dingyao Min, Keren Fu, Qijun Zhao\\rCategories: cs.CV\\r\\\\\\\\\\r  Depth can provide useful geographical cues for salient object detection\\r(SOD), and has been proven helpful in recent RGB-D SOD methods. However,\\rexisting video salient object detection (VSOD) methods only utilize\\rspatiotemporal information and seldom exploit depth information for detection.\\rIn this paper, we propose a depth-cooperated trimodal network, called DCTNet\\rfor VSOD, which is a pioneering work to incorporate depth information to assist\\rVSOD. To this end, we first generate depth from RGB frames, and then propose an\\rapproach to treat the three modalities unequally. Specifically, a multi-modal\\rattention module (MAM) is designed to model multi-modal long-range dependencies\\rbetween the main modality (RGB) and the two auxiliary modalities (depth,\\roptical flow). We also introduce a refinement fusion module (RFM) to suppress\\rnoises in each modality and select useful information dynamically for further\\rfeature refinement. Lastly, a progressive fusion strategy is adopted after the\\rrefined features to achieve final cross-modal fusion. Experiments on five\\rbenchmark datasets demonstrate the superiority of our depth-cooperated model\\ragainst 12 state-of-the-art methods, and the necessity of depth is also\\rvalidated.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06060 ,  644kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06076\\rDate: Sat, 12 Feb 2022 14:23:30 GMT   (1260kb,D)\\r\\rTitle: Indication as Prior Knowledge for Multimodal Disease Classification in\\r  Chest Radiographs with Transformers\\rAuthors: Grzegorz Jacenk\\\\'ow, Alison Q. O'Neil, Sotirios A. Tsaftaris\\rCategories: cs.CV cs.LG\\rComments: Accepted at the IEEE International Symposium on Biomedical Imaging\\r  (ISBI) 2022 as an oral presentation\\r\\\\\\\\\\r  When a clinician refers a patient for an imaging exam, they include the\\rreason (e.g. relevant patient history, suspected disease) in the scan request;\\rthis appears as the indication field in the radiology report. The\\rinterpretation and reporting of the image are substantially influenced by this\\rrequest text, steering the radiologist to focus on particular aspects of the\\rimage. We use the indication field to drive better image classification, by\\rtaking a transformer network which is unimodally pre-trained on text (BERT) and\\rfine-tuning it for multimodal classification of a dual image-text input. We\\revaluate the method on the MIMIC-CXR dataset, and present ablation studies to\\rinvestigate the effect of the indication field on the classification\\rperformance. The experimental results show our approach achieves 87.8 average\\rmicro AUROC, outperforming the state-of-the-art methods for unimodal (84.4) and\\rmultimodal (86.0) classification. Our code is available at\\rhttps://github.com/jacenkow/mmbt.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06076 ,  1260kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06079\\rDate: Sat, 12 Feb 2022 14:37:29 GMT   (2398kb,D)\\r\\rTitle: Text and Image Guided 3D Avatar Generation and Manipulation\\rAuthors: Zehranaz Canfes, M. Furkan Atasoy, Alara Dirik, Pinar Yanardag\\rCategories: cs.CV cs.GR cs.LG\\r\\\\\\\\\\r  The manipulation of latent space has recently become an interesting topic in\\rthe field of generative models. Recent research shows that latent directions\\rcan be used to manipulate images towards certain attributes. However,\\rcontrolling the generation process of 3D generative models remains a challenge.\\rIn this work, we propose a novel 3D manipulation method that can manipulate\\rboth the shape and texture of the model using text or image-based prompts such\\ras 'a young face' or 'a surprised face'. We leverage the power of Contrastive\\rLanguage-Image Pre-training (CLIP) model and a pre-trained 3D GAN model\\rdesigned to generate face avatars, and create a fully differentiable rendering\\rpipeline to manipulate meshes. More specifically, our method takes an input\\rlatent code and modifies it such that the target attribute specified by a text\\ror image prompt is present or enhanced, while leaving other attributes largely\\runaffected. Our method requires only 5 minutes per manipulation, and we\\rdemonstrate the effectiveness of our approach with extensive results and\\rcomparisons.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06079 ,  2398kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06080\\rDate: Sat, 12 Feb 2022 14:47:44 GMT   (1493kb,D)\\r\\rTitle: Recognition-free Question Answering on Handwritten Document Collections\\rAuthors: Oliver T\\\\uselmann, Friedrich M\\\\uller, Fabian Wolf and Gernot A. Fink\\rCategories: cs.CV\\r\\\\\\\\\\r  In recent years, considerable progress has been made in the research area of\\rQuestion Answering (QA) on document images. Current QA approaches from the\\rDocument Image Analysis community are mainly focusing on machine-printed\\rdocuments and perform rather limited on handwriting. This is mainly due to the\\rreduced recognition performance on handwritten documents. To tackle this\\rproblem, we propose a recognition-free QA approach, especially designed for\\rhandwritten document image collections. We present a robust document retrieval\\rmethod, as well as two QA models. Our approaches outperform the\\rstate-of-the-art recognition-free models on the challenging BenthamQA and\\rHW-SQuAD datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06080 ,  1493kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06088\\rDate: Sat, 12 Feb 2022 15:23:16 GMT   (46032kb,D)\\r\\rTitle: NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing\\rAuthors: Jiakai Zhang, Liao Wang, Xinhang Liu, Fuqiang Zhao, Minzhang Li,\\r  Haizhao Dai, Boyuan Zhang, Wei Yang, Lan Xu and Jingyi Yu\\rCategories: cs.CV cs.GR\\r\\\\\\\\\\r  Some of the most exciting experiences that Metaverse promises to offer, for\\rinstance, live interactions with virtual characters in virtual environments,\\rrequire real-time photo-realistic rendering. 3D reconstruction approaches to\\rrendering, active or passive, still require extensive cleanup work to fix the\\rmeshes or point clouds. In this paper, we present a neural volumography\\rtechnique called neural volumetric video or NeuVV to support immersive,\\rinteractive, and spatial-temporal rendering of volumetric video contents with\\rphoto-realism and in real-time. The core of NeuVV is to efficiently encode a\\rdynamic neural radiance field (NeRF) into renderable and editable primitives.\\rWe introduce two types of factorization schemes: a hyper-spherical harmonics\\r(HH) decomposition for modeling smooth color variations over space and time and\\ra learnable basis representation for modeling abrupt density and color changes\\rcaused by motion. NeuVV factorization can be integrated into a Video Octree\\r(VOctree) analogous to PlenOctree to significantly accelerate training while\\rreducing memory overhead. Real-time NeuVV rendering further enables a class of\\rimmersive content editing tools. Specifically, NeuVV treats each VOctree as a\\rprimitive and implements volume-based depth ordering and alpha blending to\\rrealize spatial-temporal compositions for content re-purposing. For example, we\\rdemonstrate positioning varied manifestations of the same performance at\\rdifferent 3D locations with different timing, adjusting color/texture of the\\rperformer's clothing, casting spotlight shadows and synthesizing distance\\rfalloff lighting, etc, all at an interactive speed. We further develop a hybrid\\rneural-rasterization rendering framework to support consumer-level VR headsets\\rso that the aforementioned volumetric video viewing and editing, for the first\\rtime, can be conducted immersively in virtual 3D space.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06088 ,  46032kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06095\\rDate: Sat, 12 Feb 2022 16:22:46 GMT   (2190kb,D)\\r\\rTitle: A Review of Deep Learning-based Approaches for Deepfake Content\\r  Detection\\rAuthors: Leandro A. Passos, Danilo Jodas, Kelton A. P. da Costa, Luis A. Souza\\r  J\\\\'unior, Danilo Colombo, Jo\\\\~ao Paulo Papa\\rCategories: cs.CV cs.AI\\r\\\\\\\\\\r  The fast-spreading information over the internet is essential to support the\\rrapid supply of numerous public utility services and entertainment to users.\\rSocial networks and online media paved the way for modern,\\rtimely-communication-fashion and convenient access to all types of information.\\rHowever, it also provides new chances for ill use of the massive amount of\\ravailable data, such as spreading fake content to manipulate public opinion.\\rDetection of counterfeit content has raised attention in the last few years for\\rthe advances in deepfake generation. The rapid growth of machine learning\\rtechniques, particularly deep learning, can predict fake content in several\\rapplication domains, including fake image and video manipulation. This paper\\rpresents a comprehensive review of recent studies for deepfake content\\rdetection using deep learning-based approaches. We aim to broaden the\\rstate-of-the-art research by systematically reviewing the different categories\\rof fake content detection. Furthermore, we report the advantages and drawbacks\\rof the examined works and future directions towards the issues and shortcomings\\rstill unsolved on deepfake detection.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06095 ,  2190kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06174\\rDate: Sun, 13 Feb 2022 01:19:41 GMT   (26008kb,D)\\r\\rTitle: Source-Free Progressive Graph Learning for Open-Set Domain Adaptation\\rAuthors: Yadan Luo, Zijian Wang, Zhuoxiao Chen, Zi Huang and Mahsa\\r  Baktashmotlagh\\rCategories: cs.CV\\rComments: arXiv admin note: substantial text overlap with arXiv:2006.12087\\r\\\\\\\\\\r  Open-set domain adaptation (OSDA) has gained considerable attention in many\\rvisual recognition tasks. However, most existing OSDA approaches are limited\\rdue to three main reasons, including: (1) the lack of essential theoretical\\ranalysis of generalization bound, (2) the reliance on the coexistence of source\\rand target data during adaptation, and (3) failing to accurately estimate the\\runcertainty of model predictions. We propose a Progressive Graph Learning (PGL)\\rframework that decomposes the target hypothesis space into the shared and\\runknown subspaces, and then progressively pseudo-labels the most confident\\rknown samples from the target domain for hypothesis adaptation. Moreover, we\\rtackle a more realistic source-free open-set domain adaptation (SF-OSDA)\\rsetting that makes no assumption about the coexistence of source and target\\rdomains, and introduce a balanced pseudo-labeling (BP-L) strategy in a\\rtwo-stage framework, namely SF-PGL. Different from PGL that applies a\\rclass-agnostic constant threshold for all target samples for pseudo-labeling,\\rthe SF-PGL model uniformly selects the most confident target instances from\\reach category at a fixed ratio. The confidence thresholds in each class are\\rregarded as the 'uncertainty' of learning the semantic information, which are\\rthen used to weigh the classification loss in the adaptation step. We conducted\\runsupervised and semi-supervised OSDA and SF-OSDA experiments on the benchmark\\rimage classification and action recognition datasets. Additionally, we find\\rthat balanced pseudo-labeling plays a significant role in improving\\rcalibration, which makes the trained model less prone to over-confident or\\runder-confident predictions on the target data. Source code is available at\\rhttps://github.com/Luoyadan/SF-PGL.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06174 ,  26008kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06198\\rDate: Sun, 13 Feb 2022 04:09:21 GMT   (3023kb,D)\\r\\rTitle: Lip movements information disentanglement for lip sync\\rAuthors: Chun Wang\\rCategories: cs.CV\\r\\\\\\\\\\r  The lip movements information is critical for many audio-visual tasks.\\rHowever, extracting lip movements information from videos is challenging, as it\\rcan be easily perturbed by factors like personal identities and head poses.\\rThis paper proposes utilizing the parametric 3D face model to disentangle lip\\rmovements information explicitly. Building on top of the recent 3D face\\rreconstruction advances, we firstly offer a method that can consistently\\rdisentangle expression information, where the lip movements information lies.\\rThen we demonstrate that once the influences of perturbing factors are\\ralleviated by synthesizing faces with the disentangled lip movements\\rinformation, the lip-sync task can be done better with much fewer data.\\rFinally, we show its effectiveness in the wild by testing it on an unseen\\rdataset for the active speaker detection task and achieving competitive\\rperformance.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06198 ,  3023kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06228\\rDate: Sun, 13 Feb 2022 06:53:39 GMT   (11839kb,D)\\r\\rTitle: Robust Deepfake On Unrestricted Media: Generation And Detection\\rAuthors: Trung-Nghia Le and Huy H Nguyen and Junichi Yamagishi and Isao Echizen\\rCategories: cs.CV\\rComments: This article will appear as one chapter for a new book called\\r  Frontiers in Fake Media Generation and Detection, edited by Mahdi Khosravy,\\r  Isao Echizen, and Noboru Babaguchi\\r\\\\\\\\\\r  Recent advances in deep learning have led to substantial improvements in\\rdeepfake generation, resulting in fake media with a more realistic appearance.\\rAlthough deepfake media have potential application in a wide range of areas and\\rare drawing much attention from both the academic and industrial communities,\\rit also leads to serious social and criminal concerns. This chapter explores\\rthe evolution of and challenges in deepfake generation and detection. It also\\rdiscusses possible ways to improve the robustness of deepfake detection for a\\rwide variety of media (e.g., in-the-wild images and videos). Finally, it\\rsuggests a focus for future fake media research.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06228 ,  11839kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06240\\rDate: Sun, 13 Feb 2022 07:39:48 GMT   (14394kb,D)\\r\\rTitle: FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations\\rAuthors: Cemre Karakas, Alara Dirik, Eylul Yalcinkaya, Pinar Yanardag\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Recent advances in generative adversarial networks have shown that it is\\rpossible to generate high-resolution and hyperrealistic images. However, the\\rimages produced by GANs are only as fair and representative as the datasets on\\rwhich they are trained. In this paper, we propose a method for directly\\rmodifying a pre-trained StyleGAN2 model that can be used to generate a balanced\\rset of images with respect to one (e.g., eyeglasses) or more attributes (e.g.,\\rgender and eyeglasses). Our method takes advantage of the style space of the\\rStyleGAN2 model to perform disentangled control of the target attributes to be\\rdebiased. Our method does not require training additional models and directly\\rdebiases the GAN model, paving the way for its use in various downstream\\rapplications. Our experiments show that our method successfully debiases the\\rGAN model within a few minutes without compromising the quality of the\\rgenerated images. To promote fair generative models, we share the code and\\rdebiased models at http://catlab-team.github.io/fairstyle.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06240 ,  14394kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06256\\rDate: Sun, 13 Feb 2022 08:39:49 GMT   (893kb,D)\\r\\rTitle: RandomSEMO: Normality Learning Of Moving Objects For Video Anomaly\\r  Detection\\rAuthors: Chaewon Park, Minhyeok Lee, MyeongAh Cho and Sangyoun Lee\\rCategories: cs.CV\\rComments: 4 pages, ICIP 2022 under review\\r\\\\\\\\\\r  Recent anomaly detection algorithms have shown powerful performance by\\radopting frame predicting autoencoders. However, these methods face two\\rchallenging circumstances. First, they are likely to be trained to be\\rexcessively powerful, generating even abnormal frames well, which leads to\\rfailure in detecting anomalies. Second, they are distracted by the large number\\rof objects captured in both foreground and background. To solve these problems,\\rwe propose a novel superpixel-based video data transformation technique named\\rRandom Superpixel Erasing on Moving Objects (RandomSEMO) and Moving Object Loss\\r(MOLoss), built on top of a simple lightweight autoencoder. RandomSEMO is\\rapplied to the moving object regions by randomly erasing their superpixels. It\\renforces the network to pay attention to the foreground objects and learn the\\rnormal features more effectively, rather than simply predicting the future\\rframe. Moreover, MOLoss urges the model to focus on learning normal objects\\rcaptured within RandomSEMO by amplifying the loss on the pixels near the moving\\robjects. The experimental results show that our model outperforms\\rstate-of-the-arts on three benchmarks.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06256 ,  893kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06263\\rDate: Sun, 13 Feb 2022 08:55:53 GMT   (4569kb,D)\\r\\rTitle: LighTN: Light-weight Transformer Network for Performance-overhead\\r  Tradeoff in Point Cloud Downsampling\\rAuthors: Xu Wang, Yi Jin, Yigang Cen, Tao Wang, Bowen Tang, Yidong Li\\rCategories: cs.CV\\r\\\\\\\\\\r  Compared with traditional task-irrelevant downsampling methods, task-oriented\\rneural networks have shown improved performance in point cloud downsampling\\rrange. Recently, Transformer family of networks has shown a more powerful\\rlearning capacity in visual tasks. However, Transformer-based architectures\\rpotentially consume too many resources which are usually worthless for low\\roverhead task networks in downsampling range. This paper proposes a novel\\rlight-weight Transformer network (LighTN) for task-oriented point cloud\\rdownsampling, as an end-to-end and plug-and-play solution. In LighTN, a\\rsingle-head self-correlation module is presented to extract refined global\\rcontextual features, where three projection matrices are simultaneously\\reliminated to save resource overhead, and the output of symmetric matrix\\rsatisfies the permutation invariant. Then, we design a novel downsampling loss\\rfunction to guide LighTN focuses on critical point cloud regions with more\\runiform distribution and prominent points coverage. Furthermore, We introduce a\\rfeed-forward network scaling mechanism to enhance the learnable capacity of\\rLighTN according to the expand-reduce strategy. The result of extensive\\rexperiments on classification and registration tasks demonstrates LighTN can\\rachieve state-of-the-art performance with limited resource overhead.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06263 ,  4569kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06266\\rDate: Sun, 13 Feb 2022 09:14:52 GMT   (9675kb,D)\\r\\rTitle: Improve Deep Image Inpainting by Emphasizing the Complexity of Missing\\r  Regions\\rAuthors: Yufeng Wang, Dan Li, Cong Xu and Min Yang\\rCategories: cs.CV\\rComments: 8pages\\r\\\\\\\\\\r  Deep image inpainting research mainly focuses on constructing various neural\\rnetwork architectures or imposing novel optimization objectives. However, on\\rthe one hand, building a state-of-the-art deep inpainting model is an extremely\\rcomplex task, and on the other hand, the resulting performance gains are\\rsometimes very limited. We believe that besides the frameworks of inpainting\\rmodels, lightweight traditional image processing techniques, which are often\\roverlooked, can actually be helpful to these deep models. In this paper, we\\renhance the deep image inpainting models with the help of classical image\\rcomplexity metrics. A knowledge-assisted index composed of missingness\\rcomplexity and forward loss is presented to guide the batch selection in the\\rtraining procedure. This index helps find samples that are more conducive to\\roptimization in each iteration and ultimately boost the overall inpainting\\rperformance. The proposed approach is simple and can be plugged into many deep\\rinpainting models by changing only a few lines of code. We experimentally\\rdemonstrate the improvements for several recently developed image inpainting\\rmodels on various datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06266 ,  9675kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06268\\rDate: Sun, 13 Feb 2022 09:23:29 GMT   (5075kb,D)\\r\\rTitle: BViT: Broad Attention based Vision Transformer\\rAuthors: Nannan Li, Yaran Chen, Weifan Li, Zixiang Ding, Dongbin Zhao\\rCategories: cs.CV\\r\\\\\\\\\\r  Recent works have demonstrated that transformer can achieve promising\\rperformance in computer vision, by exploiting the relationship among image\\rpatches with self-attention. While they only consider the attention in a single\\rfeature layer, but ignore the complementarity of attention in different levels.\\rIn this paper, we propose the broad attention to improve the performance by\\rincorporating the attention relationship of different layers for vision\\rtransformer, which is called BViT. The broad attention is implemented by broad\\rconnection and parameter-free attention. Broad connection of each transformer\\rlayer promotes the transmission and integration of information for BViT.\\rWithout introducing additional trainable parameters, parameter-free attention\\rjointly focuses on the already available attention information in different\\rlayers for extracting useful information and building their relationship.\\rExperiments on image classification tasks demonstrate that BViT delivers\\rstate-of-the-art accuracy of 74.8\\\\%/81.6\\\\% top-1 accuracy on ImageNet with\\r5M/22M parameters. Moreover, we transfer BViT to downstream object recognition\\rbenchmarks to achieve 98.9\\\\% and 89.9\\\\% on CIFAR10 and CIFAR100 respectively\\rthat exceed ViT with fewer parameters. For the generalization test, the broad\\rattention in Swin Transformer and T2T-ViT also bring an improvement of more\\rthan 1\\\\%. To sum up, broad attention is promising to promote the performance of\\rattention based models. Code and pre-trained models are available at\\rhttps://github.com/DRL-CASIA/Broad_ViT.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06268 ,  5075kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06276\\rDate: Sun, 13 Feb 2022 10:05:53 GMT   (16928kb,D)\\r\\rTitle: Natural Image Stitching Using Depth Maps\\rAuthors: Tianli Liao and Nan Li\\rCategories: cs.CV\\rComments: 9 pages, 7 figures\\r\\\\\\\\\\r  Natural image stitching (NIS) aims to create one natural-looking mosaic from\\rtwo overlapping images that capture a same 3D scene from different viewing\\rpositions. Challenges inevitably arise when the scene is non-planar and the\\rcamera baseline is wide, since parallax becomes not negligible in such cases.\\rIn this paper, we propose a novel NIS method using depth maps, which generates\\rnatural-looking mosaics against parallax in both overlapping and\\rnon-overlapping regions. Firstly, we estimate a pixel-to-pixel transformation\\rbased on feature matches and their depth values. Then, we draw a triangulation\\rof the target image and estimate multiple local homographies, one per triangle,\\rbased on the locations of their vertices and the rectified depth values.\\rFinally, the warping image is composited by the backward mapping of piece-wise\\rhomographies. Experimental results demonstrate that the proposed method not\\ronly provides accurate alignment in the overlapping regions, but also virtual\\rnaturalness in the non-overlapping region.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06276 ,  16928kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06283\\rDate: Sun, 13 Feb 2022 11:12:00 GMT   (7680kb,D)\\r\\rTitle: Zero-Reference Image Restoration for Under-Display Camera of UAV\\rAuthors: Zhuoran Zheng, Xiuyi Jia and Yunliang Zhuang\\rCategories: cs.CV\\r\\\\\\\\\\r  The exposed cameras of UAV can shake, shift, or even malfunction under the\\rinfluence of harsh weather, while the add-on devices (Dupont lines) are very\\rvulnerable to damage.\\r  We can place a low-cost T-OLED overlay around the camera to protect it, but\\rthis would also introduce image degradation issues.\\r  In particular, the temperature variations in the atmosphere can create mist\\rthat adsorbs to the T-OLED, which can cause secondary disasters (i.e., more\\rsevere image degradation) during the UAV's filming process.\\r  To solve the image degradation problem caused by overlaying T-OLEDs, in this\\rpaper we propose a new method to enhance the visual experience by enhancing the\\rtexture and color of images.\\r  Specifically, our method trains a lightweight network to estimate a low-rank\\raffine grid on the input image, and then utilizes the grid to enhance the input\\rimage at block granularity.\\r  The advantages of our method are that no reference image is required and the\\rloss function is developed from visual experience.\\r  In addition, our model can perform high-quality recovery of images of\\rarbitrary resolution in real time.\\r  In the end, the limitations of our model and the collected datasets\\r(including the daytime and nighttime scenes) are discussed.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06283 ,  7680kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06300\\rDate: Sun, 13 Feb 2022 12:49:37 GMT   (48878kb,D)\\r\\rTitle: Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction\\rAuthors: Jiayang Bai, Jie Guo, Chenchen Wan, Zhenyu Chen, Zhen He, Shan Yang,\\r  Piaopiao Yu, Yan Zhang and Yanwen Guo\\rCategories: cs.CV\\r\\\\\\\\\\r  Lighting prediction from a single image is becoming increasingly important in\\rmany vision and augmented reality (AR) applications in which shading and shadow\\rconsistency between virtual and real objects should be guaranteed. However,\\rthis is a notoriously ill-posed problem, especially for indoor scenarios,\\rbecause of the complexity of indoor luminaires and the limited information\\rinvolved in 2D images. In this paper, we propose a graph learning-based\\rframework for indoor lighting estimation. At its core is a new lighting model\\r(dubbed DSGLight) based on depth-augmented Spherical Gaussians (SG) and a Graph\\rConvolutional Network (GCN) that infers the new lighting representation from a\\rsingle LDR image of limited field-of-view. Our lighting model builds 128 evenly\\rdistributed SGs over the indoor panorama, where each SG encoding the lighting\\rand the depth around that node. The proposed GCN then learns the mapping from\\rthe input image to DSGLight. Compared with existing lighting models, our\\rDSGLight encodes both direct lighting and indirect environmental lighting more\\rfaithfully and compactly. It also makes network training and inference more\\rstable. The estimated depth distribution enables temporally stable shading and\\rshadows under spatially-varying lighting. Through thorough experiments, we show\\rthat our method obviously outperforms existing methods both qualitatively and\\rquantitatively.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06300 ,  48878kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06312\\rDate: Sun, 13 Feb 2022 13:41:15 GMT   (2186kb,D)\\r\\rTitle: Adversarial Fine-tuning for Backdoor Defense: Connect Adversarial\\r  Examples to Triggered Samples\\rAuthors: Bingxu Mu and Le Wang and Zhenxing Niu\\rCategories: cs.CV\\r\\\\\\\\\\r  Deep neural networks (DNNs) are known to be vulnerable to backdoor attacks,\\ri.e., a backdoor trigger planted at training time, the infected DNN model would\\rmisclassify any testing sample embedded with the trigger as target label. Due\\rto the stealthiness of backdoor attacks, it is hard either to detect or erase\\rthe backdoor from infected models. In this paper, we propose a new Adversarial\\rFine-Tuning (AFT) approach to erase backdoor triggers by leveraging adversarial\\rexamples of the infected model. For an infected model, we observe that its\\radversarial examples have similar behaviors as its triggered samples. Based on\\rsuch observation, we design the AFT to break the foundation of the backdoor\\rattack (i.e., the strong correlation between a trigger and a target label). We\\rempirically show that, against 5 state-of-the-art backdoor attacks, AFT can\\reffectively erase the backdoor triggers without obvious performance degradation\\ron clean samples, which significantly outperforms existing defense methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06312 ,  2186kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06358\\rDate: Sun, 13 Feb 2022 16:29:45 GMT   (2860kb,D)\\r\\rTitle: Diverse facial inpainting guided by exemplars\\rAuthors: Wanglong Lu, Hanli Zhao, Xianta Jiang, Xiaogang Jin, Min Wang, Jiankai\\r  Lyu, and Kaijie Shi\\rCategories: cs.CV cs.AI cs.GR cs.MM\\rComments: There are 13 pages, 11 figures in this paper\\rACM-class: I.4.9; I.4.5\\r\\\\\\\\\\r  Facial image inpainting is a task of filling visually realistic and\\rsemantically meaningful contents for missing or masked pixels in a face image.\\rAlthough existing methods have made significant progress in achieving high\\rvisual quality, the controllable diversity of facial image inpainting remains\\ran open problem in this field. This paper introduces EXE-GAN, a novel diverse\\rand interactive facial inpainting framework, which can not only preserve the\\rhigh-quality visual effect of the whole image but also complete the face image\\rwith exemplar-like facial attributes. The proposed facial inpainting is\\rachieved based on generative adversarial networks by leveraging the global\\rstyle of input image, the stochastic style, and the exemplar style of example\\rimage. A novel attribute similarity metric is introduced to encourage networks\\rto learn the style of facial attributes from the exemplar in a self-supervised\\rway. To guarantee the natural transition across the boundary of inpainted\\rregions, a novel spatial variant gradient backpropagation technique is designed\\rto adjust the loss gradients based on the spatial location. A variety of\\rexperimental results and comparisons on public CelebA-HQ and FFHQ datasets are\\rpresented to demonstrate the superiority of the proposed method in terms of\\rboth the quality and diversity in facial inpainting.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06358 ,  2860kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06371\\rDate: Sun, 13 Feb 2022 17:43:40 GMT   (435kb)\\r\\rTitle: Omnifont Persian OCR System Using Primitives\\rAuthors: Azarakhsh Keipour, Mohammad Eshghi, Sina Mohammadzadeh Ghadikolaei,\\r  Negin Mohammadi, Shahab Ensafi\\rCategories: cs.CV cs.AI eess.IV\\rComments: Accepted in IEEE International Conference on Industrial Technology\\r  (ICIT 2013); Cape Town, South Africa, 25-27th February 2013 (Not Presented)\\r\\\\\\\\\\r  In this paper, we introduce a model-based omnifont Persian OCR system. The\\rsystem uses a set of 8 primitive elements as structural features for\\rrecognition. First, the scanned document is preprocessed. After normalizing the\\rpreprocessed image, text rows and sub-words are separated and then thinned.\\rAfter recognition of dots in sub-words, strokes are extracted and primitive\\relements of each sub-word are recognized using the strokes. Finally, the\\rprimitives are compared with a predefined set of character identification\\rvectors in order to identify sub-word characters. The separation and\\rrecognition steps of the system are concurrent, eliminating unavoidable errors\\rof independent separation of letters. The system has been tested on documents\\rwith 14 standard Persian fonts in 6 sizes. The achieved precision is 97.06%.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06371 ,  435kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06373\\rDate: Sun, 13 Feb 2022 17:52:49 GMT   (11278kb,D)\\r\\rTitle: Scheduling Techniques for Liver Segmentation: ReduceLRonPlateau Vs\\r  OneCycleLR\\rAuthors: Ayman Al-Kababji and Faycal Bensaali and Sarada Prasad Dakua\\rCategories: cs.CV cs.LG eess.IV\\rComments: 8 pages, 4 figures, 1 table, currently submitted The 2nd\\r  International Conference on Intelligent Systems and Patterns Recognition\\r  (ISPR'2022)\\r\\\\\\\\\\r  Machine learning and computer vision techniques have influenced many fields\\rincluding the biomedical one. The aim of this paper is to investigate the\\rimportant concept of schedulers in manipulating the learning rate (LR), for the\\rliver segmentation task, throughout the training process, focusing on the newly\\rdevised OneCycleLR against the ReduceLRonPlateau. A dataset, published in 2018\\rand produced by the Medical Segmentation Decathlon Challenge organizers, called\\rTask 8 Hepatic Vessel (MSDC-T8) has been used for testing and validation. The\\rreported results that have the same number of maximum epochs (75), and are the\\raverage of 5-fold cross-validation, indicate that ReduceLRonPlateau converges\\rfaster while maintaining a similar or even better loss score on the validation\\rset when compared to OneCycleLR. The epoch at which the peak LR occurs perhaps\\rshould be made early for the OneCycleLR such that the super-convergence feature\\rcan be observed. Moreover, the overall results outperform the state-of-the-art\\rresults from the researchers who published the liver masks for this dataset. To\\rconclude, both schedulers are suitable for medical segmentation challenges,\\respecially the MSDC-T8 dataset, and can be used confidently in rapidly\\rconverging the validation loss with a minimal number of epochs.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06373 ,  11278kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06406\\rDate: Sun, 13 Feb 2022 21:06:19 GMT   (34310kb,D)\\r\\rTitle: Visual Sound Localization in the Wild by Cross-Modal Interference\\r  Erasing\\rAuthors: Xian Liu, Rui Qian, Hang Zhou, Di Hu, Weiyao Lin, Ziwei Liu, Bolei\\r  Zhou, Xiaowei Zhou\\rCategories: cs.CV cs.SD eess.AS\\rComments: Accepted by AAAI Conference on Artificial Intelligence (AAAI) 2022.\\r  16 pages\\r\\\\\\\\\\r  The task of audio-visual sound source localization has been well studied\\runder constrained scenes, where the audio recordings are clean. However, in\\rreal-world scenarios, audios are usually contaminated by off-screen sound and\\rbackground noise. They will interfere with the procedure of identifying desired\\rsources and building visual-sound connections, making previous studies\\rnon-applicable. In this work, we propose the Interference Eraser (IEr)\\rframework, which tackles the problem of audio-visual sound source localization\\rin the wild. The key idea is to eliminate the interference by redefining and\\rcarving discriminative audio representations. Specifically, we observe that the\\rprevious practice of learning only a single audio representation is\\rinsufficient due to the additive nature of audio signals. We thus extend the\\raudio representation with our Audio-Instance-Identifier module, which clearly\\rdistinguishes sounding instances when audio signals of different volumes are\\runevenly mixed. Then we erase the influence of the audible but off-screen\\rsounds and the silent but visible objects by a Cross-modal Referrer module with\\rcross-modality distillation. Quantitative and qualitative evaluations\\rdemonstrate that our proposed framework achieves superior results on sound\\rlocalization tasks, especially under real-world scenarios. Code is available at\\rhttps://github.com/alvinliu0/Visual-Sound-Localization-in-the-Wild.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06406 ,  34310kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06407\\rDate: Sun, 13 Feb 2022 21:10:06 GMT   (1532kb,D)\\r\\rTitle: Hierarchical Point Cloud Encoding and Decoding with Lightweight\\r  Self-Attention based Model\\rAuthors: En Yen Puang, Hao Zhang, Hongyuan Zhu, Wei Jing\\rCategories: cs.CV\\rComments: Accepted by RA-Letters and ICRA 2022\\rACM-class: I.4\\r\\\\\\\\\\r  In this paper we present SA-CNN, a hierarchical and lightweight\\rself-attention based encoding and decoding architecture for representation\\rlearning of point cloud data. The proposed SA-CNN introduces convolution and\\rtransposed convolution stacks to capture and generate contextual information\\ramong unordered 3D points. Following conventional hierarchical pipeline, the\\rencoding process extracts feature in local-to-global manner, while the decoding\\rprocess generates feature and point cloud in coarse-to-fine, multi-resolution\\rstages. We demonstrate that SA-CNN is capable of a wide range of applications,\\rnamely classification, part segmentation, reconstruction, shape retrieval, and\\runsupervised classification. While achieving the state-of-the-art or comparable\\rperformance in the benchmarks, SA-CNN maintains its model complexity several\\rorder of magnitude lower than the others. In term of qualitative results, we\\rvisualize the multi-stage point cloud reconstructions and latent walks on rigid\\robjects as well as deformable non-rigid human and robot models.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06407 ,  1532kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06464\\rDate: Mon, 14 Feb 2022 02:41:43 GMT   (826kb,D)\\r\\rTitle: Learn by Challenging Yourself: Contrastive Visual Representation\\r  Learning with Hard Sample Generation\\rAuthors: Yawen Wu, Zhepeng Wang, Dewen Zeng, Yiyu Shi, Jingtong Hu\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Contrastive learning (CL), a self-supervised learning approach, can\\reffectively learn visual representations from unlabeled data. However, CL\\rrequires learning on vast quantities of diverse data to achieve good\\rperformance, without which the performance of CL will greatly degrade. To\\rtackle this problem, we propose a framework with two approaches to improve the\\rdata efficiency of CL training by generating beneficial samples and joint\\rlearning. The first approach generates hard samples for the main model. The\\rgenerator is jointly learned with the main model to dynamically customize hard\\rsamples based on the training state of the main model. With the progressively\\rgrowing knowledge of the main model, the generated samples also become harder\\rto constantly encourage the main model to learn better representations.\\rBesides, a pair of data generators are proposed to generate similar but\\rdistinct samples as positive pairs. In joint learning, the hardness of a\\rpositive pair is progressively increased by decreasing their similarity. In\\rthis way, the main model learns to cluster hard positives by pulling the\\rrepresentations of similar yet distinct samples together, by which the\\rrepresentations of similar samples are well-clustered and better\\rrepresentations can be learned. Comprehensive experiments show superior\\raccuracy and data efficiency of the proposed methods over the state-of-the-art\\ron multiple datasets. For example, about 5% accuracy improvement on\\rImageNet-100 and CIFAR-10, and more than 6% accuracy improvement on CIFAR-100\\rare achieved for linear classification. Besides, up to 2x data efficiency for\\rlinear classification and up to 5x data efficiency for transfer learning are\\rachieved.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06464 ,  826kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06470\\rDate: Mon, 14 Feb 2022 03:20:39 GMT   (5060kb,D)\\r\\rTitle: Tightly Coupled Learning Strategy for Weakly Supervised Hierarchical\\r  Place Recognition\\rAuthors: Y. Shen, R. Wang, W. Zuo, N. Zheng\\rCategories: cs.CV\\rComments: 8 pages, 9 figures\\rDOI: 10.1109/LRA.2022.3141663\\r\\\\\\\\\\r  Visual place recognition (VPR) is a key issue for robotics and autonomous\\rsystems. For the trade-off between time and performance, most of methods use\\rthe coarse-to-fine hierarchical architecture, which consists of retrieving\\rtop-N candidates using global features, and re-ranking top-N with local\\rfeatures. However, since the two types of features are usually processed\\rindependently, re-ranking may harm global retrieval, termed re-ranking\\rconfusion. Moreover, re-ranking is limited by global retrieval. In this paper,\\rwe propose a tightly coupled learning (TCL) strategy to train triplet models.\\rDifferent from original triplet learning (OTL) strategy, it combines global and\\rlocal descriptors for joint optimization. In addition, a bidirectional search\\rdynamic time warping (BS-DTW) algorithm is also proposed to mine locally\\rspatial information tailored to VPR in re-ranking. The experimental results on\\rpublic benchmarks show that the models using TCL outperform the models using\\rOTL, and TCL can be used as a general strategy to improve performance for\\rweakly supervised ranking tasks. Further, our lightweight unified model is\\rbetter than several state-of-the-art methods and has over an order of magnitude\\rof computational efficiency to meet the real-time requirements of robots.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06470 ,  5060kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06484\\rDate: Mon, 14 Feb 2022 05:17:38 GMT   (6075kb,D)\\r\\rTitle: ADeADA: Adaptive Density-aware Active Domain Adaptationfor Semantic\\r  Segmentation\\rAuthors: Tsung-Han Wu, Yi-Syuan Liou, Shao-Ji Yuan, Hsin-Ying Lee, Tung-I Chen,\\r  Winston H. Hsu\\rCategories: cs.CV cs.LG\\rComments: 8 pages, 5 figures\\r\\\\\\\\\\r  In the field of domain adaptation, a trade-off exists between the model\\rperformance and the number of target domain annotations. Active learning,\\rmaximizing model performance with few informative labeled data, comes in handy\\rfor such a scenario. In this work, we present ADeADA, a general active domain\\radaptation framework for semantic segmentation. To adapt the model to the\\rtarget domain with minimum queried labels, we propose acquiring labels of the\\rsamples with high probability density in the target domain yet with low\\rprobability density in the source domain, complementary to the existing source\\rdomain labeled data. To further facilitate the label efficiency, we design an\\radaptive budget allocation policy, which dynamically balances the labeling\\rbudgets among different categories as well as between density-aware and\\runcertainty-based methods. Extensive experiments show that our method\\routperforms existing active learning and domain adaptation baselines on two\\rbenchmarks, GTA5 -> Cityscapes and SYNTHIA -> Cityscapes. With less than 5%\\rtarget domain annotations, our method reaches comparable results with that of\\rfull supervision.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06484 ,  6075kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06498\\rDate: Mon, 14 Feb 2022 06:16:26 GMT   (2359kb,D)\\r\\rTitle: Task-Adaptive Feature Transformer with Semantic Enrichment for Few-Shot\\r  Segmentation\\rAuthors: Jun Seo, Young-Hyun Park, Sung Whan Yoon, Jaekyun Moon\\rCategories: cs.CV\\rComments: 8 pages, 7 figures\\r\\\\\\\\\\r  Few-shot learning allows machines to classify novel classes using only a few\\rlabeled samples. Recently, few-shot segmentation aiming at semantic\\rsegmentation on low sample data has also seen great interest. In this paper, we\\rpropose a learnable module that can be placed on top of existing segmentation\\rnetworks for performing few-shot segmentation. This module, called the\\rtask-adaptive feature transformer (TAFT), linearly transforms task-specific\\rhigh-level features to a set of task agnostic features well-suited to\\rconducting few-shot segmentation. The task-conditioned feature transformation\\rallows an effective utilization of the semantic information in novel classes to\\rgenerate tight segmentation masks. We also propose a semantic enrichment (SE)\\rmodule that utilizes a pixel-wise attention module for high-level feature and\\ran auxiliary loss from an auxiliary segmentation network conducting the\\rsemantic segmentation for all training classes. Experiments on PASCAL-$5^i$ and\\rCOCO-$20^i$ datasets confirm that the added modules successfully extend the\\rcapability of existing segmentators to yield highly competitive few-shot\\rsegmentation performances.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06498 ,  2359kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06503\\rDate: Mon, 14 Feb 2022 06:31:34 GMT   (315kb)\\r\\rTitle: Adaptive graph convolutional networks for weakly supervised anomaly\\r  detection in videos\\rAuthors: Congqi Cao, Xin Zhang, Shizhou Zhang, Peng Wang, Yanning Zhang\\rCategories: cs.CV\\r\\\\\\\\\\r  For the weakly supervised anomaly detection task, most existing work is\\rlimited to the problem of inadequate video representation due to the inability\\rto model long-time contextual information. We propose a weakly supervised\\radaptive graph convolutional network (WAGCN) to model the contextual\\rrelationships among video segments. And we fully consider the influence of\\rother video segments on the current segment when generating the anomaly\\rprobability score for each segment. Firstly, we combine the temporal\\rconsistency as well as feature similarity of video segments for composition,\\rwhich makes full use of the association information among spatial-temporal\\rfeatures of anomalous events in videos. Secondly, we propose a graph learning\\rlayer in order to break the limitation of setting topology manually, which\\radaptively extracts sparse graph adjacency matrix based on data. Extensive\\rexperiments on two public datasets (i.e., UCF-Crime dataset and ShanghaiTech\\rdataset) demonstrate the effectiveness of our approach.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06503 ,  315kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06504\\rDate: Mon, 14 Feb 2022 06:32:21 GMT   (3185kb,D)\\r\\rTitle: Analytic Learning of Convolutional Neural Network For Pattern\\r  Recognition\\rAuthors: Huiping Zhuang, Zhiping Lin, Yimin Yang and Kar-Ann Toh\\rCategories: cs.CV\\r\\\\\\\\\\r  Training convolutional neural networks (CNNs) with back-propagation (BP) is\\rtime-consuming and resource-intensive particularly in view of the need to visit\\rthe dataset multiple times. In contrast, analytic learning attempts to obtain\\rthe weights in one epoch. However, existing attempts to analytic learning\\rconsidered only the multilayer perceptron (MLP). In this article, we propose an\\ranalytic convolutional neural network learning (ACnnL). Theoretically we show\\rthat ACnnL builds a closed-form solution similar to its MLP counterpart, but\\rdiffers in their regularization constraints. Consequently, we are able to\\ranswer to a certain extent why CNNs usually generalize better than MLPs from\\rthe implicit regularization point of view. The ACnnL is validated by conducting\\rclassification tasks on several benchmark datasets. It is encouraging that the\\rACnnL trains CNNs in a significantly fast manner with reasonably close\\rprediction accuracies to those using BP. Moreover, our experiments disclose a\\runique advantage of ACnnL under the small-sample scenario when training data\\rare scarce or expensive.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06504 ,  3185kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06510\\rDate: Mon, 14 Feb 2022 06:53:48 GMT   (2023kb,D)\\r\\rTitle: Mixing and Shifting: Exploiting Global and Local Dependencies in Vision\\r  MLPs\\rAuthors: Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou\\rCategories: cs.CV\\r\\\\\\\\\\r  Token-mixing multi-layer perceptron (MLP) models have shown competitive\\rperformance in computer vision tasks with a simple architecture and relatively\\rsmall computational cost. Their success in maintaining computation efficiency\\ris mainly attributed to avoiding the use of self-attention that is often\\rcomputationally heavy, yet this is at the expense of not being able to mix\\rtokens both globally and locally. In this paper, to exploit both global and\\rlocal dependencies without self-attention, we present Mix-Shift-MLP (MS-MLP)\\rwhich makes the size of the local receptive field used for mixing increase with\\rrespect to the amount of spatial shifting. In addition to conventional mixing\\rand shifting techniques, MS-MLP mixes both neighboring and distant tokens from\\rfine- to coarse-grained levels and then gathers them via a shifting operation.\\rThis directly contributes to the interactions between global and local tokens.\\rBeing simple to implement, MS-MLP achieves competitive performance in multiple\\rvision benchmarks. For example, an MS-MLP with 85 million parameters achieves\\r83.8% top-1 classification accuracy on ImageNet-1K. Moreover, by combining\\rMS-MLP with state-of-the-art Vision Transformers such as the Swin Transformer,\\rwe show MS-MLP achieves further improvements on three different model scales,\\re.g., by 0.5% on ImageNet-1K classification with Swin-B. The code is available\\rat: https://github.com/JegZheng/MS-MLP.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06510 ,  2023kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06511\\rDate: Mon, 14 Feb 2022 06:54:15 GMT   (7272kb,D)\\r\\rTitle: GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges\\rAuthors: Junde Wu, Huihui Fang, Fei Li, Huazhu Fu, Fengbin Lin, Jiongcheng Li,\\r  Lexing Huang, Qinji Yu, Sifan Song, Xingxing Xu, Yanyu Xu, Wensai Wang,\\r  Lingxiao Wang, Shuai Lu, Huiqi Li, Shihua Huang, Zhichao Lu, Chubin Ou, Xifei\\r  Wei, Bingyuan Liu, Riadh Kobbi, Xiaoying Tang, Li Lin, Qiang Zhou, Qiang Hu,\\r  Hrvoje Bogunovic, Jos\\\\'e Ignacio Orlando, Xiulan Zhang, Yanwu Xu\\rCategories: cs.CV\\r\\\\\\\\\\r  Color fundus photography and Optical Coherence Tomography (OCT) are the two\\rmost cost-effective tools for glaucoma screening. Both two modalities of images\\rhave prominent biomarkers to indicate glaucoma suspected. Clinically, it is\\roften recommended to take both of the screenings for a more accurate and\\rreliable diagnosis. However, although numerous algorithms are proposed based on\\rfundus images or OCT volumes in computer-aided diagnosis, there are still few\\rmethods leveraging both of the modalities for the glaucoma assessment. Inspired\\rby the success of Retinal Fundus Glaucoma Challenge (REFUGE) we held\\rpreviously, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA)\\rChallenge to encourage the development of fundus \\\\& OCT-based glaucoma grading.\\rThe primary task of the challenge is to grade glaucoma from both the 2D fundus\\rimages and 3D OCT scanning volumes. As part of GAMMA, we have publicly released\\ra glaucoma annotated dataset with both 2D fundus color photography and 3D OCT\\rvolumes, which is the first multi-modality dataset for glaucoma grading. In\\raddition, an evaluation framework is also established to evaluate the\\rperformance of the submitted methods. During the challenge, 1272 results were\\rsubmitted, and finally, top-10 teams were selected to the final stage. We\\ranalysis their results and summarize their methods in the paper. Since all\\rthese teams submitted their source code in the challenge, a detailed ablation\\rstudy is also conducted to verify the effectiveness of the particular modules\\rproposed. We find many of the proposed techniques are practical for the\\rclinical diagnosis of glaucoma. As the first in-depth study of fundus \\\\& OCT\\rmulti-modality glaucoma grading, we believe the GAMMA Challenge will be an\\ressential starting point for future research.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06511 ,  7272kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06513\\rDate: Mon, 14 Feb 2022 07:01:01 GMT   (2095kb,D)\\r\\rTitle: Context-Preserving Instance-Level Augmentation and Deformable\\r  Convolution Networks for SAR Ship Detection\\rAuthors: Taeyong Song, Sunok Kim, SungTai Kim, Jaeseok Lee and Kwanghoon Sohn\\rCategories: cs.CV\\rComments: Accepted to 2022 IEEE Radar Conference\\r\\\\\\\\\\r  Shape deformation of targets in SAR image due to random orientation and\\rpartial information loss caused by occlusion of the radar signal, is an\\ressential challenge in SAR ship detection. In this paper, we propose a data\\raugmentation method to train a deep network that is robust to partial\\rinformation loss within the targets. Taking advantage of ground-truth\\rannotations for bounding box and instance segmentation mask, we present a\\rsimple and effective pipeline to simulate information loss on targets in\\rinstance-level, while preserving contextual information. Furthermore, we adopt\\rdeformable convolutional network to adaptively extract shape-invariant deep\\rfeatures from geometrically translated targets. By learning sampling offset to\\rthe grid of standard convolution, the network can robustly extract the features\\rfrom targets with shape variations for SAR ship detection. Experiments on the\\rHRSID dataset including comparisons with other deep networks and augmentation\\rmethods, as well as ablation study, demonstrate the effectiveness of our\\rproposed method.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06513 ,  2095kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06547\\rDate: Mon, 14 Feb 2022 08:37:26 GMT   (917kb,D)\\r\\rTitle: Video2IMU: Realistic IMU features and signals from videos\\rAuthors: Arttu L\\\\ams\\\\a, Jaakko Tervonen, Jussi Liikka, Constantino \\\\'Alvarez\\r  Casado, Miguel Bordallo L\\\\'opez\\rCategories: cs.CV\\rComments: 5 pages, 2 figures\\r\\\\\\\\\\r  Human Activity Recognition (HAR) from wearable sensor data identifies\\rmovements or activities in unconstrained environments. HAR is a challenging\\rproblem as it presents great variability across subjects. Obtaining large\\ramounts of labelled data is not straightforward, since wearable sensor signals\\rare not easy to label upon simple human inspection. In our work, we propose the\\ruse of neural networks for the generation of realistic signals and features\\rusing human activity monocular videos. We show how these generated features and\\rsignals can be utilized, instead of their real counterparts, to train HAR\\rmodels that can recognize activities using signals obtained with wearable\\rsensors. To prove the validity of our methods, we perform experiments on an\\ractivity recognition dataset created for the improvement of industrial work\\rsafety. We show that our model is able to realistically generate virtual sensor\\rsignals and features usable to train a HAR classifier with comparable\\rperformance as the one trained using real sensor data. Our results enable the\\ruse of available, labelled video data for training HAR models to classify\\rsignals from wearable sensors.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06547 ,  917kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06565\\rDate: Mon, 14 Feb 2022 09:07:21 GMT   (938kb,D)\\r\\rTitle: Single-stage Rotate Object Detector via Two Points with Solar Corona\\r  Heatmap\\rAuthors: Beihang Song, Jing Li, Shan Xue, Jun Chang, Jia Wu, Jun Wan and\\r  Tianpeng Liu\\rCategories: cs.CV\\rComments: 21 pages,6 figures\\r\\\\\\\\\\r  Oriented object detection is a crucial task in computer vision. Current\\rtop-down oriented detection methods usually directly detect entire objects, and\\rnot only neglecting the authentic direction of targets, but also do not fully\\rutilise the key semantic information, which causes a decrease in detection\\raccuracy. In this study, we developed a single-stage rotating object detector\\rvia two points with a solar corona heatmap (ROTP) to detect oriented objects.\\rThe ROTP predicts parts of the object and then aggregates them to form a whole\\rimage. Herein, we meticulously represent an object in a random direction using\\rthe vertex, centre point with width, and height. Specifically, we regress two\\rheatmaps that characterise the relative location of each object, which enhances\\rthe accuracy of locating objects and avoids deviations caused by angle\\rpredictions. To rectify the central misjudgement of the Gaussian heatmap on\\rhigh-aspect ratio targets, we designed a solar corona heatmap generation method\\rto improve the perception difference between the central and non-central\\rsamples. Additionally, we predicted the vertex relative to the direction of the\\rcentre point to connect two key points that belong to the same goal.\\rExperiments on the HRSC 2016, UCASAOD, and DOTA datasets show that our ROTP\\rachieves the most advanced performance with a simpler modelling and less manual\\rintervention.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06565 ,  938kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06568\\rDate: Mon, 14 Feb 2022 09:09:08 GMT   (16063kb,D)\\r\\rTitle: Online-updated High-order Collaborative Networks for Single Image\\r  Deraining\\rAuthors: Cong Wang and Jinshan Pan and Xiao-Ming Wu\\rCategories: cs.CV\\rComments: AAAI-22\\r\\\\\\\\\\r  Single image deraining is an important and challenging task for some\\rdownstream artificial intelligence applications such as video surveillance and\\rself-driving systems. Most of the existing deep-learning-based methods\\rconstrain the network to generate derained images but few of them explore\\rfeatures from intermediate layers, different levels, and different modules\\rwhich are beneficial for rain streaks removal. In this paper, we propose a\\rhigh-order collaborative network with multi-scale compact constraints and a\\rbidirectional scale-content similarity mining module to exploit features from\\rdeep networks externally and internally for rain streaks removal. Externally,\\rwe design a deraining framework with three sub-networks trained in a\\rcollaborative manner, where the bottom network transmits intermediate features\\rto the middle network which also receives shallower rainy features from the top\\rnetwork and sends back features to the bottom network. Internally, we enforce\\rmulti-scale compact constraints on the intermediate layers of deep networks to\\rlearn useful features via a Laplacian pyramid. Further, we develop a\\rbidirectional scale-content similarity mining module to explore features at\\rdifferent scales in a down-to-up and up-to-down manner. To improve the model\\rperformance on real-world images, we propose an online-update learning\\rapproach, which uses real-world rainy images to fine-tune the network and\\rupdate the deraining results in a self-supervised manner. Extensive experiments\\rdemonstrate that our proposed method performs favorably against eleven\\rstate-of-the-art methods on five public synthetic datasets and one real-world\\rdataset. The source code will be available at\\r\\\\url{https://supercong94.wixsite.com/supercong94}.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06568 ,  16063kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06639\\rDate: Mon, 14 Feb 2022 11:47:26 GMT   (3919kb,D)\\r\\rTitle: On the Complexity of Object Detection on Real-world Public\\r  Transportation Images for Social Distancing Measurement\\rAuthors: Nik Khadijah Nik Aznan, John Brennan, Daniel Bell, Jennine Jonczyk and\\r  Paul Watson\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  Social distancing in public spaces has become an essential aspect in helping\\rto reduce the impact of the COVID-19 pandemic. Exploiting recent advances in\\rmachine learning, there have been many studies in the literature implementing\\rsocial distancing via object detection through the use of surveillance cameras\\rin public spaces. However, to date, there has been no study of social distance\\rmeasurement on public transport. The public transport setting has some unique\\rchallenges, including some low-resolution images and camera locations that can\\rlead to the partial occlusion of passengers, which make it challenging to\\rperform accurate detection. Thus, in this paper, we investigate the challenges\\rof performing accurate social distance measurement on public transportation. We\\rbenchmark several state-of-the-art object detection algorithms using real-world\\rfootage taken from the London Underground and bus network. The work highlights\\rthe complexity of performing social distancing measurement on images from\\rcurrent public transportation onboard cameras. Further, exploiting domain\\rknowledge of expected passenger behaviour, we attempt to improve the quality of\\rthe detections using various strategies and show improvement over using vanilla\\robject detection alone.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06639 ,  3919kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06673\\rDate: Mon, 14 Feb 2022 12:59:23 GMT   (279kb,D)\\r\\rTitle: Convolutional Neural Network with Convolutional Block Attention Module\\r  for Finger Vein Recognition\\rAuthors: Zhongxia Zhang and Mingwen Wang\\rCategories: cs.CV\\rComments: 11 pages, 6 figures, 5 tables\\rMSC-class: 68U10\\r\\\\\\\\\\r  Convolutional neural networks have become a popular research in the field of\\rfinger vein recognition because of their powerful image feature representation.\\rHowever, most researchers focus on improving the performance of the network by\\rincreasing the CNN depth and width, which often requires high computational\\reffort. Moreover, we can notice that not only the importance of pixels in\\rdifferent channels is different, but also the importance of pixels in different\\rpositions of the same channel is different. To reduce the computational effort\\rand to take into account the different importance of pixels, we propose a\\rlightweight convolutional neural network with a convolutional block attention\\rmodule (CBAM) for finger vein recognition, which can achieve a more accurate\\rcapture of visual structures through an attention mechanism. First, image\\rsequences are fed into a lightweight convolutional neural network we designed\\rto improve visual features. Afterwards, it learns to assign feature weights in\\ran adaptive manner with the help of a convolutional block attention module. The\\rexperiments are carried out on two publicly available databases and the results\\rdemonstrate that the proposed method achieves a stable, highly accurate, and\\rrobust performance in multimodal finger recognition.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06673 ,  279kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06687\\rDate: Mon, 14 Feb 2022 13:25:46 GMT   (6567kb,D)\\r\\rTitle: Domain Adaptation via Prompt Learning\\rAuthors: Chunjiang Ge and Rui Huang and Mixue Xie and Zihang Lai and Shiji Song\\r  and Shuang Li and Gao Huang\\rCategories: cs.CV\\rComments: 10 pages, 5 figures\\r\\\\\\\\\\r  Unsupervised domain adaption (UDA) aims to adapt models learned from a\\rwell-annotated source domain to a target domain, where only unlabeled samples\\rare given. Current UDA approaches learn domain-invariant features by aligning\\rsource and target feature spaces. Such alignments are imposed by constraints\\rsuch as statistical discrepancy minimization or adversarial training. However,\\rthese constraints could lead to the distortion of semantic feature structures\\rand loss of class discriminability. In this paper, we introduce a novel prompt\\rlearning paradigm for UDA, named Domain Adaptation via Prompt Learning (DAPL).\\rIn contrast to prior works, our approach makes use of pre-trained\\rvision-language models and optimizes only very few parameters. The main idea is\\rto embed domain information into prompts, a form of representations generated\\rfrom natural language, which is then used to perform classification. This\\rdomain information is shared only by images from the same domain, thereby\\rdynamically adapting the classifier according to each domain. By adopting this\\rparadigm, we show that our model not only outperforms previous methods on\\rseveral cross-domain benchmarks but also is very efficient to train and easy to\\rimplement.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06687 ,  6567kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06688\\rDate: Mon, 14 Feb 2022 13:26:09 GMT   (9700kb,D)\\r\\rTitle: Geometric Transformer for Fast and Robust Point Cloud Registration\\rAuthors: Zheng Qin, Hao Yu, Changjian Wang, Yulan Guo, Yuxing Peng and Kai Xu\\rCategories: cs.CV\\rComments: 19 pages, 11 figures\\r\\\\\\\\\\r  We study the problem of extracting accurate correspondences for point cloud\\rregistration. Recent keypoint-free methods bypass the detection of repeatable\\rkeypoints which is difficult in low-overlap scenarios, showing great potential\\rin registration. They seek correspondences over downsampled superpoints, which\\rare then propagated to dense points. Superpoints are matched based on whether\\rtheir neighboring patches overlap. Such sparse and loose matching requires\\rcontextual features capturing the geometric structure of the point clouds. We\\rpropose Geometric Transformer to learn geometric feature for robust superpoint\\rmatching. It encodes pair-wise distances and triplet-wise angles, making it\\rrobust in low-overlap cases and invariant to rigid transformation. The\\rsimplistic design attains surprisingly high matching accuracy such that no\\rRANSAC is required in the estimation of alignment transformation, leading to\\r$100$ times acceleration. Our method improves the inlier ratio by\\r17\\\\%$\\\\sim$30\\\\% and the registration recall by over 7\\\\% on the challenging\\r3DLoMatch benchmark. The code and models will be released at\\r\\\\url{https://github.com/qinzheng93/GeoTransformer}.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06688 ,  9700kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06709\\rDate: Mon, 14 Feb 2022 13:58:43 GMT   (645kb,D)\\r\\rTitle: How Do Vision Transformers Work?\\rAuthors: Namuk Park, Songkuk Kim\\rCategories: cs.CV cs.LG\\rComments: ICLR 2022 (Spotlight)\\r\\\\\\\\\\r  The success of multi-head self-attentions (MSAs) for computer vision is now\\rindisputable. However, little is known about how MSAs work. We present\\rfundamental explanations to help better understand the nature of MSAs. In\\rparticular, we demonstrate the following properties of MSAs and Vision\\rTransformers (ViTs): (1) MSAs improve not only accuracy but also generalization\\rby flattening the loss landscapes. Such improvement is primarily attributable\\rto their data specificity, not long-range dependency. On the other hand, ViTs\\rsuffer from non-convex losses. Large datasets and loss landscape smoothing\\rmethods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.\\rFor example, MSAs are low-pass filters, but Convs are high-pass filters.\\rTherefore, MSAs and Convs are complementary; (3) Multi-stage neural networks\\rbehave like a series connection of small individual models. In addition, MSAs\\rat the end of a stage play a key role in prediction. Based on these insights,\\rwe propose AlterNet, a model in which Conv blocks at the end of a stage are\\rreplaced with MSA blocks. AlterNet outperforms CNNs not only in large data\\rregimes but also in small data regimes. The code is available at\\rhttps://github.com/xxxnell/how-do-vits-work.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06709 ,  645kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06767\\rDate: Mon, 14 Feb 2022 14:37:15 GMT   (1284kb,D)\\r\\rTitle: Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset\\r  and A Foundation Framework\\rAuthors: Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Hang Xu,\\r  Xiaodan Liang, Wei Zhang, Xin Jiang, Chunjing Xu\\rCategories: cs.CV cs.LG\\r\\\\\\\\\\r  This paper presents a large-scale Chinese cross-modal dataset for\\rbenchmarking different multi-modal pre-training methods to facilitate the\\rVision-Language Pre-training (VLP) research and community development. Recent\\rdual-stream VLP models like CLIP, ALIGN and FILIP have shown remarkable\\rperformance on various downstream tasks as well as their remarkable zero-shot\\rability in the open domain tasks. However, their success heavily relies on the\\rscale of pre-trained datasets. Though there have been both small-scale\\rvision-language English datasets like Flickr30k, CC12M as well as large-scale\\rLAION-400M, the current community lacks large-scale Vision-Language benchmarks\\rin Chinese, hindering the development of broader multilingual applications. On\\rthe other hand, there is very rare publicly available large-scale Chinese\\rcross-modal pre-training dataset that has been released, making it hard to use\\rpre-trained models as services for downstream tasks. In this work, we release a\\rLarge-Scale Chinese Cross-modal dataset named Wukong, containing 100 million\\rChinese image-text pairs from the web. Furthermore, we release a group of big\\rmodels pre-trained with advanced image encoders (ResNet/ViT/SwinT) and\\rdifferent pre-training methods (CLIP/FILIP/LiT). We provide extensive\\rexperiments, a deep benchmarking of different downstream tasks, and some\\rexciting findings. Experiments show that Wukong can serve as a promising\\rChinese pre-training dataset and benchmark for different cross-modal learning\\rmethods, which gives superior performance on various downstream tasks such as\\rzero-shot image classification and image-text retrieval benchmarks. More\\rinformation can refer to https://wukong-dataset.github.io/wukong-dataset/.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06767 ,  1284kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06768\\rDate: Mon, 14 Feb 2022 14:37:54 GMT   (1009kb,D)\\r\\rTitle: Probabilistic Embeddings Revisited\\rAuthors: Ivan Karpukhin, Stanislav Dereka, Sergey Kolesnikov\\rCategories: cs.CV\\r\\\\\\\\\\r  In recent years, deep metric learning and its probabilistic extensions\\rachieved state-of-the-art results in a face verification task. However, despite\\rimprovements in face verification, probabilistic methods received little\\rattention in the community. It is still unclear whether they can improve image\\rretrieval quality. In this paper, we present an extensive comparison of\\rprobabilistic methods in verification and retrieval tasks. Following the\\rsuggested methodology, we outperform metric learning baselines using\\rprobabilistic methods and propose several directions for future work and\\rimprovements.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06768 ,  1009kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06777\\rDate: Mon, 14 Feb 2022 14:58:05 GMT   (1730kb,D)\\r\\rTitle: Multi-scale Attention Guided Pose Transfer\\rAuthors: Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh and Umapada Pal\\rCategories: cs.CV cs.MM\\rComments: 14 pages\\r\\\\\\\\\\r  Pose transfer refers to the probabilistic image generation of a person with a\\rpreviously unseen novel pose from another image of that person having a\\rdifferent pose. Due to potential academic and commercial applications, this\\rproblem is extensively studied in recent years. Among the various approaches to\\rthe problem, attention guided progressive generation is shown to produce\\rstate-of-the-art results in most cases. In this paper, we present an improved\\rnetwork architecture for pose transfer by introducing attention links at every\\rresolution level of the encoder and decoder. By utilizing such dense\\rmulti-scale attention guided approach, we are able to achieve significant\\rimprovement over the existing methods both visually and analytically. We\\rconclude our findings with extensive qualitative and quantitative comparisons\\ragainst several existing methods on the DeepFashion dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06777 ,  1730kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06817\\rDate: Mon, 14 Feb 2022 15:54:58 GMT   (35117kb,D)\\r\\rTitle: CATs++: Boosting Cost Aggregation with Convolutions and Transformers\\rAuthors: Seokju Cho, Sunghwan Hong, Seungryong Kim\\rCategories: cs.CV\\rComments: https://sunghwanhong.github.io/CATs/. arXiv admin note: text overlap\\r  with arXiv:2106.02520\\r\\\\\\\\\\r  Cost aggregation is a highly important process in image matching tasks, which\\raims to disambiguate the noisy matching scores. Existing methods generally\\rtackle this by hand-crafted or CNN-based methods, which either lack robustness\\rto severe deformations or inherit the limitation of CNNs that fail to\\rdiscriminate incorrect matches due to limited receptive fields and\\rinadaptability. In this paper, we introduce Cost Aggregation with Transformers\\r(CATs) to tackle this by exploring global consensus among initial correlation\\rmap with the help of some architectural designs that allow us to fully enjoy\\rglobal receptive fields of self-attention mechanism. Also, to alleviate some of\\rthe limitations that CATs may face, i.e., high computational costs induced by\\rthe use of a standard transformer that its complexity grows with the size of\\rspatial and feature dimensions, which restrict its applicability only at\\rlimited resolution and result in rather limited performance, we propose CATs++,\\ran extension of CATs. Our proposed methods outperform the previous\\rstate-of-the-art methods by large margins, setting a new state-of-the-art for\\rall the benchmarks, including PF-WILLOW, PF-PASCAL, and SPair-71k. We further\\rprovide extensive ablation studies and analyses.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06817 ,  35117kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06851\\rDate: Mon, 14 Feb 2022 16:38:31 GMT   (8637kb,D)\\r\\rTitle: HAKE: A Knowledge Engine Foundation for Human Activity Understanding\\rAuthors: Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Zuoyu Qiu, Liang Xu,\\r  Yue Xu, Hao-Shu Fang, Cewu Lu\\rCategories: cs.CV cs.AI cs.LG\\rComments: HAKE 2.0 (work in progress); website:http://hake-mvig.cn/\\r\\\\\\\\\\r  Human activity understanding is of widespread interest in artificial\\rintelligence and spans diverse applications like health care and behavior\\ranalysis. Although there have been advances with deep learning, it remains\\rchallenging. The object recognition-like solutions usually try to map pixels to\\rsemantics directly, but activity patterns are much different from object\\rpatterns, thus hindering another success. In this work, we propose a novel\\rparadigm to reformulate this task in two-stage: first mapping pixels to an\\rintermediate space spanned by atomic activity primitives, then programming\\rdetected primitives with interpretable logic rules to infer semantics. To\\rafford a representative primitive space, we build a knowledge base including\\r26+ M primitive labels and logic rules from human priors or automatic\\rdiscovering. Our framework, Human Activity Knowledge Engine (HAKE), exhibits\\rsuperior generalization ability and performance upon canonical methods on\\rchallenging benchmarks. Code and data are available at http://hake-mvig.cn/.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06851 ,  8637kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06857\\rDate: Mon, 14 Feb 2022 16:43:28 GMT   (1865kb)\\r\\rTitle: A Graph-Matching Approach for Cross-view Registration of Over-view 2 and\\r  Street-view based Point Clouds\\rAuthors: Xiao Ling, Rongjun Qin\\rCategories: cs.CV\\rComments: 24 pages, 12 figures\\rJournal-ref: ISPRS Journal of Photogrammetry and Remote Sensing 185 (2022):\\r  2-15\\r\\\\\\\\\\r  In this paper, based on the assumption that the object boundaries (e.g.,\\rbuildings) from the over-view data should coincide with footprints of\\rfa\\\\c{c}ade 3D points generated from street-view photogrammetric images, we aim\\rto address this problem by proposing a fully automated geo-registration method\\rfor cross-view data, which utilizes semantically segmented object boundaries as\\rview-invariant features under a global optimization framework through\\rgraph-matching: taking the over-view point clouds generated from\\rstereo/multi-stereo satellite images and the street-view point clouds generated\\rfrom monocular video images as the inputs, the proposed method models segments\\rof buildings as nodes of graphs, both detected from the satellite-based and\\rstreet-view based point clouds, thus to form the registration as a\\rgraph-matching problem to allow non-rigid matches; to enable a robust solution\\rand fully utilize the topological relations between these segments, we propose\\rto address the graph-matching problem on its conjugate graph solved through a\\rbelief-propagation algorithm. The matched nodes will be subject to a further\\roptimization to allow precise-registration, followed by a constrained bundle\\radjustment on the street-view image to keep 2D29 3D consistencies, which yields\\rwell-registered street-view images and point clouds to the satellite point\\rclouds.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06857 ,  1865kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06858\\rDate: Mon, 14 Feb 2022 16:43:32 GMT   (6462kb,D)\\r\\rTitle: An experimental study of the vision-bottleneck in VQA\\rAuthors: Pierre Marza, Corentin Kervadec, Grigory Antipov, Moez Baccouche,\\r  Christian Wolf\\rCategories: cs.CV\\r\\\\\\\\\\r  As in many tasks combining vision and language, both modalities play a\\rcrucial role in Visual Question Answering (VQA). To properly solve the task, a\\rgiven model should both understand the content of the proposed image and the\\rnature of the question. While the fusion between modalities, which is another\\robviously important part of the problem, has been highly studied, the vision\\rpart has received less attention in recent work. Current state-of-the-art\\rmethods for VQA mainly rely on off-the-shelf object detectors delivering a set\\rof object bounding boxes and embeddings, which are then combined with question\\rword embeddings through a reasoning module. In this paper, we propose an\\rin-depth study of the vision-bottleneck in VQA, experimenting with both the\\rquantity and quality of visual objects extracted from images. We also study the\\rimpact of two methods to incorporate the information about objects necessary\\rfor answering a question, in the reasoning module directly, and earlier in the\\robject selection stage. This work highlights the importance of vision in the\\rcontext of VQA, and the interest of tailoring vision methods used in VQA to the\\rtask at hand.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06858 ,  6462kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06875\\rDate: Mon, 14 Feb 2022 17:05:22 GMT   (6049kb,D)\\r\\rTitle: Visual Acoustic Matching\\rAuthors: Changan Chen, Ruohan Gao, Paul Calamia, Kristen Grauman\\rCategories: cs.CV cs.MM cs.SD eess.AS\\rComments: Project page:\\r  https://vision.cs.utexas.edu/projects/visual-acoustic-matching\\r\\\\\\\\\\r  We introduce the visual acoustic matching task, in which an audio clip is\\rtransformed to sound like it was recorded in a target environment. Given an\\rimage of the target environment and a waveform for the source audio, the goal\\ris to re-synthesize the audio to match the target room acoustics as suggested\\rby its visible geometry and materials. To address this novel task, we propose a\\rcross-modal transformer model that uses audio-visual attention to inject visual\\rproperties into the audio and generate realistic audio output. In addition, we\\rdevise a self-supervised training objective that can learn acoustic matching\\rfrom in-the-wild Web videos, despite their lack of acoustically mismatched\\raudio. We demonstrate that our approach successfully translates human speech to\\ra variety of real-world environments depicted in images, outperforming both\\rtraditional acoustic matching and more heavily supervised baselines.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06875 ,  6049kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06884\\rDate: Mon, 14 Feb 2022 17:19:23 GMT   (1380kb,D)\\r\\rTitle: COLA: COarse LAbel pre-training for 3D semantic segmentation of sparse\\r  LiDAR datasets\\rAuthors: Jules Sanchez, Jean-Emmanuel Deschaud and Fran\\\\c{c}ois Goulette\\rCategories: cs.CV cs.RO\\rComments: preprint for IROS\\r\\\\\\\\\\r  Transfer learning is a proven technique in 2D computer vision to leverage the\\rlarge amount of data available and achieve high performance with datasets\\rlimited in size due to the cost of acquisition or annotation. In 3D, annotation\\ris known to be a costly task; nevertheless, transfer learning methods have only\\rrecently been investigated. Unsupervised pre-training has been heavily favored\\ras no very large annotated dataset are available. In this work, we tackle the\\rcase of real-time 3D semantic segmentation of sparse outdoor LiDAR scans. Such\\rdatasets have been on the rise, but with different label sets even for the same\\rtask. In this work, we propose here an intermediate-level label set called the\\rcoarse labels, which allows all the data available to be leveraged without any\\rmanual labelization. This way, we have access to a larger dataset, alongside a\\rsimpler task of semantic segmentation. With it, we introduce a new pre-training\\rtask: the coarse label pre-training, also called COLA. We thoroughly analyze\\rthe impact of COLA on various datasets and architectures and show that it\\ryields a noticeable performance improvement, especially when the finetuning\\rtask has access only to a small dataset.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06884 ,  1380kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06934\\rDate: Mon, 14 Feb 2022 18:49:12 GMT   (2550kb,D)\\r\\rTitle: Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection\\rAuthors: Fatih Cagatay Akyon, Sinan Onur Altinuc, Alptekin Temizel\\rCategories: cs.CV cs.LG\\rComments: 5 pages, 4 figures, 2 tables\\r\\\\\\\\\\r  Detection of small objects and objects far away in the scene is a major\\rchallenge in surveillance applications. Such objects are represented by small\\rnumber of pixels in the image and lack sufficient details, making them\\rdifficult to detect using conventional detectors. In this work, an open-source\\rframework called Slicing Aided Hyper Inference (SAHI) is proposed that provides\\ra generic slicing aided inference and fine-tuning pipeline for small object\\rdetection. The proposed technique is generic in the sense that it can be\\rapplied on top of any available object detector without any fine-tuning.\\rExperimental evaluations, using object detection baselines on the Visdrone and\\rxView aerial object detection datasets show that the proposed inference method\\rcan increase object detection AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and\\rTOOD detectors, respectively. Moreover, the detection accuracy can be further\\rincreased with a slicing aided fine-tuning, resulting in a cumulative increase\\rof 12.7%, 13.4% and 14.5% AP in the same order. Proposed technique has been\\rintegrated with Detectron2, MMDetection and YOLOv5 models and it is publicly\\ravailable at\\r\\\\href{https://github.com/obss/sahi.git}{https://github.com/obss/sahi.git}\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06934 ,  2550kb)\\r%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:1907.05911 (*cross-listing*)\\rDate: Fri, 12 Jul 2019 18:15:56 GMT   (1107kb,D)\\rDate (revised v2): Fri, 11 Sep 2020 16:12:37 GMT   (6820kb,D)\\rDate (revised v3): Mon, 25 Jan 2021 07:15:21 GMT   (10768kb,D)\\r\\rTitle: Vector Quantized Bayesian Neural Network Inference for Data Streams\\rAuthors: Namuk Park, Taekyu Lee, Songkuk Kim\\rCategories: cs.LG cs.CV stat.ML\\rComments: AAAI 2021\\r\\\\\\\\\\r  Bayesian neural networks (BNN) can estimate the uncertainty in predictions,\\ras opposed to non-Bayesian neural networks (NNs). However, BNNs have been far\\rless widely used than non-Bayesian NNs in practice since they need iterative NN\\rexecutions to predict a result for one data, and it gives rise to prohibitive\\rcomputational cost. This computational burden is a critical problem when\\rprocessing data streams with low-latency. To address this problem, we propose a\\rnovel model VQ-BNN, which approximates BNN inference for data streams. In order\\rto reduce the computational burden, VQ-BNN inference predicts NN only once and\\rcompensates the result with previously memorized predictions. To be specific,\\rVQ-BNN inference for data streams is given by temporal exponential smoothing of\\rrecent predictions. The computational cost of this model is almost the same as\\rthat of non-Bayesian NNs. Experiments including semantic segmentation on\\rreal-world data show that this model performs significantly faster than BNNs\\rwhile estimating predictive results comparable to or superior to the results of\\rBNNs.\\r\\\\\\\\ ( https://arxiv.org/abs/1907.05911 ,  10768kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05983 (*cross-listing*)\\rDate: Sat, 12 Feb 2022 04:51:00 GMT   (865kb,D)\\r\\rTitle: Uncalibrated Models Can Improve Human-AI Collaboration\\rAuthors: Kailas Vodrahalli, Tobias Gerstenberg, and James Zou\\rCategories: cs.AI cs.CV cs.HC cs.LG\\rComments: 15 pages, 9 figures, in submission\\r\\\\\\\\\\r  In many practical applications of AI, an AI model is used as a decision aid\\rfor human users. The AI provides advice that a human (sometimes) incorporates\\rinto their decision-making process. The AI advice is often presented with some\\rmeasure of confidence that the human can use to calibrate how much they\\rdepend on or trust the advice. In this paper, we demonstrate that presenting AI\\rmodels as more confident than they actually are, even when the original AI is\\rwell-calibrated, can improve human-AI performance (measured as the accuracy and\\rconfidence of the human's final prediction after seeing the AI advice). We\\rfirst learn a model for how humans incorporate AI advice using data from\\rthousands of human interactions. This enables us to explicitly estimate how to\\rtransform the AI's prediction confidence, making the AI uncalibrated, in order\\rto improve the final human prediction. We empirically validate our results\\racross four different tasks -- dealing with images, text and tabular data --\\rinvolving hundreds of human participants. We further support our findings with\\rsimulation analysis. Our findings suggest the importance of and a framework for\\rjointly optimizing the human-AI system as opposed to the standard paradigm of\\roptimizing the AI model alone.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05983 ,  865kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06027 (*cross-listing*)\\rDate: Sat, 12 Feb 2022 09:58:09 GMT   (6570kb,D)\\r\\rTitle: End-to-end Reinforcement Learning of Robotic Manipulation with Robust\\r  Keypoints Representation\\rAuthors: Tianying Wang, En Yen Puang, Marcus Lee, Yan Wu, Wei Jing\\rCategories: cs.RO cs.CV\\rComments: 8 pages\\r\\\\\\\\\\r  We present an end-to-end Reinforcement Learning(RL) framework for robotic\\rmanipulation tasks, using a robust and efficient keypoints representation. The\\rproposed method learns keypoints from camera images as the state\\rrepresentation, through a self-supervised autoencoder architecture. The\\rkeypoints encode the geometric information, as well as the relationship of the\\rtool and target in a compact representation to ensure efficient and robust\\rlearning. After keypoints learning, the RL step then learns the robot motion\\rfrom the extracted keypoints state representation. The keypoints and RL\\rlearning processes are entirely done in the simulated environment. We\\rdemonstrate the effectiveness of the proposed method on robotic manipulation\\rtasks including grasping and pushing, in different scenarios. We also\\rinvestigate the generalization capability of the trained model. In addition to\\rthe robust keypoints representation, we further apply domain randomization and\\radversarial training examples to achieve zero-shot sim-to-real transfer in\\rreal-world robotic manipulation tasks.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06027 ,  6570kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06073 (*cross-listing*)\\rDate: Sat, 12 Feb 2022 14:12:13 GMT   (829kb)\\r\\rTitle: Classification of Microscopy Images of Breast Tissue: Region Duplication\\r  based Self-Supervision vs. Off-the Shelf Deep Representations\\rAuthors: Aravind Ravi\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Breast cancer is one of the leading causes of female mortality in the world.\\rThis can be reduced when diagnoses are performed at the early stages of\\rprogression. Further, the efficiency of the process can be significantly\\rimproved with computer aided diagnosis. Deep learning based approaches have\\rbeen successfully applied to achieve this. One of the limiting factors for\\rtraining deep networks in a supervised manner is the dependency on large\\ramounts of expert annotated data. In reality, large amounts of unlabelled data\\rand only small amounts of expert annotated data are available. In such\\rscenarios, transfer learning approaches and self-supervised learning (SSL)\\rbased approaches can be leveraged. In this study, we propose a novel\\rself-supervision pretext task to train a convolutional neural network (CNN) and\\rextract domain specific features. This method was compared with deep features\\rextracted using pre-trained CNNs such as DenseNet-121 and ResNet-50 trained on\\rImageNet. Additionally, two types of patch-combination methods were introduced\\rand compared with majority voting. The methods were validated on the BACH\\rmicroscopy images dataset. Results indicated that the best performance of 99%\\rsensitivity was achieved for the deep features extracted using ResNet50 with\\rconcatenation of patch-level embedding. Preliminary results of SSL to extract\\rdomain specific features indicated that with just 15% of unlabelled data a high\\rsensitivity of 94% can be achieved for a four class classification of\\rmicroscopy images.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06073 ,  829kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06104 (*cross-listing*)\\rDate: Sat, 12 Feb 2022 17:07:17 GMT   (759kb)\\r\\rTitle: Semi-supervised Medical Image Segmentation via Geometry-aware\\r  Consistency Training\\rAuthors: Zihang Liu, Chunhui Zhao\\rCategories: eess.IV cs.CV\\rComments: 9 pages, 4 figures\\r\\\\\\\\\\r  The performance of supervised deep learning methods for medical image\\rsegmentation is often limited by the scarcity of labeled data. As a promising\\rresearch direction, semi-supervised learning addresses this dilemma by\\rleveraging unlabeled data information to assist the learning process. In this\\rpaper, a novel geometry-aware semi-supervised learning framework is proposed\\rfor medical image segmentation, which is a consistency-based method.\\rConsidering that the hard-to-segment regions are mainly located around the\\robject boundary, we introduce an auxiliary prediction task to learn the global\\rgeometric information. Based on the geometric constraint, the ambiguous\\rboundary regions are emphasized through an exponentially weighted strategy for\\rthe model training to better exploit both labeled and unlabeled data. In\\raddition, a dual-view network is designed to perform segmentation from\\rdifferent perspectives and reduce the prediction uncertainty. The proposed\\rmethod is evaluated on the public left atrium benchmark dataset and improves\\rfully supervised method by 8.7% in Dice with 10% labeled images, while 4.3%\\rwith 20% labeled images. Meanwhile, our framework outperforms six\\rstate-of-the-art semi-supervised segmentation methods.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06104 ,  759kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06109 (*cross-listing*)\\rDate: Sat, 12 Feb 2022 17:45:43 GMT   (369kb)\\r\\rTitle: Breast Cancer Detection using Histopathological Images\\rAuthors: Jitendra Maan, Harsh Maan\\rCategories: eess.IV cs.CV\\rComments: 6 pages, 10 figures, Published with International Journal of Computer\\r  Science Trends and Technology (IJCST)\\rJournal-ref: International Journal of Computer Science Trends and Technology\\r  (IJCST) V10(1):Page(53-58) Jan-Feb 2022. ISSN: 2347-8578.www.ijcstjournal.org\\r\\\\\\\\\\r  Cancer is one of the most common and fatal diseases in the world. Breast\\rcancer affects one in every eight women and one in every eight hundred men.\\rHence, our prime target should be early detection of cancer because the early\\rdetection of cancer can be helpful to cure cancer effectively. Therefore, we\\rpropose a saliency detection system with the help of advanced deep learning\\rtechniques, such that the machine will be taught to emulate actions of\\rpathologists for localization of diagnostically pertinent regions. We study\\ridentification of five diagnostic categories of breast cancer by training a CNN\\r(VGG16, ResNet architecture). We have used BreakHis dataset to train our model.\\rWe focus on both detection and classification of cancerous regions in\\rhistopathology images. The diagnostically relevant regions are salient. The\\rdetection system will be available as an open source web application which can\\rbe used by pathologists and medical institutions.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06109 ,  369kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06142 (*cross-listing*)\\rDate: Sat, 12 Feb 2022 21:02:45 GMT   (1490kb,D)\\r\\rTitle: Multi-task Deep Learning for Cerebrovascular Disease Classification and\\r  MRI-to-PET Translation\\rAuthors: Ramy Hussein, Moss Zhao, David Shin, Jia Guo, Kevin T. Chen, Rui D.\\r  Armindo, Guido Davidzon, Michael Moseley, and Greg Zaharchuk\\rCategories: eess.IV cs.CV cs.LG\\rComments: 7 pages, 6 figures\\r\\\\\\\\\\r  Accurate quantification of cerebral blood flow (CBF) is essential for the\\rdiagnosis and assessment of cerebrovascular diseases such as Moyamoya, carotid\\rstenosis, aneurysms, and stroke. Positron emission tomography (PET) is\\rcurrently regarded as the gold standard for the measurement of CBF in the human\\rbrain. PET imaging, however, is not widely available because of its prohibitive\\rcosts, use of ionizing radiation, and logistical challenges, which require a\\rco-localized cyclotron to deliver the 2 min half-life Oxygen-15 radioisotope.\\rMagnetic resonance imaging (MRI), in contrast, is more readily available and\\rdoes not involve ionizing radiation. In this study, we propose a multi-task\\rlearning framework for brain MRI-to-PET translation and disease diagnosis. The\\rproposed framework comprises two prime networks: (1) an attention-based 3D\\rencoder-decoder convolutional neural network (CNN) that synthesizes\\rhigh-quality PET CBF maps from multi-contrast MRI images, and (2) a multi-scale\\r3D CNN that identifies the brain disease corresponding to the input MRI images.\\rOur multi-task framework yields promising results on the task of MRI-to-PET\\rtranslation, achieving an average structural similarity index (SSIM) of 0.94\\rand peak signal-to-noise ratio (PSNR) of 38dB on a cohort of 120 subjects. In\\raddition, we show that integrating multiple MRI modalities can improve the\\rclinical diagnosis of brain diseases.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06142 ,  1490kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06165 (*cross-listing*)\\rDate: Sat, 12 Feb 2022 23:45:18 GMT   (17496kb,D)\\r\\rTitle: InfraredTags: Embedding Invisible AR Markers and Barcodes Using\\r  Low-Cost, Infrared-Based 3D Printing and Imaging Tools\\rAuthors: Mustafa Doga Dogan (1), Ahmad Taka (1), Michael Lu (1), Yunyi Zhu (1),\\r  Akshat Kumar (1), Aakar Gupta (2), Stefanie Mueller (1) ((1) MIT CSAIL,\\r  Cambridge, MA, USA, (2) Facebook Reality Labs, Redmond, WA, USA)\\rCategories: cs.HC cs.CV\\rComments: 12 pages, 10 figures. To appear in the Proceedings of the 2022 ACM\\r  Conference on Human Factors in Computing Systems\\rACM-class: H.5.0; H.5.2\\rDOI: 10.1145/3491102.3501951\\r\\\\\\\\\\r  Existing approaches for embedding unobtrusive tags inside 3D objects require\\reither complex fabrication or high-cost imaging equipment. We present\\rInfraredTags, which are 2D markers and barcodes imperceptible to the naked eye\\rthat can be 3D printed as part of objects, and detected rapidly by low-cost\\rnear-infrared cameras. We achieve this by printing objects from an\\rinfrared-transmitting filament, which infrared cameras can see through, and by\\rhaving air gaps inside for the tag's bits, which appear at a different\\rintensity in the infrared image.\\r  We built a user interface that facilitates the integration of common tags (QR\\rcodes, ArUco markers) with the object geometry to make them 3D printable as\\rInfraredTags. We also developed a low-cost infrared imaging module that\\raugments existing mobile devices and decodes tags using our image processing\\rpipeline. Our evaluation shows that the tags can be detected with little\\rnear-infrared illumination (0.2lux) and from distances as far as 250cm. We\\rdemonstrate how our method enables various applications, such as object\\rtracking and embedding metadata for augmented reality and tangible\\rinteractions.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06165 ,  17496kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06201 (*cross-listing*)\\rDate: Sun, 13 Feb 2022 04:23:12 GMT   (5960kb,D)\\r\\rTitle: Unsupervised Disentanglement with Tensor Product Representations on the\\r  Torus\\rAuthors: Michael Rotman, Amit Dekel, Shir Gur, Yaron Oz, Lior Wolf\\rCategories: cs.LG cs.CV\\rComments: Accepted to ICLR 2022\\r\\\\\\\\\\r  The current methods for learning representations with auto-encoders almost\\rexclusively employ vectors as the latent representations. In this work, we\\rpropose to employ a tensor product structure for this purpose. This way, the\\robtained representations are naturally disentangled. In contrast to the\\rconventional variations methods, which are targeted toward normally distributed\\rfeatures, the latent space in our representation is distributed uniformly over\\ra set of unit circles. We argue that the torus structure of the latent space\\rcaptures the generative factors effectively. We employ recent tools for\\rmeasuring unsupervised disentanglement, and in an extensive set of experiments\\rdemonstrate the advantage of our method in terms of disentanglement,\\rcompleteness, and informativeness. The code for our proposed method is\\ravailable at https://github.com/rotmanmi/Unsupervised-Disentanglement-Torus.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06201 ,  5960kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06250 (*cross-listing*)\\rDate: Sun, 13 Feb 2022 08:11:04 GMT   (1536kb)\\r\\rTitle: Privacy protection based on mask template\\rAuthors: Hao Wang (1), Yu Bai (2), Guangmin Sun (1), Jie Liu (1) ((1) Beijing\\r  University of Technology,(2) Beijing Friendship Hospital)\\rCategories: cs.CR cs.AI cs.CV\\r\\\\\\\\\\r  Powerful recognition algorithms are widely used in the Internet or important\\rmedical systems, which poses a serious threat to personal privacy. Although the\\rlaw provides for diversity protection, e.g. The General Data Protection\\rRegulation (GDPR) in Europe and Articles 1032 to 1039 of the civil code in\\rChina. However, as an important privacy disclosure event, biometric data is\\roften hidden, which is difficult for the owner to detect and trace to the\\rsource. Human biometrics generally exist in images. In order to avoid the\\rdisclosure of personal privacy, we should prevent unauthorized recognition\\ralgorithms from acquiring the real features of the original image.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06250 ,  1536kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06253 (*cross-listing*)\\rDate: Sun, 13 Feb 2022 08:26:28 GMT   (995kb)\\r\\rTitle: Autonomous Drone Swarm Navigation and Multi-target Tracking in 3D\\r  Environments with Dynamic Obstacles\\rAuthors: Suleman Qamar, Saddam Hussain Khan, Muhammad Arif Arshad, Maryam\\r  Qamar, and Asifullah Khan\\rCategories: cs.RO cs.AI cs.CV\\rComments: Pages: 19, Figures: 17, Tables: 8\\r\\\\\\\\\\r  Autonomous modeling of artificial swarms is necessary because manual creation\\ris a time intensive and complicated procedure which makes it impractical. An\\rautonomous approach employing deep reinforcement learning is presented in this\\rstudy for swarm navigation. In this approach, complex 3D environments with\\rstatic and dynamic obstacles and resistive forces (like linear drag, angular\\rdrag, and gravity) are modeled to track multiple dynamic targets. Moreover,\\rreward functions for robust swarm formation and target tracking are devised for\\rlearning complex swarm behaviors. Since the number of agents is not fixed and\\rhas only the partial observance of the environment, swarm formation and\\rnavigation become challenging. In this regard, the proposed strategy consists\\rof three main phases to tackle the aforementioned challenges: 1) A methodology\\rfor dynamic swarm management, 2) Avoiding obstacles, Finding the shortest path\\rtowards the targets, 3) Tracking the targets and Island modeling. The dynamic\\rswarm management phase translates basic sensory input to high level commands to\\renhance swarm navigation and decentralized setup while maintaining the swarms\\rsize fluctuations. While, in the island modeling, the swarm can split into\\rindividual subswarms according to the number of targets, conversely, these\\rsubswarms may join to form a single huge swarm, giving the swarm ability to\\rtrack multiple targets. Customized state of the art policy based deep\\rreinforcement learning algorithms are employed to achieve significant results.\\rThe promising results show that our proposed strategy enhances swarm navigation\\rand can track multiple static and dynamic targets in complex dynamic\\renvironments.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06253 ,  995kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06260 (*cross-listing*)\\rDate: Sun, 13 Feb 2022 08:47:01 GMT   (2996kb,D)\\r\\rTitle: LTSP: Long-Term Slice Propagation for Accurate Airway Segmentation\\rAuthors: Yangqian Wu, Minghui Zhang, Weihao Yu, Hao Zheng, Jiasheng Xu and Yun\\r  Gu\\rCategories: eess.IV cs.CV\\rComments: Accepted by IPCAI 2022\\r\\\\\\\\\\r  Purpose: Bronchoscopic intervention is a widely-used clinical technique for\\rpulmonary diseases, which requires an accurate and topological complete airway\\rmap for its localization and guidance. The airway map could be extracted from\\rchest computed tomography (CT) scans automatically by airway segmentation\\rmethods. Due to the complex tree-like structure of the airway, preserving its\\rtopology completeness while maintaining the segmentation accuracy is a\\rchallenging task.\\r  Methods: In this paper, a long-term slice propagation (LTSP) method is\\rproposed for accurate airway segmentation from pathological CT scans. We also\\rdesign a two-stage end-to-end segmentation framework utilizing the LTSP method\\rin the decoding process. Stage 1 is used to generate a coarse feature map by an\\rencoder-decoder architecture. Stage 2 is to adopt the proposed LTSP method for\\rexploiting the continuity information and enhancing the weak airway features in\\rthe coarse feature map. The final segmentation result is predicted from the\\rrefined feature map.\\r  Results: Extensive experiments were conducted to evaluate the performance of\\rthe proposed method on 70 clinical CT scans. The results demonstrate the\\rconsiderable improvements of the proposed method compared to some\\rstate-of-the-art methods as most breakages are eliminated and more tiny bronchi\\rare detected. The ablation studies further confirm the effectiveness of the\\rconstituents of the proposed method.\\r  Conclusion: Slice continuity information is beneficial to accurate airway\\rsegmentation. Furthermore, by propagating the long-term slice feature, the\\rairway topology connectivity is preserved with overall segmentation accuracy\\rmaintained.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06260 ,  2996kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06299 (*cross-listing*)\\rDate: Sun, 13 Feb 2022 12:46:19 GMT   (4516kb,D)\\r\\rTitle: Motion Sickness Modeling with Visual Vertical Estimation and Its\\r  Application to Autonomous Personal Mobility Vehicles\\rAuthors: Hailong Liu and Shota Inoue and Takahiro Wada\\rCategories: cs.HC cs.CV\\r\\\\\\\\\\r  Passengers (drivers) of level 3-5 autonomous personal mobility vehicles\\r(APMV) and cars can perform non-driving tasks, such as reading books and\\rsmartphones, while driving. It has been pointed out that such activities may\\rincrease motion sickness. Many studies have been conducted to build\\rcountermeasures, of which various computational motion sickness models have\\rbeen developed. Many of these are based on subjective vertical conflict (SVC)\\rtheory, which describes vertical changes in direction sensed by human sensory\\rorgans vs. those expected by the central nervous system. Such models are\\rexpected to be applied to autonomous driving scenarios. However, no current\\rcomputational model can integrate visual vertical information with vestibular\\rsensations.\\r  We proposed a 6 DoF SVC-VV model which add a visually perceived vertical\\rblock into a conventional six-degrees-of-freedom SVC model to predict VV\\rdirections from image data simulating the visual input of a human. Hence, a\\rsimple image-based VV estimation method is proposed.\\r  As the validation of the proposed model, this paper focuses on describing the\\rfact that the motion sickness increases as a passenger reads a book while using\\ran AMPV, assuming that visual vertical (VV) plays an important role. In the\\rstatic experiment, it is demonstrated that the estimated VV by the proposed\\rmethod accurately described the gravitational acceleration direction with a low\\rmean absolute deviation. In addition, the results of the driving experiment\\rusing an APMV demonstrated that the proposed 6 DoF SVC-VV model could describe\\rthat the increased motion sickness experienced when the VV and gravitational\\racceleration directions were different.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06299 ,  4516kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06344 (*cross-listing*)\\rDate: Sun, 13 Feb 2022 15:28:49 GMT   (1149kb)\\r\\rTitle: A Data Augmentation Method for Fully Automatic Brain Tumor Segmentation\\rAuthors: Yu Wang, Yarong Ji, Hongbing Xiao\\rCategories: eess.IV cs.CV cs.LG\\rComments: 15 pages, 7 figures, 4tables\\r\\\\\\\\\\r  Automatic segmentation of glioma and its subregions is of great significance\\rfor diagnosis, treatment and monitoring of disease. In this paper, an\\raugmentation method, called TensorMixup, was proposed and applied to the three\\rdimensional U-Net architecture for brain tumor segmentation. The main ideas\\rincluded that first, two image patches with size of 128 in three dimensions\\rwere selected according to glioma information of ground truth labels from the\\rmagnetic resonance imaging data of any two patients with the same modality.\\rNext, a tensor in which all elements were independently sampled from Beta\\rdistribution was used to mix the image patches. Then the tensor was mapped to a\\rmatrix which was used to mix the one-hot encoded labels of the above image\\rpatches. Therefore, a new image and its one-hot encoded label were synthesized.\\rFinally, the new data was used to train the model which could be used to\\rsegment glioma. The experimental results show that the mean accuracy of Dice\\rscores are 91.32%, 85.67%, and 82.20% respectively on the whole tumor, tumor\\rcore, and enhancing tumor segmentation, which proves that the proposed\\rTensorMixup is feasible and effective for brain tumor segmentation.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06344 ,  1149kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06366 (*cross-listing*)\\rDate: Sun, 13 Feb 2022 17:25:01 GMT   (6028kb,D)\\r\\rTitle: Learning Perspective Deformation in X-Ray Transmission Imaging\\rAuthors: Yixing Huang, Andreas Maier, Rainer Fietkau, Christoph Bert, Florian\\r  Putz\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  In cone-beam X-ray transmission imaging, due to the divergence of X-rays,\\rimaged structures with different depths have different magnification factors on\\ran X-ray detector, which results in perspective deformation. Perspective\\rdeformation causes difficulty in direct, accurate geometric assessments of\\ranatomical structures. In this work, to reduce perspective deformation in X-ray\\rimages acquired from regular cone-beam computed tomography (CBCT) systems, we\\rinvestigate on learning perspective deformation, i.e., converting perspective\\rprojections into orthogonal projections. Directly converting a single\\rperspective projection image into an orthogonal projection image is extremely\\rchallenging due to the lack of depth information. Therefore, we propose to\\rutilize one additional perspective projection, a complementary (180-degree) or\\rorthogonal (90-degree) view, to provide a certain degree of depth information.\\rFurthermore, learning perspective deformation in different spatial domains is\\rinvestigated. Our proposed method is evaluated on numerical spherical bead\\rphantoms as well as patients' chest and head X-ray data. The experiments on\\rnumerical bead phantom data demonstrate that learning perspective deformation\\rin polar coordinates has significant advantages over learning in Cartesian\\rcoordinates, as root-mean-square error (RMSE) decreases from 5.31 to 1.40,\\rwhile learning in log-polar coordinates has no further considerable improvement\\r(RMSE = 1.85). In addition, using a complementary view (RMSE = 1.40) is better\\rthan an orthogonal view (RMSE = 3.87). The experiments on patients' chest and\\rhead data demonstrate that learning perspective deformation using dual\\rcomplementary views is also applicable in anatomical X-ray data, allowing\\raccurate cardiothoracic ratio measurements in chest X-ray images and\\rcephalometric analysis in synthetic cephalograms from cone-beam X-ray\\rprojections.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06366 ,  6028kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06372 (*cross-listing*)\\rDate: Sun, 13 Feb 2022 17:44:33 GMT   (1045kb)\\r\\rTitle: A Survey of Deep Learning Techniques for the Analysis of COVID-19 and\\r  their usability for Detecting Omicron\\rAuthors: Asifullah Khan, Saddam Hussain Khan, Mahrukh Saif, Asiya Batool,\\r  Anabia Sohail and Muhammad Waleed Khan\\rCategories: eess.IV cs.CV\\rComments: Pages: 36, Figures: 7, Tables: 14\\r\\\\\\\\\\r  The Coronavirus (COVID-19) outbreak in December 2019 has become an ongoing\\rthreat to humans worldwide, creating a health crisis that infected millions of\\rlives, as well as devastating the global economy. Deep learning (DL) techniques\\rhave proved helpful in analysis and delineation of infectious regions in\\rradiological images in a timely manner. This paper makes an in-depth survey of\\rDL techniques and draws a taxonomy based on diagnostic strategies and learning\\rapproaches. DL techniques are systematically categorized into classification,\\rsegmentation, and multi-stage approaches for COVID-19 diagnosis at image and\\rregion level analysis. Each category includes pre-trained and custom-made\\rConvolutional Neural Network architectures for detecting COVID-19 infection in\\rradiographic imaging modalities; X-Ray, and Computer Tomography (CT).\\rFurthermore, a discussion is made on challenges in developing diagnostic\\rtechniques in pandemic, cross-platform interoperability, and examining imaging\\rmodality, in addition to reviewing methodologies and performance measures used\\rin these techniques. This survey provides an insight into promising areas of\\rresearch in DL for analyzing radiographic images and thus, may further\\raccelerate the research in designing of customized DL based diagnostic tools\\rfor effectively dealing with new variants of COVID-19 and emerging challenges.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06372 ,  1045kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06431 (*cross-listing*)\\rDate: Sun, 13 Feb 2022 22:40:46 GMT   (11400kb,D)\\r\\rTitle: AI can evolve without labels: self-evolving vision transformer for chest\\r  X-ray diagnosis through knowledge distillation\\rAuthors: Sangjoon Park, Gwanghyun Kim, Yujin Oh, Joon Beom Seo, Sang Min Lee,\\r  Jin Hwan Kim, Sungjun Moon, Jae-Kwang Lim, Chang Min Park, and Jong Chul Ye\\rCategories: eess.IV cs.CV cs.LG\\rComments: 24 pages\\r\\\\\\\\\\r  Although deep learning-based computer-aided diagnosis systems have recently\\rachieved expert-level performance, developing a robust deep learning model\\rrequires large, high-quality data with manual annotation, which is expensive to\\robtain. This situation poses the problem that the chest x-rays collected\\rannually in hospitals cannot be used due to the lack of manual labeling by\\rexperts, especially in deprived areas. To address this, here we present a novel\\rdeep learning framework that uses knowledge distillation through\\rself-supervised learning and self-training, which shows that the performance of\\rthe original model trained with a small number of labels can be gradually\\rimproved with more unlabeled data. Experimental results show that the proposed\\rframework maintains impressive robustness against a real-world environment and\\rhas general applicability to several diagnostic tasks such as tuberculosis,\\rpneumothorax, and COVID-19. Notably, we demonstrated that our model performs\\reven better than those trained with the same amount of labeled data. The\\rproposed framework has a great potential for medical imaging, where plenty of\\rdata is accumulated every year, but ground truth annotations are expensive to\\robtain.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06431 ,  11400kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06434 (*cross-listing*)\\rDate: Sun, 13 Feb 2022 23:15:53 GMT   (5067kb,D)\\r\\rTitle: Perception-Aware Perching on Powerlines with Multirotors\\rAuthors: Julio L. Paneque, Jose Ramiro Mart\\\\'inez de Dios, and An\\\\'ibal Ollero.\\r  Drew Hanover, Sihao Sun, \\\\'Angel Romero, and Davide Scaramuzza\\rCategories: cs.RO cs.CV\\rComments: IEEE Robotics and Automation Letters (2022)\\rDOI: 10.1109/LRA.2022.3145514\\r\\\\\\\\\\r  Multirotor aerial robots are becoming widely used for the inspection of\\rpowerlines. To enable continuous, robust inspection without human intervention,\\rthe robots must be able to perch on the powerlines to recharge their batteries.\\rHighly versatile perching capabilities are necessary to adapt to the variety of\\rconfigurations and constraints that are present in real powerline systems. This\\rpaper presents a novel perching trajectory generation framework that computes\\rperception-aware, collision-free, and dynamically-feasible maneuvers to guide\\rthe robot to the desired final state. Trajectory generation is achieved via\\rsolving a Nonlinear Programming problem using the Primal-Dual Interior Point\\rmethod. The problem considers the full dynamic model of the robot down to its\\rsingle rotor thrusts and minimizes the final pose and velocity errors while\\ravoiding collisions and maximizing the visibility of the powerline during the\\rmaneuver. The generated maneuvers consider both the perching and the posterior\\rrecovery trajectories. The framework adopts costs and constraints defined by\\refficient mathematical representations of powerlines, enabling online onboard\\rexecution in resource-constrained hardware. The method is validated on-board an\\ragile quadrotor conducting powerline inspection and various perching maneuvers\\rwith final pitch values of up to 180 degrees. The developed code is available\\ronline at: https://github.com/grvcPerception/pa_powerline_perching\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06434 ,  5067kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06458 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 02:14:50 GMT   (325kb)\\r\\rTitle: Faster hyperspectral image classification based on selective kernel\\r  mechanism using deep convolutional networks\\rAuthors: Guandong Li, Chunju Zhang\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Hyperspectral imagery is rich in spatial and spectral information. Using\\r3D-CNN can simultaneously acquire features of spatial and spectral dimensions\\rto facilitate classification of features, but hyperspectral image information\\rspectral dimensional information redundancy. The use of continuous 3D-CNN will\\rresult in a high amount of parameters, and the computational power requirements\\rof the device are high, and the training takes too long. This letter designed\\rthe Faster selective kernel mechanism network (FSKNet), FSKNet can balance this\\rproblem. It designs 3D-CNN and 2D-CNN conversion modules, using 3D-CNN to\\rcomplete feature extraction while reducing the dimensionality of spatial and\\rspectrum. However, such a model is not lightweight enough. In the converted\\r2D-CNN, a selective kernel mechanism is proposed, which allows each neuron to\\radjust the receptive field size based on the two-way input information scale.\\rUnder the Selective kernel mechanism, it mainly includes two components, se\\rmodule and variable convolution. Se acquires channel dimensional attention and\\rvariable convolution to obtain spatial dimension deformation information of\\rground objects. The model is more accurate, faster, and less computationally\\rintensive. FSKNet achieves high accuracy on the IN, UP, Salinas, and Botswana\\rdata sets with very small parameters.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06458 ,  325kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06465 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 02:52:53 GMT   (6287kb,D)\\r\\rTitle: A State-of-the-art Survey of U-Net in Microscopic Image Analysis: from\\r  Simple Usage to Structure Mortification\\rAuthors: Jian Wu, Wanli Liu, Chen Li, Tao Jiang, Islam Mohammad Shariful,\\r  Hongzan Sun, Xiaoqi Li, Xintong Li, Xinyu Huang, Marcin Grzegorzek\\rCategories: eess.IV cs.CV\\r\\\\\\\\\\r  Image analysis technology is used to solve the inadvertences of artificial\\rtraditional methods in disease, wastewater treatment, environmental change\\rmonitoring analysis and convolutional neural networks (CNN) play an important\\rrole in microscopic image analysis. An important step in detection, tracking,\\rmonitoring, feature extraction, modeling and analysis is image segmentation, in\\rwhich U-Net has increasingly applied in microscopic image segmentation. This\\rpaper comprehensively reviews the development history of U-Net, and analyzes\\rvarious research results of various segmentation methods since the emergence of\\rU-Net and conducts a comprehensive review of related papers. First, This paper\\rhas summarizes the improved methods of U-Net and then listed the existing\\rsignificances of image segmentation techniques and their improvements that has\\rintroduced over the years. Finally, focusing on the different improvement\\rstrategies of U-Net in different papers, the related work of each application\\rtarget is reviewed according to detailed technical categories to facilitate\\rfuture research. Researchers can clearly see the dynamics of transmission of\\rtechnological development and keep up with future trends in this\\rinterdisciplinary field.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06465 ,  6287kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06467 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 03:01:05 GMT   (1630kb,D)\\r\\rTitle: Optimizing Random Mixup with Gaussian Differential Privacy\\rAuthors: Donghao Li, Yang Cao and Yuan Yao\\rCategories: cs.LG cs.CV\\rComments: 28 pages, 9 figures\\r\\\\\\\\\\r  Differentially private data release receives rising attention in machine\\rlearning community. Recently, an algorithm called DPMix is proposed to release\\rhigh-dimensional data after a random mixup of degree $m$ with differential\\rprivacy. However, limited theoretical justifications are given about the sweet\\rspot $m$ phenomenon, and directly applying DPMix to image data suffers from\\rsevere loss of utility. In this paper, we revisit random mixup with recent\\rprogress on differential privacy. In theory, equipped with Gaussian\\rDifferential Privacy with Poisson subsampling, a tight closed form analysis is\\rpresented that enables a quantitative characterization of optimal mixup $m^*$\\rbased on linear regression models. In practice, mixup of features, extracted by\\rhandcraft or pre-trained neural networks such as self-supervised learning\\rwithout labels, is adopted to significantly boost the performance with privacy\\rprotection. We name it as Differentially Private Feature Mixup (DPFMix).\\rExperiments on MNIST, CIFAR10/100 are conducted to demonstrate its remarkable\\rutility improvement and protection against attacks.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06467 ,  1630kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06505 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 06:33:05 GMT   (3513kb,D)\\r\\rTitle: Opinions Vary? Diagnosis First!\\rAuthors: Junde Wu, Huihui Fang, Binghong Wu, Dalu Yang, Yehui Yang, Yanwu Xu\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  In medical image segmentation, images are usually annotated by several\\rdifferent clinical experts. This clinical routine helps to mitigate the\\rpersonal bias. However, Computer Vision models often assume there has a unique\\rground-truth for each of the instance. This research gap between Computer\\rVision and medical routine is commonly existed but less explored by the current\\rresearch.In this paper, we try to answer the following two questions: 1. How to\\rlearn an optimal combination of the multiple segmentation labels? and 2. How to\\restimate this segmentation mask from the raw image? We note that in clinical\\rpractice, the image segmentation mask usually exists as an auxiliary\\rinformation for disease diagnosis. Adhering to this mindset, we propose a\\rframework taking the diagnosis result as the gold standard, to estimate the\\rsegmentation mask upon the multi-rater segmentation labels, named DiFF\\r(Diagnosis First segmentation Framework).DiFF is implemented by two novelty\\rtechniques. First, DFSim (Diagnosis First Simulation of gold label) is learned\\ras an optimal combination of multi-rater segmentation labels for the disease\\rdiagnosis. Then, toward estimating DFSim mask from the raw image, we further\\rpropose T\\\\&G Module (Take and Give Module) to instill the diagnosis knowledge\\rinto the segmentation network. The experiments show that compared with commonly\\rused majority vote, the proposed DiFF is able to segment the masks with 6%\\rimprovement on diagnosis AUC score, which also outperforms various\\rstate-of-the-art multi-rater methods by a large margin.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06505 ,  3513kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06523 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 07:40:03 GMT   (9862kb,D)\\r\\rTitle: MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution\\r  Shifts and Training Conflicts\\rAuthors: Weixin Liang and James Zou\\rCategories: cs.LG cs.AI cs.CL cs.CV\\rComments: ICLR 2022. Code & data available at\\r  https://github.com/Weixin-Liang/MetaShift\\r\\\\\\\\\\r  Understanding the performance of machine learning models across diverse data\\rdistributions is critically important for reliable applications. Motivated by\\rthis, there is a growing focus on curating benchmark datasets that capture\\rdistribution shifts. While valuable, the existing benchmarks are limited in\\rthat many of them only contain a small number of shifts and they lack\\rsystematic annotation about what is different across different shifts. We\\rpresent MetaShift--a collection of 12,868 sets of natural images across 410\\rclasses--to address this challenge. We leverage the natural heterogeneity of\\rVisual Genome and its annotations to construct MetaShift. The key construction\\ridea is to cluster images using its metadata, which provides context for each\\rimage (e.g. cats with cars or cats in bathroom) that represent distinct\\rdata distributions. MetaShift has two important benefits: first, it contains\\rorders of magnitude more natural data shifts than previously available. Second,\\rit provides explicit explanations of what is unique about each of its data sets\\rand a distance score that measures the amount of distribution shift between any\\rtwo of its data sets. We demonstrate the utility of MetaShift in benchmarking\\rseveral recent proposals for training models to be robust to data shifts. We\\rfind that the simple empirical risk minimization performs the best when shifts\\rare moderate and no method had a systematic advantage for large shifts. We also\\rshow how MetaShift can help to visualize conflicts between data subsets during\\rmodel training.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06523 ,  9862kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06574 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 09:36:50 GMT   (1276kb,D)\\r\\rTitle: I-Tuning: Tuning Language Models with Image for Caption Generation\\rAuthors: Ziyang Luo, Yadong Xi, Rongsheng Zhang, Jing Ma\\rCategories: cs.CL cs.CV\\rComments: Work in progress\\r\\\\\\\\\\r  Recently, tuning the pre-trained language model (PLM) in a\\rparameter-efficient manner becomes a popular topic in the natural language\\rprocessing area. However, most of them focus on tuning the PLM with the\\rtext-only information. In this work, we propose a new perspective to tune the\\rfrozen PLM with images for caption generation. We denote our method as\\rI-Tuning, which can automatically filter the vision information from images to\\radjust the output hidden states of PLM. Evaluating on the image captioning\\rtasks (MSCOCO and Flickr30k Captioning), our method achieves comparable or even\\rbetter performance than the previous models which have 2-4 times more trainable\\rparameters and/or consume a large amount of cross-modal pre-training data.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06574 ,  1276kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06590 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 10:22:10 GMT   (10814kb)\\r\\rTitle: A Pragmatic Machine Learning Approach to Quantify Tumor Infiltrating\\r  Lymphocytes in Whole Slide Images\\rAuthors: Nikita Shvetsov, Morten Gr{\\\\o}nnesby, Edvard Pedersen, Kajsa\\r  M{\\\\o}llersen, Lill-Tove Rasmussen Busund, Ruth Schwienbacher, Lars Ailo\\r  Bongo, Thomas K. Kilvaer\\rCategories: eess.IV cs.CV q-bio.QM\\rComments: 19 pages, 5 figures, 2 tables, 11 supplementary pages\\rMSC-class: 68T07\\rACM-class: I.4.6; I.4.9; J.3\\r\\\\\\\\\\r  Increased levels of tumor infiltrating lymphocytes (TILs) in cancer tissue\\rindicate favourable outcomes in many types of cancer. Manual quantification of\\rimmune cells is inaccurate and time consuming for pathologists. Our aim is to\\rleverage a computational solution to automatically quantify TILs in whole slide\\rimages (WSIs) of standard diagnostic haematoxylin and eosin stained sections\\r(H&E slides) from lung cancer patients. Our approach is to transfer an open\\rsource machine learning method for segmentation and classification of nuclei in\\rH&E slides trained on public data to TIL quantification without manual labeling\\rof our data. Our results show that additional augmentation improves model\\rtransferability when training on few samples/limited tissue types. Models\\rtrained with sufficient samples/tissue types do not benefit from our additional\\raugmentation policy. Further, the resulting TIL quantification correlates to\\rpatient prognosis and compares favorably to the current state-of-the-art method\\rfor immune cell detection in non-small lung cancer (current standard CD8 cells\\rin DAB stained TMAs HR 0.34 95% CI 0.17-0.68 vs TILs in HE WSIs: HoVer-Net\\rPanNuke Aug Model HR 0.30 95% CI 0.15-0.60, HoVer-Net MoNuSAC Aug model HR 0.27\\r95% CI 0.14-0.53). Moreover, we implemented a cloud based system to train,\\rdeploy and visually inspect machine learning based annotation for H&E slides.\\rOur pragmatic approach bridges the gap between machine learning research,\\rtranslational clinical research and clinical implementation. However,\\rvalidation in prospective studies is needed to assert that the method works in\\ra clinical setting.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06590 ,  10814kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06599 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 10:40:51 GMT   (2203kb,D)\\r\\rTitle: Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in\\r  First Trimester 3D Ultrasound\\rAuthors: W.A.P. Bastiaansen, M. Rousian, R.P.M. Steegers-Theunissen, W.J.\\r  Niessen, A.H.J. Koning, S. Klein\\rCategories: eess.IV cs.CV cs.LG\\rComments: submitted to Melba (The Journal of Machine Learning for Biomedical\\r  Imaging)\\r\\\\\\\\\\r  Segmentation and spatial alignment of ultrasound (US) imaging data acquired\\rin the in first trimester are crucial for monitoring human embryonic growth and\\rdevelopment throughout this crucial period of life. Current approaches are\\reither manual or semi-automatic and are therefore very time-consuming and prone\\rto errors. To automate these tasks, we propose a multi-atlas framework for\\rautomatic segmentation and spatial alignment of the embryo using deep learning\\rwith minimal supervision. Our framework learns to register the embryo to an\\ratlas, which consists of the US images acquired at a range of gestational age\\r(GA), segmented and spatially aligned to a predefined standard orientation.\\r>From this, we can derive the segmentation of the embryo and put the embryo in\\rstandard orientation. US images acquired at 8+0 till 12+6 weeks GA were used\\rand eight pregnancies were selected as atlas images. We evaluated different\\rfusion strategies to incorporate multiple atlases: 1) training the framework\\rusing atlas images from a single pregnancy, 2) training the framework with data\\rof all available atlases and 3) ensembling of the frameworks trained per\\rpregnancy. To evaluate the performance, we calculated the Dice score over the\\rtest set. We found that training the framework using all available atlases\\routperformed ensembling and gave similar results compared to the best of all\\rframeworks trained on a single subject. Furthermore, we found that selecting\\rimages from the four atlases closest in GA out of all available atlases,\\rregardless of the individual quality, gave the best results with a median Dice\\rscore of 0.72. We conclude that our framework can accurately segment and\\rspatially align the embryo in first trimester 3D US images and is robust for\\rthe variation in quality that existed in the available atlases. Our code is\\rpublicly available at: https://github.com/wapbastiaansen/multi-atlas-seg-reg.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06599 ,  2203kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06626 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 11:27:27 GMT   (295kb,D)\\r\\rTitle: MuZero with Self-competition for Rate Control in VP9 Video Compression\\rAuthors: Amol Mandhane, Anton Zhernov, Maribeth Rauh, Chenjie Gu, Miaosen Wang,\\r  Flora Xue, Wendy Shang, Derek Pang, Rene Claus, Ching-Han Chiang, Cheng Chen,\\r  Jingning Han, Angie Chen, Daniel J. Mankowitz, Jackson Broshear, Julian\\r  Schrittwieser, Thomas Hubert, Oriol Vinyals, Timothy Mann\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\\\r  Video streaming usage has seen a significant rise as entertainment,\\reducation, and business increasingly rely on online video. Optimizing video\\rcompression has the potential to increase access and quality of content to\\rusers, and reduce energy use and costs overall. In this paper, we present an\\rapplication of the MuZero algorithm to the challenge of video compression.\\rSpecifically, we target the problem of learning a rate control policy to select\\rthe quantization parameters (QP) in the encoding process of libvpx, an open\\rsource VP9 video compression library widely used by popular video-on-demand\\r(VOD) services. We treat this as a sequential decision making problem to\\rmaximize the video quality with an episodic constraint imposed by the target\\rbitrate. Notably, we introduce a novel self-competition based reward mechanism\\rto solve constrained RL with variable constraint satisfaction difficulty, which\\ris challenging for existing constrained RL methods. We demonstrate that the\\rMuZero-based rate control achieves an average 6.28% reduction in size of the\\rcompressed videos for the same delivered video quality level (measured as PSNR\\rBD-rate) compared to libvpx's two-pass VBR rate control policy, while having\\rbetter constraint satisfaction behavior.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06626 ,  295kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06675 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 13:00:31 GMT   (9442kb,D)\\r\\rTitle: Can Machines Help Us Answering Question 16 in Datasheets, and In Turn\\r  Reflecting on Inappropriate Content?\\rAuthors: Patrick Schramowski, Christopher Tauchmann, and Kristian Kersting\\rCategories: cs.AI cs.CV cs.CY\\rComments: arXiv admin note: text overlap with arXiv:2110.04222\\r\\\\\\\\\\r  Large datasets underlying much of current machine learning raise serious\\rissues concerning inappropriate content such as offensive, insulting,\\rthreatening, or might otherwise cause anxiety. This calls for increased dataset\\rdocumentation, e.g., using datasheets. They, among other topics, encourage to\\rreflect on the composition of the datasets. So far, this documentation,\\rhowever, is done manually and therefore can be tedious and error-prone,\\respecially for large image datasets. Here we ask the arguably circular\\rquestion of whether a machine can help us reflect on inappropriate content,\\ranswering Question 16 in Datasheets. To this end, we propose to use the\\rinformation stored in pre-trained transformer models to assist us in the\\rdocumentation process. Specifically, prompt-tuning based on a dataset of\\rsocio-moral values steers CLIP to identify potentially inappropriate content,\\rtherefore reducing human labor. We then document the inappropriate images found\\rusing word clouds, based on captions generated using a vision-language model.\\rThe documentations of two popular, large-scale computer vision datasets --\\rImageNet and OpenImages -- produced this way suggest that machines can indeed\\rhelp dataset creators to answer Question 16 on inappropriate image content.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06675 ,  9442kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06707 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 13:58:13 GMT   (4964kb,D)\\r\\rTitle: Spiking Cochlea with System-level Local Automatic Gain Control\\rAuthors: Ilya Kiselev, Chang Gao, Shih-Chii Liu\\rCategories: eess.SP cs.CV cs.LG cs.SD cs.SY eess.AS eess.SY\\rComments: Accepted for publication at the IEEE Transactions on Circuits and\\r  Systems I - Regular Papers, 2022\\rDOI: 10.1109/TCSI.2022.3150165\\r\\\\\\\\\\r  Including local automatic gain control (AGC) circuitry into a silicon cochlea\\rdesign has been challenging because of transistor mismatch and model\\rcomplexity. To address this, we present an alternative system-level algorithm\\rthat implements channel-specific AGC in a silicon spiking cochlea by measuring\\rthe output spike activity of individual channels. The bandpass filter gain of a\\rchannel is adapted dynamically to the input amplitude so that the average\\routput spike rate stays within a defined range. Because this AGC mechanism only\\rneeds counting and adding operations, it can be implemented at low hardware\\rcost in a future design. We evaluate the impact of the local AGC algorithm on a\\rclassification task where the input signal varies over 32 dB input range. Two\\rclassifier types receiving cochlea spike features were tested on a speech\\rversus noise classification task. The logistic regression classifier achieves\\ran average of 6% improvement and 40.8% relative improvement in accuracy when\\rthe AGC is enabled. The deep neural network classifier shows a similar\\rimprovement for the AGC case and achieves a higher mean accuracy of 96%\\rcompared to the best accuracy of 91% from the logistic regression classifier.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06707 ,  4964kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06876 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 17:06:32 GMT   (600kb,D)\\r\\rTitle: A Graphical Approach For Brain Haemorrhage Segmentation\\rAuthors: Dr. Ninad Mehendale, Pragya Gupta, Nishant Rajadhyaksha, Ansh Dagha,\\r  Mihir Hundiwala, Aditi Paretkar, Sakshi Chavan, and Tanmay Mishra\\rCategories: eess.IV cs.CV cs.LG\\rComments: 10 pages 6 figures 3 tables preprint\\r\\\\\\\\\\r  Haemorrhaging of the brain is the leading cause of death in people between\\rthe ages of 15 and 24 and the third leading cause of death in people older than\\rthat. Computed tomography (CT) is an imaging modality used to diagnose\\rneurological emergencies, including stroke and traumatic brain injury. Recent\\radvances in Deep Learning and Image Processing have utilised different\\rmodalities like CT scans to help automate the detection and segmentation of\\rbrain haemorrhage occurrences. In this paper, we propose a novel implementation\\rof an architecture consisting of traditional Convolutional Neural Networks(CNN)\\ralong with Graph Neural Networks(GNN) to produce a holistic model for the task\\rof brain haemorrhage segmentation.GNNs work on the principle of neighbourhood\\raggregation thus providing a reliable estimate of global structures present in\\rimages. GNNs work with few layers thus in turn requiring fewer parameters to\\rwork with. We were able to achieve a dice coefficient score of around 0.81 with\\rlimited data with our implementation.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06876 ,  600kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06914 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 18:09:43 GMT   (2410kb,D)\\r\\rTitle: A Generic Self-Supervised Framework of Learning Invariant Discriminative\\r  Features\\rAuthors: Foivos Ntelemis, Yaochu Jin, Spencer A. Thomas\\rCategories: cs.LG cs.CV\\r\\\\\\\\\\r  Self-supervised learning (SSL) has become a popular method for generating\\rinvariant representations without the need for human annotations. Nonetheless,\\rthe desired invariant representation is achieved by utilising prior online\\rtransformation functions on the input data. As a result, each SSL framework is\\rcustomised for a particular data type, e.g., visual data, and further\\rmodifications are required if it is used for other dataset types. On the other\\rhand, autoencoder (AE), which is a generic and widely applicable framework,\\rmainly focuses on dimension reduction and is not suited for learning invariant\\rrepresentation. This paper proposes a generic SSL framework based on a\\rconstrained self-labelling assignment process that prevents degenerate\\rsolutions. Specifically, the prior transformation functions are replaced with a\\rself-transformation mechanism, derived through an unsupervised training process\\rof adversarial training, for imposing invariant representations. Via the\\rself-transformation mechanism, pairs of augmented instances can be generated\\rfrom the same input data. Finally, a training objective based on contrastive\\rlearning is designed by leveraging both the self-labelling assignment and the\\rself-transformation mechanism. Despite the fact that the self-transformation\\rprocess is very generic, the proposed training strategy outperforms a majority\\rof state-of-the-art representation learning methods based on AE structures. To\\rvalidate the performance of our method, we conduct experiments on four types of\\rdata, namely visual, audio, text, and mass spectrometry data, and compare them\\rin terms of four quantitative metrics. Our comparison results indicate that the\\rproposed method demonstrate robustness and successfully identify patterns\\rwithin the datasets.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06914 ,  2410kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.06924 (*cross-listing*)\\rDate: Mon, 14 Feb 2022 18:33:12 GMT   (10135kb,D)\\r\\rTitle: Do Gradient Inversion Attacks Make Federated Learning Unsafe?\\rAuthors: Ali Hatamizadeh, Hongxu Yin, Pavlo Molchanov, Andriy Myronenko, Wenqi\\r  Li, Prerna Dogra, Andrew Feng, Mona G. Flores, Jan Kautz, Daguang Xu, Holger\\r  R. Roth\\rCategories: cs.LG cs.CR cs.CV cs.DC\\rComments: Improved and reformatted version of\\r  https://www.researchsquare.com/article/rs-1147182/v2\\r\\\\\\\\\\r  Federated learning (FL) allows the collaborative training of AI models\\rwithout needing to share raw data. This capability makes it especially\\rinteresting for healthcare applications where patient and data privacy is of\\rutmost concern. However, recent works on the inversion of deep neural networks\\rfrom model gradients raised concerns about the security of FL in preventing the\\rleakage of training data. In this work, we show that these attacks presented in\\rthe literature are impractical in real FL use-cases and provide a new baseline\\rattack that works for more realistic scenarios where the clients' training\\rinvolves updating the Batch Normalization (BN) statistics. Furthermore, we\\rpresent new ways to measure and visualize potential data leakage in FL. Our\\rwork is a step towards establishing reproducible methods of measuring data\\rleakage in FL and could help determine the optimal tradeoffs between\\rprivacy-preserving techniques, such as differential privacy, and model accuracy\\rbased on quantifiable metrics.\\r\\\\\\\\ ( https://arxiv.org/abs/2202.06924 ,  10135kb)\\r%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:1806.11230\\rreplaced with revised version Sun, 13 Feb 2022 04:11:52 GMT   (40882kb,D)\\r\\rTitle: Human Action Recognition and Prediction: A Survey\\rAuthors: Yu Kong, Yun Fu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/1806.11230 ,  40882kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:1810.11641\\rreplaced with revised version Sat, 12 Feb 2022 11:18:51 GMT   (2440kb,D)\\r\\rTitle: Cross-Modal Distillation for RGB-Depth Person Re-Identification\\rAuthors: Frank Hafner, Amran Bhuiyan, Julian F. P. Kooij, Eric Granger\\rCategories: cs.CV eess.IV\\rJournal-ref: Computer Vision and Image Understanding, 103352 (2022)\\r\\\\\\\\ ( https://arxiv.org/abs/1810.11641 ,  2440kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:1912.11616\\rreplaced with revised version Mon, 14 Feb 2022 05:08:56 GMT   (8703kb,D)\\r\\rTitle: Concise and Effective Network for 3D Human Modeling from Orthogonal\\r  Silhouettes\\rAuthors: Bin Liu, Xiuping Liu, Zhixin Yang, Charlie C.L. Wang\\rCategories: cs.CV cs.GR eess.IV\\rComments: 13 pages, 15 figures\\r\\\\\\\\ ( https://arxiv.org/abs/1912.11616 ,  8703kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2001.09284\\rreplaced with revised version Sun, 13 Feb 2022 14:58:42 GMT   (6375kb,D)\\r\\rTitle: Towards High Performance Low Complexity Calibration in Appearance Based\\r  Gaze Estimation\\rAuthors: Zhaokang Chen and Bertram E. Shi\\rCategories: cs.CV\\rComments: Accepted by IEEE Transactions on Pattern Analysis and Machine\\r  Intelligence (TPAMI)\\rDOI: 10.1109/TPAMI.2022.3148386\\r\\\\\\\\ ( https://arxiv.org/abs/2001.09284 ,  6375kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2006.10724\\rreplaced with revised version Sun, 13 Feb 2022 09:12:10 GMT   (2417kb,D)\\r\\rTitle: Cyclic Differentiable Architecture Search\\rAuthors: Hongyuan Yu, Houwen Peng, Yan Huang, Jianlong Fu, Hao Du, Liang Wang,\\r  Haibin Ling\\rCategories: cs.CV\\rComments: Accepted by IEEE Transactions on Pattern Analysis and Machine\\r  Intelligence (TPAMI)\\r\\\\\\\\ ( https://arxiv.org/abs/2006.10724 ,  2417kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2012.06737\\rreplaced with revised version Mon, 14 Feb 2022 04:22:41 GMT   (2660kb,D)\\r\\rTitle: Computer Vision and Normalizing Flow-Based Defect Detection\\rAuthors: Zijian Kuang, Xinran Tie, Lihang Ying, Shi Jin\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2012.06737 ,  2660kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2102.06386\\rreplaced with revised version Mon, 14 Feb 2022 06:34:29 GMT   (16938kb,D)\\r\\rTitle: Multi-source Pseudo-label Learning of Semantic Segmentation for the\\r  Scene Recognition of Agricultural Mobile Robots\\rAuthors: Shigemichi Matsuzaki, Jun Miura and Hiroaki Masuzawa\\rCategories: cs.CV cs.RO\\rComments: Submitted to Advanced Robotics\\r\\\\\\\\ ( https://arxiv.org/abs/2102.06386 ,  16938kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2102.09546\\rreplaced with revised version Sun, 13 Feb 2022 20:30:25 GMT   (2157kb,D)\\r\\rTitle: Deep Gait Recognition: A Survey\\rAuthors: Alireza Sepas-Moghaddam, Ali Etemad\\rCategories: cs.CV\\rComments: Accepted to IEEE Transactions on Pattern Analysis and Machine\\r  Intelligence (T-PAMI)\\r\\\\\\\\ ( https://arxiv.org/abs/2102.09546 ,  2157kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2102.10543\\rreplaced with revised version Mon, 14 Feb 2022 11:39:53 GMT   (47553kb,D)\\r\\rTitle: Learning Disentangled Representation by Exploiting Pretrained Generative\\r  Models: A Contrastive Learning View\\rAuthors: Xuanchi Ren, Tao Yang, Yuwang Wang, Wenjun Zeng\\rCategories: cs.CV cs.AI cs.LG\\rComments: Accepted to ICLR 2022. Source code: https://github.com/xrenaa/DisCo\\r\\\\\\\\ ( https://arxiv.org/abs/2102.10543 ,  47553kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2103.16565\\rreplaced with revised version Mon, 14 Feb 2022 17:23:46 GMT   (1026kb,D)\\r\\rTitle: Learning Representational Invariances for Data-Efficient Action\\r  Recognition\\rAuthors: Yuliang Zou, Jinwoo Choi, Qitong Wang, Jia-Bin Huang\\rCategories: cs.CV\\rComments: Under review at CVIU. Project page:\\r  https://yuliang.vision/video-data-aug\\r\\\\\\\\ ( https://arxiv.org/abs/2103.16565 ,  1026kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2106.00186\\rreplaced with revised version Mon, 14 Feb 2022 12:22:59 GMT   (28868kb,D)\\r\\rTitle: Towards Real-time and Light-weight Line Segment Detection\\rAuthors: Geonmo Gu, Byungsoo Ko, SeoungHyun Go, Sung-Hyun Lee, Jingeun Lee,\\r  Minchul Shin\\rCategories: cs.CV cs.LG\\rComments: Accepted by AAAI2022\\r\\\\\\\\ ( https://arxiv.org/abs/2106.00186 ,  28868kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2106.12011\\rreplaced with revised version Sat, 12 Feb 2022 11:35:47 GMT   (423kb,D)\\r\\rTitle: P2T: Pyramid Pooling Transformer for Scene Understanding\\rAuthors: Yu-Huan Wu, Yun Liu, Xin Zhan, Ming-Ming Cheng\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2106.12011 ,  423kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2107.02041\\rreplaced with revised version Sat, 12 Feb 2022 12:30:08 GMT   (3251kb,D)\\r\\rTitle: No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh\\r  Models\\rAuthors: Zicheng Zhang, Wei Sun, Xiongkuo Min, Tao Wang, Wei Lu, and Guangtao\\r  Zhai\\rCategories: cs.CV cs.GR eess.IV\\r\\\\\\\\ ( https://arxiv.org/abs/2107.02041 ,  3251kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2107.11049\\rreplaced with revised version Mon, 14 Feb 2022 07:27:04 GMT   (4182kb,D)\\r\\rTitle: MCDAL: Maximum Classifier Discrepancy for Active Learning\\rAuthors: Jae Won Cho, Dong-Jin Kim, Yunjae Jung, In So Kweon\\rCategories: cs.CV cs.AI cs.LG\\rComments: 11 pages, Accepted to IEEE-TNNLS\\r\\\\\\\\ ( https://arxiv.org/abs/2107.11049 ,  4182kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2108.03443\\rreplaced with revised version Sun, 13 Feb 2022 17:21:14 GMT   (58469kb,D)\\r\\rTitle: NODEO: A Neural Ordinary Differential Equation Based Optimization\\r  Framework for Deformable Image Registration\\rAuthors: Yifan Wu, Tom Z.Jiahao, Jiancong Wang, Paul A.Yushkevich, M.Ani Hsieh,\\r  James C.Gee\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2108.03443 ,  58469kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2108.06932\\rreplaced with revised version Sun, 13 Feb 2022 11:27:55 GMT   (1863kb,D)\\r\\rTitle: Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers\\rAuthors: Bo Dong, Wenhai Wang, Jinpeng Li, Deng-Ping Fan\\rCategories: cs.CV\\rComments: Technical Report\\r\\\\\\\\ ( https://arxiv.org/abs/2108.06932 ,  1863kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2109.02167\\rreplaced with revised version Sat, 12 Feb 2022 04:48:00 GMT   (18589kb,D)\\r\\rTitle: Robust Attentive Deep Neural Network for Exposing GAN-generated Faces\\rAuthors: Hui Guo, Shu Hu, Xin Wang, Ming-Ching Chang, Siwei Lyu\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2109.02167 ,  18589kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2109.04468\\rreplaced with revised version Mon, 14 Feb 2022 17:16:53 GMT   (15240kb,D)\\r\\rTitle: Leveraging Local Domains for Image-to-Image Translation\\rAuthors: Anthony Dell'Eva, Fabio Pizzati, Massimo Bertozzi, Raoul de Charette\\rCategories: cs.CV cs.AI cs.LG cs.RO\\rComments: VISAPP 2022 Best Paper Award\\r\\\\\\\\ ( https://arxiv.org/abs/2109.04468 ,  15240kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2110.03220\\rreplaced with revised version Mon, 14 Feb 2022 10:55:19 GMT   (15144kb,D)\\r\\rTitle: Gradient Step Denoiser for convergent Plug-and-Play\\rAuthors: Samuel Hurault, Arthur Leclaire, Nicolas Papadakis\\rCategories: cs.CV eess.IV math.OC\\r\\\\\\\\ ( https://arxiv.org/abs/2110.03220 ,  15144kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2110.12978\\rreplaced with revised version Sat, 12 Feb 2022 05:55:43 GMT   (3227kb,D)\\r\\rTitle: MoDeRNN: Towards Fine-grained Motion Details for Spatiotemporal\\r  Predictive Learning\\rAuthors: Zenghao Chai, Zhengzhuo Xu, Chun Yuan\\rCategories: cs.CV\\rComments: Accepted at ICASSP 2022\\r\\\\\\\\ ( https://arxiv.org/abs/2110.12978 ,  3227kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2111.05080\\rreplaced with revised version Mon, 14 Feb 2022 01:28:45 GMT   (28092kb,D)\\r\\rTitle: Residual Quantity in Percentage of Factory Machines Using Computer\\r  Vision and Mathematical Methods\\rAuthors: Seunghyeon Kim, Jihoon Ryoo, Dongyeob Lee, Youngho Kim\\rCategories: cs.CV\\rComments: 4 pages, 13 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2111.05080 ,  28092kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2111.05170\\rreplaced with revised version Sat, 12 Feb 2022 08:32:47 GMT   (1296kb)\\r\\rTitle: Exploiting Robust Unsupervised Video Person Re-identification\\rAuthors: Xianghao Zang, Ge Li, Wei Gao, Xiujun Shu\\rCategories: cs.CV\\rComments: Preprint version; Accepted by IET Image Processing\\rJournal-ref: IET Image Processing 2022\\rDOI: 10.1049/ipr2.12380\\r\\\\\\\\ ( https://arxiv.org/abs/2111.05170 ,  1296kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2112.03603\\rreplaced with revised version Mon, 14 Feb 2022 04:19:45 GMT   (1506kb,D)\\r\\rTitle: Handwritten Mathematical Expression Recognition via Attention\\r  Aggregation based Bi-directional Mutual Learning\\rAuthors: Xiaohang Bian, Bo Qin, Xiaozhe Xin, Jianwu Li, Xuefeng Su, Yanfeng\\r  Wang\\rCategories: cs.CV cs.AI cs.LG\\rComments: 9 pages,5 figures, have been accepted in AAAI 2022\\rJournal-ref: AAAI 2022\\r\\\\\\\\ ( https://arxiv.org/abs/2112.03603 ,  1506kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2112.04283\\rreplaced with revised version Mon, 14 Feb 2022 01:27:02 GMT   (3283kb,D)\\r\\rTitle: Adverse Weather Image Translation with Asymmetric and Uncertainty-aware\\r  GAN\\rAuthors: Jeong-gi Kwak, Youngsaeng Jin, Yuanming Li, Dongsik Yoon, Donghyeon\\r  Kim, Hanseok Ko\\rCategories: cs.CV cs.GR\\rComments: BMVC 2021, ver.2 including supplementary material / codes are\\r  available in here: https://github.com/jgkwak95/AU-GAN\\r\\\\\\\\ ( https://arxiv.org/abs/2112.04283 ,  3283kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2112.06406\\rreplaced with revised version Sat, 12 Feb 2022 10:58:36 GMT   (9162kb,D)\\r\\rTitle: Hybrid Atlas Building with Deep Registration Priors\\rAuthors: Nian Wu, Jian Wang, Miaomiao Zhang, Guixu Zhang, Yaxin Peng and\\r  Chaomin Shen\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2112.06406 ,  9162kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.04756\\rreplaced with revised version Sat, 12 Feb 2022 23:21:53 GMT   (1303kb)\\r\\rTitle: Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity\\r  Background Subtraction\\rAuthors: Tianya Zhang and Peter J. Jin\\rCategories: cs.CV eess.SP\\r\\\\\\\\ ( https://arxiv.org/abs/2201.04756 ,  1303kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.06220\\rreplaced with revised version Sun, 13 Feb 2022 15:49:33 GMT   (2181kb)\\r\\rTitle: Face Detection in Extreme Conditions: A Machine-learning Approach\\rAuthors: Sameer Aqib Hashmi\\rCategories: cs.CV cs.AI\\rComments: 6 pages, 9 figures\\r\\\\\\\\ ( https://arxiv.org/abs/2201.06220 ,  2181kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.09168\\rreplaced with revised version Mon, 14 Feb 2022 07:45:42 GMT   (1717kb,D)\\r\\rTitle: Reading-strategy Inspired Visual Representation Learning for\\r  Text-to-Video Retrieval\\rAuthors: Jianfeng Dong, Yabing Wang, Xianke Chen, Xiaoye Qu, Xirong Li, Yuan\\r  He, Xun Wang\\rCategories: cs.CV cs.AI cs.MM\\rComments: Accepted by IEEE Transactions on Circuits and Systems for Video\\r  Technology. Code is available at https://github.com/LiJiaBei-7/rivrl\\rDOI: 10.1109/TCSVT.2022.3150959\\r\\\\\\\\ ( https://arxiv.org/abs/2201.09168 ,  1717kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.10448\\rreplaced with revised version Sun, 13 Feb 2022 19:41:52 GMT   (17946kb,D)\\r\\rTitle: How Low Can We Go? Pixel Annotation for Semantic Segmentation\\rAuthors: Daniel Kigli, Ariel Shamir, Shai Avidan\\rCategories: cs.CV\\rComments: Paper and Supplementary\\r\\\\\\\\ ( https://arxiv.org/abs/2201.10448 ,  17946kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.00158\\rreplaced with revised version Sun, 13 Feb 2022 08:27:49 GMT   (2784kb,D)\\r\\rTitle: Learning-Based Framework for Camera Calibration with Distortion\\r  Correction and High Precision Feature Detection\\rAuthors: Yesheng Zhang, Xu Zhao and Dahong Qian\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2202.00158 ,  2784kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.01829\\rreplaced with revised version Sun, 13 Feb 2022 18:37:26 GMT   (40837kb,D)\\r\\rTitle: HRBF-Fusion: Accurate 3D reconstruction from RGB-D data using on-the-fly\\r  implicits\\rAuthors: Yabin Xu and Liangliang Nan and Laishui Zhou and Jun Wang and Charlie\\r  C.L. Wang\\rCategories: cs.CV\\rDOI: 10.1145/3516521\\r\\\\\\\\ ( https://arxiv.org/abs/2202.01829 ,  40837kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.02980\\rreplaced with revised version Sat, 12 Feb 2022 19:43:31 GMT   (7875kb,D)\\r\\rTitle: 3D Object Detection from Images for Autonomous Driving: A Survey\\rAuthors: Xinzhu Ma, Wanli Ouyang, Andrea Simonelli, Elisa Ricci\\rCategories: cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2202.02980 ,  7875kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.03126\\rreplaced with revised version Sat, 12 Feb 2022 20:47:45 GMT   (11113kb,D)\\r\\rTitle: Reasoning for Complex Data through Ensemble-based Self-Supervised\\r  Learning\\rAuthors: Gabriel Bertocco, Ant\\\\^onio Theophilo, Fernanda Andal\\\\'o and Anderson\\r  Rocha\\rCategories: cs.CV\\rComments: This work has been submitted to the IEEE for possible publication.\\r  Copyright may be transferred without notice, after which this version may no\\r  longer be accessible\\r\\\\\\\\ ( https://arxiv.org/abs/2202.03126 ,  11113kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.04901\\rreplaced with revised version Sat, 12 Feb 2022 02:45:42 GMT   (49104kb,D)\\r\\rTitle: FILM: Frame Interpolation for Large Motion\\rAuthors: Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline\\r  Pantofaru, Brian Curless\\rCategories: cs.CV\\rComments: Project page: https://film-net.github.io. Code link:\\r  https://github.com/google-research/frame-interpolation. YouTube link:\\r  https://www.youtube.com/watch?v=OAD-BieIjH4\\r\\\\\\\\ ( https://arxiv.org/abs/2202.04901 ,  49104kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.04947\\rreplaced with revised version Mon, 14 Feb 2022 15:30:49 GMT   (2920kb,D)\\r\\rTitle: OWL (Observe, Watch, Listen): Localizing Actions in Egocentric Video via\\r  Audiovisual Temporal Context\\rAuthors: Merey Ramazanova, Victor Escorcia, Fabian Caba Heilbron, Chen Zhao,\\r  Bernard Ghanem\\rCategories: cs.CV cs.SD eess.AS\\r\\\\\\\\ ( https://arxiv.org/abs/2202.04947 ,  2920kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05411\\rreplaced with revised version Mon, 14 Feb 2022 09:16:29 GMT   (3028kb,D)\\r\\rTitle: Incremental Learning of Structured Memory via Closed-Loop Transcription\\rAuthors: Shengbang Tong, Xili Dai, Ziyang Wu, Mingyang Li, Brent Yi, Yi Ma\\rCategories: cs.CV\\rComments: 13 pages\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05411 ,  3028kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05508\\rreplaced with revised version Mon, 14 Feb 2022 05:55:25 GMT   (12107kb,D)\\r\\rTitle: Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer\\rAuthors: Yair Kittenplon, Inbal Lavi, Sharon Fogel, Yarin Bar, R. Manmatha,\\r  Pietro Perona\\rCategories: cs.CV cs.CL cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05508 ,  12107kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05592\\rreplaced with revised version Mon, 14 Feb 2022 07:01:55 GMT   (35364kb,D)\\r\\rTitle: Video-driven Neural Physically-based Facial Asset for Production\\rAuthors: Longwen Zhang, Chuxiao Zeng, Qixuan Zhang, Hongyang Lin, Ruixiang Cao,\\r  Wei Yang, Lan Xu, and Jingyi Yu\\rCategories: cs.CV\\rComments: For project page, see https://sites.google.com/view/npfa/\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05592 ,  35364kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2101.07036 (*cross-listing*)\\rreplaced with revised version Sun, 13 Feb 2022 11:11:59 GMT   (29501kb,D)\\r\\rTitle: Iterative Facial Image Inpainting Based on an Encoder-Generator\\r  Architecture\\rAuthors: Yahya Dogan and Hacer Yalim Keles\\rCategories: eess.IV cs.CV\\rComments: This paper is the preprint of the accepted manuscript in Neural\\r  Computing and Applications Journal\\r\\\\\\\\ ( https://arxiv.org/abs/2101.07036 ,  29501kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2104.01687 (*cross-listing*)\\rreplaced with revised version Mon, 14 Feb 2022 14:55:01 GMT   (3921kb,D)\\r\\rTitle: 3D Convolutional Neural Networks for Stalled Brain Capillary Detection\\rAuthors: Roman Solovyev, Alexandr A. Kalinin, Tatiana Gabruseva\\rCategories: eess.IV cs.CV\\rJournal-ref: Computers in biology and medicine. 2022\\rDOI: 10.1016/j.compbiomed.2021.105089\\r\\\\\\\\ ( https://arxiv.org/abs/2104.01687 ,  3921kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2104.14655 (*cross-listing*)\\rreplaced with revised version Sat, 12 Feb 2022 18:28:07 GMT   (832kb)\\r\\rTitle: Lung Cancer Diagnosis Using Deep Attention Based on Multiple Instance\\r  Learning and Radiomics\\rAuthors: Junhua Chen, Haiyan Zeng, Chong Zhang, Zhenwei Shi, Andre Dekker,\\r  Leonard Wee, Inigo Bermejo\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2104.14655 ,  832kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2105.12639\\rreplaced with revised version Mon, 14 Feb 2022 08:47:34 GMT   (2493kb,D)\\r\\rTitle: Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy,\\r  Uncertainty, and Robustness\\rAuthors: Namuk Park, Songkuk Kim\\rCategories: cs.LG cs.AI cs.CV stat.ML\\r\\\\\\\\ ( https://arxiv.org/abs/2105.12639 ,  2493kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2107.08221\\rreplaced with revised version Sat, 12 Feb 2022 16:37:54 GMT   (6492kb,D)\\r\\rTitle: Visual Representation Learning Does Not Generalize Strongly Within the\\r  Same Domain\\rAuthors: Lukas Schott, Julius von K\\\\ugelgen, Frederik Tr\\\\auble, Peter Gehler,\\r  Chris Russell, Matthias Bethge, Bernhard Sch\\\\olkopf, Francesco Locatello,\\r  Wieland Brendel\\rCategories: cs.LG cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2107.08221 ,  6492kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2109.08853 (*cross-listing*)\\rreplaced with revised version Sun, 13 Feb 2022 21:40:18 GMT   (310kb,D)\\r\\rTitle: A survey on deep learning approaches for breast cancer diagnosis\\rAuthors: Timothy Kwong, Samaneh Mazaheri\\rCategories: eess.IV cs.CV cs.LG\\r\\\\\\\\ ( https://arxiv.org/abs/2109.08853 ,  310kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2110.14013 (*cross-listing*)\\rreplaced with revised version Mon, 14 Feb 2022 06:53:35 GMT   (978kb,D)\\r\\rTitle: Deep Integrated Pipeline of Segmentation Guided Classification of Breast\\r  Cancer from Ultrasound Images\\rAuthors: Muhammad Sakib Khan Inan, Fahim Irfan Alam, Rizwan Hasan\\rCategories: eess.IV cs.CV cs.LG\\rComments: Accepted for publication as a Research Paper (Journal Article) in\\r  Biomedical Signal Processing and Control, Elsevier\\rDOI: 10.1016/j.bspc.2022.103553\\r\\\\\\\\ ( https://arxiv.org/abs/2110.14013 ,  978kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2111.10734\\rreplaced with revised version Mon, 14 Feb 2022 02:53:23 GMT   (20974kb,D)\\r\\rTitle: Deep Probability Estimation\\rAuthors: Sheng Liu, Aakash Kaku, Weicheng Zhu, Matan Leibovich, Sreyas Mohan,\\r  Boyang Yu, Haoxiang Huang, Laure Zanna, Narges Razavian, Jonathan Niles-Weed,\\r  Carlos Fernandez-Granda\\rCategories: cs.LG cs.AI cs.CV stat.ML\\rComments: SL, AK, WZ, ML, SM contributed equally to this work; 36 pages, 16\\r  figures, 11 tables\\r\\\\\\\\ ( https://arxiv.org/abs/2111.10734 ,  20974kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.09360 (*cross-listing*)\\rreplaced with revised version Sat, 12 Feb 2022 08:33:50 GMT   (25201kb,D)\\r\\rTitle: POTHER: Patch-Voted Deep Learning-based Chest X-ray Bias Analysis for\\r  COVID-19 Detection\\rAuthors: Tomasz Szczepa\\\\'nski, Arkadiusz Sitek, Tomasz Trzci\\\\'nski, Szymon\\r  P{\\\\l}otka\\rCategories: eess.IV cs.CV cs.LG\\rComments: Submitted to International Conference on Computational Science (ICCS)\\r  2022 in London\\r\\\\\\\\ ( https://arxiv.org/abs/2201.09360 ,  25201kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2201.10424 (*cross-listing*)\\rreplaced with revised version Mon, 14 Feb 2022 07:13:23 GMT   (308kb,D)\\r\\rTitle: Beam Stack Search-based reconstruction of unhealthy coronary artery wall\\r  segmentations in CCTA-CPR scans\\rAuthors: Antonio Tejero-de-Pablos, Hiroaki Yamane, Yusuke Kurose, Junichi Iho,\\r  Youji Tokunaga, Makoto Horie, Keisuke Nishizawa, Yusaku Hayashi, Yasushi\\r  Koyama, Tatsuya Harada\\rCategories: eess.IV cs.CV\\rJournal-ref: 2021 IEEE 18th International Symposium on Biomedical Imaging\\r  (ISBI). IEEE, 2021\\r\\\\\\\\ ( https://arxiv.org/abs/2201.10424 ,  308kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.00263\\rreplaced with revised version Mon, 14 Feb 2022 06:53:32 GMT   (1182kb,D)\\r\\rTitle: Fully Online Meta-Learning Without Task Boundaries\\rAuthors: Jathushan Rajasegaran, Chelsea Finn, Sergey Levine\\rCategories: cs.LG cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2202.00263 ,  1182kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.03570 (*cross-listing*)\\rreplaced with revised version Sat, 12 Feb 2022 00:09:54 GMT   (2329kb)\\r\\rTitle: Phase-Stretch Adaptive Gradient-Field Extractor (PAGE)\\rAuthors: Callen MacPhee, Madhuri Suthar, Bahram Jalali\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2202.03570 ,  2329kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.04074 (*cross-listing*)\\rreplaced with revised version Sun, 13 Feb 2022 21:31:13 GMT   (570kb,D)\\r\\rTitle: Cross-level Contrastive Learning and Consistency Constraint for\\r  Semi-supervised Medical Image Segmentation\\rAuthors: Xinkai Zhao, Chaowei Fang, De-Jun Fan, Xutao Lin, Feng Gao, Guanbin Li\\rCategories: eess.IV cs.CV\\r\\\\\\\\ ( https://arxiv.org/abs/2202.04074 ,  570kb)\\r------------------------------------------------------------------------------\\r\\\\\\\\\\rarXiv:2202.05062 (*cross-listing*)\\rreplaced with revised version Sat, 12 Feb 2022 15:30:04 GMT   (670kb)\\r\\rTitle: Equivariance Regularization for Image Reconstruction\\rAuthors: Junqi Tang\\rCategories: math.OC cs.CV cs.LG eess.IV\\r\\\\\\\\ ( https://arxiv.org/abs/2202.05062 ,  670kb)\\r%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%--- \""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b662e3-d5d6-4024-ab12-c4a9acf51282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the arxiv emails into a txt file\n",
    "with open(\"arxiv_contents.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    " f.write(arxiv_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20084d-d394-4bb6-baba-86834ebefe41",
   "metadata": {},
   "source": [
    "### convert text document to langchain document format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7efa239-673a-4252-873e-b7ab6d5596f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████| 45.9M/45.9M [00:02<00:00, 18.4MiB/s]\n",
      "Verifying: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 45.9M/45.9M [00:00<00:00, 399MiB/s]\n"
     ]
    }
   ],
   "source": [
    "doc = Document(page_content=arxiv_contents, \n",
    "                metadata={\"source\": \"local\"})\n",
    "\n",
    "# split into different chunks\n",
    "# chunk_size and chunk_overlap are a hyperparameters we choose \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "# split the documents and convert to vector stores\n",
    "all_splits = text_splitter.split_documents([doc])\n",
    "vector_store = Chroma.from_documents(documents=all_splits, \n",
    "                                     embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a32570-d9c7-4af1-ac0f-91eb9099b6ba",
   "metadata": {},
   "source": [
    "### If using fine-tuned model, quantize to GGUF Format\n",
    "For fine-tuning a given LLM, checkout [this video](https://youtu.be/_xxGMSVLwU8?feature=shared)\n",
    "\n",
    "For quantizing, we can either: \n",
    "- Use the llama.cpp library written in C,C++ for this. Checkout [this video](https://youtu.be/j7ahltwlFH0?feature=shared)\n",
    "- Or we can use LangChain's functionality. \n",
    "\n",
    "In any case we need the model to be converted to `gguf` format to run on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898ca93-a27c-480e-8bea-7aafe403ba9f",
   "metadata": {},
   "source": [
    "## Load the quantize LLM model \n",
    "For the quantized model, we can either, \n",
    "- Use the LlamaCpp class from LangChain\n",
    "- Use the GPT4All library\n",
    "\n",
    "In any case, the models need to be quantized models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9daab2-07f3-4206-8f03-06ea59a30712",
   "metadata": {},
   "source": [
    "### Either use LlamaCpp as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "993118d5-bf85-4613-a5cb-1bac4d2827e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/llms/llamacpp.py:140\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m quantized_gguf_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../generative-ai-course/quantized_models/ft-Q8_K_M.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Initiate the LlamaCpp class to run the LLM\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantized_gguf_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_gpu_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf16_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/pydantic/v1/main.py:1100\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1102\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/llms/llamacpp.py:142\u001b[0m, in \u001b[0;36mLlamaCpp.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama, LlamaGrammar\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import llama-cpp-python library. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install the llama-cpp-python library to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    148\u001b[0m model_path \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    149\u001b[0m model_param_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_freq_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_freq_base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    166\u001b[0m ]\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1 \n",
    "\n",
    "n_batch = 512\n",
    "quantized_gguf_model = \"../generative-ai-course/quantized_models/ft-Q8_K_M.gguf\"\n",
    "\n",
    "# Initiate the LlamaCpp class to run the LLM\n",
    "llm = LlamaCpp(\n",
    "    model_path=quantized_gguf_model,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=1024,\n",
    "    f16_kv=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb94348-9858-4717-a920-6dee8edac982",
   "metadata": {},
   "source": [
    "### Or use GPT4All as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b221bc34-2467-4865-bf55-a1029b98e42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRetrieval augmented generation (RAG) is a technique used in natural language processing to generate new text by retrieving and combining existing text. It involves using a large corpus of text to train a machine learning model that can predict the most likely next word or phrase in a given sentence. The model is then used to generate new text by selecting the most probable words or phrases from the training data. RAG has been applied to many different domains, including news summarization, language translation, and chatbot generation.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=r\"./models/orca-mini-3b-gguf2-q4_0.gguf\",\n",
    "    max_tokens=2048,\n",
    ")\n",
    "llm.invoke(\"what is Retrieval Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a3f0b-6f77-49db-a5ef-6121a2f7b852",
   "metadata": {},
   "source": [
    "## Create the LangChain with and without RAG\n",
    "For a given prompt, \n",
    "- Create a langchain without retrieval and see the response\n",
    "- Create a langchain with the retrieval object\n",
    "\n",
    "And see how the response differs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69637a08-d520-476d-a5f2-4c02f4f6cb9c",
   "metadata": {},
   "source": [
    "### LLM response without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55283aa7-0835-4d05-9d8b-d4fd164e45c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' There is no specific information provided in the context to determine the title of a paper.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve relevant docs\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Create the langchain with retriever\n",
    "qa_chain = (\n",
    "    {\"context\": {}, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "qa_chain.invoke(\"what is the title of the paper that talks about awareness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9ac6a-6161-40c6-b6b9-106f5b7a7ec6",
   "metadata": {},
   "source": [
    "### LLM response with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ec52ba4-8995-4bca-90d8-1d2880a87e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve relevant docs\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "rag_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562a98c-4d85-49b7-b3d2-ec9c44ce81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_documents(documents):\n",
    "    return \"\\n\\n\".join(documents.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27916a43-8eb0-4341-b1bd-18446886dbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The title of the paper that talks about awareness is \"I Think, Therefore I am: Awareness in Large Language Models\".'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Create the langchain with retriever,\n",
    "# prompt template and LLM\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "qa_chain.invoke(\"what is the title of the paper that talks about awareness?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
