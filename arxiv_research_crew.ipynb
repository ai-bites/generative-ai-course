{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7234401-25ab-4a2f-9464-733cdde3be0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crewai in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (0.86.0)\n",
      "Requirement already satisfied: crewai_tools in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (0.25.5)\n",
      "Requirement already satisfied: arxiv in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (1.4.4)\n",
      "Requirement already satisfied: auth0-python>=4.7.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (4.7.2)\n",
      "Requirement already satisfied: chromadb>=0.5.18 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (0.5.23)\n",
      "Requirement already satisfied: click>=8.1.7 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (8.1.8)\n",
      "Requirement already satisfied: instructor>=1.3.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (1.7.2)\n",
      "Requirement already satisfied: json-repair>=0.25.2 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (0.35.0)\n",
      "Requirement already satisfied: jsonref>=1.1.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (1.1.0)\n",
      "Requirement already satisfied: litellm>=1.44.22 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (1.56.8)\n",
      "Requirement already satisfied: openai>=1.13.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (1.58.1)\n",
      "Requirement already satisfied: openpyxl>=3.1.5 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (3.1.5)\n",
      "Requirement already satisfied: opentelemetry-api>=1.22.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http>=1.22.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.22.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (1.29.0)\n",
      "Requirement already satisfied: pdfplumber>=0.11.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (0.11.5)\n",
      "Requirement already satisfied: pydantic>=2.4.2 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (2.10.4)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (1.0.1)\n",
      "Requirement already satisfied: pyvis>=0.3.2 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (0.3.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (2024.11.6)\n",
      "Requirement already satisfied: tomli-w>=1.1.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (1.1.0)\n",
      "Requirement already satisfied: tomli>=2.0.2 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (2.2.1)\n",
      "Requirement already satisfied: uv>=0.4.25 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai) (0.5.14)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (4.12.3)\n",
      "Requirement already satisfied: docker>=7.1.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (7.1.0)\n",
      "Requirement already satisfied: docx2txt>=0.8 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (0.8)\n",
      "Requirement already satisfied: embedchain>=0.1.114 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (0.1.126)\n",
      "Requirement already satisfied: lancedb>=0.5.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (0.17.0)\n",
      "Requirement already satisfied: pyright>=1.1.350 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (1.1.391)\n",
      "Requirement already satisfied: pytest>=8.0.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (8.3.4)\n",
      "Requirement already satisfied: pytube>=15.0.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (15.0.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (2.32.3)\n",
      "Requirement already satisfied: scrapegraph-py>=1.8.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (1.8.0)\n",
      "Requirement already satisfied: selenium>=4.18.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (4.27.1)\n",
      "Requirement already satisfied: serpapi>=0.1.5 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from crewai_tools) (0.1.5)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.5 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from auth0-python>=4.7.1->crewai) (3.11.11)\n",
      "Requirement already satisfied: cryptography<44.0.0,>=43.0.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from auth0-python>=4.7.1->crewai) (43.0.3)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from auth0-python>=4.7.1->crewai) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=2.0.7 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from auth0-python>=4.7.1->crewai) (2.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from beautifulsoup4>=4.12.3->crewai_tools) (2.6)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (0.115.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (3.7.4)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (0.50b0)\n",
      "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (1.68.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (3.10.13)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from chromadb>=0.5.18->crewai) (13.9.4)\n",
      "Requirement already satisfied: alembic<2.0.0,>=1.13.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (1.14.0)\n",
      "Requirement already satisfied: cohere<6.0,>=5.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (5.13.4)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (1.75.0)\n",
      "Requirement already satisfied: gptcache<0.2.0,>=0.1.43 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (0.1.44)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (0.3.13)\n",
      "Requirement already satisfied: langchain-cohere<0.4.0,>=0.3.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (0.3.4)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (0.3.13)\n",
      "Requirement already satisfied: langchain-openai<0.3.0,>=0.2.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (0.2.14)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (0.1.147)\n",
      "Requirement already satisfied: mem0ai<0.2.0,>=0.1.37 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (0.1.38)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.0.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (5.1.0)\n",
      "Requirement already satisfied: pysbd<0.4.0,>=0.3.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (0.3.4)\n",
      "Requirement already satisfied: schema<0.8.0,>=0.7.5 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (0.7.7)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.27 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (2.0.36)\n",
      "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from embedchain>=0.1.114->crewai_tools) (0.7.0)\n",
      "Requirement already satisfied: sgmllib3k in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from instructor>=1.3.3->crewai) (0.16)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from instructor>=1.3.3->crewai) (3.1.5)\n",
      "Requirement already satisfied: jiter<0.9,>=0.6.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from instructor>=1.3.3->crewai) (0.8.2)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from instructor>=1.3.3->crewai) (2.27.2)\n",
      "Requirement already satisfied: deprecation in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from lancedb>=0.5.4->crewai_tools) (2.1.0)\n",
      "Requirement already satisfied: pylance==0.20.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from lancedb>=0.5.4->crewai_tools) (0.20.0)\n",
      "Requirement already satisfied: packaging in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from lancedb>=0.5.4->crewai_tools) (24.2)\n",
      "Requirement already satisfied: pyarrow>=14 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pylance==0.20.0->lancedb>=0.5.4->crewai_tools) (18.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from litellm>=1.44.22->crewai) (8.5.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from litellm>=1.44.22->crewai) (4.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from openai>=1.13.3->crewai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from openai>=1.13.3->crewai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from openai>=1.13.3->crewai) (1.3.1)\n",
      "Requirement already satisfied: et-xmlfile in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from openpyxl>=3.1.5->crewai) (2.0.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from opentelemetry-api>=1.22.0->crewai) (1.2.15)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.29.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai) (1.29.0)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from opentelemetry-proto==1.29.0->opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai) (5.29.2)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from opentelemetry-sdk>=1.22.0->crewai) (0.50b0)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pdfplumber>=0.11.4->crewai) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pdfplumber>=0.11.4->crewai) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pdfplumber>=0.11.4->crewai) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pdfminer.six==20231228->pdfplumber>=0.11.4->crewai) (3.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pydantic>=2.4.2->crewai) (0.7.0)\n",
      "Requirement already satisfied: nodeenv>=1.6.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pyright>=1.1.350->crewai_tools) (1.9.1)\n",
      "Requirement already satisfied: iniconfig in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pytest>=8.0.0->crewai_tools) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pytest>=8.0.0->crewai_tools) (1.5.0)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pyvis>=0.3.2->crewai) (8.31.0)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pyvis>=0.3.2->crewai) (4.0.1)\n",
      "Requirement already satisfied: networkx>=1.11 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pyvis>=0.3.2->crewai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from requests>=2.31.0->crewai_tools) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from requests>=2.31.0->crewai_tools) (2024.12.14)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from selenium>=4.18.1->crewai_tools) (0.28.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from selenium>=4.18.1->crewai_tools) (0.11.1)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from selenium>=4.18.1->crewai_tools) (1.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.18.3)\n",
      "Requirement already satisfied: Mako in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from alembic<2.0.0,>=1.13.1->embedchain>=0.1.114->crewai_tools) (1.3.8)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from build>=1.0.3->chromadb>=0.5.18->crewai) (1.2.0)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools) (1.10.0)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools) (0.4.0)\n",
      "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools) (0.9.0)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools) (2.32.0.20241016)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai) (1.17.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.22.0->crewai) (1.17.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb>=0.5.18->crewai) (0.41.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.24.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.37.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (1.25.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (3.27.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (1.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.0.6)\n",
      "Requirement already satisfied: cachetools in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from gptcache<0.2.0,>=0.1.43->embedchain>=0.1.114->crewai_tools) (5.5.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb>=0.5.18->crewai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.5.18->crewai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm>=1.44.22->crewai) (3.21.0)\n",
      "Requirement already satisfied: decorator in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.18.0)\n",
      "Requirement already satisfied: stack_data in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.14.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.4->instructor>=1.3.3->crewai) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (0.22.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (0.9)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.26 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai_tools) (0.3.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai_tools) (0.3.4)\n",
      "Requirement already satisfied: langchain-experimental<0.4.0,>=0.3.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools) (0.3.4)\n",
      "Requirement already satisfied: pandas>=1.4.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools) (2.2.3)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools) (0.9.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai_tools) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai_tools) (2.7.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->embedchain>=0.1.114->crewai_tools) (1.0.0)\n",
      "Requirement already satisfied: pytz<2025.0,>=2024.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai_tools) (2024.2)\n",
      "Requirement already satisfied: qdrant-client<2.0.0,>=1.9.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai_tools) (1.12.2)\n",
      "Requirement already satisfied: coloredlogs in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (24.12.23)\n",
      "Requirement already satisfied: sympy in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (1.13.3)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai) (0.50b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.50b0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai) (0.50b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb>=0.5.18->crewai) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb>=0.5.18->crewai) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from rich>=10.11.0->chromadb>=0.5.18->crewai) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb>=0.5.18->crewai) (0.27.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from trio~=0.17->selenium>=4.18.1->crewai_tools) (2.4.0)\n",
      "Requirement already satisfied: outcome in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from trio~=0.17->selenium>=4.18.1->crewai_tools) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium>=4.18.1->crewai_tools) (1.2.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from typer>=0.9.0->chromadb>=0.5.18->crewai) (1.5.4)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.18.1->crewai_tools) (1.7.1)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai) (1.0.3)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai) (14.1)\n",
      "Requirement already satisfied: pycparser in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from cffi>=1.12->cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai) (2.22)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai_tools) (3.23.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai_tools) (0.9.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (1.68.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.7.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (0.14.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (1.6.0)\n",
      "Requirement already satisfied: filelock in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb>=0.5.18->crewai) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb>=0.5.18->crewai) (2024.12.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.26->langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai_tools) (1.33)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.18->crewai) (0.1.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pandas>=1.4.3->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools) (2024.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai_tools) (1.68.1)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai_tools) (2.10.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (10.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai_tools) (75.1.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai_tools) (4.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai_tools) (3.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (0.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai_tools) (1.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai_tools) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.37->embedchain>=0.1.114->crewai_tools) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install crewai crewai_tools arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f3e7ba-f2de-43d2-9e1d-49cf07969271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8760da75-4df2-4bcc-950e-764c097529b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssankar/miniconda3/envs/crewai/lib/python3.11/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from typing import Type, List\n",
    "from pydantic import BaseModel, Field \n",
    "from crewai.tools import BaseTool \n",
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33848045-8581-4f93-83ef-613388d1e907",
   "metadata": {},
   "source": [
    "## Define the Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8e7297e-3d10-4557-8600-42f2eec4bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchArxivPapersInput(BaseModel):\n",
    "    \"\"\"Input schema for SearchArxivPapersTool.\"\"\"\n",
    "    start_date: datetime.date = Field(..., description=\"Starat date to fetch papers for.\")\n",
    "    end_date: datetime.date = Field(..., description=\"End date to fetch papers for.\")\n",
    "\n",
    "class SearchArxivPapersTool(BaseTool):\n",
    "    name: str = \"search_arxiv_papers\"\n",
    "    description: str = \"Searches all ArXiv papers from selected categories submitted from the start date to end date.\"\n",
    "    args_schema: Type[BaseModel] = SearchArxivPapersInput\n",
    "\n",
    "    def _run(self, start_date: datetime.date, end_date: datetime.date = None) -> List[dict]:\n",
    "        arxiv_ai_categories = [\"cs.AI\"]\n",
    "        # arxiv_ai_categories = [\"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"cs.RO\"]\n",
    "\n",
    "        # Define the date range for the target date\n",
    "        start_date = start_date.strftime('%Y%m%d%H%M')\n",
    "        if end_date:\n",
    "            end_date = end_date.strftime('%Y%m%d%H%M')\n",
    "        else:\n",
    "            end_date = (start_date + datetime.timedelta(days=1)).strftime('%Y%m%d%H%M')\n",
    "\n",
    "        # Initialize the ArXiv client\n",
    "        client = arxiv.Client(\n",
    "            page_size=100,  # Search 100 results per page\n",
    "            delay_seconds=3  # Delay between requests to respect rate limits\n",
    "        )\n",
    "\n",
    "        all_papers = []\n",
    "        for category in arxiv_ai_categories:\n",
    "            print(f\"Searching papers for category: {category}\")\n",
    "\n",
    "            search_query = f\"cat:{category} AND submittedDate:[{start_date} TO {end_date}]\"\n",
    "\n",
    "            search = arxiv.Search(\n",
    "                query=search_query,\n",
    "                sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "                max_results=20  # Search all results\n",
    "            )\n",
    "\n",
    "            # Collect results for the category\n",
    "            category_papers = []\n",
    "            for result in client.results(search):\n",
    "                category_papers.append({\n",
    "                    'title': result.title,\n",
    "                    'authors': [author.name for author in result.authors],\n",
    "                    'summary': result.summary,\n",
    "                    'published': result.published,\n",
    "                    'url': result.entry_id,\n",
    "                    'comment':result.comment\n",
    "                })\n",
    "\n",
    "                # Delay between requests to respect rate limits\n",
    "                time.sleep(1)\n",
    "\n",
    "            print(f\"Searched {len(category_papers)} papers from {category}\")\n",
    "            all_papers.extend(category_papers)\n",
    "\n",
    "        return all_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61f11de9-dc11-4a02-bdce-73076371d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_search_tool = SearchArxivPapersTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5b5ee-d776-4448-9959-f92985db7dd2",
   "metadata": {},
   "source": [
    "## Create our Agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7eb9de31-6df6-45f4-a8ba-c6e2c8a7d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1: ArXiv Researcher\n",
    "researcher = Agent(\n",
    "    role = \"Senior Researcher\",\n",
    "    goal = \"Find the top 10 papers from the search results from ArXiv between dates {start_date} and {end_date}.\"\n",
    "            \"Rank them appropirately.\",\n",
    "    backstory = \"You are a senior researcher with a deep understanding of all topics in AI and AI research.\"\n",
    "                \"You can identify the best research papers based on the title, abstract and comments.\"\n",
    "                \"Give higher priority to papers that have CVPR, ECCV, ICCV, ICLR, ICML, NeurIPS, ICRA, IROS, ACL, EMNLP mentioned in the comments\"\n",
    "                \"Ignore papers which mention workshop in the comments\",\n",
    "    verbose = True,\n",
    "    tools = [arxiv_search_tool],\n",
    ")\n",
    "\n",
    "# Agent 2: Frontend Engineer\n",
    "frontend_engineer = Agent(\n",
    "    role = \"Senior Frontend & AI Engineer\",\n",
    "    goal = \"Compile the results into a HTML file.\",\n",
    "    backstory = \"You are a competent frontend engineer writing HTML, CSS and Markdown with decades of experience.\"\n",
    "                \"You have also been working with AI for decades and understand it well.\",\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a8ec6-ef4b-4f2d-8704-c621fec4155d",
   "metadata": {},
   "source": [
    "## Create our Tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ab082e7-ad6a-4d12-901f-ab297f8229fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task for ArXiv Researcher\n",
    "research_task = Task(\n",
    "    description = (\"Find the top 10 research papers from the search results from ArXiv between dates: {start_date} and {end_date}.\"),\n",
    "    expected_output = (\n",
    "        \"A list of top 10 research papers with the following information in the following format:\"\n",
    "        \"- Title\"\n",
    "        \"- Authors\"\n",
    "        \"- Abstract\"\n",
    "        \"- Link to the paper\"\n",
    "    ),\n",
    "    agent = researcher,\n",
    "    human_input = True,\n",
    ")\n",
    "\n",
    "# Task for Frontend Engineer\n",
    "reporting_task = Task(\n",
    "    description = (\"Compile the results into a detailed report in HTML file format.\"),\n",
    "    expected_output = (\n",
    "        \"An HTML file with the results in the following format:\"\n",
    "        \"Top 10 AI Research Papers published between dates {start_date} and {end_date}\"\n",
    "        \"- Title (which on clicking opens the paper in a new tab)\"\n",
    "        \"- Authors\"\n",
    "        \"- Short summary of the abstract (2-4 sentences)\"\n",
    "    ),\n",
    "    agent = frontend_engineer,\n",
    "    context = [research_task],\n",
    "    output_file = \"./report.html\",\n",
    "    human_input = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c844daa-7254-4454-a4b1-d9ba8ae0cd5b",
   "metadata": {},
   "source": [
    "## Crate the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ff4a74f-b02a-4aa8-86d2-10b8ed0991f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "arxiv_research_crew = Crew(\n",
    "    agents = [researcher, frontend_engineer],\n",
    "    tasks = [research_task, reporting_task],\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8881a-495d-4136-9f76-0b330b6bd493",
   "metadata": {},
   "source": [
    "## Run the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3544782-07a0-4e93-be7c-7130d6b0f900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mFind the top 10 research papers from the search results from ArXiv between dates: 2025-06-17 and 2025-06-18.\u001b[00m\n",
      "Searching papers for category: cs.AI\n",
      "Searched 20 papers from cs.AI\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Researcher\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to search for research papers on ArXiv from the specified date range of 2025-06-17 to 2025-06-18. I will use the tool `search_arxiv_papers` to gather this information.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92msearch_arxiv_papers\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"start_date\\\": \\\"2025-06-17\\\", \\\"end_date\\\": \\\"2025-06-18\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "[{'title': 'Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size', 'authors': ['Soufiane Hayou', 'Liyuan Liu'], 'summary': 'Pretraining large language models is a costly process. To make this process\\nmore efficient, several methods have been proposed to optimize model\\narchitecture/parametrization and hardware use. On the parametrization side,\\n$\\\\mu P$ (Maximal Update Parametrization) parametrizes model weights and\\nlearning rate (LR) in a way that makes hyperparameters (HPs) transferable with\\nwidth (embedding dimension): HPs can be tuned for a small model and used for\\nlarger models without additional tuning. While $\\\\mu$P showed impressive results\\nin practice, recent empirical studies have reported conflicting observations\\nwhen applied to LLMs. One limitation of the theory behind $\\\\mu$P is the fact\\nthat input dimension (vocabulary size in LLMs) is considered fixed when taking\\nthe width to infinity. This is unrealistic since vocabulary size is generally\\nmuch larger than width in practice. In this work, we provide a theoretical\\nanalysis of the effect of vocabulary size on training dynamics, and\\nsubsequently show that as vocabulary size increases, the training dynamics\\n\\\\emph{interpolate between the $\\\\mu$P regime and another regime that we call\\nLarge Vocab (LV) Regime}, where optimal scaling rules are different from those\\npredicted by $\\\\mu$P. Our analysis reveals that in the LV regime, the optimal\\nembedding LR to hidden LR ratio should roughly scale as $\\\\Theta(\\\\sqrt{width})$,\\nsurprisingly close to the empirical findings previously reported in the\\nliterature, and different from the $\\\\Theta(width)$ ratio predicted by $\\\\mu$P.\\nWe conduct several experiments to validate our theory, and pretrain a 1B model\\nfrom scratch to show the benefit of our suggested scaling rule for the\\nembedding LR.', 'published': datetime.datetime(2025, 6, 17, 23, 57, 30, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.15025v1', 'comment': 'TD,LR: How to set the learning rate for emebdding layer in LLMs?'}, {'title': 'SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models', 'authors': ['Gyuhak Kim', 'Sumiran Singh Thakur', 'Su Min Park', 'Wei Wei', 'Yujia Bao'], 'summary': \"Supervised fine-tuning (SFT) has become an essential step in tailoring large\\nlanguage models (LLMs) to align with human expectations and specific downstream\\ntasks. However, existing SFT methods typically treat each training instance as\\na uniform sequence, giving equal importance to all tokens regardless of their\\nrelevance. This overlooks the fact that only a subset of tokens often contains\\ncritical, task-specific information. To address this limitation, we introduce\\nSupervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that\\ntreats groups of tokens differently based on their importance.SFT-GO groups\\ntokens in each sample based on their importance values and optimizes the LLM\\nusing a weighted combination of the worst-group loss and the standard\\ncross-entropy loss. This mechanism adaptively emphasizes the most challenging\\ntoken groups and guides the model to better handle different group\\ndistributions, thereby improving overall learning dynamics. We provide a\\ntheoretical analysis of SFT-GO's convergence rate, demonstrating its\\nefficiency. Empirically, we apply SFT-GO with three different token grouping\\nstrategies and show that models trained with SFT-GO consistently outperform\\nbaseline approaches across popular LLM benchmarks. These improvements hold\\nacross various datasets and base models, demonstrating the robustness and the\\neffectiveness of our method.\", 'published': datetime.datetime(2025, 6, 17, 23, 12, 28, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.15021v1', 'comment': None}, {'title': 'Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment', 'authors': ['Yue Gao'], 'summary': 'Effective reinforcement learning (RL) for sepsis treatment depends on\\nlearning stable, clinically meaningful state representations from irregular ICU\\ntime series. While previous works have explored representation learning for\\nthis task, the critical challenge of training instability in sequential\\nrepresentations and its detrimental impact on policy performance has been\\noverlooked. This work demonstrates that Controlled Differential Equations (CDE)\\nstate representation can achieve strong RL policies when two key factors are\\nmet: (1) ensuring training stability through early stopping or stabilization\\nmethods, and (2) enforcing acuity-aware representations by correlation\\nregularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the\\nMIMIC-III sepsis cohort reveal that stable CDE autoencoder produces\\nrepresentations strongly correlated with acuity scores and enables RL policies\\nwith superior performance (WIS return $> 0.9$). In contrast, unstable CDE\\nrepresentation leads to degraded representations and policy failure (WIS return\\n$\\\\sim$ 0). Visualizations of the latent space show that stable CDEs not only\\nseparate survivor and non-survivor trajectories but also reveal clear acuity\\nscore gradients, whereas unstable training fails to capture either pattern.\\nThese findings highlight practical guidelines for using CDEs to encode\\nirregular medical time series in clinical RL, emphasizing the need for training\\nstability in sequential representation learning.', 'published': datetime.datetime(2025, 6, 17, 23, 10, 51, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.15019v1', 'comment': 'Accepted to IJCAI2025 AI4TS'}, {'title': 'Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output', 'authors': ['Richa Gupta', 'Alexander Htet Kyaw'], 'summary': 'Generative AI, specifically text-to-image models, have revolutionized\\ninterior architectural design by enabling the rapid translation of conceptual\\nideas into visual representations from simple text prompts. While generative AI\\ncan produce visually appealing images they often lack actionable data for\\ndesigners In this work, we propose a novel pipeline that integrates DALL-E 3\\nwith a materials dataset to enrich AI-generated designs with sustainability\\nmetrics and material usage insights. After the model generates an interior\\ndesign image, a post-processing module identifies the top ten materials present\\nand pairs them with carbon dioxide equivalent (CO2e) values from a general\\nmaterials dictionary. This approach allows designers to immediately evaluate\\nenvironmental impacts and refine prompts accordingly. We evaluate the system\\nthrough three user tests: (1) no mention of sustainability to the user prior to\\nthe prompting process with generative AI, (2) sustainability goals communicated\\nto the user before prompting, and (3) sustainability goals communicated along\\nwith quantitative CO2e data included in the generative AI outputs. Our\\nqualitative and quantitative analyses reveal that the introduction of\\nsustainability metrics in the third test leads to more informed design\\ndecisions, however, it can also trigger decision fatigue and lower overall\\nsatisfaction. Nevertheless, the majority of participants reported incorporating\\nsustainability principles into their workflows in the third test, underscoring\\nthe potential of integrated metrics to guide more ecologically responsible\\npractices. Our findings showcase the importance of balancing design freedom\\nwith practical constraints, offering a clear path toward holistic, data-driven\\nsolutions in AI-assisted architectural design.', 'published': datetime.datetime(2025, 6, 17, 22, 33, 11, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.15008v1', 'comment': '15 Pages, 6 figures, CAAD Futures 2025'}, {'title': 'Scaling Intelligence: Designing Data Centers for Next-Gen Language Models', 'authors': ['Jesmin Jahan Tithi', 'Hanjiang Wu', 'Avishaii Abuhatzera', 'Fabrizio Petrini'], 'summary': 'The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8\\ntrillion parameters - demands a radical rethinking of data center architecture\\nto ensure scalability, efficiency, and cost-effectiveness. Our work provides a\\ncomprehensive co-design framework that jointly explores FLOPS, HBM bandwidth\\nand capacity, multiple network topologies (two-tier vs. FullFlat optical), the\\nsize of the scale-out domain, and popular parallelism/optimization strategies\\nused in LLMs. We introduce and evaluate FullFlat network architectures, which\\nprovide uniform high-bandwidth, low-latency connectivity between all nodes, and\\ndemonstrate their transformative impact on performance and scalability. Through\\ndetailed sensitivity analyses, we quantify the benefits of overlapping compute\\nand communication, leveraging hardware-accelerated collectives, wider scale-out\\ndomains, and larger memory capacity. Our study spans both sparse (mixture of\\nexperts) and dense transformer-based LLMs, revealing how system design choices\\naffect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens\\nper sec / Peak flops of the hardware) and overall throughput. For the co-design\\nstudy, we extended and validated a performance modeling tool capable of\\npredicting LLM runtime within 10% of real-world measurements. Our findings\\noffer actionable insights and a practical roadmap for designing AI data centers\\nthat can efficiently support trillion-parameter models, reduce optimization\\ncomplexity, and sustain the rapid evolution of AI capabilities.', 'published': datetime.datetime(2025, 6, 17, 22, 29, 37, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.15006v1', 'comment': '14 pages, submitted to SC25 for review'}, {'title': 'Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings', 'authors': ['Ignacio Sastre', 'Aiala Ros'], 'summary': \"In this work, we observe an interesting phenomenon: it is possible to\\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\\noriginal text exactly, without modifying the model's weights. This is achieved\\nby introducing a special memory token, whose embedding is optimized through\\ntraining on a fixed sequence. When prompted with this embedding, the model\\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\\nsuccessfully reconstructs all tested sequences. Our findings highlight an\\ninteresting capability of LLMs and suggest potential applications in\\nmemory-based retrieval, compression, and controlled text generation.\", 'published': datetime.datetime(2025, 6, 17, 22, 13, 34, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.15001v1', 'comment': 'This paper will be presented at The First Workshop on Large Language\\n  Model Memorization (L2M2) at ACL 2025'}, {'title': 'Improved Image Reconstruction and Diffusion Parameter Estimation Using a Temporal Convolutional Network Model of Gradient Trajectory Errors', 'authors': ['Jonathan B. Martin', 'Hannah E. Alderson', 'John C. Gore', 'Mark D. Does', 'Kevin D. Harkins'], 'summary': 'Summary: Errors in gradient trajectories introduce significant artifacts and\\ndistortions in magnetic resonance images, particularly in non-Cartesian imaging\\nsequences, where imperfect gradient waveforms can greatly reduce image quality.\\nPurpose: Our objective is to develop a general, nonlinear gradient system model\\nthat can accurately predict gradient distortions using convolutional networks.\\nMethods: A set of training gradient waveforms were measured on a small animal\\nimaging system, and used to train a temporal convolutional network to predict\\nthe gradient waveforms produced by the imaging system. Results: The trained\\nnetwork was able to accurately predict nonlinear distortions produced by the\\ngradient system. Network prediction of gradient waveforms was incorporated into\\nthe image reconstruction pipeline and provided improvements in image quality\\nand diffusion parameter mapping compared to both the nominal gradient waveform\\nand the gradient impulse response function. Conclusion: Temporal convolutional\\nnetworks can more accurately model gradient system behavior than existing\\nlinear methods and may be used to retrospectively correct gradient errors.', 'published': datetime.datetime(2025, 6, 17, 22, 1, 6, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14995v1', 'comment': None}, {'title': 'MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning', 'authors': ['Tristan Tomilin', 'Luka van den Boogaard', 'Samuel Garcin', 'Bram Grooten', 'Meng Fang', 'Mykola Pechenizkiy'], 'summary': 'Benchmarks play a crucial role in the development and analysis of\\nreinforcement learning (RL) algorithms, with environment availability strongly\\nimpacting research. One particularly underexplored intersection is continual\\nlearning (CL) in cooperative multi-agent settings. To remedy this, we introduce\\nMEAL (Multi-agent Environments for Adaptive Learning), the first benchmark\\ntailored for continual multi-agent reinforcement learning (CMARL). Existing CL\\nbenchmarks run environments on the CPU, leading to computational bottlenecks\\nand limiting the length of task sequences. MEAL leverages JAX for GPU\\nacceleration, enabling continual learning across sequences of 100 tasks on a\\nstandard desktop PC in a few hours. We show that naively combining popular CL\\nand MARL methods yields strong performance on simple environments, but fails to\\nscale to more complex settings requiring sustained coordination and adaptation.\\nOur ablation study identifies architectural and algorithmic features critical\\nfor CMARL on MEAL.', 'published': datetime.datetime(2025, 6, 17, 21, 50, 4, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14990v1', 'comment': None}, {'title': 'Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits', 'authors': ['Tianyi Xu', 'Jiaxin Liu', 'Zizhan Zheng'], 'summary': 'We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at\\nensuring fair outcomes across agents while maximizing overall system\\nperformance. A key challenge in this setting is decision-making under limited\\ninformation about arm rewards. To address this, we introduce a novel probing\\nframework that strategically gathers information about selected arms before\\nallocation. In the offline setting, where reward distributions are known, we\\nleverage submodular properties to design a greedy probing algorithm with a\\nprovable performance bound. For the more complex online setting, we develop an\\nalgorithm that achieves sublinear regret while maintaining fairness. Extensive\\nexperiments on synthetic and real-world datasets show that our approach\\noutperforms baseline methods, achieving better fairness and efficiency.', 'published': datetime.datetime(2025, 6, 17, 21, 43, 21, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14988v1', 'comment': None}, {'title': 'Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition', 'authors': ['Jiamin Xie', 'Ju Lin', 'Yiteng Huang', 'Tyler Vuong', 'Zhaojiang Lin', 'Zhaojun Yang', 'Peng Su', 'Prashant Rawat', 'Sangeeta Srivastava', 'Ming Sun', 'Florian Metze'], 'summary': \"Recent studies have demonstrated that prompting large language models (LLM)\\nwith audio encodings enables effective speech recognition capabilities.\\nHowever, the ability of Speech LLMs to comprehend and process multi-channel\\naudio with spatial cues remains a relatively uninvestigated area of research.\\nIn this work, we present directional-SpeechLlama, a novel approach that\\nleverages the microphone array of smart glasses to achieve directional speech\\nrecognition, source localization, and bystander cross-talk suppression. To\\nenhance the model's ability to understand directivity, we propose two key\\ntechniques: serialized directional output training (S-DOT) and contrastive\\ndirection data augmentation (CDDA). Experimental results show that our proposed\\ndirectional-SpeechLlama effectively captures the relationship between textual\\ncues and spatial audio, yielding strong performance in both speech recognition\\nand source localization tasks.\", 'published': datetime.datetime(2025, 6, 17, 20, 49, 41, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14973v1', 'comment': 'Accepted to Interspeech 2025'}, {'title': 'FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization', 'authors': ['Rajat Kumar Jenamani', 'Tom Silver', 'Ben Dodson', 'Shiqin Tong', 'Anthony Song', 'Yuting Yang', 'Ziang Liu', 'Benjamin Howe', 'Aimee Whitneck', 'Tapomayukh Bhattacharjee'], 'summary': \"Physical caregiving robots hold promise for improving the quality of life of\\nmillions worldwide who require assistance with feeding. However, in-home meal\\nassistance remains challenging due to the diversity of activities (e.g.,\\neating, drinking, mouth wiping), contexts (e.g., socializing, watching TV),\\nfood items, and user preferences that arise during deployment. In this work, we\\npropose FEAST, a flexible mealtime-assistance system that can be personalized\\nin-the-wild to meet the unique needs of individual care recipients. Developed\\nin collaboration with two community researchers and informed by a formative\\nstudy with a diverse group of care recipients, our system is guided by three\\nkey tenets for in-the-wild personalization: adaptability, transparency, and\\nsafety. FEAST embodies these principles through: (i) modular hardware that\\nenables switching between assisted feeding, drinking, and mouth-wiping, (ii)\\ndiverse interaction methods, including a web interface, head gestures, and\\nphysical buttons, to accommodate diverse functional abilities and preferences,\\nand (iii) parameterized behavior trees that can be safely and transparently\\nadapted using a large language model. We evaluate our system based on the\\npersonalization requirements identified in our formative study, demonstrating\\nthat FEAST offers a wide range of transparent and safe adaptations and\\noutperforms a state-of-the-art baseline limited to fixed customizations. To\\ndemonstrate real-world applicability, we conduct an in-home user study with two\\ncare recipients (who are community researchers), feeding them three meals each\\nacross three diverse scenarios. We further assess FEAST's ecological validity\\nby evaluating with an Occupational Therapist previously unfamiliar with the\\nsystem. In all cases, users successfully personalize FEAST to meet their\\nindividual needs and preferences. Website: https://emprise.cs.cornell.edu/feast\", 'published': datetime.datetime(2025, 6, 17, 20, 30, 11, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14968v1', 'comment': 'RSS 2025 - Outstanding Paper Award & Outstanding Systems Paper Award\\n  Finalist'}, {'title': 'Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective', 'authors': ['Zhoujun Cheng', 'Shibo Hao', 'Tianyang Liu', 'Fan Zhou', 'Yutao Xie', 'Feng Yao', 'Yuexin Bian', 'Yonghao Zhuang', 'Nilabjo Dey', 'Yuheng Zha', 'Yi Gu', 'Kun Zhou', 'Yuqi Wang', 'Yuan Li', 'Richard Fan', 'Jianshu She', 'Chengqian Gao', 'Abulhair Saparov', 'Haonan Li', 'Taylor W. Killian', 'Mikhail Yurochkin', 'Zhengzhong Liu', 'Eric P. Xing', 'Zhiting Hu'], 'summary': 'Reinforcement learning (RL) has emerged as a promising approach to improve\\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\\nmath and code, limiting our understanding of its broader applicability to\\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\\nreward signals across diverse reasoning domains. We introduce Guru, a curated\\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\\nthrough domain-specific reward design, deduplication, and filtering to ensure\\nreliability and effectiveness for RL training. Based on Guru, we systematically\\nrevisit established findings in RL for LLM reasoning and observe significant\\nvariation across domains. For example, while prior work suggests that RL\\nprimarily elicits existing knowledge from pretrained models, our results reveal\\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\\nScience) easily benefit from cross-domain RL training, while domains with\\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\\ntraining to achieve meaningful performance gains, suggesting that RL is likely\\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\\nGuru-32B, two models that achieve state-of-the-art performance among open\\nmodels RL-trained with publicly available data, outperforming best baselines by\\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\\nalso show that our models effectively improve the Pass@k performance of their\\nbase models, particularly on complex tasks less likely to appear in pretraining\\ndata. We release data, models, training and evaluation code to facilitate\\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360', 'published': datetime.datetime(2025, 6, 17, 20, 24, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14965v1', 'comment': '38 pages, 9 figures. Under review'}, {'title': 'Flat Channels to Infinity in Neural Loss Landscapes', 'authors': ['Flavio Martinelli', 'Alexander Van Meegen', 'Berfin imek', 'Wulfram Gerstner', 'Johanni Brea'], 'summary': \"The loss landscapes of neural networks contain minima and saddle points that\\nmay be connected in flat regions or appear in isolation. We identify and\\ncharacterize a special structure in the loss landscape: channels along which\\nthe loss decreases extremely slowly, while the output weights of at least two\\nneurons, $a_i$ and $a_j$, diverge to $\\\\pm$infinity, and their input weight\\nvectors, $\\\\mathbf{w_i}$ and $\\\\mathbf{w_j}$, become equal to each other. At\\nconvergence, the two neurons implement a gated linear unit:\\n$a_i\\\\sigma(\\\\mathbf{w_i} \\\\cdot \\\\mathbf{x}) + a_j\\\\sigma(\\\\mathbf{w_j} \\\\cdot\\n\\\\mathbf{x}) \\\\rightarrow \\\\sigma(\\\\mathbf{w} \\\\cdot \\\\mathbf{x}) + (\\\\mathbf{v} \\\\cdot\\n\\\\mathbf{x}) \\\\sigma'(\\\\mathbf{w} \\\\cdot \\\\mathbf{x})$. Geometrically, these\\nchannels to infinity are asymptotically parallel to symmetry-induced lines of\\ncritical points. Gradient flow solvers, and related optimization methods like\\nSGD or ADAM, reach the channels with high probability in diverse regression\\nsettings, but without careful inspection they look like flat local minima with\\nfinite parameter values. Our characterization provides a comprehensive picture\\nof these quasi-flat regions in terms of gradient dynamics, geometry, and\\nfunctional interpretation. The emergence of gated linear units at the end of\\nthe channels highlights a surprising aspect of the computational capabilities\\nof fully connected layers.\", 'published': datetime.datetime(2025, 6, 17, 20, 4, 15, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14951v1', 'comment': None}, {'title': 'Determinao Automtica de Limiar de Deteco de Ataques em Redes de Computadores Utilizando Autoencoders', 'authors': ['Luan Gonalves Miranda', 'Pedro Ivo da Cruz', 'Murilo Bellezoni Loiola'], 'summary': 'Currently, digital security mechanisms like Anomaly Detection Systems using\\nAutoencoders (AE) show great potential for bypassing problems intrinsic to the\\ndata, such as data imbalance. Because AE use a non-trivial and nonstandardized\\nseparation threshold to classify the extracted reconstruction error, the\\ndefinition of this threshold directly impacts the performance of the detection\\nprocess. Thus, this work proposes the automatic definition of this threshold\\nusing some machine learning algorithms. For this, three algorithms were\\nevaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.', 'published': datetime.datetime(2025, 6, 17, 19, 45, 25, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14937v1', 'comment': 'This work was accepted at SBrT 2022 (Brazilian Symposium on\\n  Telecommunications and Signal Processing), though it was not included in the\\n  official proceedings. in Portuguese language'}, {'title': 'CALM: Contextual Analog Logic with Multimodality', 'authors': ['Maxwell J. Jacobson', 'Corey J. Maley', 'Yexiang Xue'], 'summary': 'In this work, we introduce Contextual Analog Logic with Multimodality (CALM).\\nCALM unites symbolic reasoning with neural generation, enabling systems to make\\ncontext-sensitive decisions grounded in real-world multi-modal data.\\n  Background: Classic bivalent logic systems cannot capture the nuance of human\\ndecision-making. They also require human grounding in multi-modal environments,\\nwhich can be ad-hoc, rigid, and brittle. Neural networks are good at extracting\\nrich contextual information from multi-modal data, but lack interpretable\\nstructures for reasoning.\\n  Objectives: CALM aims to bridge the gap between logic and neural perception,\\ncreating an analog logic that can reason over multi-modal inputs. Without this\\nintegration, AI systems remain either brittle or unstructured, unable to\\ngeneralize robustly to real-world tasks. In CALM, symbolic predicates evaluate\\nto analog truth values computed by neural networks and constrained search.\\n  Methods: CALM represents each predicate using a domain tree, which\\niteratively refines its analog truth value when the contextual groundings of\\nits entities are determined. The iterative refinement is predicted by neural\\nnetworks capable of capturing multi-modal information and is filtered through a\\nsymbolic reasoning module to ensure constraint satisfaction.\\n  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2%\\naccuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It\\nalso demonstrated spatial heatmap generation aligned with logical constraints\\nand delicate human preferences, as shown by a human study.\\n  Conclusions: CALM demonstrates the potential to reason with logic structure\\nwhile aligning with preferences in multi-modal environments. It lays the\\nfoundation for next-gen AI systems that require the precision and\\ninterpretation of logic and the multimodal information processing of neural\\nnetworks.', 'published': datetime.datetime(2025, 6, 17, 19, 40, 32, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14936v1', 'comment': None}, {'title': 'Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection', 'authors': ['Adriana Watson'], 'summary': 'The decentralized finance (DeFi) community has grown rapidly in recent years,\\npushed forward by cryptocurrency enthusiasts interested in the vast untapped\\npotential of new markets. The surge in popularity of cryptocurrency has ushered\\nin a new era of financial crime. Unfortunately, the novelty of the technology\\nmakes the task of catching and prosecuting offenders particularly challenging.\\nThus, it is necessary to implement automated detection tools related to\\npolicies to address the growing criminality in the cryptocurrency realm.', 'published': datetime.datetime(2025, 6, 17, 19, 30, 21, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14933v1', 'comment': '6 pages, 4 figures. Code available at:\\n  https://github.com/awatson246/crypto-anomaly-detection-policy'}, {'title': 'MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance', 'authors': ['Joseph J. Peper', 'Wenzhao Qiu', 'Ali Payani', 'Lu Wang'], 'summary': 'Natural language processing evaluation has made significant progress, largely\\ndriven by the proliferation of powerful large language mod-els (LLMs). New\\nevaluation benchmarks are of increasing priority as the reasoning capabilities\\nof LLMs are expanding at a rapid pace. In particular, while multi-document (MD)\\nreasoning is an area of extreme relevance given LLM capabilities in handling\\nlonger-context inputs, few benchmarks exist to rigorously examine model\\nbehavior in this setting. Moreover, the multi-document setting is historically\\nchallenging for benchmark creation due to the expensive cost of annotating long\\ninputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs\\non the task of multi-document reasoning. Notably, MDBench is created through a\\nnovel synthetic generation process, allowing us to controllably and efficiently\\ngenerate challenging document sets and the corresponding question-answer (QA)\\nexamples. Our novel technique operates on condensed structured seed knowledge,\\nmodifying it through LLM-assisted edits to induce MD-specific reasoning\\nchallenges. We then convert this structured knowledge into a natural text\\nsurface form, generating a document set and corresponding QA example. We\\nanalyze the behavior of popular LLMs and prompting techniques, finding that\\nMDBENCH poses significant challenges for all methods, even with relatively\\nshort document sets. We also see our knowledge-guided generation technique (1)\\nallows us to readily perform targeted analysis of MD-specific reasoning\\ncapabilities and (2) can be adapted quickly to account for new challenges and\\nfuture modeling improvements.', 'published': datetime.datetime(2025, 6, 17, 19, 14, 30, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14927v1', 'comment': 'ACL 2025 Findings'}, {'title': 'Forecasting the spatiotemporal evolution of fluid-induced microearthquakes with deep learning', 'authors': ['Jaehong Chung', 'Michael Manga', 'Timothy Kneafsey', 'Tapan Mukerji', 'Mengsu Hu'], 'summary': 'Microearthquakes (MEQs) generated by subsurface fluid injection record the\\nevolving stress state and permeability of reservoirs. Forecasting their full\\nspatiotemporal evolution is therefore critical for applications such as\\nenhanced geothermal systems (EGS), CO$_2$ sequestration and other\\ngeo-engineering applications. We present a transformer-based deep learning\\nmodel that ingests hydraulic stimulation history and prior MEQ observations to\\nforecast four key quantities: cumulative MEQ count, cumulative logarithmic\\nseismic moment, and the 50th- and 95th-percentile extents ($P_{50}, P_{95}$) of\\nthe MEQ cloud. Applied to the EGS Collab Experiment 1 dataset, the model\\nachieves $R^2 >0.98$ for the 1-second forecast horizon and $R^2 >0.88$ for the\\n15-second forecast horizon across all targets, and supplies uncertainty\\nestimates through a learned standard deviation term. These accurate,\\nuncertainty-quantified forecasts enable real-time inference of fracture\\npropagation and permeability evolution, demonstrating the strong potential of\\ndeep-learning approaches to improve seismic-risk assessment and guide\\nmitigation strategies in future fluid-injection operations.', 'published': datetime.datetime(2025, 6, 17, 19, 10, 5, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14923v1', 'comment': None}, {'title': 'Foundation Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face)', 'authors': ['Fridolin Haugg', 'Grace Lee', 'John He', 'Leonard Nrnberg', 'Dennis Bontempi', 'Danielle S. Bitterman', 'Paul Catalano', 'Vasco Prudente', 'Dmitrii Glubokov', 'Andrew Warrington', 'Suraj Pai', 'Dirk De Ruysscher', 'Christian Guthier', 'Benjamin H. Kann', 'Vadim N. Gladyshev', 'Hugo JWL Aerts', 'Raymond H. Mak'], 'summary': 'Background: Facial appearance offers a noninvasive window into health. We\\nbuilt FAHR-Face, a foundation model trained on >40 million facial images and\\nfine-tuned it for two distinct tasks: biological age estimation (FAHR-FaceAge)\\nand survival risk prediction (FAHR-FaceSurvival).\\n  Methods: FAHR-FaceAge underwent a two-stage, age-balanced fine-tuning on\\n749,935 public images; FAHR-FaceSurvival was fine-tuned on 34,389 photos of\\ncancer patients. Model robustness (cosmetic surgery, makeup, pose, lighting)\\nand independence (saliency mapping) was tested extensively. Both models were\\nclinically tested in two independent cancer patient datasets with survival\\nanalyzed by multivariable Cox models and adjusted for clinical prognostic\\nfactors.\\n  Findings: For age estimation, FAHR-FaceAge had the lowest mean absolute error\\nof 5.1 years on public datasets, outperforming benchmark models and maintaining\\naccuracy across the full human lifespan. In cancer patients, FAHR-FaceAge\\noutperformed a prior facial age estimation model in survival prognostication.\\nFAHR-FaceSurvival demonstrated robust prediction of mortality, and the\\nhighest-risk quartile had more than triple the mortality of the lowest\\n(adjusted hazard ratio 3.22; P<0.001). These findings were validated in the\\nindependent cohort and both models showed generalizability across age, sex,\\nrace and cancer subgroups. The two algorithms provided distinct, complementary\\nprognostic information; saliency mapping revealed each model relied on distinct\\nfacial regions. The combination of FAHR-FaceAge and FAHR-FaceSurvival improved\\nprognostic accuracy.\\n  Interpretation: A single foundation model can generate inexpensive, scalable\\nfacial biomarkers that capture both biological ageing and disease-related\\nmortality risk. The foundation model enabled effective training using\\nrelatively small clinical datasets.', 'published': datetime.datetime(2025, 6, 17, 18, 28, 11, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14909v1', 'comment': None}, {'title': 'PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning', 'authors': ['Yizhen Zhang', 'Yang Ding', 'Shuoshuo Zhang', 'Xinchen Zhang', 'Haoling Li', 'Zhong-zhi Li', 'Peijie Wang', 'Jie Wu', 'Lei Ji', 'Yelong Shen', 'Yujiu Yang', 'Yeyun Gong'], 'summary': 'Inspired by the impressive reasoning capabilities demonstrated by\\nreinforcement learning approaches like DeepSeek-R1, recent emerging research\\nhas begun exploring the use of reinforcement learning (RL) to enhance\\nvision-language models (VLMs) for multimodal reasoning tasks. However, most\\nexisting multimodal reinforcement learning approaches remain limited to spatial\\nreasoning within single-image contexts, yet still struggle to generalize to\\nmore complex and real-world scenarios involving multi-image positional\\nreasoning, where understanding the relationships across images is crucial. To\\naddress this challenge, we propose a general reinforcement learning approach\\nPeRL tailored for interleaved multimodal tasks, and a multi-stage strategy\\ndesigned to enhance the exploration-exploitation trade-off, thereby improving\\nlearning efficiency and task performance. Specifically, we introduce\\npermutation of image sequences to simulate varied positional relationships to\\nexplore more spatial and positional diversity. Furthermore, we design a rollout\\nfiltering mechanism for resampling to focus on trajectories that contribute\\nmost to learning optimal behaviors to exploit learned policies effectively. We\\nevaluate our model on 5 widely-used multi-image benchmarks and 3 single-image\\nbenchmarks. Our experiments confirm that PeRL trained model consistently\\nsurpasses R1-related and interleaved VLM baselines by a large margin, achieving\\nstate-of-the-art performance on multi-image benchmarks, while preserving\\ncomparable performance on single-image tasks.', 'published': datetime.datetime(2025, 6, 17, 18, 25, 56, tzinfo=datetime.timezone.utc), 'url': 'http://arxiv.org/abs/2506.14907v1', 'comment': None}]\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "1. **Title:** Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size  \n",
      "   **Authors:** Soufiane Hayou, Liyuan Liu  \n",
      "   **Abstract:** Pretraining large language models is a costly process. To make this process more efficient, several methods have been proposed to optimize model architecture/parametrization and hardware use.   \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.15025v1)  \n",
      "   **Comment:** TD,LR: How to set the learning rate for embedding layer in LLMs?\n",
      "\n",
      "2. **Title:** Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment  \n",
      "   **Authors:** Yue Gao  \n",
      "   **Abstract:** Effective reinforcement learning (RL) for sepsis treatment depends on learning stable, clinically meaningful state representations from irregular ICU time series.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.15019v1)  \n",
      "   **Comment:** Accepted to IJCAI2025 AI4TS\n",
      "\n",
      "3. **Title:** Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output  \n",
      "   **Authors:** Richa Gupta, Alexander Htet Kyaw  \n",
      "   **Abstract:** Generative AI, specifically text-to-image models, have revolutionized interior architectural design by enabling the rapid translation of conceptual ideas into visual representations from simple text prompts.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.15008v1)  \n",
      "   **Comment:** 15 Pages, 6 figures, CAAD Futures 2025\n",
      "\n",
      "4. **Title:** Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings  \n",
      "   **Authors:** Ignacio Sastre, Aiala Ros  \n",
      "   **Abstract:** In this work, we observe an interesting phenomenon: it is possible to generate reversible sentence embeddings that allow an LLM to reconstruct the original text exactly, without modifying the model's weights.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.15001v1)  \n",
      "   **Comment:** This paper will be presented at The First Workshop on Large Language Model Memorization (L2M2) at ACL 2025\n",
      "\n",
      "5. **Title:** Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition  \n",
      "   **Authors:** Jiamin Xie et al.  \n",
      "   **Abstract:** Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech recognition capabilities.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.14973v1)  \n",
      "   **Comment:** Accepted to Interspeech 2025\n",
      "\n",
      "6. **Title:** MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance  \n",
      "   **Authors:** Joseph J. Peper et al.  \n",
      "   **Abstract:** Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language models (LLMs).  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.14927v1)  \n",
      "   **Comment:** ACL 2025 Findings\n",
      "\n",
      "7. **Title:** Forecasting the spatiotemporal evolution of fluid-induced microearthquakes with deep learning  \n",
      "   **Authors:** Jaehong Chung et al.  \n",
      "   **Abstract:** Microearthquakes (MEQs) generated by subsurface fluid injection record the evolving stress state and permeability of reservoirs.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.14923v1)  \n",
      "   **Comment:** None\n",
      "\n",
      "8. **Title:** FOUNDATION Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face)  \n",
      "   **Authors:** Fridolin Haugg et al.  \n",
      "   **Abstract:** Background: Facial appearance offers a noninvasive window into health.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.14909v1)  \n",
      "   **Comment:** None\n",
      "\n",
      "9. **Title:** PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning  \n",
      "   **Authors:** Yizhen Zhang et al.  \n",
      "   **Abstract:** Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1...  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.14907v1)  \n",
      "   **Comment:** None\n",
      "\n",
      "10. **Title:** Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits  \n",
      "    **Authors:** Tianyi Xu et al.  \n",
      "    **Abstract:** We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance.  \n",
      "    **Link:** [Read more](http://arxiv.org/abs/2506.14988v1)  \n",
      "    **Comment:** None\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m ## Final Result:\u001b[00m \u001b[92m1. **Title:** Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size  \n",
      "   **Authors:** Soufiane Hayou, Liyuan Liu  \n",
      "   **Abstract:** Pretraining large language models is a costly process. To make this process more efficient, several methods have been proposed to optimize model architecture/parametrization and hardware use.   \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.15025v1)  \n",
      "   **Comment:** TD,LR: How to set the learning rate for embedding layer in LLMs?\n",
      "\n",
      "2. **Title:** Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment  \n",
      "   **Authors:** Yue Gao  \n",
      "   **Abstract:** Effective reinforcement learning (RL) for sepsis treatment depends on learning stable, clinically meaningful state representations from irregular ICU time series.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.15019v1)  \n",
      "   **Comment:** Accepted to IJCAI2025 AI4TS\n",
      "\n",
      "3. **Title:** Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output  \n",
      "   **Authors:** Richa Gupta, Alexander Htet Kyaw  \n",
      "   **Abstract:** Generative AI, specifically text-to-image models, have revolutionized interior architectural design by enabling the rapid translation of conceptual ideas into visual representations from simple text prompts.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.15008v1)  \n",
      "   **Comment:** 15 Pages, 6 figures, CAAD Futures 2025\n",
      "\n",
      "4. **Title:** Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings  \n",
      "   **Authors:** Ignacio Sastre, Aiala Ros  \n",
      "   **Abstract:** In this work, we observe an interesting phenomenon: it is possible to generate reversible sentence embeddings that allow an LLM to reconstruct the original text exactly, without modifying the model's weights.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.15001v1)  \n",
      "   **Comment:** This paper will be presented at The First Workshop on Large Language Model Memorization (L2M2) at ACL 2025\n",
      "\n",
      "5. **Title:** Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition  \n",
      "   **Authors:** Jiamin Xie et al.  \n",
      "   **Abstract:** Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech recognition capabilities.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.14973v1)  \n",
      "   **Comment:** Accepted to Interspeech 2025\n",
      "\n",
      "6. **Title:** MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance  \n",
      "   **Authors:** Joseph J. Peper et al.  \n",
      "   **Abstract:** Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language models (LLMs).  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.14927v1)  \n",
      "   **Comment:** ACL 2025 Findings\n",
      "\n",
      "7. **Title:** Forecasting the spatiotemporal evolution of fluid-induced microearthquakes with deep learning  \n",
      "   **Authors:** Jaehong Chung et al.  \n",
      "   **Abstract:** Microearthquakes (MEQs) generated by subsurface fluid injection record the evolving stress state and permeability of reservoirs.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.14923v1)  \n",
      "   **Comment:** None\n",
      "\n",
      "8. **Title:** FOUNDATION Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face)  \n",
      "   **Authors:** Fridolin Haugg et al.  \n",
      "   **Abstract:** Background: Facial appearance offers a noninvasive window into health.  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.14909v1)  \n",
      "   **Comment:** None\n",
      "\n",
      "9. **Title:** PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning  \n",
      "   **Authors:** Yizhen Zhang et al.  \n",
      "   **Abstract:** Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1...  \n",
      "   **Link:** [Read more](http://arxiv.org/abs/2506.14907v1)  \n",
      "   **Comment:** None\n",
      "\n",
      "10. **Title:** Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits  \n",
      "    **Authors:** Tianyi Xu et al.  \n",
      "    **Abstract:** We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance.  \n",
      "    **Link:** [Read more](http://arxiv.org/abs/2506.14988v1)  \n",
      "    **Comment:** None\u001b[00m\n",
      "\u001b[1m\u001b[93m \n",
      "\n",
      "=====\n",
      "## Please provide feedback on the Final Result and the Agent's actions. Respond with 'looks good' or a similar phrase when you're satisfied.\n",
      "=====\n",
      "\u001b[00m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " looks good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human feedback:  looks good\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Frontend & AI Engineer\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mCompile the results into a detailed report in HTML file format.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Frontend & AI Engineer\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Top 10 AI Research Papers (June 17-18, 2025)</title>\n",
      "    <style>\n",
      "        body {\n",
      "            font-family: Arial, sans-serif;\n",
      "            margin: 20px;\n",
      "            background-color: #f9f9f9;\n",
      "        }\n",
      "        \n",
      "        h1 {\n",
      "            color: #333;\n",
      "        }\n",
      "        \n",
      "        .paper {\n",
      "            border: 1px solid #ccc;\n",
      "            border-radius: 5px;\n",
      "            margin: 10px 0;\n",
      "            padding: 15px;\n",
      "            background-color: #fff;\n",
      "        }\n",
      "        \n",
      "        .title {\n",
      "            font-size: 1.5em;\n",
      "            color: #007bff;\n",
      "        }\n",
      "        \n",
      "        .authors {\n",
      "            font-weight: bold;\n",
      "        }\n",
      "        \n",
      "        .summary {\n",
      "            margin-top: 10px;\n",
      "            color: #555;\n",
      "        }\n",
      "        \n",
      "        .link {\n",
      "            margin-top: 10px;\n",
      "            display: inline-block;\n",
      "            color: #007bff;\n",
      "            text-decoration: none;\n",
      "        }\n",
      "        \n",
      "        .link:hover {\n",
      "            text-decoration: underline;\n",
      "        }\n",
      "    </style>\n",
      "</head>\n",
      "\n",
      "<body>\n",
      "    <h1>Top 10 AI Research Papers (June 17-18, 2025)</h1>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.15025v1\" target=\"_blank\">Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size</a>\n",
      "        <div class=\"authors\">Authors: Soufiane Hayou, Liyuan Liu</div>\n",
      "        <div class=\"summary\">Pretraining large language models is a costly process. To make this process more efficient, several methods have been proposed to optimize model architecture/parametrization and hardware use.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.15019v1\" target=\"_blank\">Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment</a>\n",
      "        <div class=\"authors\">Authors: Yue Gao</div>\n",
      "        <div class=\"summary\">Effective reinforcement learning (RL) for sepsis treatment depends on learning stable, clinically meaningful state representations from irregular ICU time series.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.15008v1\" target=\"_blank\">Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output</a>\n",
      "        <div class=\"authors\">Authors: Richa Gupta, Alexander Htet Kyaw</div>\n",
      "        <div class=\"summary\">Generative AI, specifically text-to-image models, have revolutionized interior architectural design by enabling the rapid translation of conceptual ideas into visual representations from simple text prompts.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.15001v1\" target=\"_blank\">Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings</a>\n",
      "        <div class=\"authors\">Authors: Ignacio Sastre, Aiala Ros</div>\n",
      "        <div class=\"summary\">In this work, we observe an interesting phenomenon: it is possible to generate reversible sentence embeddings that allow an LLM to reconstruct the original text exactly, without modifying the model's weights.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14973v1\" target=\"_blank\">Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition</a>\n",
      "        <div class=\"authors\">Authors: Jiamin Xie et al.</div>\n",
      "        <div class=\"summary\">Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech recognition capabilities.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14927v1\" target=\"_blank\">MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance</a>\n",
      "        <div class=\"authors\">Authors: Joseph J. Peper et al.</div>\n",
      "        <div class=\"summary\">Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language models (LLMs).</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14923v1\" target=\"_blank\">Forecasting the spatiotemporal evolution of fluid-induced microearthquakes with deep learning</a>\n",
      "        <div class=\"authors\">Authors: Jaehong Chung et al.</div>\n",
      "        <div class=\"summary\">Microearthquakes (MEQs) generated by subsurface fluid injection record the evolving stress state and permeability of reservoirs.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14909v1\" target=\"_blank\">FOUNDATION Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face)</a>\n",
      "        <div class=\"authors\">Authors: Fridolin Haugg et al.</div>\n",
      "        <div class=\"summary\">Background: Facial appearance offers a noninvasive window into health.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14907v1\" target=\"_blank\">PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning</a>\n",
      "        <div class=\"authors\">Authors: Yizhen Zhang et al.</div>\n",
      "        <div class=\"summary\">Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1...</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14988v1\" target=\"_blank\">Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits</a>\n",
      "        <div class=\"authors\">Authors: Tianyi Xu et al.</div>\n",
      "        <div class=\"summary\">We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance.</div>\n",
      "    </div>\n",
      "\n",
      "</body>\n",
      "\n",
      "</html>\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m ## Final Result:\u001b[00m \u001b[92m```html\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Top 10 AI Research Papers (June 17-18, 2025)</title>\n",
      "    <style>\n",
      "        body {\n",
      "            font-family: Arial, sans-serif;\n",
      "            margin: 20px;\n",
      "            background-color: #f9f9f9;\n",
      "        }\n",
      "        \n",
      "        h1 {\n",
      "            color: #333;\n",
      "        }\n",
      "        \n",
      "        .paper {\n",
      "            border: 1px solid #ccc;\n",
      "            border-radius: 5px;\n",
      "            margin: 10px 0;\n",
      "            padding: 15px;\n",
      "            background-color: #fff;\n",
      "        }\n",
      "        \n",
      "        .title {\n",
      "            font-size: 1.5em;\n",
      "            color: #007bff;\n",
      "        }\n",
      "        \n",
      "        .authors {\n",
      "            font-weight: bold;\n",
      "        }\n",
      "        \n",
      "        .summary {\n",
      "            margin-top: 10px;\n",
      "            color: #555;\n",
      "        }\n",
      "        \n",
      "        .link {\n",
      "            margin-top: 10px;\n",
      "            display: inline-block;\n",
      "            color: #007bff;\n",
      "            text-decoration: none;\n",
      "        }\n",
      "        \n",
      "        .link:hover {\n",
      "            text-decoration: underline;\n",
      "        }\n",
      "    </style>\n",
      "</head>\n",
      "\n",
      "<body>\n",
      "    <h1>Top 10 AI Research Papers (June 17-18, 2025)</h1>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.15025v1\" target=\"_blank\">Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size</a>\n",
      "        <div class=\"authors\">Authors: Soufiane Hayou, Liyuan Liu</div>\n",
      "        <div class=\"summary\">Pretraining large language models is a costly process. To make this process more efficient, several methods have been proposed to optimize model architecture/parametrization and hardware use.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.15019v1\" target=\"_blank\">Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment</a>\n",
      "        <div class=\"authors\">Authors: Yue Gao</div>\n",
      "        <div class=\"summary\">Effective reinforcement learning (RL) for sepsis treatment depends on learning stable, clinically meaningful state representations from irregular ICU time series.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.15008v1\" target=\"_blank\">Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output</a>\n",
      "        <div class=\"authors\">Authors: Richa Gupta, Alexander Htet Kyaw</div>\n",
      "        <div class=\"summary\">Generative AI, specifically text-to-image models, have revolutionized interior architectural design by enabling the rapid translation of conceptual ideas into visual representations from simple text prompts.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.15001v1\" target=\"_blank\">Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings</a>\n",
      "        <div class=\"authors\">Authors: Ignacio Sastre, Aiala Ros</div>\n",
      "        <div class=\"summary\">In this work, we observe an interesting phenomenon: it is possible to generate reversible sentence embeddings that allow an LLM to reconstruct the original text exactly, without modifying the model's weights.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14973v1\" target=\"_blank\">Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition</a>\n",
      "        <div class=\"authors\">Authors: Jiamin Xie et al.</div>\n",
      "        <div class=\"summary\">Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech recognition capabilities.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14927v1\" target=\"_blank\">MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance</a>\n",
      "        <div class=\"authors\">Authors: Joseph J. Peper et al.</div>\n",
      "        <div class=\"summary\">Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language models (LLMs).</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14923v1\" target=\"_blank\">Forecasting the spatiotemporal evolution of fluid-induced microearthquakes with deep learning</a>\n",
      "        <div class=\"authors\">Authors: Jaehong Chung et al.</div>\n",
      "        <div class=\"summary\">Microearthquakes (MEQs) generated by subsurface fluid injection record the evolving stress state and permeability of reservoirs.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14909v1\" target=\"_blank\">FOUNDATION Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face)</a>\n",
      "        <div class=\"authors\">Authors: Fridolin Haugg et al.</div>\n",
      "        <div class=\"summary\">Background: Facial appearance offers a noninvasive window into health.</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14907v1\" target=\"_blank\">PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning</a>\n",
      "        <div class=\"authors\">Authors: Yizhen Zhang et al.</div>\n",
      "        <div class=\"summary\">Inspired by the impressive reasoning capabilities demonstrated by reinforcement learning approaches like DeepSeek-R1...</div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"paper\">\n",
      "        <a class=\"title\" href=\"http://arxiv.org/abs/2506.14988v1\" target=\"_blank\">Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits</a>\n",
      "        <div class=\"authors\">Authors: Tianyi Xu et al.</div>\n",
      "        <div class=\"summary\">We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance.</div>\n",
      "    </div>\n",
      "\n",
      "</body>\n",
      "\n",
      "</html>\n",
      "```\u001b[00m\n",
      "\u001b[1m\u001b[93m \n",
      "\n",
      "=====\n",
      "## Please provide feedback on the Final Result and the Agent's actions. Respond with 'looks good' or a similar phrase when you're satisfied.\n",
      "=====\n",
      "\u001b[00m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human feedback:  good\n"
     ]
    }
   ],
   "source": [
    "crew_inputs = {\n",
    "    \"start_date\" : \"2025-06-17\", \n",
    "    \"end_date\" : \"2025-06-18\",\n",
    "}\n",
    "result = arxiv_research_crew.kickoff(inputs = crew_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43090fd4-a8af-4092-ade1-edc2fc70847d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
